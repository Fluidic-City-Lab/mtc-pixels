{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1375af72",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Bad key text.latex.preview in file /home/michael/anaconda3/envs/flow/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 123 ('text.latex.preview : False')\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "https://github.com/matplotlib/matplotlib/blob/v3.5.1/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n",
      "\n",
      "Bad key mathtext.fallback_to_cm in file /home/michael/anaconda3/envs/flow/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 155 ('mathtext.fallback_to_cm : True  # When True, use symbols from the Computer Modern')\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "https://github.com/matplotlib/matplotlib/blob/v3.5.1/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n",
      "\n",
      "Bad key savefig.jpeg_quality in file /home/michael/anaconda3/envs/flow/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 418 ('savefig.jpeg_quality: 95       # when a jpeg is saved, the default quality parameter.')\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "https://github.com/matplotlib/matplotlib/blob/v3.5.1/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n",
      "\n",
      "Bad key keymap.all_axes in file /home/michael/anaconda3/envs/flow/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 466 ('keymap.all_axes : a                 # enable all axes')\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "https://github.com/matplotlib/matplotlib/blob/v3.5.1/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n",
      "\n",
      "Bad key animation.avconv_path in file /home/michael/anaconda3/envs/flow/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 477 ('animation.avconv_path: avconv     # Path to avconv binary. Without full path')\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "https://github.com/matplotlib/matplotlib/blob/v3.5.1/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n",
      "\n",
      "Bad key animation.avconv_args in file /home/michael/anaconda3/envs/flow/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 479 ('animation.avconv_args:            # Additional arguments to pass to avconv')\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "https://github.com/matplotlib/matplotlib/blob/v3.5.1/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n",
      "{'num_workers': 5, 'num_envs_per_worker': 1, 'create_env_on_driver': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'gamma': 0.999, 'lr': 5e-05, 'train_batch_size': 15000, 'model': {'_use_default_native_models': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': [[16, [8, 8], 4], [32, [4, 4], 2], [256, [11, 11], 1]], 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'num_framestacks': 'auto', 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1, 'framestack': True}, 'optimizer': {}, 'horizon': 3000, 'soft_horizon': False, 'no_done_at_end': False, 'env': None, 'observation_space': None, 'action_space': None, 'env_config': {}, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'torch', 'eager_tracing': False, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_num_episodes': 10, 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'collect_metrics_timeout': 180, 'metrics_smoothing_episodes': 100, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'min_iter_time_s': 0, 'timesteps_per_iteration': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {}, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, 'simple_optimizer': -1, 'monitor': -1, 'use_critic': True, 'use_gae': True, 'lambda': 0.97, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 10, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.02, 'vf_share_layers': -1}\n",
      "2022-09-29 12:09:21,491\tINFO services.py:1247 -- View the Ray dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265\u001b[39m\u001b[22m\n",
      "== Status ==\n",
      "Memory usage on this node: 11.0/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 0/6 CPUs, 0/6 GPUs, 0.0/4.72 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Result logdir: /home/michael/ray_results/singleagent_onelane_ring\n",
      "Number of trials: 1/1 (1 PENDING)\n",
      "+-----------------------------------------+----------+-------+\n",
      "| Trial name                              | status   | loc   |\n",
      "|-----------------------------------------+----------+-------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_76534_00000 | PENDING  |       |\n",
      "+-----------------------------------------+----------+-------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=29799)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=29799)\u001b[0m Bad key text.latex.preview in file /home/michael/anaconda3/envs/flow/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 123 ('text.latex.preview : False')\n",
      "\u001b[2m\u001b[36m(pid=29799)\u001b[0m You probably need to get an updated matplotlibrc file from\n",
      "\u001b[2m\u001b[36m(pid=29799)\u001b[0m https://github.com/matplotlib/matplotlib/blob/v3.5.1/matplotlibrc.template\n",
      "\u001b[2m\u001b[36m(pid=29799)\u001b[0m or from the matplotlib source distribution\n",
      "\u001b[2m\u001b[36m(pid=29799)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=29799)\u001b[0m Bad key mathtext.fallback_to_cm in file /home/michael/anaconda3/envs/flow/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 155 ('mathtext.fallback_to_cm : True  # When True, use symbols from the Computer Modern')\n",
      "\u001b[2m\u001b[36m(pid=29799)\u001b[0m You probably need to get an updated matplotlibrc file from\n",
      "\u001b[2m\u001b[36m(pid=29799)\u001b[0m https://github.com/matplotlib/matplotlib/blob/v3.5.1/matplotlibrc.template\n",
      "\u001b[2m\u001b[36m(pid=29799)\u001b[0m or from the matplotlib source distribution\n",
      "\u001b[2m\u001b[36m(pid=29799)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=29799)\u001b[0m Bad key savefig.jpeg_quality in file /home/michael/anaconda3/envs/flow/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 418 ('savefig.jpeg_quality: 95       # when a jpeg is saved, the default quality parameter.')\n",
      "\u001b[2m\u001b[36m(pid=29799)\u001b[0m You probably need to get an updated matplotlibrc file from\n",
      "\u001b[2m\u001b[36m(pid=29799)\u001b[0m https://github.com/matplotlib/matplotlib/blob/v3.5.1/matplotlibrc.template\n",
      "\u001b[2m\u001b[36m(pid=29799)\u001b[0m or from the matplotlib source distribution\n",
      "\u001b[2m\u001b[36m(pid=29799)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=29799)\u001b[0m Bad key keymap.all_axes in file /home/michael/anaconda3/envs/flow/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 466 ('keymap.all_axes : a                 # enable all axes')\n",
      "\u001b[2m\u001b[36m(pid=29799)\u001b[0m You probably need to get an updated matplotlibrc file from\n",
      "\u001b[2m\u001b[36m(pid=29799)\u001b[0m https://github.com/matplotlib/matplotlib/blob/v3.5.1/matplotlibrc.template\n",
      "\u001b[2m\u001b[36m(pid=29799)\u001b[0m or from the matplotlib source distribution\n",
      "\u001b[2m\u001b[36m(pid=29799)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=29799)\u001b[0m Bad key animation.avconv_path in file /home/michael/anaconda3/envs/flow/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 477 ('animation.avconv_path: avconv     # Path to avconv binary. Without full path')\n",
      "\u001b[2m\u001b[36m(pid=29799)\u001b[0m You probably need to get an updated matplotlibrc file from\n",
      "\u001b[2m\u001b[36m(pid=29799)\u001b[0m https://github.com/matplotlib/matplotlib/blob/v3.5.1/matplotlibrc.template\n",
      "\u001b[2m\u001b[36m(pid=29799)\u001b[0m or from the matplotlib source distribution\n",
      "\u001b[2m\u001b[36m(pid=29799)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=29799)\u001b[0m Bad key animation.avconv_args in file /home/michael/anaconda3/envs/flow/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 479 ('animation.avconv_args:            # Additional arguments to pass to avconv')\n",
      "\u001b[2m\u001b[36m(pid=29799)\u001b[0m You probably need to get an updated matplotlibrc file from\n",
      "\u001b[2m\u001b[36m(pid=29799)\u001b[0m https://github.com/matplotlib/matplotlib/blob/v3.5.1/matplotlibrc.template\n",
      "\u001b[2m\u001b[36m(pid=29799)\u001b[0m or from the matplotlib source distribution\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=29799)\u001b[0m 2022-09-29 12:09:24,062\tINFO trainer.py:720 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=29798)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=29798)\u001b[0m Bad key text.latex.preview in file /home/michael/anaconda3/envs/flow/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 123 ('text.latex.preview : False')\n",
      "\u001b[2m\u001b[36m(pid=29798)\u001b[0m You probably need to get an updated matplotlibrc file from\n",
      "\u001b[2m\u001b[36m(pid=29798)\u001b[0m https://github.com/matplotlib/matplotlib/blob/v3.5.1/matplotlibrc.template\n",
      "\u001b[2m\u001b[36m(pid=29798)\u001b[0m or from the matplotlib source distribution\n",
      "\u001b[2m\u001b[36m(pid=29798)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=29798)\u001b[0m Bad key mathtext.fallback_to_cm in file /home/michael/anaconda3/envs/flow/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 155 ('mathtext.fallback_to_cm : True  # When True, use symbols from the Computer Modern')\n",
      "\u001b[2m\u001b[36m(pid=29798)\u001b[0m You probably need to get an updated matplotlibrc file from\n",
      "\u001b[2m\u001b[36m(pid=29798)\u001b[0m https://github.com/matplotlib/matplotlib/blob/v3.5.1/matplotlibrc.template\n",
      "\u001b[2m\u001b[36m(pid=29798)\u001b[0m or from the matplotlib source distribution\n",
      "\u001b[2m\u001b[36m(pid=29798)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=29798)\u001b[0m Bad key savefig.jpeg_quality in file /home/michael/anaconda3/envs/flow/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 418 ('savefig.jpeg_quality: 95       # when a jpeg is saved, the default quality parameter.')\n",
      "\u001b[2m\u001b[36m(pid=29798)\u001b[0m You probably need to get an updated matplotlibrc file from\n",
      "\u001b[2m\u001b[36m(pid=29798)\u001b[0m https://github.com/matplotlib/matplotlib/blob/v3.5.1/matplotlibrc.template\n",
      "\u001b[2m\u001b[36m(pid=29798)\u001b[0m or from the matplotlib source distribution\n",
      "\u001b[2m\u001b[36m(pid=29798)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=29798)\u001b[0m Bad key keymap.all_axes in file /home/michael/anaconda3/envs/flow/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 466 ('keymap.all_axes : a                 # enable all axes')\n",
      "\u001b[2m\u001b[36m(pid=29798)\u001b[0m You probably need to get an updated matplotlibrc file from\n",
      "\u001b[2m\u001b[36m(pid=29798)\u001b[0m https://github.com/matplotlib/matplotlib/blob/v3.5.1/matplotlibrc.template\n",
      "\u001b[2m\u001b[36m(pid=29798)\u001b[0m or from the matplotlib source distribution\n",
      "\u001b[2m\u001b[36m(pid=29798)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=29798)\u001b[0m Bad key animation.avconv_path in file /home/michael/anaconda3/envs/flow/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 477 ('animation.avconv_path: avconv     # Path to avconv binary. Without full path')\n",
      "\u001b[2m\u001b[36m(pid=29798)\u001b[0m You probably need to get an updated matplotlibrc file from\n",
      "\u001b[2m\u001b[36m(pid=29798)\u001b[0m https://github.com/matplotlib/matplotlib/blob/v3.5.1/matplotlibrc.template\n",
      "\u001b[2m\u001b[36m(pid=29798)\u001b[0m or from the matplotlib source distribution\n",
      "\u001b[2m\u001b[36m(pid=29798)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=29798)\u001b[0m Bad key animation.avconv_args in file /home/michael/anaconda3/envs/flow/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 479 ('animation.avconv_args:            # Additional arguments to pass to avconv')\n",
      "\u001b[2m\u001b[36m(pid=29798)\u001b[0m You probably need to get an updated matplotlibrc file from\n",
      "\u001b[2m\u001b[36m(pid=29798)\u001b[0m https://github.com/matplotlib/matplotlib/blob/v3.5.1/matplotlibrc.template\n",
      "\u001b[2m\u001b[36m(pid=29798)\u001b[0m or from the matplotlib source distribution\n",
      "\u001b[2m\u001b[36m(pid=29796)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=29796)\u001b[0m Bad key text.latex.preview in file /home/michael/anaconda3/envs/flow/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 123 ('text.latex.preview : False')\n",
      "\u001b[2m\u001b[36m(pid=29796)\u001b[0m You probably need to get an updated matplotlibrc file from\n",
      "\u001b[2m\u001b[36m(pid=29796)\u001b[0m https://github.com/matplotlib/matplotlib/blob/v3.5.1/matplotlibrc.template\n",
      "\u001b[2m\u001b[36m(pid=29796)\u001b[0m or from the matplotlib source distribution\n",
      "\u001b[2m\u001b[36m(pid=29796)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=29796)\u001b[0m Bad key mathtext.fallback_to_cm in file /home/michael/anaconda3/envs/flow/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 155 ('mathtext.fallback_to_cm : True  # When True, use symbols from the Computer Modern')\n",
      "\u001b[2m\u001b[36m(pid=29796)\u001b[0m You probably need to get an updated matplotlibrc file from\n",
      "\u001b[2m\u001b[36m(pid=29796)\u001b[0m https://github.com/matplotlib/matplotlib/blob/v3.5.1/matplotlibrc.template\n",
      "\u001b[2m\u001b[36m(pid=29796)\u001b[0m or from the matplotlib source distribution\n",
      "\u001b[2m\u001b[36m(pid=29796)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=29796)\u001b[0m Bad key savefig.jpeg_quality in file /home/michael/anaconda3/envs/flow/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 418 ('savefig.jpeg_quality: 95       # when a jpeg is saved, the default quality parameter.')\n",
      "\u001b[2m\u001b[36m(pid=29796)\u001b[0m You probably need to get an updated matplotlibrc file from\n",
      "\u001b[2m\u001b[36m(pid=29796)\u001b[0m https://github.com/matplotlib/matplotlib/blob/v3.5.1/matplotlibrc.template\n",
      "\u001b[2m\u001b[36m(pid=29796)\u001b[0m or from the matplotlib source distribution\n",
      "\u001b[2m\u001b[36m(pid=29796)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=29796)\u001b[0m Bad key keymap.all_axes in file /home/michael/anaconda3/envs/flow/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 466 ('keymap.all_axes : a                 # enable all axes')\n",
      "\u001b[2m\u001b[36m(pid=29796)\u001b[0m You probably need to get an updated matplotlibrc file from\n",
      "\u001b[2m\u001b[36m(pid=29796)\u001b[0m https://github.com/matplotlib/matplotlib/blob/v3.5.1/matplotlibrc.template\n",
      "\u001b[2m\u001b[36m(pid=29796)\u001b[0m or from the matplotlib source distribution\n",
      "\u001b[2m\u001b[36m(pid=29796)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=29796)\u001b[0m Bad key animation.avconv_path in file /home/michael/anaconda3/envs/flow/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 477 ('animation.avconv_path: avconv     # Path to avconv binary. Without full path')\n",
      "\u001b[2m\u001b[36m(pid=29796)\u001b[0m You probably need to get an updated matplotlibrc file from\n",
      "\u001b[2m\u001b[36m(pid=29796)\u001b[0m https://github.com/matplotlib/matplotlib/blob/v3.5.1/matplotlibrc.template\n",
      "\u001b[2m\u001b[36m(pid=29796)\u001b[0m or from the matplotlib source distribution\n",
      "\u001b[2m\u001b[36m(pid=29796)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=29796)\u001b[0m Bad key animation.avconv_args in file /home/michael/anaconda3/envs/flow/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 479 ('animation.avconv_args:            # Additional arguments to pass to avconv')\n",
      "\u001b[2m\u001b[36m(pid=29796)\u001b[0m You probably need to get an updated matplotlibrc file from\n",
      "\u001b[2m\u001b[36m(pid=29796)\u001b[0m https://github.com/matplotlib/matplotlib/blob/v3.5.1/matplotlibrc.template\n",
      "\u001b[2m\u001b[36m(pid=29796)\u001b[0m or from the matplotlib source distribution\n",
      "\u001b[2m\u001b[36m(pid=29797)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=29797)\u001b[0m Bad key text.latex.preview in file /home/michael/anaconda3/envs/flow/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 123 ('text.latex.preview : False')\n",
      "\u001b[2m\u001b[36m(pid=29797)\u001b[0m You probably need to get an updated matplotlibrc file from\n",
      "\u001b[2m\u001b[36m(pid=29797)\u001b[0m https://github.com/matplotlib/matplotlib/blob/v3.5.1/matplotlibrc.template\n",
      "\u001b[2m\u001b[36m(pid=29797)\u001b[0m or from the matplotlib source distribution\n",
      "\u001b[2m\u001b[36m(pid=29797)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=29797)\u001b[0m Bad key mathtext.fallback_to_cm in file /home/michael/anaconda3/envs/flow/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 155 ('mathtext.fallback_to_cm : True  # When True, use symbols from the Computer Modern')\n",
      "\u001b[2m\u001b[36m(pid=29797)\u001b[0m You probably need to get an updated matplotlibrc file from\n",
      "\u001b[2m\u001b[36m(pid=29797)\u001b[0m https://github.com/matplotlib/matplotlib/blob/v3.5.1/matplotlibrc.template\n",
      "\u001b[2m\u001b[36m(pid=29797)\u001b[0m or from the matplotlib source distribution\n",
      "\u001b[2m\u001b[36m(pid=29797)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=29797)\u001b[0m Bad key savefig.jpeg_quality in file /home/michael/anaconda3/envs/flow/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 418 ('savefig.jpeg_quality: 95       # when a jpeg is saved, the default quality parameter.')\n",
      "\u001b[2m\u001b[36m(pid=29797)\u001b[0m You probably need to get an updated matplotlibrc file from\n",
      "\u001b[2m\u001b[36m(pid=29797)\u001b[0m https://github.com/matplotlib/matplotlib/blob/v3.5.1/matplotlibrc.template\n",
      "\u001b[2m\u001b[36m(pid=29797)\u001b[0m or from the matplotlib source distribution\n",
      "\u001b[2m\u001b[36m(pid=29797)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=29797)\u001b[0m Bad key keymap.all_axes in file /home/michael/anaconda3/envs/flow/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 466 ('keymap.all_axes : a                 # enable all axes')\n",
      "\u001b[2m\u001b[36m(pid=29797)\u001b[0m You probably need to get an updated matplotlibrc file from\n",
      "\u001b[2m\u001b[36m(pid=29797)\u001b[0m https://github.com/matplotlib/matplotlib/blob/v3.5.1/matplotlibrc.template\n",
      "\u001b[2m\u001b[36m(pid=29797)\u001b[0m or from the matplotlib source distribution\n",
      "\u001b[2m\u001b[36m(pid=29797)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=29797)\u001b[0m Bad key animation.avconv_path in file /home/michael/anaconda3/envs/flow/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 477 ('animation.avconv_path: avconv     # Path to avconv binary. Without full path')\n",
      "\u001b[2m\u001b[36m(pid=29797)\u001b[0m You probably need to get an updated matplotlibrc file from\n",
      "\u001b[2m\u001b[36m(pid=29797)\u001b[0m https://github.com/matplotlib/matplotlib/blob/v3.5.1/matplotlibrc.template\n",
      "\u001b[2m\u001b[36m(pid=29797)\u001b[0m or from the matplotlib source distribution\n",
      "\u001b[2m\u001b[36m(pid=29797)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=29797)\u001b[0m Bad key animation.avconv_args in file /home/michael/anaconda3/envs/flow/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 479 ('animation.avconv_args:            # Additional arguments to pass to avconv')\n",
      "\u001b[2m\u001b[36m(pid=29797)\u001b[0m You probably need to get an updated matplotlibrc file from\n",
      "\u001b[2m\u001b[36m(pid=29797)\u001b[0m https://github.com/matplotlib/matplotlib/blob/v3.5.1/matplotlibrc.template\n",
      "\u001b[2m\u001b[36m(pid=29797)\u001b[0m or from the matplotlib source distribution\n",
      "\u001b[2m\u001b[36m(pid=29795)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=29795)\u001b[0m Bad key text.latex.preview in file /home/michael/anaconda3/envs/flow/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 123 ('text.latex.preview : False')\n",
      "\u001b[2m\u001b[36m(pid=29795)\u001b[0m You probably need to get an updated matplotlibrc file from\n",
      "\u001b[2m\u001b[36m(pid=29795)\u001b[0m https://github.com/matplotlib/matplotlib/blob/v3.5.1/matplotlibrc.template\n",
      "\u001b[2m\u001b[36m(pid=29795)\u001b[0m or from the matplotlib source distribution\n",
      "\u001b[2m\u001b[36m(pid=29795)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=29795)\u001b[0m Bad key mathtext.fallback_to_cm in file /home/michael/anaconda3/envs/flow/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 155 ('mathtext.fallback_to_cm : True  # When True, use symbols from the Computer Modern')\n",
      "\u001b[2m\u001b[36m(pid=29795)\u001b[0m You probably need to get an updated matplotlibrc file from\n",
      "\u001b[2m\u001b[36m(pid=29795)\u001b[0m https://github.com/matplotlib/matplotlib/blob/v3.5.1/matplotlibrc.template\n",
      "\u001b[2m\u001b[36m(pid=29795)\u001b[0m or from the matplotlib source distribution\n",
      "\u001b[2m\u001b[36m(pid=29795)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=29795)\u001b[0m Bad key savefig.jpeg_quality in file /home/michael/anaconda3/envs/flow/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 418 ('savefig.jpeg_quality: 95       # when a jpeg is saved, the default quality parameter.')\n",
      "\u001b[2m\u001b[36m(pid=29795)\u001b[0m You probably need to get an updated matplotlibrc file from\n",
      "\u001b[2m\u001b[36m(pid=29795)\u001b[0m https://github.com/matplotlib/matplotlib/blob/v3.5.1/matplotlibrc.template\n",
      "\u001b[2m\u001b[36m(pid=29795)\u001b[0m or from the matplotlib source distribution\n",
      "\u001b[2m\u001b[36m(pid=29795)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=29795)\u001b[0m Bad key keymap.all_axes in file /home/michael/anaconda3/envs/flow/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 466 ('keymap.all_axes : a                 # enable all axes')\n",
      "\u001b[2m\u001b[36m(pid=29795)\u001b[0m You probably need to get an updated matplotlibrc file from\n",
      "\u001b[2m\u001b[36m(pid=29795)\u001b[0m https://github.com/matplotlib/matplotlib/blob/v3.5.1/matplotlibrc.template\n",
      "\u001b[2m\u001b[36m(pid=29795)\u001b[0m or from the matplotlib source distribution\n",
      "\u001b[2m\u001b[36m(pid=29795)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=29795)\u001b[0m Bad key animation.avconv_path in file /home/michael/anaconda3/envs/flow/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 477 ('animation.avconv_path: avconv     # Path to avconv binary. Without full path')\n",
      "\u001b[2m\u001b[36m(pid=29795)\u001b[0m You probably need to get an updated matplotlibrc file from\n",
      "\u001b[2m\u001b[36m(pid=29795)\u001b[0m https://github.com/matplotlib/matplotlib/blob/v3.5.1/matplotlibrc.template\n",
      "\u001b[2m\u001b[36m(pid=29795)\u001b[0m or from the matplotlib source distribution\n",
      "\u001b[2m\u001b[36m(pid=29795)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=29795)\u001b[0m Bad key animation.avconv_args in file /home/michael/anaconda3/envs/flow/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 479 ('animation.avconv_args:            # Additional arguments to pass to avconv')\n",
      "\u001b[2m\u001b[36m(pid=29795)\u001b[0m You probably need to get an updated matplotlibrc file from\n",
      "\u001b[2m\u001b[36m(pid=29795)\u001b[0m https://github.com/matplotlib/matplotlib/blob/v3.5.1/matplotlibrc.template\n",
      "\u001b[2m\u001b[36m(pid=29795)\u001b[0m or from the matplotlib source distribution\n",
      "\u001b[2m\u001b[36m(pid=29794)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=29794)\u001b[0m Bad key text.latex.preview in file /home/michael/anaconda3/envs/flow/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 123 ('text.latex.preview : False')\n",
      "\u001b[2m\u001b[36m(pid=29794)\u001b[0m You probably need to get an updated matplotlibrc file from\n",
      "\u001b[2m\u001b[36m(pid=29794)\u001b[0m https://github.com/matplotlib/matplotlib/blob/v3.5.1/matplotlibrc.template\n",
      "\u001b[2m\u001b[36m(pid=29794)\u001b[0m or from the matplotlib source distribution\n",
      "\u001b[2m\u001b[36m(pid=29794)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=29794)\u001b[0m Bad key mathtext.fallback_to_cm in file /home/michael/anaconda3/envs/flow/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 155 ('mathtext.fallback_to_cm : True  # When True, use symbols from the Computer Modern')\n",
      "\u001b[2m\u001b[36m(pid=29794)\u001b[0m You probably need to get an updated matplotlibrc file from\n",
      "\u001b[2m\u001b[36m(pid=29794)\u001b[0m https://github.com/matplotlib/matplotlib/blob/v3.5.1/matplotlibrc.template\n",
      "\u001b[2m\u001b[36m(pid=29794)\u001b[0m or from the matplotlib source distribution\n",
      "\u001b[2m\u001b[36m(pid=29794)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=29794)\u001b[0m Bad key savefig.jpeg_quality in file /home/michael/anaconda3/envs/flow/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 418 ('savefig.jpeg_quality: 95       # when a jpeg is saved, the default quality parameter.')\n",
      "\u001b[2m\u001b[36m(pid=29794)\u001b[0m You probably need to get an updated matplotlibrc file from\n",
      "\u001b[2m\u001b[36m(pid=29794)\u001b[0m https://github.com/matplotlib/matplotlib/blob/v3.5.1/matplotlibrc.template\n",
      "\u001b[2m\u001b[36m(pid=29794)\u001b[0m or from the matplotlib source distribution\n",
      "\u001b[2m\u001b[36m(pid=29794)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=29794)\u001b[0m Bad key keymap.all_axes in file /home/michael/anaconda3/envs/flow/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 466 ('keymap.all_axes : a                 # enable all axes')\n",
      "\u001b[2m\u001b[36m(pid=29794)\u001b[0m You probably need to get an updated matplotlibrc file from\n",
      "\u001b[2m\u001b[36m(pid=29794)\u001b[0m https://github.com/matplotlib/matplotlib/blob/v3.5.1/matplotlibrc.template\n",
      "\u001b[2m\u001b[36m(pid=29794)\u001b[0m or from the matplotlib source distribution\n",
      "\u001b[2m\u001b[36m(pid=29794)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=29794)\u001b[0m Bad key animation.avconv_path in file /home/michael/anaconda3/envs/flow/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 477 ('animation.avconv_path: avconv     # Path to avconv binary. Without full path')\n",
      "\u001b[2m\u001b[36m(pid=29794)\u001b[0m You probably need to get an updated matplotlibrc file from\n",
      "\u001b[2m\u001b[36m(pid=29794)\u001b[0m https://github.com/matplotlib/matplotlib/blob/v3.5.1/matplotlibrc.template\n",
      "\u001b[2m\u001b[36m(pid=29794)\u001b[0m or from the matplotlib source distribution\n",
      "\u001b[2m\u001b[36m(pid=29794)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=29794)\u001b[0m Bad key animation.avconv_args in file /home/michael/anaconda3/envs/flow/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 479 ('animation.avconv_args:            # Additional arguments to pass to avconv')\n",
      "\u001b[2m\u001b[36m(pid=29794)\u001b[0m You probably need to get an updated matplotlibrc file from\n",
      "\u001b[2m\u001b[36m(pid=29794)\u001b[0m https://github.com/matplotlib/matplotlib/blob/v3.5.1/matplotlibrc.template\n",
      "\u001b[2m\u001b[36m(pid=29794)\u001b[0m or from the matplotlib source distribution\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=29799)\u001b[0m 2022-09-29 12:09:27,547\tWARNING util.py:55 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(pid=29795)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=29795)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29795)\u001b[0m ring length: 248\n",
      "\u001b[2m\u001b[36m(pid=29795)\u001b[0m v_max: 4.569659527528101\n",
      "\u001b[2m\u001b[36m(pid=29795)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29796)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=29796)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29796)\u001b[0m ring length: 255\n",
      "\u001b[2m\u001b[36m(pid=29796)\u001b[0m v_max: 4.902299776966978\n",
      "\u001b[2m\u001b[36m(pid=29796)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29797)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=29797)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29797)\u001b[0m ring length: 238\n",
      "\u001b[2m\u001b[36m(pid=29797)\u001b[0m v_max: 4.094180836186086\n",
      "\u001b[2m\u001b[36m(pid=29797)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29798)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=29798)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29798)\u001b[0m ring length: 236\n",
      "\u001b[2m\u001b[36m(pid=29798)\u001b[0m v_max: 3.999052674951912\n",
      "\u001b[2m\u001b[36m(pid=29798)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29794)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=29794)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29794)\u001b[0m ring length: 248\n",
      "\u001b[2m\u001b[36m(pid=29794)\u001b[0m v_max: 4.569659527528101\n",
      "\u001b[2m\u001b[36m(pid=29794)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29795)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=29795)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29795)\u001b[0m ring length: 256\n",
      "\u001b[2m\u001b[36m(pid=29795)\u001b[0m v_max: 4.949804327743507\n",
      "\u001b[2m\u001b[36m(pid=29795)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29796)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=29796)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29796)\u001b[0m ring length: 231\n",
      "\u001b[2m\u001b[36m(pid=29796)\u001b[0m v_max: 3.761192925277308\n",
      "\u001b[2m\u001b[36m(pid=29796)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29797)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=29797)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29797)\u001b[0m ring length: 257\n",
      "\u001b[2m\u001b[36m(pid=29797)\u001b[0m v_max: 4.997304682316517\n",
      "\u001b[2m\u001b[36m(pid=29797)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29798)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=29798)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29798)\u001b[0m ring length: 253\n",
      "\u001b[2m\u001b[36m(pid=29798)\u001b[0m v_max: 4.807278529691987\n",
      "\u001b[2m\u001b[36m(pid=29798)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29794)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=29794)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29794)\u001b[0m ring length: 248\n",
      "\u001b[2m\u001b[36m(pid=29794)\u001b[0m v_max: 4.569659527528101\n",
      "\u001b[2m\u001b[36m(pid=29794)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29795)\u001b[0m ./michael_files/emission_collection/singleagent_onelane_ring_20220929-1209271664471367.5581455-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=29796)\u001b[0m ./michael_files/emission_collection/singleagent_onelane_ring_20220929-1209271664471367.558048-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=29797)\u001b[0m ./michael_files/emission_collection/singleagent_onelane_ring_20220929-1209271664471367.5580838-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=29798)\u001b[0m ./michael_files/emission_collection/singleagent_onelane_ring_20220929-1209271664471367.5581636-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=29794)\u001b[0m ./michael_files/emission_collection/singleagent_onelane_ring_20220929-1209271664471367.5579946-0_emission.csv ./michael_files/emission_collection/\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_76534_00000:\n",
      "  agent_timesteps_total: 15000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-09-29_12-15-25\n",
      "  done: false\n",
      "  episode_len_mean: 3000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -4553.108932706082\n",
      "  episode_reward_mean: -4865.720531234294\n",
      "  episode_reward_min: -5104.719297832834\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 5\n",
      "  experiment_id: 939d1d5e80b64de89564600c497902cc\n",
      "  hostname: michael-home\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.19999999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.3562973079034837\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.005131190146617512\n",
      "          policy_loss: -0.0036096321004521798\n",
      "          total_loss: 1596.8351916297008\n",
      "          vf_explained_var: 3.581330076940503e-08\n",
      "          vf_loss: 1596.837776623742\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 15000\n",
      "    num_steps_sampled: 15000\n",
      "    num_steps_trained: 15000\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 192.168.1.76\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 27.840508806262232\n",
      "    ram_util_percent: 44.7119373776908\n",
      "  pid: 29799\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09024738272362175\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 65.44791234012287\n",
      "    mean_inference_ms: 1.8113217485701785\n",
      "    mean_raw_obs_processing_ms: 34.87183425633838\n",
      "  time_since_restore: 357.9421977996826\n",
      "  time_this_iter_s: 357.9421977996826\n",
      "  time_total_s: 357.9421977996826\n",
      "  timers:\n",
      "    learn_throughput: 296.741\n",
      "    learn_time_ms: 50549.102\n",
      "    sample_throughput: 48.8\n",
      "    sample_time_ms: 307376.332\n",
      "    update_time_ms: 3.272\n",
      "  timestamp: 1664471725\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 15000\n",
      "  training_iteration: 1\n",
      "  trial_id: '76534_00000'\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 14.7/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/4.72 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Result logdir: /home/michael/ray_results/singleagent_onelane_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |    ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_76534_00000 | RUNNING  | 192.168.1.76:29799 |      1 |          357.942 | 15000 | -4865.72 |             -4553.11 |             -5104.72 |               3000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=29799)\u001b[0m 2022-09-29 12:15:25,491\tWARNING ppo.py:242 -- The magnitude of your environment rewards are more than 487.0x the scale of `vf_clip_param`. This means that it will take more than 487.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=29795)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=29795)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29795)\u001b[0m ring length: 245\n",
      "\u001b[2m\u001b[36m(pid=29795)\u001b[0m v_max: 4.427046998576354\n",
      "\u001b[2m\u001b[36m(pid=29795)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29796)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=29796)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29796)\u001b[0m ring length: 226\n",
      "\u001b[2m\u001b[36m(pid=29796)\u001b[0m v_max: 3.5232840694769565\n",
      "\u001b[2m\u001b[36m(pid=29796)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29797)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=29797)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29797)\u001b[0m ring length: 262\n",
      "\u001b[2m\u001b[36m(pid=29797)\u001b[0m v_max: 5.234739483008334\n",
      "\u001b[2m\u001b[36m(pid=29797)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29798)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=29798)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29798)\u001b[0m ring length: 266\n",
      "\u001b[2m\u001b[36m(pid=29798)\u001b[0m v_max: 5.42459972166245\n",
      "\u001b[2m\u001b[36m(pid=29798)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29794)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=29794)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29794)\u001b[0m ring length: 220\n",
      "\u001b[2m\u001b[36m(pid=29794)\u001b[0m v_max: 3.2377399006821497\n",
      "\u001b[2m\u001b[36m(pid=29794)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29795)\u001b[0m ./michael_files/emission_collection/singleagent_onelane_ring_20220929-1212501664471570.5059524-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=29796)\u001b[0m ./michael_files/emission_collection/singleagent_onelane_ring_20220929-1212501664471570.5092082-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=29797)\u001b[0m ./michael_files/emission_collection/singleagent_onelane_ring_20220929-1212501664471570.5087414-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=29798)\u001b[0m ./michael_files/emission_collection/singleagent_onelane_ring_20220929-1212501664471570.5048978-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=29794)\u001b[0m ./michael_files/emission_collection/singleagent_onelane_ring_20220929-1212501664471570.5090265-0_emission.csv ./michael_files/emission_collection/\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_WaveAttenuationPOEnv-v0_76534_00000:\n",
      "  agent_timesteps_total: 30000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-09-29_12-19-40\n",
      "  done: false\n",
      "  episode_len_mean: 3000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -4553.108932706082\n",
      "  episode_reward_mean: -4952.496011512117\n",
      "  episode_reward_min: -5111.392282507102\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 10\n",
      "  experiment_id: 939d1d5e80b64de89564600c497902cc\n",
      "  hostname: michael-home\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.09999999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.27788199226735\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009937891185533726\n",
      "          policy_loss: -0.006411516624187135\n",
      "          total_loss: 1639.7789792852886\n",
      "          vf_explained_var: -9.950944956926833e-09\n",
      "          vf_loss: 1639.7844040692862\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 30000\n",
      "    num_steps_sampled: 30000\n",
      "    num_steps_trained: 30000\n",
      "  iterations_since_restore: 2\n",
      "  node_ip: 192.168.1.76\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 27.245054945054946\n",
      "    ram_util_percent: 49.17472527472527\n",
      "  pid: 29799\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09062090897082672\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 56.906646152582006\n",
      "    mean_inference_ms: 1.821701821157815\n",
      "    mean_raw_obs_processing_ms: 34.87792647824709\n",
      "  time_since_restore: 612.924711227417\n",
      "  time_this_iter_s: 254.98251342773438\n",
      "  time_total_s: 612.924711227417\n",
      "  timers:\n",
      "    learn_throughput: 298.054\n",
      "    learn_time_ms: 50326.388\n",
      "    sample_throughput: 58.566\n",
      "    sample_time_ms: 256120.729\n",
      "    update_time_ms: 3.166\n",
      "  timestamp: 1664471980\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 30000\n",
      "  training_iteration: 2\n",
      "  trial_id: '76534_00000'\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 15.4/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/4.72 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_4_7b3716726cd22aa6f9a7a4f69638d7d8, 0.0/1.0 CPU_group_2_7b3716726cd22aa6f9a7a4f69638d7d8, 0.0/1.0 CPU_group_1_7b3716726cd22aa6f9a7a4f69638d7d8, 0.0/6.0 CPU_group_7b3716726cd22aa6f9a7a4f69638d7d8, 0.0/1.0 CPU_group_0_7b3716726cd22aa6f9a7a4f69638d7d8, 0.0/1.0 CPU_group_3_7b3716726cd22aa6f9a7a4f69638d7d8, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_5_7b3716726cd22aa6f9a7a4f69638d7d8)\n",
      "Result logdir: /home/michael/ray_results/singleagent_onelane_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |    ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_76534_00000 | RUNNING  | 192.168.1.76:29799 |      2 |          612.925 | 30000 |  -4952.5 |             -4553.11 |             -5111.39 |               3000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=29799)\u001b[0m 2022-09-29 12:19:40,541\tWARNING ppo.py:242 -- The magnitude of your environment rewards are more than 495.0x the scale of `vf_clip_param`. This means that it will take more than 495.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=29795)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=29795)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29795)\u001b[0m ring length: 253\n",
      "\u001b[2m\u001b[36m(pid=29795)\u001b[0m v_max: 4.807278529691987\n",
      "\u001b[2m\u001b[36m(pid=29795)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29796)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=29796)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29796)\u001b[0m ring length: 257\n",
      "\u001b[2m\u001b[36m(pid=29796)\u001b[0m v_max: 4.997304682316517\n",
      "\u001b[2m\u001b[36m(pid=29796)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29797)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=29797)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29797)\u001b[0m ring length: 260\n",
      "\u001b[2m\u001b[36m(pid=29797)\u001b[0m v_max: 5.139779427502188\n",
      "\u001b[2m\u001b[36m(pid=29797)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29798)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=29798)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29798)\u001b[0m ring length: 241\n",
      "\u001b[2m\u001b[36m(pid=29798)\u001b[0m v_max: 4.236854288051661\n",
      "\u001b[2m\u001b[36m(pid=29798)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29794)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=29794)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29794)\u001b[0m ring length: 226\n",
      "\u001b[2m\u001b[36m(pid=29794)\u001b[0m v_max: 3.5232840694769565\n",
      "\u001b[2m\u001b[36m(pid=29794)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29795)\u001b[0m ./michael_files/emission_collection/singleagent_onelane_ring_20220929-1217061664471826.0310953-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=29796)\u001b[0m ./michael_files/emission_collection/singleagent_onelane_ring_20220929-1217061664471826.0318687-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=29797)\u001b[0m ./michael_files/emission_collection/singleagent_onelane_ring_20220929-1217061664471826.0301507-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=29798)\u001b[0m ./michael_files/emission_collection/singleagent_onelane_ring_20220929-1217061664471826.0319047-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=29794)\u001b[0m ./michael_files/emission_collection/singleagent_onelane_ring_20220929-1217061664471826.0293562-0_emission.csv ./michael_files/emission_collection/\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_76534_00000:\n",
      "  agent_timesteps_total: 45000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-09-29_12-23-55\n",
      "  done: false\n",
      "  episode_len_mean: 3000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -4553.108932706082\n",
      "  episode_reward_mean: -4926.877228937498\n",
      "  episode_reward_min: -5111.392282507102\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 15\n",
      "  experiment_id: 939d1d5e80b64de89564600c497902cc\n",
      "  hostname: michael-home\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.049999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.2023822344965853\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011513690018895338\n",
      "          policy_loss: -0.004103976297902607\n",
      "          total_loss: 1487.965859933627\n",
      "          vf_explained_var: -4.596629299413735e-09\n",
      "          vf_loss: 1487.9693838345802\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 45000\n",
      "    num_steps_sampled: 45000\n",
      "    num_steps_trained: 45000\n",
      "  iterations_since_restore: 3\n",
      "  node_ip: 192.168.1.76\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 27.21071428571429\n",
      "    ram_util_percent: 45.8989010989011\n",
      "  pid: 29799\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09080062252220186\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 52.15983098438426\n",
      "    mean_inference_ms: 1.8292486217573662\n",
      "    mean_raw_obs_processing_ms: 34.87925558407181\n",
      "  time_since_restore: 868.1373195648193\n",
      "  time_this_iter_s: 255.21260833740234\n",
      "  time_total_s: 868.1373195648193\n",
      "  timers:\n",
      "    learn_throughput: 297.858\n",
      "    learn_time_ms: 50359.552\n",
      "    sample_throughput: 62.76\n",
      "    sample_time_ms: 239004.655\n",
      "    update_time_ms: 3.112\n",
      "  timestamp: 1664472235\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 45000\n",
      "  training_iteration: 3\n",
      "  trial_id: '76534_00000'\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 14.4/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/4.72 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_2_7b3716726cd22aa6f9a7a4f69638d7d8, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_1_7b3716726cd22aa6f9a7a4f69638d7d8, 0.0/1.0 CPU_group_0_7b3716726cd22aa6f9a7a4f69638d7d8, 0.0/1.0 CPU_group_5_7b3716726cd22aa6f9a7a4f69638d7d8, 0.0/1.0 CPU_group_3_7b3716726cd22aa6f9a7a4f69638d7d8, 0.0/6.0 CPU_group_7b3716726cd22aa6f9a7a4f69638d7d8, 0.0/1.0 CPU_group_4_7b3716726cd22aa6f9a7a4f69638d7d8)\n",
      "Result logdir: /home/michael/ray_results/singleagent_onelane_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |    ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_76534_00000 | RUNNING  | 192.168.1.76:29799 |      3 |          868.137 | 45000 | -4926.88 |             -4553.11 |             -5111.39 |               3000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=29799)\u001b[0m 2022-09-29 12:23:55,794\tWARNING ppo.py:242 -- The magnitude of your environment rewards are more than 493.0x the scale of `vf_clip_param`. This means that it will take more than 493.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=29795)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=29795)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29795)\u001b[0m ring length: 223\n",
      "\u001b[2m\u001b[36m(pid=29795)\u001b[0m v_max: 3.3805185755587788\n",
      "\u001b[2m\u001b[36m(pid=29795)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29796)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=29796)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29796)\u001b[0m ring length: 223\n",
      "\u001b[2m\u001b[36m(pid=29796)\u001b[0m v_max: 3.3805185755587788\n",
      "\u001b[2m\u001b[36m(pid=29796)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29797)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=29797)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29797)\u001b[0m ring length: 251\n",
      "\u001b[2m\u001b[36m(pid=29797)\u001b[0m v_max: 4.71224180704279\n",
      "\u001b[2m\u001b[36m(pid=29797)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29798)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=29798)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29798)\u001b[0m ring length: 231\n",
      "\u001b[2m\u001b[36m(pid=29798)\u001b[0m v_max: 3.761192925277308\n",
      "\u001b[2m\u001b[36m(pid=29798)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29794)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=29794)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29794)\u001b[0m ring length: 259\n",
      "\u001b[2m\u001b[36m(pid=29794)\u001b[0m v_max: 5.092292348291853\n",
      "\u001b[2m\u001b[36m(pid=29794)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29795)\u001b[0m ./michael_files/emission_collection/singleagent_onelane_ring_20220929-1221211664472081.05306-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=29796)\u001b[0m ./michael_files/emission_collection/singleagent_onelane_ring_20220929-1221211664472081.0500224-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=29798)\u001b[0m ./michael_files/emission_collection/singleagent_onelane_ring_20220929-1221211664472081.0501828-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=29797)\u001b[0m ./michael_files/emission_collection/singleagent_onelane_ring_20220929-1221211664472081.054397-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=29794)\u001b[0m ./michael_files/emission_collection/singleagent_onelane_ring_20220929-1221211664472081.054703-0_emission.csv ./michael_files/emission_collection/\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_76534_00000:\n",
      "  agent_timesteps_total: 60000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-09-29_12-28-11\n",
      "  done: false\n",
      "  episode_len_mean: 3000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -4553.108932706082\n",
      "  episode_reward_mean: -4893.0982477052785\n",
      "  episode_reward_min: -5111.392282507102\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 20\n",
      "  experiment_id: 939d1d5e80b64de89564600c497902cc\n",
      "  hostname: michael-home\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.049999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.126540728342735\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01194098207468923\n",
      "          policy_loss: -0.005449865249348647\n",
      "          total_loss: 1402.0176990573689\n",
      "          vf_explained_var: 1.0102482045359906e-10\n",
      "          vf_loss: 1402.0225562144135\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 60000\n",
      "    num_steps_sampled: 60000\n",
      "    num_steps_trained: 60000\n",
      "  iterations_since_restore: 4\n",
      "  node_ip: 192.168.1.76\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 27.018852459016394\n",
      "    ram_util_percent: 46.01639344262295\n",
      "  pid: 29799\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09090642080947779\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 49.07573199641563\n",
      "    mean_inference_ms: 1.8324676264007056\n",
      "    mean_raw_obs_processing_ms: 34.899858082457534\n",
      "  time_since_restore: 1124.2112863063812\n",
      "  time_this_iter_s: 256.0739667415619\n",
      "  time_total_s: 1124.2112863063812\n",
      "  timers:\n",
      "    learn_throughput: 298.298\n",
      "    learn_time_ms: 50285.269\n",
      "    sample_throughput: 65.005\n",
      "    sample_time_ms: 230752.835\n",
      "    update_time_ms: 3.126\n",
      "  timestamp: 1664472491\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 60000\n",
      "  training_iteration: 4\n",
      "  trial_id: '76534_00000'\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 14.4/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/4.72 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_5_7b3716726cd22aa6f9a7a4f69638d7d8, 0.0/1.0 CPU_group_0_7b3716726cd22aa6f9a7a4f69638d7d8, 0.0/1.0 CPU_group_1_7b3716726cd22aa6f9a7a4f69638d7d8, 0.0/1.0 CPU_group_3_7b3716726cd22aa6f9a7a4f69638d7d8, 0.0/6.0 CPU_group_7b3716726cd22aa6f9a7a4f69638d7d8, 0.0/1.0 CPU_group_4_7b3716726cd22aa6f9a7a4f69638d7d8, 0.0/1.0 CPU_group_2_7b3716726cd22aa6f9a7a4f69638d7d8)\n",
      "Result logdir: /home/michael/ray_results/singleagent_onelane_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |    ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_76534_00000 | RUNNING  | 192.168.1.76:29799 |      4 |          1124.21 | 60000 |  -4893.1 |             -4553.11 |             -5111.39 |               3000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=29799)\u001b[0m 2022-09-29 12:28:11,915\tWARNING ppo.py:242 -- The magnitude of your environment rewards are more than 489.0x the scale of `vf_clip_param`. This means that it will take more than 489.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=29795)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=29795)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29795)\u001b[0m ring length: 236\n",
      "\u001b[2m\u001b[36m(pid=29795)\u001b[0m v_max: 3.999052674951912\n",
      "\u001b[2m\u001b[36m(pid=29795)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29796)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=29796)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29796)\u001b[0m ring length: 255\n",
      "\u001b[2m\u001b[36m(pid=29796)\u001b[0m v_max: 4.902299776966978\n",
      "\u001b[2m\u001b[36m(pid=29796)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29797)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=29797)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29797)\u001b[0m ring length: 252\n",
      "\u001b[2m\u001b[36m(pid=29797)\u001b[0m v_max: 4.759762049824181\n",
      "\u001b[2m\u001b[36m(pid=29797)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29798)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=29798)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29798)\u001b[0m ring length: 263\n",
      "\u001b[2m\u001b[36m(pid=29798)\u001b[0m v_max: 5.282212215151455\n",
      "\u001b[2m\u001b[36m(pid=29798)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29794)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=29794)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29794)\u001b[0m ring length: 224\n",
      "\u001b[2m\u001b[36m(pid=29794)\u001b[0m v_max: 3.4281086136538996\n",
      "\u001b[2m\u001b[36m(pid=29794)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29795)\u001b[0m ./michael_files/emission_collection/singleagent_onelane_ring_20220929-1225361664472336.3041515-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=29796)\u001b[0m ./michael_files/emission_collection/singleagent_onelane_ring_20220929-1225361664472336.3057287-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=29797)\u001b[0m ./michael_files/emission_collection/singleagent_onelane_ring_20220929-1225361664472336.305258-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=29798)\u001b[0m ./michael_files/emission_collection/singleagent_onelane_ring_20220929-1225361664472336.3021007-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=29794)\u001b[0m ./michael_files/emission_collection/singleagent_onelane_ring_20220929-1225361664472336.3031511-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=29798)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=29794)\u001b[0m  Retrying in 1 seconds\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_76534_00000:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  agent_timesteps_total: 75000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-09-29_12-32-28\n",
      "  done: false\n",
      "  episode_len_mean: 3000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3973.9839617183966\n",
      "  episode_reward_mean: -4766.540557886204\n",
      "  episode_reward_min: -5111.392282507102\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 25\n",
      "  experiment_id: 939d1d5e80b64de89564600c497902cc\n",
      "  hostname: michael-home\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.049999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.0359447436817621\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009688501921685626\n",
      "          policy_loss: -0.004847394639105236\n",
      "          total_loss: 1051.5654203253278\n",
      "          vf_explained_var: -8.081985636287925e-10\n",
      "          vf_loss: 1051.5697835114042\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 75000\n",
      "    num_steps_sampled: 75000\n",
      "    num_steps_trained: 75000\n",
      "  iterations_since_restore: 5\n",
      "  node_ip: 192.168.1.76\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 27.136612021857925\n",
      "    ram_util_percent: 46.05846994535519\n",
      "  pid: 29799\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09096793202229976\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 46.88506736169761\n",
      "    mean_inference_ms: 1.8352839798211154\n",
      "    mean_raw_obs_processing_ms: 34.920116382660346\n",
      "  time_since_restore: 1381.0053157806396\n",
      "  time_this_iter_s: 256.7940294742584\n",
      "  time_total_s: 1381.0053157806396\n",
      "  timers:\n",
      "    learn_throughput: 298.04\n",
      "    learn_time_ms: 50328.824\n",
      "    sample_throughput: 66.414\n",
      "    sample_time_ms: 225857.649\n",
      "    update_time_ms: 3.122\n",
      "  timestamp: 1664472748\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 75000\n",
      "  training_iteration: 5\n",
      "  trial_id: '76534_00000'\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 14.4/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/4.72 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_4_7b3716726cd22aa6f9a7a4f69638d7d8, 0.0/1.0 CPU_group_2_7b3716726cd22aa6f9a7a4f69638d7d8, 0.0/6.0 CPU_group_7b3716726cd22aa6f9a7a4f69638d7d8, 0.0/1.0 CPU_group_1_7b3716726cd22aa6f9a7a4f69638d7d8, 0.0/1.0 CPU_group_0_7b3716726cd22aa6f9a7a4f69638d7d8, 0.0/1.0 CPU_group_3_7b3716726cd22aa6f9a7a4f69638d7d8, 0.0/1.0 CPU_group_5_7b3716726cd22aa6f9a7a4f69638d7d8)\n",
      "Result logdir: /home/michael/ray_results/singleagent_onelane_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |    ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_76534_00000 | RUNNING  | 192.168.1.76:29799 |      5 |          1381.01 | 75000 | -4766.54 |             -3973.98 |             -5111.39 |               3000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=29799)\u001b[0m 2022-09-29 12:32:28,757\tWARNING ppo.py:242 -- The magnitude of your environment rewards are more than 477.0x the scale of `vf_clip_param`. This means that it will take more than 477.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=29795)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=29795)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29795)\u001b[0m ring length: 237\n",
      "\u001b[2m\u001b[36m(pid=29795)\u001b[0m v_max: 4.04661795519353\n",
      "\u001b[2m\u001b[36m(pid=29795)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29796)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=29796)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29796)\u001b[0m ring length: 239\n",
      "\u001b[2m\u001b[36m(pid=29796)\u001b[0m v_max: 4.141741239205832\n",
      "\u001b[2m\u001b[36m(pid=29796)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29797)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=29797)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29797)\u001b[0m ring length: 236\n",
      "\u001b[2m\u001b[36m(pid=29797)\u001b[0m v_max: 3.999052674951912\n",
      "\u001b[2m\u001b[36m(pid=29797)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29798)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=29798)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29798)\u001b[0m ring length: 254\n",
      "\u001b[2m\u001b[36m(pid=29798)\u001b[0m v_max: 4.854791141191876\n",
      "\u001b[2m\u001b[36m(pid=29798)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29794)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=29794)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29794)\u001b[0m ring length: 252\n",
      "\u001b[2m\u001b[36m(pid=29794)\u001b[0m v_max: 4.759762049824181\n",
      "\u001b[2m\u001b[36m(pid=29794)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29795)\u001b[0m ./michael_files/emission_collection/singleagent_onelane_ring_20220929-1229521664472592.5926275-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=29796)\u001b[0m ./michael_files/emission_collection/singleagent_onelane_ring_20220929-1229521664472592.5891607-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=29797)\u001b[0m ./michael_files/emission_collection/singleagent_onelane_ring_20220929-1229521664472592.5934815-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=29798)\u001b[0m ./michael_files/emission_collection/singleagent_onelane_ring_20220929-1229521664472592.5894778-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=29794)\u001b[0m ./michael_files/emission_collection/singleagent_onelane_ring_20220929-1229521664472592.5884805-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=29796)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=29797)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=29795)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=29798)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=29794)\u001b[0m  Retrying in 1 seconds\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_76534_00000:\n",
      "  agent_timesteps_total: 90000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-09-29_12-36-44\n",
      "  done: false\n",
      "  episode_len_mean: 3000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2412.554907567702\n",
      "  episode_reward_mean: -4513.402531217313\n",
      "  episode_reward_min: -5111.392282507102\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 30\n",
      "  experiment_id: 939d1d5e80b64de89564600c497902cc\n",
      "  hostname: michael-home\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.024999999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.9316345843218141\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015906169006397467\n",
      "          policy_loss: -0.005149937847676545\n",
      "          total_loss: 578.5780767667092\n",
      "          vf_explained_var: -2.0710089199127424e-09\n",
      "          vf_loss: 578.5828298536397\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 90000\n",
      "    num_steps_sampled: 90000\n",
      "    num_steps_trained: 90000\n",
      "  iterations_since_restore: 6\n",
      "  node_ip: 192.168.1.76\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 27.13123287671233\n",
      "    ram_util_percent: 46.1282191780822\n",
      "  pid: 29799\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09099749770992631\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 45.23506064623025\n",
      "    mean_inference_ms: 1.8368525279456205\n",
      "    mean_raw_obs_processing_ms: 34.939577687315605\n",
      "  time_since_restore: 1636.7376301288605\n",
      "  time_this_iter_s: 255.73231434822083\n",
      "  time_total_s: 1636.7376301288605\n",
      "  timers:\n",
      "    learn_throughput: 298.486\n",
      "    learn_time_ms: 50253.601\n",
      "    sample_throughput: 67.409\n",
      "    sample_time_ms: 222521.359\n",
      "    update_time_ms: 3.127\n",
      "  timestamp: 1664473004\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 90000\n",
      "  training_iteration: 6\n",
      "  trial_id: '76534_00000'\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 14.4/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/4.72 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_5_7b3716726cd22aa6f9a7a4f69638d7d8, 0.0/1.0 CPU_group_4_7b3716726cd22aa6f9a7a4f69638d7d8, 0.0/6.0 CPU_group_7b3716726cd22aa6f9a7a4f69638d7d8, 0.0/1.0 CPU_group_1_7b3716726cd22aa6f9a7a4f69638d7d8, 0.0/1.0 CPU_group_2_7b3716726cd22aa6f9a7a4f69638d7d8, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_3_7b3716726cd22aa6f9a7a4f69638d7d8, 0.0/1.0 CPU_group_0_7b3716726cd22aa6f9a7a4f69638d7d8)\n",
      "Result logdir: /home/michael/ray_results/singleagent_onelane_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |    ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_76534_00000 | RUNNING  | 192.168.1.76:29799 |      6 |          1636.74 | 90000 |  -4513.4 |             -2412.55 |             -5111.39 |               3000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=29799)\u001b[0m 2022-09-29 12:36:44,535\tWARNING ppo.py:242 -- The magnitude of your environment rewards are more than 451.0x the scale of `vf_clip_param`. This means that it will take more than 451.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=29795)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=29795)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29795)\u001b[0m ring length: 260\n",
      "\u001b[2m\u001b[36m(pid=29795)\u001b[0m v_max: 5.139779427502188\n",
      "\u001b[2m\u001b[36m(pid=29795)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29796)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=29796)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29796)\u001b[0m ring length: 255\n",
      "\u001b[2m\u001b[36m(pid=29796)\u001b[0m v_max: 4.902299776966978\n",
      "\u001b[2m\u001b[36m(pid=29796)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29797)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=29797)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29797)\u001b[0m ring length: 269\n",
      "\u001b[2m\u001b[36m(pid=29797)\u001b[0m v_max: 5.566938458220921\n",
      "\u001b[2m\u001b[36m(pid=29797)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29798)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=29798)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29798)\u001b[0m ring length: 227\n",
      "\u001b[2m\u001b[36m(pid=29798)\u001b[0m v_max: 3.570869368805752\n",
      "\u001b[2m\u001b[36m(pid=29798)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29794)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=29794)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29794)\u001b[0m ring length: 241\n",
      "\u001b[2m\u001b[36m(pid=29794)\u001b[0m v_max: 4.236854288051661\n",
      "\u001b[2m\u001b[36m(pid=29794)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29795)\u001b[0m ./michael_files/emission_collection/singleagent_onelane_ring_20220929-1234091664472849.277709-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=29796)\u001b[0m ./michael_files/emission_collection/singleagent_onelane_ring_20220929-1234091664472849.2727427-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=29797)\u001b[0m ./michael_files/emission_collection/singleagent_onelane_ring_20220929-1234091664472849.2650313-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=29798)\u001b[0m ./michael_files/emission_collection/singleagent_onelane_ring_20220929-1234091664472849.2761369-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=29794)\u001b[0m ./michael_files/emission_collection/singleagent_onelane_ring_20220929-1234091664472849.2758682-0_emission.csv ./michael_files/emission_collection/\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_76534_00000:\n",
      "  agent_timesteps_total: 105000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-09-29_12-40-59\n",
      "  done: false\n",
      "  episode_len_mean: 3000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2412.554907567702\n",
      "  episode_reward_mean: -4375.670961493747\n",
      "  episode_reward_min: -5111.392282507102\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 35\n",
      "  experiment_id: 939d1d5e80b64de89564600c497902cc\n",
      "  hostname: michael-home\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.024999999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.866731061016099\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014403981738902633\n",
      "          policy_loss: -0.005910000059570155\n",
      "          total_loss: 656.8850743439237\n",
      "          vf_explained_var: -1.0102482184137784e-09\n",
      "          vf_loss: 656.8906261638059\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 105000\n",
      "    num_steps_sampled: 105000\n",
      "    num_steps_trained: 105000\n",
      "  iterations_since_restore: 7\n",
      "  node_ip: 192.168.1.76\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 27.31675824175825\n",
      "    ram_util_percent: 46.111538461538466\n",
      "  pid: 29799\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09102054307281264\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 43.94019597743822\n",
      "    mean_inference_ms: 1.8380174344205813\n",
      "    mean_raw_obs_processing_ms: 34.951063023064684\n",
      "  time_since_restore: 1891.8167068958282\n",
      "  time_this_iter_s: 255.07907676696777\n",
      "  time_total_s: 1891.8167068958282\n",
      "  timers:\n",
      "    learn_throughput: 298.552\n",
      "    learn_time_ms: 50242.439\n",
      "    sample_throughput: 68.181\n",
      "    sample_time_ms: 220002.494\n",
      "    update_time_ms: 3.13\n",
      "  timestamp: 1664473259\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 105000\n",
      "  training_iteration: 7\n",
      "  trial_id: '76534_00000'\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 14.5/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/4.72 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_5_7b3716726cd22aa6f9a7a4f69638d7d8, 0.0/1.0 CPU_group_4_7b3716726cd22aa6f9a7a4f69638d7d8, 0.0/1.0 CPU_group_3_7b3716726cd22aa6f9a7a4f69638d7d8, 0.0/1.0 CPU_group_0_7b3716726cd22aa6f9a7a4f69638d7d8, 0.0/6.0 CPU_group_7b3716726cd22aa6f9a7a4f69638d7d8, 0.0/1.0 CPU_group_1_7b3716726cd22aa6f9a7a4f69638d7d8, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_2_7b3716726cd22aa6f9a7a4f69638d7d8)\n",
      "Result logdir: /home/michael/ray_results/singleagent_onelane_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_76534_00000 | RUNNING  | 192.168.1.76:29799 |      7 |          1891.82 | 105000 | -4375.67 |             -2412.55 |             -5111.39 |               3000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=29799)\u001b[0m 2022-09-29 12:40:59,662\tWARNING ppo.py:242 -- The magnitude of your environment rewards are more than 438.0x the scale of `vf_clip_param`. This means that it will take more than 438.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=29795)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=29795)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29795)\u001b[0m ring length: 227\n",
      "\u001b[2m\u001b[36m(pid=29795)\u001b[0m v_max: 3.570869368805752\n",
      "\u001b[2m\u001b[36m(pid=29795)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29796)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=29796)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29796)\u001b[0m ring length: 270\n",
      "\u001b[2m\u001b[36m(pid=29796)\u001b[0m v_max: 5.6143732387852054\n",
      "\u001b[2m\u001b[36m(pid=29796)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29797)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=29797)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29797)\u001b[0m ring length: 240\n",
      "\u001b[2m\u001b[36m(pid=29797)\u001b[0m v_max: 4.189299083856172\n",
      "\u001b[2m\u001b[36m(pid=29797)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29798)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=29798)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29798)\u001b[0m ring length: 263\n",
      "\u001b[2m\u001b[36m(pid=29798)\u001b[0m v_max: 5.282212215151455\n",
      "\u001b[2m\u001b[36m(pid=29798)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29794)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=29794)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29794)\u001b[0m ring length: 259\n",
      "\u001b[2m\u001b[36m(pid=29794)\u001b[0m v_max: 5.092292348291853\n",
      "\u001b[2m\u001b[36m(pid=29794)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29795)\u001b[0m ./michael_files/emission_collection/singleagent_onelane_ring_20220929-1238251664473105.0568967-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=29796)\u001b[0m ./michael_files/emission_collection/singleagent_onelane_ring_20220929-1238251664473105.0567183-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=29797)\u001b[0m ./michael_files/emission_collection/singleagent_onelane_ring_20220929-1238251664473105.0566072-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=29798)\u001b[0m ./michael_files/emission_collection/singleagent_onelane_ring_20220929-1238251664473105.056671-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=29794)\u001b[0m ./michael_files/emission_collection/singleagent_onelane_ring_20220929-1238251664473105.0520532-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=29796)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=29797)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=29798)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=29795)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=29794)\u001b[0m  Retrying in 1 seconds\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_76534_00000:\n",
      "  agent_timesteps_total: 120000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-09-29_12-45-15\n",
      "  done: false\n",
      "  episode_len_mean: 3000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -1685.4845579003083\n",
      "  episode_reward_mean: -4093.2780196676017\n",
      "  episode_reward_min: -5111.392282507102\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 40\n",
      "  experiment_id: 939d1d5e80b64de89564600c497902cc\n",
      "  hostname: michael-home\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.024999999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.728960135427572\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.022631066147343556\n",
      "          policy_loss: -0.008810190431442174\n",
      "          total_loss: 243.74740980762547\n",
      "          vf_explained_var: -2.1720336640385085e-09\n",
      "          vf_loss: 243.75565368523033\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 120000\n",
      "    num_steps_sampled: 120000\n",
      "    num_steps_trained: 120000\n",
      "  iterations_since_restore: 8\n",
      "  node_ip: 192.168.1.76\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 27.19808219178082\n",
      "    ram_util_percent: 46.16027397260274\n",
      "  pid: 29799\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09103817483594434\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 42.892762045233226\n",
      "    mean_inference_ms: 1.8388818613817843\n",
      "    mean_raw_obs_processing_ms: 34.96287300509367\n",
      "  time_since_restore: 2147.619206428528\n",
      "  time_this_iter_s: 255.80249953269958\n",
      "  time_total_s: 2147.619206428528\n",
      "  timers:\n",
      "    learn_throughput: 298.758\n",
      "    learn_time_ms: 50207.878\n",
      "    sample_throughput: 68.735\n",
      "    sample_time_ms: 218229.898\n",
      "    update_time_ms: 3.12\n",
      "  timestamp: 1664473515\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 120000\n",
      "  training_iteration: 8\n",
      "  trial_id: '76534_00000'\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 14.4/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/4.72 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_1_7b3716726cd22aa6f9a7a4f69638d7d8, 0.0/6.0 CPU_group_7b3716726cd22aa6f9a7a4f69638d7d8, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_0_7b3716726cd22aa6f9a7a4f69638d7d8, 0.0/1.0 CPU_group_2_7b3716726cd22aa6f9a7a4f69638d7d8, 0.0/1.0 CPU_group_5_7b3716726cd22aa6f9a7a4f69638d7d8, 0.0/1.0 CPU_group_4_7b3716726cd22aa6f9a7a4f69638d7d8, 0.0/1.0 CPU_group_3_7b3716726cd22aa6f9a7a4f69638d7d8)\n",
      "Result logdir: /home/michael/ray_results/singleagent_onelane_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_76534_00000 | RUNNING  | 192.168.1.76:29799 |      8 |          2147.62 | 120000 | -4093.28 |             -1685.48 |             -5111.39 |               3000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=29799)\u001b[0m 2022-09-29 12:45:15,508\tWARNING ppo.py:242 -- The magnitude of your environment rewards are more than 409.0x the scale of `vf_clip_param`. This means that it will take more than 409.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=29795)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=29795)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29795)\u001b[0m ring length: 266\n",
      "\u001b[2m\u001b[36m(pid=29795)\u001b[0m v_max: 5.42459972166245\n",
      "\u001b[2m\u001b[36m(pid=29795)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29796)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=29796)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29796)\u001b[0m ring length: 252\n",
      "\u001b[2m\u001b[36m(pid=29796)\u001b[0m v_max: 4.759762049824181\n",
      "\u001b[2m\u001b[36m(pid=29796)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29797)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=29797)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29797)\u001b[0m ring length: 226\n",
      "\u001b[2m\u001b[36m(pid=29797)\u001b[0m v_max: 3.5232840694769565\n",
      "\u001b[2m\u001b[36m(pid=29797)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29798)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=29798)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29798)\u001b[0m ring length: 267\n",
      "\u001b[2m\u001b[36m(pid=29798)\u001b[0m v_max: 5.472051563171826\n",
      "\u001b[2m\u001b[36m(pid=29798)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29794)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=29794)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29794)\u001b[0m ring length: 232\n",
      "\u001b[2m\u001b[36m(pid=29794)\u001b[0m v_max: 3.8087690783250063\n",
      "\u001b[2m\u001b[36m(pid=29794)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29795)\u001b[0m ./michael_files/emission_collection/singleagent_onelane_ring_20220929-1242401664473360.1783154-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=29796)\u001b[0m ./michael_files/emission_collection/singleagent_onelane_ring_20220929-1242401664473360.1804924-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=29797)\u001b[0m ./michael_files/emission_collection/singleagent_onelane_ring_20220929-1242401664473360.1754644-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=29798)\u001b[0m ./michael_files/emission_collection/singleagent_onelane_ring_20220929-1242401664473360.1796227-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=29794)\u001b[0m ./michael_files/emission_collection/singleagent_onelane_ring_20220929-1242401664473360.1773028-0_emission.csv ./michael_files/emission_collection/\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_76534_00000:\n",
      "  agent_timesteps_total: 135000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-09-29_12-49-30\n",
      "  done: false\n",
      "  episode_len_mean: 3000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -702.7114185584205\n",
      "  episode_reward_mean: -3795.005926564698\n",
      "  episode_reward_min: -5111.392282507102\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 45\n",
      "  experiment_id: 939d1d5e80b64de89564600c497902cc\n",
      "  hostname: michael-home\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.024999999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.6090737135228464\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014392983636636963\n",
      "          policy_loss: -0.006152804654833498\n",
      "          total_loss: 140.5650479041924\n",
      "          vf_explained_var: -4.0409928181439625e-10\n",
      "          vf_loss: 140.57084124613615\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 135000\n",
      "    num_steps_sampled: 135000\n",
      "    num_steps_trained: 135000\n",
      "  iterations_since_restore: 9\n",
      "  node_ip: 192.168.1.76\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 27.397252747252743\n",
      "    ram_util_percent: 46.22142857142857\n",
      "  pid: 29799\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09104535820489047\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 42.025268027292995\n",
      "    mean_inference_ms: 1.8396717238778364\n",
      "    mean_raw_obs_processing_ms: 34.970297656343874\n",
      "  time_since_restore: 2402.8038368225098\n",
      "  time_this_iter_s: 255.18463039398193\n",
      "  time_total_s: 2402.8038368225098\n",
      "  timers:\n",
      "    learn_throughput: 298.747\n",
      "    learn_time_ms: 50209.682\n",
      "    sample_throughput: 69.203\n",
      "    sample_time_ms: 216753.946\n",
      "    update_time_ms: 3.119\n",
      "  timestamp: 1664473770\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 135000\n",
      "  training_iteration: 9\n",
      "  trial_id: '76534_00000'\n",
      "  \n",
      "\u001b[2m\u001b[36m(pid=29799)\u001b[0m 2022-09-29 12:49:30,743\tWARNING ppo.py:242 -- The magnitude of your environment rewards are more than 380.0x the scale of `vf_clip_param`. This means that it will take more than 380.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "== Status ==\n",
      "Memory usage on this node: 14.5/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/4.72 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_3_7b3716726cd22aa6f9a7a4f69638d7d8, 0.0/1.0 CPU_group_5_7b3716726cd22aa6f9a7a4f69638d7d8, 0.0/1.0 CPU_group_2_7b3716726cd22aa6f9a7a4f69638d7d8, 0.0/6.0 CPU_group_7b3716726cd22aa6f9a7a4f69638d7d8, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_1_7b3716726cd22aa6f9a7a4f69638d7d8, 0.0/1.0 CPU_group_4_7b3716726cd22aa6f9a7a4f69638d7d8, 0.0/1.0 CPU_group_0_7b3716726cd22aa6f9a7a4f69638d7d8)\n",
      "Result logdir: /home/michael/ray_results/singleagent_onelane_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_76534_00000 | RUNNING  | 192.168.1.76:29799 |      9 |           2402.8 | 135000 | -3795.01 |             -702.711 |             -5111.39 |               3000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=29795)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=29795)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29795)\u001b[0m ring length: 260\n",
      "\u001b[2m\u001b[36m(pid=29795)\u001b[0m v_max: 5.139779427502188\n",
      "\u001b[2m\u001b[36m(pid=29795)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29796)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=29796)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29796)\u001b[0m ring length: 243\n",
      "\u001b[2m\u001b[36m(pid=29796)\u001b[0m v_max: 4.331956438196479\n",
      "\u001b[2m\u001b[36m(pid=29796)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29797)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=29797)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29797)\u001b[0m ring length: 222\n",
      "\u001b[2m\u001b[36m(pid=29797)\u001b[0m v_max: 3.3329270738617227\n",
      "\u001b[2m\u001b[36m(pid=29797)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29798)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=29798)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29798)\u001b[0m ring length: 253\n",
      "\u001b[2m\u001b[36m(pid=29798)\u001b[0m v_max: 4.807278529691987\n",
      "\u001b[2m\u001b[36m(pid=29798)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29794)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=29794)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29794)\u001b[0m ring length: 257\n",
      "\u001b[2m\u001b[36m(pid=29794)\u001b[0m v_max: 4.997304682316517\n",
      "\u001b[2m\u001b[36m(pid=29794)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29795)\u001b[0m ./michael_files/emission_collection/singleagent_onelane_ring_20220929-1246561664473616.0313578-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=29796)\u001b[0m ./michael_files/emission_collection/singleagent_onelane_ring_20220929-1246561664473616.0361607-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=29797)\u001b[0m ./michael_files/emission_collection/singleagent_onelane_ring_20220929-1246561664473616.0310853-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=29798)\u001b[0m ./michael_files/emission_collection/singleagent_onelane_ring_20220929-1246561664473616.0382876-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=29794)\u001b[0m ./michael_files/emission_collection/singleagent_onelane_ring_20220929-1246561664473616.036432-0_emission.csv ./michael_files/emission_collection/\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_76534_00000:\n",
      "  agent_timesteps_total: 150000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-09-29_12-53-45\n",
      "  done: false\n",
      "  episode_len_mean: 3000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -702.7114185584205\n",
      "  episode_reward_mean: -3535.575102786128\n",
      "  episode_reward_min: -5111.392282507102\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 50\n",
      "  experiment_id: 939d1d5e80b64de89564600c497902cc\n",
      "  hostname: michael-home\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.024999999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.4584184756218377\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01591141222601807\n",
      "          policy_loss: -0.006059239870647631\n",
      "          total_loss: 135.30682396646273\n",
      "          vf_explained_var: -2.1720336640385085e-09\n",
      "          vf_loss: 135.3124854362617\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 150000\n",
      "    num_steps_sampled: 150000\n",
      "    num_steps_trained: 150000\n",
      "  iterations_since_restore: 10\n",
      "  node_ip: 192.168.1.76\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 27.238461538461536\n",
      "    ram_util_percent: 46.348351648351645\n",
      "  pid: 29799\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09104910238433117\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 41.293340901251355\n",
      "    mean_inference_ms: 1.8402390961647763\n",
      "    mean_raw_obs_processing_ms: 34.974784545476595\n",
      "  time_since_restore: 2657.9407086372375\n",
      "  time_this_iter_s: 255.13687181472778\n",
      "  time_total_s: 2657.9407086372375\n",
      "  timers:\n",
      "    learn_throughput: 298.69\n",
      "    learn_time_ms: 50219.283\n",
      "    sample_throughput: 69.586\n",
      "    sample_time_ms: 215560.204\n",
      "    update_time_ms: 3.122\n",
      "  timestamp: 1664474025\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 150000\n",
      "  training_iteration: 10\n",
      "  trial_id: '76534_00000'\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 14.5/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/4.72 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_5_7b3716726cd22aa6f9a7a4f69638d7d8, 0.0/1.0 CPU_group_0_7b3716726cd22aa6f9a7a4f69638d7d8, 0.0/1.0 CPU_group_4_7b3716726cd22aa6f9a7a4f69638d7d8, 0.0/1.0 CPU_group_3_7b3716726cd22aa6f9a7a4f69638d7d8, 0.0/1.0 CPU_group_2_7b3716726cd22aa6f9a7a4f69638d7d8, 0.0/6.0 CPU_group_7b3716726cd22aa6f9a7a4f69638d7d8, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_1_7b3716726cd22aa6f9a7a4f69638d7d8)\n",
      "Result logdir: /home/michael/ray_results/singleagent_onelane_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_76534_00000 | RUNNING  | 192.168.1.76:29799 |     10 |          2657.94 | 150000 | -3535.58 |             -702.711 |             -5111.39 |               3000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=29799)\u001b[0m 2022-09-29 12:53:45,928\tWARNING ppo.py:242 -- The magnitude of your environment rewards are more than 354.0x the scale of `vf_clip_param`. This means that it will take more than 354.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=29795)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=29795)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29795)\u001b[0m ring length: 263\n",
      "\u001b[2m\u001b[36m(pid=29795)\u001b[0m v_max: 5.282212215151455\n",
      "\u001b[2m\u001b[36m(pid=29795)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29796)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=29796)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29796)\u001b[0m ring length: 248\n",
      "\u001b[2m\u001b[36m(pid=29796)\u001b[0m v_max: 4.569659527528101\n",
      "\u001b[2m\u001b[36m(pid=29796)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29797)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=29797)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29797)\u001b[0m ring length: 253\n",
      "\u001b[2m\u001b[36m(pid=29797)\u001b[0m v_max: 4.807278529691987\n",
      "\u001b[2m\u001b[36m(pid=29797)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29798)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=29798)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29798)\u001b[0m ring length: 246\n",
      "\u001b[2m\u001b[36m(pid=29798)\u001b[0m v_max: 4.474587708995648\n",
      "\u001b[2m\u001b[36m(pid=29798)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29794)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=29794)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29794)\u001b[0m ring length: 268\n",
      "\u001b[2m\u001b[36m(pid=29794)\u001b[0m v_max: 5.5194978538368025\n",
      "\u001b[2m\u001b[36m(pid=29794)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29795)\u001b[0m ./michael_files/emission_collection/singleagent_onelane_ring_20220929-1251111664473871.2592869-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=29796)\u001b[0m ./michael_files/emission_collection/singleagent_onelane_ring_20220929-1251111664473871.2579863-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=29797)\u001b[0m ./michael_files/emission_collection/singleagent_onelane_ring_20220929-1251111664473871.2596037-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=29798)\u001b[0m ./michael_files/emission_collection/singleagent_onelane_ring_20220929-1251111664473871.261121-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=29794)\u001b[0m ./michael_files/emission_collection/singleagent_onelane_ring_20220929-1251111664473871.2560227-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=29796)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=29797)\u001b[0m  Retrying in 1 seconds\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_76534_00000:\n",
      "  agent_timesteps_total: 165000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-09-29_12-58-01\n",
      "  done: false\n",
      "  episode_len_mean: 3000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -437.41526475832575\n",
      "  episode_reward_mean: -3291.055007775157\n",
      "  episode_reward_min: -5111.392282507102\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 55\n",
      "  experiment_id: 939d1d5e80b64de89564600c497902cc\n",
      "  hostname: michael-home\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.024999999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.3724977912791705\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011868835000850448\n",
      "          policy_loss: -0.0035209288919281403\n",
      "          total_loss: 110.72111821902\n",
      "          vf_explained_var: -2.121521180953323e-09\n",
      "          vf_loss: 110.72434261936253\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 165000\n",
      "    num_steps_sampled: 165000\n",
      "    num_steps_trained: 165000\n",
      "  iterations_since_restore: 11\n",
      "  node_ip: 192.168.1.76\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 27.238082191780823\n",
      "    ram_util_percent: 46.21287671232876\n",
      "  pid: 29799\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09105058647175927\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 40.66610887773361\n",
      "    mean_inference_ms: 1.8408277365652943\n",
      "    mean_raw_obs_processing_ms: 34.97896406086067\n",
      "  time_since_restore: 2913.960571527481\n",
      "  time_this_iter_s: 256.01986289024353\n",
      "  time_total_s: 2913.960571527481\n",
      "  timers:\n",
      "    learn_throughput: 298.99\n",
      "    learn_time_ms: 50168.845\n",
      "    sample_throughput: 73.022\n",
      "    sample_time_ms: 205418.646\n",
      "    update_time_ms: 3.126\n",
      "  timestamp: 1664474281\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 165000\n",
      "  training_iteration: 11\n",
      "  trial_id: '76534_00000'\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 14.5/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/4.72 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_1_7b3716726cd22aa6f9a7a4f69638d7d8, 0.0/1.0 CPU_group_2_7b3716726cd22aa6f9a7a4f69638d7d8, 0.0/1.0 CPU_group_4_7b3716726cd22aa6f9a7a4f69638d7d8, 0.0/1.0 CPU_group_5_7b3716726cd22aa6f9a7a4f69638d7d8, 0.0/1.0 CPU_group_3_7b3716726cd22aa6f9a7a4f69638d7d8, 0.0/1.0 CPU_group_0_7b3716726cd22aa6f9a7a4f69638d7d8, 0.0/6.0 CPU_group_7b3716726cd22aa6f9a7a4f69638d7d8)\n",
      "Result logdir: /home/michael/ray_results/singleagent_onelane_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_76534_00000 | RUNNING  | 192.168.1.76:29799 |     11 |          2913.96 | 165000 | -3291.06 |             -437.415 |             -5111.39 |               3000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=29799)\u001b[0m 2022-09-29 12:58:01,992\tWARNING ppo.py:242 -- The magnitude of your environment rewards are more than 329.0x the scale of `vf_clip_param`. This means that it will take more than 329.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=29795)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=29795)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29795)\u001b[0m ring length: 249\n",
      "\u001b[2m\u001b[36m(pid=29795)\u001b[0m v_max: 4.617190445131122\n",
      "\u001b[2m\u001b[36m(pid=29795)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29796)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=29796)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29796)\u001b[0m ring length: 226\n",
      "\u001b[2m\u001b[36m(pid=29796)\u001b[0m v_max: 3.5232840694769565\n",
      "\u001b[2m\u001b[36m(pid=29796)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29797)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=29797)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29797)\u001b[0m ring length: 230\n",
      "\u001b[2m\u001b[36m(pid=29797)\u001b[0m v_max: 3.7136148111012934\n",
      "\u001b[2m\u001b[36m(pid=29797)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29798)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=29798)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29798)\u001b[0m ring length: 255\n",
      "\u001b[2m\u001b[36m(pid=29798)\u001b[0m v_max: 4.902299776966978\n",
      "\u001b[2m\u001b[36m(pid=29798)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29794)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=29794)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29794)\u001b[0m ring length: 269\n",
      "\u001b[2m\u001b[36m(pid=29794)\u001b[0m v_max: 5.566938458220921\n",
      "\u001b[2m\u001b[36m(pid=29794)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29795)\u001b[0m ./michael_files/emission_collection/singleagent_onelane_ring_20220929-1255261664474126.4450438-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=29796)\u001b[0m ./michael_files/emission_collection/singleagent_onelane_ring_20220929-1255261664474126.4475813-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=29797)\u001b[0m ./michael_files/emission_collection/singleagent_onelane_ring_20220929-1255261664474126.4491196-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=29798)\u001b[0m ./michael_files/emission_collection/singleagent_onelane_ring_20220929-1255261664474126.4473524-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=29794)\u001b[0m ./michael_files/emission_collection/singleagent_onelane_ring_20220929-1255261664474126.4481602-0_emission.csv ./michael_files/emission_collection/\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_WaveAttenuationPOEnv-v0_76534_00000:\n",
      "  agent_timesteps_total: 180000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-09-29_13-02-17\n",
      "  done: false\n",
      "  episode_len_mean: 3000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.46616539439007\n",
      "  episode_reward_mean: -3038.6550772261267\n",
      "  episode_reward_min: -5111.392282507102\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 60\n",
      "  experiment_id: 939d1d5e80b64de89564600c497902cc\n",
      "  hostname: michael-home\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.024999999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.2479903430630595\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015650439574949566\n",
      "          policy_loss: -0.005045164960941647\n",
      "          total_loss: 99.7701988107067\n",
      "          vf_explained_var: -2.222546147123694e-09\n",
      "          vf_loss: 99.77485307273218\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 180000\n",
      "    num_steps_sampled: 180000\n",
      "    num_steps_trained: 180000\n",
      "  iterations_since_restore: 12\n",
      "  node_ip: 192.168.1.76\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 27.237534246575343\n",
      "    ram_util_percent: 46.35013698630138\n",
      "  pid: 29799\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09105260298475155\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 40.122066440810485\n",
      "    mean_inference_ms: 1.8414192357533998\n",
      "    mean_raw_obs_processing_ms: 34.98154441092975\n",
      "  time_since_restore: 3169.310095310211\n",
      "  time_this_iter_s: 255.3495237827301\n",
      "  time_total_s: 3169.310095310211\n",
      "  timers:\n",
      "    learn_throughput: 298.864\n",
      "    learn_time_ms: 50190.037\n",
      "    sample_throughput: 73.016\n",
      "    sample_time_ms: 205434.061\n",
      "    update_time_ms: 3.131\n",
      "  timestamp: 1664474537\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 180000\n",
      "  training_iteration: 12\n",
      "  trial_id: '76534_00000'\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 14.5/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/4.72 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_4_7b3716726cd22aa6f9a7a4f69638d7d8, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_5_7b3716726cd22aa6f9a7a4f69638d7d8, 0.0/1.0 CPU_group_1_7b3716726cd22aa6f9a7a4f69638d7d8, 0.0/1.0 CPU_group_3_7b3716726cd22aa6f9a7a4f69638d7d8, 0.0/6.0 CPU_group_7b3716726cd22aa6f9a7a4f69638d7d8, 0.0/1.0 CPU_group_2_7b3716726cd22aa6f9a7a4f69638d7d8, 0.0/1.0 CPU_group_0_7b3716726cd22aa6f9a7a4f69638d7d8)\n",
      "Result logdir: /home/michael/ray_results/singleagent_onelane_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_76534_00000 | RUNNING  | 192.168.1.76:29799 |     12 |          3169.31 | 180000 | -3038.66 |              10.4662 |             -5111.39 |               3000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=29799)\u001b[0m 2022-09-29 13:02:17,383\tWARNING ppo.py:242 -- The magnitude of your environment rewards are more than 304.0x the scale of `vf_clip_param`. This means that it will take more than 304.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=29795)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=29795)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29795)\u001b[0m ring length: 229\n",
      "\u001b[2m\u001b[36m(pid=29795)\u001b[0m v_max: 3.666034803266416\n",
      "\u001b[2m\u001b[36m(pid=29795)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29796)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=29796)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29796)\u001b[0m ring length: 250\n",
      "\u001b[2m\u001b[36m(pid=29796)\u001b[0m v_max: 4.664717904914125\n",
      "\u001b[2m\u001b[36m(pid=29796)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29797)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=29797)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29797)\u001b[0m ring length: 247\n",
      "\u001b[2m\u001b[36m(pid=29797)\u001b[0m v_max: 4.522125250095652\n",
      "\u001b[2m\u001b[36m(pid=29797)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29798)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=29798)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29798)\u001b[0m ring length: 232\n",
      "\u001b[2m\u001b[36m(pid=29798)\u001b[0m v_max: 3.8087690783250063\n",
      "\u001b[2m\u001b[36m(pid=29798)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29794)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=29794)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29794)\u001b[0m ring length: 265\n",
      "\u001b[2m\u001b[36m(pid=29794)\u001b[0m v_max: 5.37714246265477\n",
      "\u001b[2m\u001b[36m(pid=29794)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29795)\u001b[0m ./michael_files/emission_collection/singleagent_onelane_ring_20220929-1259421664474382.6530802-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=29796)\u001b[0m ./michael_files/emission_collection/singleagent_onelane_ring_20220929-1259421664474382.6528249-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=29797)\u001b[0m ./michael_files/emission_collection/singleagent_onelane_ring_20220929-1259421664474382.6529276-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=29798)\u001b[0m ./michael_files/emission_collection/singleagent_onelane_ring_20220929-1259421664474382.6536016-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=29794)\u001b[0m ./michael_files/emission_collection/singleagent_onelane_ring_20220929-1259421664474382.6477723-0_emission.csv ./michael_files/emission_collection/\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_76534_00000:\n",
      "  agent_timesteps_total: 195000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-09-29_13-06-32\n",
      "  done: false\n",
      "  episode_len_mean: 3000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 434.8916005238218\n",
      "  episode_reward_mean: -2815.068503005963\n",
      "  episode_reward_min: -5111.392282507102\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 65\n",
      "  experiment_id: 939d1d5e80b64de89564600c497902cc\n",
      "  hostname: michael-home\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.024999999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.07877568762623152\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.02159590687455638\n",
      "          policy_loss: -0.006944038075694846\n",
      "          total_loss: 103.34544127351147\n",
      "          vf_explained_var: -6.061489088438066e-10\n",
      "          vf_loss: 103.35184534040548\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 195000\n",
      "    num_steps_sampled: 195000\n",
      "    num_steps_trained: 195000\n",
      "  iterations_since_restore: 13\n",
      "  node_ip: 192.168.1.76\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 27.241322314049587\n",
      "    ram_util_percent: 46.34848484848485\n",
      "  pid: 29799\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09105183908592959\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 39.644817727004586\n",
      "    mean_inference_ms: 1.8419523054994753\n",
      "    mean_raw_obs_processing_ms: 34.98296832174584\n",
      "  time_since_restore: 3424.118988275528\n",
      "  time_this_iter_s: 254.80889296531677\n",
      "  time_total_s: 3424.118988275528\n",
      "  timers:\n",
      "    learn_throughput: 299.124\n",
      "    learn_time_ms: 50146.412\n",
      "    sample_throughput: 73.015\n",
      "    sample_time_ms: 205437.253\n",
      "    update_time_ms: 3.149\n",
      "  timestamp: 1664474792\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 195000\n",
      "  training_iteration: 13\n",
      "  trial_id: '76534_00000'\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 14.5/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/4.72 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_5_7b3716726cd22aa6f9a7a4f69638d7d8, 0.0/1.0 CPU_group_4_7b3716726cd22aa6f9a7a4f69638d7d8, 0.0/1.0 CPU_group_0_7b3716726cd22aa6f9a7a4f69638d7d8, 0.0/1.0 CPU_group_3_7b3716726cd22aa6f9a7a4f69638d7d8, 0.0/1.0 CPU_group_1_7b3716726cd22aa6f9a7a4f69638d7d8, 0.0/1.0 CPU_group_2_7b3716726cd22aa6f9a7a4f69638d7d8, 0.0/6.0 CPU_group_7b3716726cd22aa6f9a7a4f69638d7d8)\n",
      "Result logdir: /home/michael/ray_results/singleagent_onelane_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_76534_00000 | RUNNING  | 192.168.1.76:29799 |     13 |          3424.12 | 195000 | -2815.07 |              434.892 |             -5111.39 |               3000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=29799)\u001b[0m 2022-09-29 13:06:32,234\tWARNING ppo.py:242 -- The magnitude of your environment rewards are more than 282.0x the scale of `vf_clip_param`. This means that it will take more than 282.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=29795)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=29795)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29795)\u001b[0m ring length: 221\n",
      "\u001b[2m\u001b[36m(pid=29795)\u001b[0m v_max: 3.285334164169417\n",
      "\u001b[2m\u001b[36m(pid=29795)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29796)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=29796)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29796)\u001b[0m ring length: 246\n",
      "\u001b[2m\u001b[36m(pid=29796)\u001b[0m v_max: 4.474587708995648\n",
      "\u001b[2m\u001b[36m(pid=29796)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29797)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=29797)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29797)\u001b[0m ring length: 246\n",
      "\u001b[2m\u001b[36m(pid=29797)\u001b[0m v_max: 4.474587708995648\n",
      "\u001b[2m\u001b[36m(pid=29797)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29798)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=29798)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29798)\u001b[0m ring length: 241\n",
      "\u001b[2m\u001b[36m(pid=29798)\u001b[0m v_max: 4.236854288051661\n",
      "\u001b[2m\u001b[36m(pid=29798)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29794)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=29794)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29794)\u001b[0m ring length: 263\n",
      "\u001b[2m\u001b[36m(pid=29794)\u001b[0m v_max: 5.282212215151455\n",
      "\u001b[2m\u001b[36m(pid=29794)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29795)\u001b[0m ./michael_files/emission_collection/singleagent_onelane_ring_20220929-1303571664474637.8892522-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=29796)\u001b[0m ./michael_files/emission_collection/singleagent_onelane_ring_20220929-1303571664474637.890812-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=29797)\u001b[0m ./michael_files/emission_collection/singleagent_onelane_ring_20220929-1303571664474637.893956-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=29798)\u001b[0m ./michael_files/emission_collection/singleagent_onelane_ring_20220929-1303571664474637.8940597-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=29794)\u001b[0m ./michael_files/emission_collection/singleagent_onelane_ring_20220929-1303571664474637.892037-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=29795)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=29798)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=29794)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=29796)\u001b[0m  Retrying in 1 seconds\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_76534_00000:\n",
      "  agent_timesteps_total: 210000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-09-29_13-10-48\n",
      "  done: false\n",
      "  episode_len_mean: 3000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 546.2845302483471\n",
      "  episode_reward_mean: -2611.918764392536\n",
      "  episode_reward_min: -5111.392282507102\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 70\n",
      "  experiment_id: 939d1d5e80b64de89564600c497902cc\n",
      "  hostname: michael-home\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.024999999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.006422302135165487\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013065869511877036\n",
      "          policy_loss: -0.003959095446092976\n",
      "          total_loss: 90.38906519776684\n",
      "          vf_explained_var: -3.687405936148025e-09\n",
      "          vf_loss: 90.39269761392626\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 210000\n",
      "    num_steps_sampled: 210000\n",
      "    num_steps_trained: 210000\n",
      "  iterations_since_restore: 14\n",
      "  node_ip: 192.168.1.76\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 27.06986301369863\n",
      "    ram_util_percent: 46.413698630137\n",
      "  pid: 29799\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09104871344400615\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 39.22236579664174\n",
      "    mean_inference_ms: 1.842318430329338\n",
      "    mean_raw_obs_processing_ms: 34.98537892470726\n",
      "  time_since_restore: 3679.971312046051\n",
      "  time_this_iter_s: 255.85232377052307\n",
      "  time_total_s: 3679.971312046051\n",
      "  timers:\n",
      "    learn_throughput: 299.262\n",
      "    learn_time_ms: 50123.366\n",
      "    sample_throughput: 73.015\n",
      "    sample_time_ms: 205438.052\n",
      "    update_time_ms: 3.141\n",
      "  timestamp: 1664475048\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 210000\n",
      "  training_iteration: 14\n",
      "  trial_id: '76534_00000'\n",
      "  \n",
      "\u001b[2m\u001b[36m(pid=29799)\u001b[0m 2022-09-29 13:10:48,134\tWARNING ppo.py:242 -- The magnitude of your environment rewards are more than 261.0x the scale of `vf_clip_param`. This means that it will take more than 261.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "== Status ==\n",
      "Memory usage on this node: 14.5/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/4.72 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_4_7b3716726cd22aa6f9a7a4f69638d7d8, 0.0/1.0 CPU_group_2_7b3716726cd22aa6f9a7a4f69638d7d8, 0.0/6.0 CPU_group_7b3716726cd22aa6f9a7a4f69638d7d8, 0.0/1.0 CPU_group_3_7b3716726cd22aa6f9a7a4f69638d7d8, 0.0/1.0 CPU_group_1_7b3716726cd22aa6f9a7a4f69638d7d8, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_5_7b3716726cd22aa6f9a7a4f69638d7d8, 0.0/1.0 CPU_group_0_7b3716726cd22aa6f9a7a4f69638d7d8)\n",
      "Result logdir: /home/michael/ray_results/singleagent_onelane_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_76534_00000 | RUNNING  | 192.168.1.76:29799 |     14 |          3679.97 | 210000 | -2611.92 |              546.285 |             -5111.39 |               3000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=29795)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=29795)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29795)\u001b[0m ring length: 246\n",
      "\u001b[2m\u001b[36m(pid=29795)\u001b[0m v_max: 4.474587708995648\n",
      "\u001b[2m\u001b[36m(pid=29795)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29796)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=29796)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29796)\u001b[0m ring length: 236\n",
      "\u001b[2m\u001b[36m(pid=29796)\u001b[0m v_max: 3.999052674951912\n",
      "\u001b[2m\u001b[36m(pid=29796)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29797)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=29797)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29797)\u001b[0m ring length: 234\n",
      "\u001b[2m\u001b[36m(pid=29797)\u001b[0m v_max: 3.903915223349057\n",
      "\u001b[2m\u001b[36m(pid=29797)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29798)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=29798)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29798)\u001b[0m ring length: 248\n",
      "\u001b[2m\u001b[36m(pid=29798)\u001b[0m v_max: 4.569659527528101\n",
      "\u001b[2m\u001b[36m(pid=29798)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29794)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=29794)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29794)\u001b[0m ring length: 252\n",
      "\u001b[2m\u001b[36m(pid=29794)\u001b[0m v_max: 4.759762049824181\n",
      "\u001b[2m\u001b[36m(pid=29794)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29795)\u001b[0m ./michael_files/emission_collection/singleagent_onelane_ring_20220929-1308121664474892.7255545-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=29796)\u001b[0m ./michael_files/emission_collection/singleagent_onelane_ring_20220929-1308121664474892.7253084-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=29797)\u001b[0m ./michael_files/emission_collection/singleagent_onelane_ring_20220929-1308121664474892.7272108-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=29798)\u001b[0m ./michael_files/emission_collection/singleagent_onelane_ring_20220929-1308121664474892.728255-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=29794)\u001b[0m ./michael_files/emission_collection/singleagent_onelane_ring_20220929-1308121664474892.7257423-0_emission.csv ./michael_files/emission_collection/\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_76534_00000:\n",
      "  agent_timesteps_total: 225000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-09-29_13-15-02\n",
      "  done: false\n",
      "  episode_len_mean: 3000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 576.1779594862682\n",
      "  episode_reward_mean: -2423.575176544746\n",
      "  episode_reward_min: -5111.392282507102\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 75\n",
      "  experiment_id: 939d1d5e80b64de89564600c497902cc\n",
      "  hostname: michael-home\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.024999999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -0.10230100585573937\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01535893792318087\n",
      "          policy_loss: -0.004450013508262523\n",
      "          total_loss: 85.88137659784091\n",
      "          vf_explained_var: 0.0281955748796463\n",
      "          vf_loss: 85.88544250261985\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 225000\n",
      "    num_steps_sampled: 225000\n",
      "    num_steps_trained: 225000\n",
      "  iterations_since_restore: 15\n",
      "  node_ip: 192.168.1.76\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 27.325344352617087\n",
      "    ram_util_percent: 46.329752066115695\n",
      "  pid: 29799\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09104881879771531\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 38.84533286470321\n",
      "    mean_inference_ms: 1.8426628963664968\n",
      "    mean_raw_obs_processing_ms: 34.986877265022244\n",
      "  time_since_restore: 3934.394568681717\n",
      "  time_this_iter_s: 254.4232566356659\n",
      "  time_total_s: 3934.394568681717\n",
      "  timers:\n",
      "    learn_throughput: 299.825\n",
      "    learn_time_ms: 50029.132\n",
      "    sample_throughput: 73.066\n",
      "    sample_time_ms: 205295.114\n",
      "    update_time_ms: 3.169\n",
      "  timestamp: 1664475302\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 225000\n",
      "  training_iteration: 15\n",
      "  trial_id: '76534_00000'\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Memory usage on this node: 14.6/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/4.72 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_5_7b3716726cd22aa6f9a7a4f69638d7d8, 0.0/1.0 CPU_group_2_7b3716726cd22aa6f9a7a4f69638d7d8, 0.0/1.0 CPU_group_4_7b3716726cd22aa6f9a7a4f69638d7d8, 0.0/1.0 CPU_group_3_7b3716726cd22aa6f9a7a4f69638d7d8, 0.0/1.0 CPU_group_1_7b3716726cd22aa6f9a7a4f69638d7d8, 0.0/1.0 CPU_group_0_7b3716726cd22aa6f9a7a4f69638d7d8, 0.0/6.0 CPU_group_7b3716726cd22aa6f9a7a4f69638d7d8)\n",
      "Result logdir: /home/michael/ray_results/singleagent_onelane_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_76534_00000 | RUNNING  | 192.168.1.76:29799 |     15 |          3934.39 | 225000 | -2423.58 |              576.178 |             -5111.39 |               3000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=29799)\u001b[0m 2022-09-29 13:15:02,609\tWARNING ppo.py:242 -- The magnitude of your environment rewards are more than 242.0x the scale of `vf_clip_param`. This means that it will take more than 242.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=29795)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=29795)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29795)\u001b[0m ring length: 225\n",
      "\u001b[2m\u001b[36m(pid=29795)\u001b[0m v_max: 3.4756971311168834\n",
      "\u001b[2m\u001b[36m(pid=29795)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29796)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=29796)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29796)\u001b[0m ring length: 262\n",
      "\u001b[2m\u001b[36m(pid=29796)\u001b[0m v_max: 5.234739483008334\n",
      "\u001b[2m\u001b[36m(pid=29796)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29797)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=29797)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29797)\u001b[0m ring length: 236\n",
      "\u001b[2m\u001b[36m(pid=29797)\u001b[0m v_max: 3.999052674951912\n",
      "\u001b[2m\u001b[36m(pid=29797)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29798)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=29798)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29798)\u001b[0m ring length: 223\n",
      "\u001b[2m\u001b[36m(pid=29798)\u001b[0m v_max: 3.3805185755587788\n",
      "\u001b[2m\u001b[36m(pid=29798)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29794)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=29794)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29794)\u001b[0m ring length: 251\n",
      "\u001b[2m\u001b[36m(pid=29794)\u001b[0m v_max: 4.71224180704279\n",
      "\u001b[2m\u001b[36m(pid=29794)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29795)\u001b[0m ./michael_files/emission_collection/singleagent_onelane_ring_20220929-1312281664475148.6481874-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=29796)\u001b[0m ./michael_files/emission_collection/singleagent_onelane_ring_20220929-1312281664475148.6465518-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=29798)\u001b[0m ./michael_files/emission_collection/singleagent_onelane_ring_20220929-1312281664475148.6493073-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=29797)\u001b[0m ./michael_files/emission_collection/singleagent_onelane_ring_20220929-1312281664475148.6431549-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=29794)\u001b[0m ./michael_files/emission_collection/singleagent_onelane_ring_20220929-1312281664475148.6502256-0_emission.csv ./michael_files/emission_collection/\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_76534_00000:\n",
      "  agent_timesteps_total: 240000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-09-29_13-19-17\n",
      "  done: false\n",
      "  episode_len_mean: 3000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 576.1779594862682\n",
      "  episode_reward_mean: -2337.069653689315\n",
      "  episode_reward_min: -5111.392282507102\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 80\n",
      "  experiment_id: 939d1d5e80b64de89564600c497902cc\n",
      "  hostname: michael-home\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.024999999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -0.1313101069938581\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01228539844626125\n",
      "          policy_loss: -0.002879399982265244\n",
      "          total_loss: 79.19305585117664\n",
      "          vf_explained_var: 0.1838216781616211\n",
      "          vf_loss: 79.19562821792344\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 240000\n",
      "    num_steps_sampled: 240000\n",
      "    num_steps_trained: 240000\n",
      "  iterations_since_restore: 16\n",
      "  node_ip: 192.168.1.76\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 27.19945054945055\n",
      "    ram_util_percent: 46.4782967032967\n",
      "  pid: 29799\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09105067200392236\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 38.506491397903396\n",
      "    mean_inference_ms: 1.8429665694284036\n",
      "    mean_raw_obs_processing_ms: 34.9876740366433\n",
      "  time_since_restore: 4189.278388500214\n",
      "  time_this_iter_s: 254.8838198184967\n",
      "  time_total_s: 4189.278388500214\n",
      "  timers:\n",
      "    learn_throughput: 299.71\n",
      "    learn_time_ms: 50048.433\n",
      "    sample_throughput: 73.103\n",
      "    sample_time_ms: 205190.986\n",
      "    update_time_ms: 3.161\n",
      "  timestamp: 1664475557\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 240000\n",
      "  training_iteration: 16\n",
      "  trial_id: '76534_00000'\n",
      "  \n",
      "\u001b[2m\u001b[36m(pid=29799)\u001b[0m 2022-09-29 13:19:17,541\tWARNING ppo.py:242 -- The magnitude of your environment rewards are more than 234.0x the scale of `vf_clip_param`. This means that it will take more than 234.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "== Status ==\n",
      "Memory usage on this node: 14.5/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/4.72 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_1_7b3716726cd22aa6f9a7a4f69638d7d8, 0.0/1.0 CPU_group_5_7b3716726cd22aa6f9a7a4f69638d7d8, 0.0/1.0 CPU_group_0_7b3716726cd22aa6f9a7a4f69638d7d8, 0.0/1.0 CPU_group_4_7b3716726cd22aa6f9a7a4f69638d7d8, 0.0/1.0 CPU_group_2_7b3716726cd22aa6f9a7a4f69638d7d8, 0.0/6.0 CPU_group_7b3716726cd22aa6f9a7a4f69638d7d8, 0.0/1.0 CPU_group_3_7b3716726cd22aa6f9a7a4f69638d7d8, 0.0/1.0 accelerator_type:G)\n",
      "Result logdir: /home/michael/ray_results/singleagent_onelane_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_76534_00000 | RUNNING  | 192.168.1.76:29799 |     16 |          4189.28 | 240000 | -2337.07 |              576.178 |             -5111.39 |               3000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=29795)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=29795)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29795)\u001b[0m ring length: 259\n",
      "\u001b[2m\u001b[36m(pid=29795)\u001b[0m v_max: 5.092292348291853\n",
      "\u001b[2m\u001b[36m(pid=29795)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29796)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=29796)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29796)\u001b[0m ring length: 233\n",
      "\u001b[2m\u001b[36m(pid=29796)\u001b[0m v_max: 3.856343201216534\n",
      "\u001b[2m\u001b[36m(pid=29796)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29797)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=29797)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29797)\u001b[0m ring length: 245\n",
      "\u001b[2m\u001b[36m(pid=29797)\u001b[0m v_max: 4.427046998576354\n",
      "\u001b[2m\u001b[36m(pid=29797)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29798)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=29798)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29798)\u001b[0m ring length: 226\n",
      "\u001b[2m\u001b[36m(pid=29798)\u001b[0m v_max: 3.5232840694769565\n",
      "\u001b[2m\u001b[36m(pid=29798)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29794)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=29794)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29794)\u001b[0m ring length: 237\n",
      "\u001b[2m\u001b[36m(pid=29794)\u001b[0m v_max: 4.04661795519353\n",
      "\u001b[2m\u001b[36m(pid=29794)\u001b[0m -----------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=29795)\u001b[0m ./michael_files/emission_collection/singleagent_onelane_ring_20220929-1316431664475403.114435-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=29798)\u001b[0m ./michael_files/emission_collection/singleagent_onelane_ring_20220929-1316431664475403.1138704-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=29796)\u001b[0m ./michael_files/emission_collection/singleagent_onelane_ring_20220929-1316431664475403.1199822-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=29797)\u001b[0m ./michael_files/emission_collection/singleagent_onelane_ring_20220929-1316431664475403.1207669-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=29794)\u001b[0m ./michael_files/emission_collection/singleagent_onelane_ring_20220929-1316431664475403.1211004-0_emission.csv ./michael_files/emission_collection/\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_76534_00000:\n",
      "  agent_timesteps_total: 255000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-09-29_13-23-32\n",
      "  done: false\n",
      "  episode_len_mean: 3000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 576.1779594862682\n",
      "  episode_reward_mean: -2198.6251733464574\n",
      "  episode_reward_min: -5111.392282507102\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 85\n",
      "  experiment_id: 939d1d5e80b64de89564600c497902cc\n",
      "  hostname: michael-home\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.024999999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -0.19511137335103448\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009094168615842684\n",
      "          policy_loss: -0.0012958445562586442\n",
      "          total_loss: 99.6547185962483\n",
      "          vf_explained_var: 0.26788634061813354\n",
      "          vf_loss: 99.65578694101107\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 255000\n",
      "    num_steps_sampled: 255000\n",
      "    num_steps_trained: 255000\n",
      "  iterations_since_restore: 17\n",
      "  node_ip: 192.168.1.76\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 27.247252747252748\n",
      "    ram_util_percent: 46.364010989010985\n",
      "  pid: 29799\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09105459910126548\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 38.200305067933826\n",
      "    mean_inference_ms: 1.8431855662341932\n",
      "    mean_raw_obs_processing_ms: 34.98807407987632\n",
      "  time_since_restore: 4444.0207579135895\n",
      "  time_this_iter_s: 254.74236941337585\n",
      "  time_total_s: 4444.0207579135895\n",
      "  timers:\n",
      "    learn_throughput: 300.012\n",
      "    learn_time_ms: 49998.052\n",
      "    sample_throughput: 73.097\n",
      "    sample_time_ms: 205207.628\n",
      "    update_time_ms: 3.151\n",
      "  timestamp: 1664475812\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 255000\n",
      "  training_iteration: 17\n",
      "  trial_id: '76534_00000'\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 14.6/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/4.72 GiB heap, 0.0/14.65 GiB objects (0.0/6.0 CPU_group_7b3716726cd22aa6f9a7a4f69638d7d8, 0.0/1.0 CPU_group_5_7b3716726cd22aa6f9a7a4f69638d7d8, 0.0/1.0 CPU_group_3_7b3716726cd22aa6f9a7a4f69638d7d8, 0.0/1.0 CPU_group_0_7b3716726cd22aa6f9a7a4f69638d7d8, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_4_7b3716726cd22aa6f9a7a4f69638d7d8, 0.0/1.0 CPU_group_1_7b3716726cd22aa6f9a7a4f69638d7d8, 0.0/1.0 CPU_group_2_7b3716726cd22aa6f9a7a4f69638d7d8)\n",
      "Result logdir: /home/michael/ray_results/singleagent_onelane_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_76534_00000 | RUNNING  | 192.168.1.76:29799 |     17 |          4444.02 | 255000 | -2198.63 |              576.178 |             -5111.39 |               3000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=29799)\u001b[0m 2022-09-29 13:23:32,340\tWARNING ppo.py:242 -- The magnitude of your environment rewards are more than 220.0x the scale of `vf_clip_param`. This means that it will take more than 220.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=29795)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=29795)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29795)\u001b[0m ring length: 230\n",
      "\u001b[2m\u001b[36m(pid=29795)\u001b[0m v_max: 3.7136148111012934\n",
      "\u001b[2m\u001b[36m(pid=29795)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29796)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=29796)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29796)\u001b[0m ring length: 268\n",
      "\u001b[2m\u001b[36m(pid=29796)\u001b[0m v_max: 5.5194978538368025\n",
      "\u001b[2m\u001b[36m(pid=29796)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29797)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=29797)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29797)\u001b[0m ring length: 268\n",
      "\u001b[2m\u001b[36m(pid=29797)\u001b[0m v_max: 5.5194978538368025\n",
      "\u001b[2m\u001b[36m(pid=29797)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29798)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=29798)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29798)\u001b[0m ring length: 262\n",
      "\u001b[2m\u001b[36m(pid=29798)\u001b[0m v_max: 5.234739483008334\n",
      "\u001b[2m\u001b[36m(pid=29798)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29794)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=29794)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29794)\u001b[0m ring length: 226\n",
      "\u001b[2m\u001b[36m(pid=29794)\u001b[0m v_max: 3.5232840694769565\n",
      "\u001b[2m\u001b[36m(pid=29794)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29796)\u001b[0m ./michael_files/emission_collection/singleagent_onelane_ring_20220929-1320581664475658.2012575-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=29798)\u001b[0m ./michael_files/emission_collection/singleagent_onelane_ring_20220929-1320581664475658.1897287-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=29795)\u001b[0m ./michael_files/emission_collection/singleagent_onelane_ring_20220929-1320581664475658.1967895-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=29797)\u001b[0m ./michael_files/emission_collection/singleagent_onelane_ring_20220929-1320581664475658.2021506-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=29794)\u001b[0m ./michael_files/emission_collection/singleagent_onelane_ring_20220929-1320581664475658.1985798-0_emission.csv ./michael_files/emission_collection/\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_76534_00000:\n",
      "  agent_timesteps_total: 270000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-09-29_13-27-47\n",
      "  done: false\n",
      "  episode_len_mean: 3000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 657.078816305578\n",
      "  episode_reward_mean: -2063.8167915924864\n",
      "  episode_reward_min: -5111.392282507102\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 90\n",
      "  experiment_id: 939d1d5e80b64de89564600c497902cc\n",
      "  hostname: michael-home\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.012499999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -0.24469915084162\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010218319238388738\n",
      "          policy_loss: -0.0013297840526674764\n",
      "          total_loss: 80.5150050729008\n",
      "          vf_explained_var: 0.41782426834106445\n",
      "          vf_loss: 80.51620734424914\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 270000\n",
      "    num_steps_sampled: 270000\n",
      "    num_steps_trained: 270000\n",
      "  iterations_since_restore: 18\n",
      "  node_ip: 192.168.1.76\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 27.23250688705234\n",
      "    ram_util_percent: 46.518457300275486\n",
      "  pid: 29799\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09105887690651\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 37.92193027397444\n",
      "    mean_inference_ms: 1.843347409050347\n",
      "    mean_raw_obs_processing_ms: 34.98816250613045\n",
      "  time_since_restore: 4698.966933012009\n",
      "  time_this_iter_s: 254.9461750984192\n",
      "  time_total_s: 4698.966933012009\n",
      "  timers:\n",
      "    learn_throughput: 299.962\n",
      "    learn_time_ms: 50006.266\n",
      "    sample_throughput: 73.13\n",
      "    sample_time_ms: 205113.802\n",
      "    update_time_ms: 3.151\n",
      "  timestamp: 1664476067\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 270000\n",
      "  training_iteration: 18\n",
      "  trial_id: '76534_00000'\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=29799)\u001b[0m 2022-09-29 13:27:47,336\tWARNING ppo.py:242 -- The magnitude of your environment rewards are more than 206.0x the scale of `vf_clip_param`. This means that it will take more than 206.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "== Status ==\n",
      "Memory usage on this node: 14.5/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/4.72 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_2_7b3716726cd22aa6f9a7a4f69638d7d8, 0.0/1.0 CPU_group_3_7b3716726cd22aa6f9a7a4f69638d7d8, 0.0/1.0 CPU_group_4_7b3716726cd22aa6f9a7a4f69638d7d8, 0.0/1.0 CPU_group_0_7b3716726cd22aa6f9a7a4f69638d7d8, 0.0/1.0 CPU_group_5_7b3716726cd22aa6f9a7a4f69638d7d8, 0.0/6.0 CPU_group_7b3716726cd22aa6f9a7a4f69638d7d8, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_1_7b3716726cd22aa6f9a7a4f69638d7d8)\n",
      "Result logdir: /home/michael/ray_results/singleagent_onelane_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_76534_00000 | RUNNING  | 192.168.1.76:29799 |     18 |          4698.97 | 270000 | -2063.82 |              657.079 |             -5111.39 |               3000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=29795)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=29795)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29795)\u001b[0m ring length: 239\n",
      "\u001b[2m\u001b[36m(pid=29795)\u001b[0m v_max: 4.141741239205832\n",
      "\u001b[2m\u001b[36m(pid=29795)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29796)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=29796)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29796)\u001b[0m ring length: 270\n",
      "\u001b[2m\u001b[36m(pid=29796)\u001b[0m v_max: 5.6143732387852054\n",
      "\u001b[2m\u001b[36m(pid=29796)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29797)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=29797)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29797)\u001b[0m ring length: 224\n",
      "\u001b[2m\u001b[36m(pid=29797)\u001b[0m v_max: 3.4281086136538996\n",
      "\u001b[2m\u001b[36m(pid=29797)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29798)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=29798)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29798)\u001b[0m ring length: 257\n",
      "\u001b[2m\u001b[36m(pid=29798)\u001b[0m v_max: 4.997304682316517\n",
      "\u001b[2m\u001b[36m(pid=29798)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29794)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=29794)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29794)\u001b[0m ring length: 228\n",
      "\u001b[2m\u001b[36m(pid=29794)\u001b[0m v_max: 3.618452967700356\n",
      "\u001b[2m\u001b[36m(pid=29794)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29795)\u001b[0m ./michael_files/emission_collection/singleagent_onelane_ring_20220929-1325121664475912.849293-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=29798)\u001b[0m ./michael_files/emission_collection/singleagent_onelane_ring_20220929-1325121664475912.8450813-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=29796)\u001b[0m ./michael_files/emission_collection/singleagent_onelane_ring_20220929-1325121664475912.8478916-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=29797)\u001b[0m ./michael_files/emission_collection/singleagent_onelane_ring_20220929-1325121664475912.8493037-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=29794)\u001b[0m ./michael_files/emission_collection/singleagent_onelane_ring_20220929-1325121664475912.8478193-0_emission.csv ./michael_files/emission_collection/\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_76534_00000:\n",
      "  agent_timesteps_total: 285000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-09-29_13-32-02\n",
      "  done: false\n",
      "  episode_len_mean: 3000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 657.078816305578\n",
      "  episode_reward_mean: -1960.9061607901867\n",
      "  episode_reward_min: -5111.392282507102\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 95\n",
      "  experiment_id: 939d1d5e80b64de89564600c497902cc\n",
      "  hostname: michael-home\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.012499999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -0.3247235548193172\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010731900761336108\n",
      "          policy_loss: -0.001162456618069466\n",
      "          total_loss: 68.55224046383874\n",
      "          vf_explained_var: 0.4986482262611389\n",
      "          vf_loss: 68.55326867669315\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 285000\n",
      "    num_steps_sampled: 285000\n",
      "    num_steps_trained: 285000\n",
      "  iterations_since_restore: 19\n",
      "  node_ip: 192.168.1.76\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 27.284615384615382\n",
      "    ram_util_percent: 46.43049450549451\n",
      "  pid: 29799\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09106243212254003\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 37.66770687600681\n",
      "    mean_inference_ms: 1.8434929043094703\n",
      "    mean_raw_obs_processing_ms: 34.98795772360466\n",
      "  time_since_restore: 4953.924997806549\n",
      "  time_this_iter_s: 254.9580647945404\n",
      "  time_total_s: 4953.924997806549\n",
      "  timers:\n",
      "    learn_throughput: 300.122\n",
      "    learn_time_ms: 49979.713\n",
      "    sample_throughput: 73.129\n",
      "    sample_time_ms: 205117.711\n",
      "    update_time_ms: 3.155\n",
      "  timestamp: 1664476322\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 285000\n",
      "  training_iteration: 19\n",
      "  trial_id: '76534_00000'\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 14.6/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/4.72 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_2_7b3716726cd22aa6f9a7a4f69638d7d8, 0.0/6.0 CPU_group_7b3716726cd22aa6f9a7a4f69638d7d8, 0.0/1.0 CPU_group_1_7b3716726cd22aa6f9a7a4f69638d7d8, 0.0/1.0 CPU_group_4_7b3716726cd22aa6f9a7a4f69638d7d8, 0.0/1.0 CPU_group_3_7b3716726cd22aa6f9a7a4f69638d7d8, 0.0/1.0 CPU_group_0_7b3716726cd22aa6f9a7a4f69638d7d8, 0.0/1.0 CPU_group_5_7b3716726cd22aa6f9a7a4f69638d7d8, 0.0/1.0 accelerator_type:G)\n",
      "Result logdir: /home/michael/ray_results/singleagent_onelane_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_76534_00000 | RUNNING  | 192.168.1.76:29799 |     19 |          4953.92 | 285000 | -1960.91 |              657.079 |             -5111.39 |               3000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=29795)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=29795)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29795)\u001b[0m ring length: 266\n",
      "\u001b[2m\u001b[36m(pid=29795)\u001b[0m v_max: 5.42459972166245\n",
      "\u001b[2m\u001b[36m(pid=29795)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29796)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=29796)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29796)\u001b[0m ring length: 237\n",
      "\u001b[2m\u001b[36m(pid=29796)\u001b[0m v_max: 4.04661795519353\n",
      "\u001b[2m\u001b[36m(pid=29796)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29797)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=29797)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29797)\u001b[0m ring length: 249\n",
      "\u001b[2m\u001b[36m(pid=29797)\u001b[0m v_max: 4.617190445131122\n",
      "\u001b[2m\u001b[36m(pid=29797)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29798)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=29798)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29798)\u001b[0m ring length: 244\n",
      "\u001b[2m\u001b[36m(pid=29798)\u001b[0m v_max: 4.379503211387676\n",
      "\u001b[2m\u001b[36m(pid=29798)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29794)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=29794)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29794)\u001b[0m ring length: 235\n",
      "\u001b[2m\u001b[36m(pid=29794)\u001b[0m v_max: 3.9514850725282584\n",
      "\u001b[2m\u001b[36m(pid=29794)\u001b[0m -----------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=29795)\u001b[0m ./michael_files/emission_collection/singleagent_onelane_ring_20220929-1329271664476167.9805293-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=29796)\u001b[0m ./michael_files/emission_collection/singleagent_onelane_ring_20220929-1329271664476167.9969757-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=29797)\u001b[0m ./michael_files/emission_collection/singleagent_onelane_ring_20220929-1329271664476167.9813814-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=29798)\u001b[0m ./michael_files/emission_collection/singleagent_onelane_ring_20220929-1329271664476167.9815662-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=29794)\u001b[0m ./michael_files/emission_collection/singleagent_onelane_ring_20220929-1329271664476167.9969928-0_emission.csv ./michael_files/emission_collection/\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_76534_00000:\n",
      "  agent_timesteps_total: 300000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-09-29_13-36-17\n",
      "  done: false\n",
      "  episode_len_mean: 3000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 657.078816305578\n",
      "  episode_reward_mean: -1868.8156920896567\n",
      "  episode_reward_min: -5111.392282507102\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 100\n",
      "  experiment_id: 939d1d5e80b64de89564600c497902cc\n",
      "  hostname: michael-home\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.012499999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -0.38829715057449826\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.007883981361488119\n",
      "          policy_loss: 0.0005610384264076917\n",
      "          total_loss: 65.33775833905753\n",
      "          vf_explained_var: 0.6387473940849304\n",
      "          vf_loss: 65.33709854837191\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 300000\n",
      "    num_steps_sampled: 300000\n",
      "    num_steps_trained: 300000\n",
      "  iterations_since_restore: 20\n",
      "  node_ip: 192.168.1.76\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 27.228021978021978\n",
      "    ram_util_percent: 46.65521978021978\n",
      "  pid: 29799\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09106759802234361\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 37.434378695519065\n",
      "    mean_inference_ms: 1.8436293070827383\n",
      "    mean_raw_obs_processing_ms: 34.98757876930163\n",
      "  time_since_restore: 5208.937789678574\n",
      "  time_this_iter_s: 255.01279187202454\n",
      "  time_total_s: 5208.937789678574\n",
      "  timers:\n",
      "    learn_throughput: 300.221\n",
      "    learn_time_ms: 49963.233\n",
      "    sample_throughput: 73.127\n",
      "    sample_time_ms: 205121.809\n",
      "    update_time_ms: 3.144\n",
      "  timestamp: 1664476577\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 300000\n",
      "  training_iteration: 20\n",
      "  trial_id: '76534_00000'\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 14.6/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/4.72 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_2_7b3716726cd22aa6f9a7a4f69638d7d8, 0.0/1.0 CPU_group_4_7b3716726cd22aa6f9a7a4f69638d7d8, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_5_7b3716726cd22aa6f9a7a4f69638d7d8, 0.0/6.0 CPU_group_7b3716726cd22aa6f9a7a4f69638d7d8, 0.0/1.0 CPU_group_3_7b3716726cd22aa6f9a7a4f69638d7d8, 0.0/1.0 CPU_group_0_7b3716726cd22aa6f9a7a4f69638d7d8, 0.0/1.0 CPU_group_1_7b3716726cd22aa6f9a7a4f69638d7d8)\n",
      "Result logdir: /home/michael/ray_results/singleagent_onelane_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_76534_00000 | RUNNING  | 192.168.1.76:29799 |     20 |          5208.94 | 300000 | -1868.82 |              657.079 |             -5111.39 |               3000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=29795)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=29795)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29795)\u001b[0m ring length: 224\n",
      "\u001b[2m\u001b[36m(pid=29795)\u001b[0m v_max: 3.4281086136538996\n",
      "\u001b[2m\u001b[36m(pid=29795)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29796)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=29796)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29796)\u001b[0m ring length: 237\n",
      "\u001b[2m\u001b[36m(pid=29796)\u001b[0m v_max: 4.04661795519353\n",
      "\u001b[2m\u001b[36m(pid=29796)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29797)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=29797)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29797)\u001b[0m ring length: 246\n",
      "\u001b[2m\u001b[36m(pid=29797)\u001b[0m v_max: 4.474587708995648\n",
      "\u001b[2m\u001b[36m(pid=29797)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29798)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=29798)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29798)\u001b[0m ring length: 247\n",
      "\u001b[2m\u001b[36m(pid=29798)\u001b[0m v_max: 4.522125250095652\n",
      "\u001b[2m\u001b[36m(pid=29798)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29794)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=29794)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29794)\u001b[0m ring length: 267\n",
      "\u001b[2m\u001b[36m(pid=29794)\u001b[0m v_max: 5.472051563171826\n",
      "\u001b[2m\u001b[36m(pid=29794)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29795)\u001b[0m ./michael_files/emission_collection/singleagent_onelane_ring_20220929-1333421664476422.8576272-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=29798)\u001b[0m ./michael_files/emission_collection/singleagent_onelane_ring_20220929-1333421664476422.8579974-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=29796)\u001b[0m ./michael_files/emission_collection/singleagent_onelane_ring_20220929-1333421664476422.8583093-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=29797)\u001b[0m ./michael_files/emission_collection/singleagent_onelane_ring_20220929-1333421664476422.85256-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=29794)\u001b[0m ./michael_files/emission_collection/singleagent_onelane_ring_20220929-1333421664476422.852559-0_emission.csv ./michael_files/emission_collection/\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_76534_00000:\n",
      "  agent_timesteps_total: 315000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-09-29_13-40-32\n",
      "  done: false\n",
      "  episode_len_mean: 3000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1000.629739874041\n",
      "  episode_reward_mean: -1598.9528941982976\n",
      "  episode_reward_min: -5111.392282507102\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 105\n",
      "  experiment_id: 939d1d5e80b64de89564600c497902cc\n",
      "  hostname: michael-home\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.0062499999999999995\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -0.5319372710535082\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014474770578336765\n",
      "          policy_loss: -0.0017644718403817487\n",
      "          total_loss: 92.31377139010672\n",
      "          vf_explained_var: 0.4709567725658417\n",
      "          vf_loss: 92.31544528896526\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 315000\n",
      "    num_steps_sampled: 315000\n",
      "    num_steps_trained: 315000\n",
      "  iterations_since_restore: 21\n",
      "  node_ip: 192.168.1.76\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 27.376923076923077\n",
      "    ram_util_percent: 46.591483516483514\n",
      "  pid: 29799\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0911147485403475\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 35.80787417999574\n",
      "    mean_inference_ms: 1.8454517958826029\n",
      "    mean_raw_obs_processing_ms: 34.99289167294798\n",
      "  time_since_restore: 5464.024999380112\n",
      "  time_this_iter_s: 255.0872097015381\n",
      "  time_total_s: 5464.024999380112\n",
      "  timers:\n",
      "    learn_throughput: 300.167\n",
      "    learn_time_ms: 49972.124\n",
      "    sample_throughput: 73.164\n",
      "    sample_time_ms: 205019.677\n",
      "    update_time_ms: 3.121\n",
      "  timestamp: 1664476832\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 315000\n",
      "  training_iteration: 21\n",
      "  trial_id: '76534_00000'\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Memory usage on this node: 14.6/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/4.72 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_5_7b3716726cd22aa6f9a7a4f69638d7d8, 0.0/1.0 CPU_group_3_7b3716726cd22aa6f9a7a4f69638d7d8, 0.0/6.0 CPU_group_7b3716726cd22aa6f9a7a4f69638d7d8, 0.0/1.0 CPU_group_2_7b3716726cd22aa6f9a7a4f69638d7d8, 0.0/1.0 CPU_group_0_7b3716726cd22aa6f9a7a4f69638d7d8, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_1_7b3716726cd22aa6f9a7a4f69638d7d8, 0.0/1.0 CPU_group_4_7b3716726cd22aa6f9a7a4f69638d7d8)\n",
      "Result logdir: /home/michael/ray_results/singleagent_onelane_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_76534_00000 | RUNNING  | 192.168.1.76:29799 |     21 |          5464.02 | 315000 | -1598.95 |              1000.63 |             -5111.39 |               3000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=29795)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=29795)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29795)\u001b[0m ring length: 260\n",
      "\u001b[2m\u001b[36m(pid=29795)\u001b[0m v_max: 5.139779427502188\n",
      "\u001b[2m\u001b[36m(pid=29795)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29796)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=29796)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29796)\u001b[0m ring length: 264\n",
      "\u001b[2m\u001b[36m(pid=29796)\u001b[0m v_max: 5.329679917416892\n",
      "\u001b[2m\u001b[36m(pid=29796)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29797)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=29797)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29797)\u001b[0m ring length: 267\n",
      "\u001b[2m\u001b[36m(pid=29797)\u001b[0m v_max: 5.472051563171826\n",
      "\u001b[2m\u001b[36m(pid=29797)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29798)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=29798)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29798)\u001b[0m ring length: 235\n",
      "\u001b[2m\u001b[36m(pid=29798)\u001b[0m v_max: 3.9514850725282584\n",
      "\u001b[2m\u001b[36m(pid=29798)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29794)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=29794)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29794)\u001b[0m ring length: 255\n",
      "\u001b[2m\u001b[36m(pid=29794)\u001b[0m v_max: 4.902299776966978\n",
      "\u001b[2m\u001b[36m(pid=29794)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29795)\u001b[0m ./michael_files/emission_collection/singleagent_onelane_ring_20220929-1337571664476677.9312177-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=29796)\u001b[0m ./michael_files/emission_collection/singleagent_onelane_ring_20220929-1337571664476677.932766-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=29797)\u001b[0m ./michael_files/emission_collection/singleagent_onelane_ring_20220929-1337571664476677.9290917-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=29794)\u001b[0m ./michael_files/emission_collection/singleagent_onelane_ring_20220929-1337571664476677.9316185-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=29798)\u001b[0m ./michael_files/emission_collection/singleagent_onelane_ring_20220929-1337571664476677.926774-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=29795)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=29796)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=29794)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=29797)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=29798)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=29795)\u001b[0m Could not connect to TraCI server at localhost:50613 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=29795)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=29796)\u001b[0m Could not connect to TraCI server at localhost:43529 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=29796)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=29794)\u001b[0m Could not connect to TraCI server at localhost:58815 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=29794)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=29797)\u001b[0m Could not connect to TraCI server at localhost:52941 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=29797)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=29798)\u001b[0m Could not connect to TraCI server at localhost:35709 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=29798)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=29795)\u001b[0m Could not connect to TraCI server at localhost:50613 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=29795)\u001b[0m  Retrying in 3 seconds\n",
      "\u001b[2m\u001b[36m(pid=29796)\u001b[0m Could not connect to TraCI server at localhost:43529 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=29796)\u001b[0m  Retrying in 3 seconds\n",
      "\u001b[2m\u001b[36m(pid=29794)\u001b[0m Could not connect to TraCI server at localhost:58815 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=29794)\u001b[0m  Retrying in 3 seconds\n",
      "\u001b[2m\u001b[36m(pid=29797)\u001b[0m Could not connect to TraCI server at localhost:52941 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=29797)\u001b[0m  Retrying in 3 seconds\n",
      "\u001b[2m\u001b[36m(pid=29798)\u001b[0m Could not connect to TraCI server at localhost:35709 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=29798)\u001b[0m  Retrying in 3 seconds\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_76534_00000:\n",
      "  agent_timesteps_total: 330000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-09-29_13-44-53\n",
      "  done: false\n",
      "  episode_len_mean: 3000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1000.629739874041\n",
      "  episode_reward_mean: -1324.334160447256\n",
      "  episode_reward_min: -5091.922368519463\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 110\n",
      "  experiment_id: 939d1d5e80b64de89564600c497902cc\n",
      "  hostname: michael-home\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.0062499999999999995\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -0.6121541006585299\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01029281014812347\n",
      "          policy_loss: -0.0010833402285628589\n",
      "          total_loss: 76.61765153044361\n",
      "          vf_explained_var: 0.6445322036743164\n",
      "          vf_loss: 76.61867057186062\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 330000\n",
      "    num_steps_sampled: 330000\n",
      "    num_steps_trained: 330000\n",
      "  iterations_since_restore: 22\n",
      "  node_ip: 192.168.1.76\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 26.71209677419355\n",
      "    ram_util_percent: 46.64543010752688\n",
      "  pid: 29799\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09112678433894054\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 35.031729332453736\n",
      "    mean_inference_ms: 1.8462981098595173\n",
      "    mean_raw_obs_processing_ms: 35.00191207698795\n",
      "  time_since_restore: 5724.885528326035\n",
      "  time_this_iter_s: 260.86052894592285\n",
      "  time_total_s: 5724.885528326035\n",
      "  timers:\n",
      "    learn_throughput: 300.332\n",
      "    learn_time_ms: 49944.717\n",
      "    sample_throughput: 72.958\n",
      "    sample_time_ms: 205597.976\n",
      "    update_time_ms: 3.158\n",
      "  timestamp: 1664477093\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 330000\n",
      "  training_iteration: 22\n",
      "  trial_id: '76534_00000'\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 14.6/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/4.72 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_2_7b3716726cd22aa6f9a7a4f69638d7d8, 0.0/1.0 CPU_group_4_7b3716726cd22aa6f9a7a4f69638d7d8, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_1_7b3716726cd22aa6f9a7a4f69638d7d8, 0.0/1.0 CPU_group_0_7b3716726cd22aa6f9a7a4f69638d7d8, 0.0/1.0 CPU_group_5_7b3716726cd22aa6f9a7a4f69638d7d8, 0.0/6.0 CPU_group_7b3716726cd22aa6f9a7a4f69638d7d8, 0.0/1.0 CPU_group_3_7b3716726cd22aa6f9a7a4f69638d7d8)\n",
      "Result logdir: /home/michael/ray_results/singleagent_onelane_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_76534_00000 | RUNNING  | 192.168.1.76:29799 |     22 |          5724.89 | 330000 | -1324.33 |              1000.63 |             -5091.92 |               3000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=29795)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=29795)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29795)\u001b[0m ring length: 227\n",
      "\u001b[2m\u001b[36m(pid=29795)\u001b[0m v_max: 3.570869368805752\n",
      "\u001b[2m\u001b[36m(pid=29795)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29796)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=29796)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29796)\u001b[0m ring length: 244\n",
      "\u001b[2m\u001b[36m(pid=29796)\u001b[0m v_max: 4.379503211387676\n",
      "\u001b[2m\u001b[36m(pid=29796)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29797)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=29797)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29797)\u001b[0m ring length: 240\n",
      "\u001b[2m\u001b[36m(pid=29797)\u001b[0m v_max: 4.189299083856172\n",
      "\u001b[2m\u001b[36m(pid=29797)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29798)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=29798)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29798)\u001b[0m ring length: 258\n",
      "\u001b[2m\u001b[36m(pid=29798)\u001b[0m v_max: 5.044800727535787\n",
      "\u001b[2m\u001b[36m(pid=29798)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29794)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=29794)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29794)\u001b[0m ring length: 244\n",
      "\u001b[2m\u001b[36m(pid=29794)\u001b[0m v_max: 4.379503211387676\n",
      "\u001b[2m\u001b[36m(pid=29794)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29796)\u001b[0m ./michael_files/emission_collection/singleagent_onelane_ring_20220929-1342131664476933.0792747-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=29798)\u001b[0m ./michael_files/emission_collection/singleagent_onelane_ring_20220929-1342131664476933.0767496-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=29795)\u001b[0m ./michael_files/emission_collection/singleagent_onelane_ring_20220929-1342131664476933.075458-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=29797)\u001b[0m ./michael_files/emission_collection/singleagent_onelane_ring_20220929-1342131664476933.0800948-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=29794)\u001b[0m ./michael_files/emission_collection/singleagent_onelane_ring_20220929-1342131664476933.07579-0_emission.csv ./michael_files/emission_collection/\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_76534_00000:\n",
      "  agent_timesteps_total: 345000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-09-29_13-49-08\n",
      "  done: false\n",
      "  episode_len_mean: 3000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1012.5500830982905\n",
      "  episode_reward_mean: -1049.6762479931153\n",
      "  episode_reward_min: -4818.214679835756\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 115\n",
      "  experiment_id: 939d1d5e80b64de89564600c497902cc\n",
      "  hostname: michael-home\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.0062499999999999995\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -0.6421055980153003\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009296272489444774\n",
      "          policy_loss: -0.0010641153219108613\n",
      "          total_loss: 81.13465537378343\n",
      "          vf_explained_var: 0.5685025453567505\n",
      "          vf_loss: 81.13566153574799\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 345000\n",
      "    num_steps_sampled: 345000\n",
      "    num_steps_trained: 345000\n",
      "  iterations_since_restore: 23\n",
      "  node_ip: 192.168.1.76\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 27.301098901098907\n",
      "    ram_util_percent: 46.611263736263744\n",
      "  pid: 29799\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.091130016790724\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 34.53716879784258\n",
      "    mean_inference_ms: 1.8465168998619483\n",
      "    mean_raw_obs_processing_ms: 35.01062790666448\n",
      "  time_since_restore: 5979.703444957733\n",
      "  time_this_iter_s: 254.8179166316986\n",
      "  time_total_s: 5979.703444957733\n",
      "  timers:\n",
      "    learn_throughput: 300.329\n",
      "    learn_time_ms: 49945.223\n",
      "    sample_throughput: 72.958\n",
      "    sample_time_ms: 205598.358\n",
      "    update_time_ms: 3.159\n",
      "  timestamp: 1664477348\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 345000\n",
      "  training_iteration: 23\n",
      "  trial_id: '76534_00000'\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 14.6/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/4.72 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_5_7b3716726cd22aa6f9a7a4f69638d7d8, 0.0/1.0 CPU_group_0_7b3716726cd22aa6f9a7a4f69638d7d8, 0.0/6.0 CPU_group_7b3716726cd22aa6f9a7a4f69638d7d8, 0.0/1.0 CPU_group_3_7b3716726cd22aa6f9a7a4f69638d7d8, 0.0/1.0 CPU_group_1_7b3716726cd22aa6f9a7a4f69638d7d8, 0.0/1.0 CPU_group_4_7b3716726cd22aa6f9a7a4f69638d7d8, 0.0/1.0 CPU_group_2_7b3716726cd22aa6f9a7a4f69638d7d8)\n",
      "Result logdir: /home/michael/ray_results/singleagent_onelane_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_76534_00000 | RUNNING  | 192.168.1.76:29799 |     23 |           5979.7 | 345000 | -1049.68 |              1012.55 |             -4818.21 |               3000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=29795)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=29795)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29795)\u001b[0m ring length: 269\n",
      "\u001b[2m\u001b[36m(pid=29795)\u001b[0m v_max: 5.566938458220921\n",
      "\u001b[2m\u001b[36m(pid=29795)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29796)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=29796)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29796)\u001b[0m ring length: 221\n",
      "\u001b[2m\u001b[36m(pid=29796)\u001b[0m v_max: 3.285334164169417\n",
      "\u001b[2m\u001b[36m(pid=29796)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29797)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=29797)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29797)\u001b[0m ring length: 229\n",
      "\u001b[2m\u001b[36m(pid=29797)\u001b[0m v_max: 3.666034803266416\n",
      "\u001b[2m\u001b[36m(pid=29797)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29798)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=29798)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29798)\u001b[0m ring length: 246\n",
      "\u001b[2m\u001b[36m(pid=29798)\u001b[0m v_max: 4.474587708995648\n",
      "\u001b[2m\u001b[36m(pid=29798)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29794)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=29794)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29794)\u001b[0m ring length: 256\n",
      "\u001b[2m\u001b[36m(pid=29794)\u001b[0m v_max: 4.949804327743507\n",
      "\u001b[2m\u001b[36m(pid=29794)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29795)\u001b[0m ./michael_files/emission_collection/singleagent_onelane_ring_20220929-1346331664477193.9714491-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=29796)\u001b[0m ./michael_files/emission_collection/singleagent_onelane_ring_20220929-1346331664477193.9689283-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=29797)\u001b[0m ./michael_files/emission_collection/singleagent_onelane_ring_20220929-1346331664477193.9759-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=29798)\u001b[0m ./michael_files/emission_collection/singleagent_onelane_ring_20220929-1346331664477193.9761527-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=29794)\u001b[0m ./michael_files/emission_collection/singleagent_onelane_ring_20220929-1346331664477193.9748464-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=29795)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=29794)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=29796)\u001b[0m  Retrying in 1 seconds\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_76534_00000:\n",
      "  agent_timesteps_total: 360000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-09-29_13-53-24\n",
      "  done: false\n",
      "  episode_len_mean: 3000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1330.3099843155796\n",
      "  episode_reward_mean: -798.9228905638981\n",
      "  episode_reward_min: -4483.639875489541\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 120\n",
      "  experiment_id: 939d1d5e80b64de89564600c497902cc\n",
      "  hostname: michael-home\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.0031249999999999997\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -0.6712405676053742\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008409804717089055\n",
      "          policy_loss: -0.0004820114865867516\n",
      "          total_loss: 54.28305734375776\n",
      "          vf_explained_var: 0.6968640089035034\n",
      "          vf_loss: 54.283513001264154\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 360000\n",
      "    num_steps_sampled: 360000\n",
      "    num_steps_trained: 360000\n",
      "  iterations_since_restore: 24\n",
      "  node_ip: 192.168.1.76\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 27.156284153005465\n",
      "    ram_util_percent: 46.70737704918032\n",
      "  pid: 29799\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09113138782786676\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 34.18174195218096\n",
      "    mean_inference_ms: 1.846847404045757\n",
      "    mean_raw_obs_processing_ms: 35.01553008647591\n",
      "  time_since_restore: 6236.234850406647\n",
      "  time_this_iter_s: 256.5314054489136\n",
      "  time_total_s: 6236.234850406647\n",
      "  timers:\n",
      "    learn_throughput: 300.004\n",
      "    learn_time_ms: 49999.414\n",
      "    sample_throughput: 72.953\n",
      "    sample_time_ms: 205612.125\n",
      "    update_time_ms: 3.164\n",
      "  timestamp: 1664477604\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 360000\n",
      "  training_iteration: 24\n",
      "  trial_id: '76534_00000'\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Memory usage on this node: 14.6/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/4.72 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_4_7b3716726cd22aa6f9a7a4f69638d7d8, 0.0/1.0 CPU_group_5_7b3716726cd22aa6f9a7a4f69638d7d8, 0.0/1.0 CPU_group_0_7b3716726cd22aa6f9a7a4f69638d7d8, 0.0/6.0 CPU_group_7b3716726cd22aa6f9a7a4f69638d7d8, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_2_7b3716726cd22aa6f9a7a4f69638d7d8, 0.0/1.0 CPU_group_3_7b3716726cd22aa6f9a7a4f69638d7d8, 0.0/1.0 CPU_group_1_7b3716726cd22aa6f9a7a4f69638d7d8)\n",
      "Result logdir: /home/michael/ray_results/singleagent_onelane_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_76534_00000 | RUNNING  | 192.168.1.76:29799 |     24 |          6236.23 | 360000 | -798.923 |              1330.31 |             -4483.64 |               3000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=29795)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=29795)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29795)\u001b[0m ring length: 235\n",
      "\u001b[2m\u001b[36m(pid=29795)\u001b[0m v_max: 3.9514850725282584\n",
      "\u001b[2m\u001b[36m(pid=29795)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29796)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=29796)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29796)\u001b[0m ring length: 269\n",
      "\u001b[2m\u001b[36m(pid=29796)\u001b[0m v_max: 5.566938458220921\n",
      "\u001b[2m\u001b[36m(pid=29796)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29797)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=29797)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29797)\u001b[0m ring length: 268\n",
      "\u001b[2m\u001b[36m(pid=29797)\u001b[0m v_max: 5.5194978538368025\n",
      "\u001b[2m\u001b[36m(pid=29797)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29798)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=29798)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29798)\u001b[0m ring length: 252\n",
      "\u001b[2m\u001b[36m(pid=29798)\u001b[0m v_max: 4.759762049824181\n",
      "\u001b[2m\u001b[36m(pid=29798)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29794)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=29794)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29794)\u001b[0m ring length: 244\n",
      "\u001b[2m\u001b[36m(pid=29794)\u001b[0m v_max: 4.379503211387676\n",
      "\u001b[2m\u001b[36m(pid=29794)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29796)\u001b[0m ./michael_files/emission_collection/singleagent_onelane_ring_20220929-1350481664477449.000548-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=29797)\u001b[0m ./michael_files/emission_collection/singleagent_onelane_ring_20220929-1350481664477449.0005682-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=29794)\u001b[0m ./michael_files/emission_collection/singleagent_onelane_ring_20220929-1350481664477448.9964473-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=29795)\u001b[0m ./michael_files/emission_collection/singleagent_onelane_ring_20220929-1350481664477448.9992192-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=29798)\u001b[0m ./michael_files/emission_collection/singleagent_onelane_ring_20220929-1350481664477449.0005708-0_emission.csv ./michael_files/emission_collection/\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_76534_00000:\n",
      "  agent_timesteps_total: 375000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-09-29_13-57-39\n",
      "  done: false\n",
      "  episode_len_mean: 3000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1828.8910832151173\n",
      "  episode_reward_mean: -544.9538314722691\n",
      "  episode_reward_min: -3902.1085320846323\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 125\n",
      "  experiment_id: 939d1d5e80b64de89564600c497902cc\n",
      "  hostname: michael-home\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.0015624999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -0.7500125237440659\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009734039651380465\n",
      "          policy_loss: -0.0004839606417374591\n",
      "          total_loss: 73.15096565181926\n",
      "          vf_explained_var: 0.8227866888046265\n",
      "          vf_loss: 73.15143429384393\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 375000\n",
      "    num_steps_sampled: 375000\n",
      "    num_steps_trained: 375000\n",
      "  iterations_since_restore: 25\n",
      "  node_ip: 192.168.1.76\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 27.238292011019283\n",
      "    ram_util_percent: 46.696969696969695\n",
      "  pid: 29799\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09113182277966579\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 33.908525775213555\n",
      "    mean_inference_ms: 1.8469301336107689\n",
      "    mean_raw_obs_processing_ms: 35.01811029213952\n",
      "  time_since_restore: 6490.785718202591\n",
      "  time_this_iter_s: 254.5508677959442\n",
      "  time_total_s: 6490.785718202591\n",
      "  timers:\n",
      "    learn_throughput: 299.91\n",
      "    learn_time_ms: 50014.991\n",
      "    sample_throughput: 72.954\n",
      "    sample_time_ms: 205609.346\n",
      "    update_time_ms: 3.136\n",
      "  timestamp: 1664477859\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 375000\n",
      "  training_iteration: 25\n",
      "  trial_id: '76534_00000'\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 14.7/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/4.72 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_5_7b3716726cd22aa6f9a7a4f69638d7d8, 0.0/1.0 CPU_group_0_7b3716726cd22aa6f9a7a4f69638d7d8, 0.0/1.0 CPU_group_1_7b3716726cd22aa6f9a7a4f69638d7d8, 0.0/1.0 CPU_group_2_7b3716726cd22aa6f9a7a4f69638d7d8, 0.0/6.0 CPU_group_7b3716726cd22aa6f9a7a4f69638d7d8, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_3_7b3716726cd22aa6f9a7a4f69638d7d8, 0.0/1.0 CPU_group_4_7b3716726cd22aa6f9a7a4f69638d7d8)\n",
      "Result logdir: /home/michael/ray_results/singleagent_onelane_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_76534_00000 | RUNNING  | 192.168.1.76:29799 |     25 |          6490.79 | 375000 | -544.954 |              1828.89 |             -3902.11 |               3000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=29795)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=29795)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29795)\u001b[0m ring length: 254\n",
      "\u001b[2m\u001b[36m(pid=29795)\u001b[0m v_max: 4.854791141191876\n",
      "\u001b[2m\u001b[36m(pid=29795)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29796)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=29796)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29796)\u001b[0m ring length: 249\n",
      "\u001b[2m\u001b[36m(pid=29796)\u001b[0m v_max: 4.617190445131122\n",
      "\u001b[2m\u001b[36m(pid=29796)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29797)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=29797)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29797)\u001b[0m ring length: 225\n",
      "\u001b[2m\u001b[36m(pid=29797)\u001b[0m v_max: 3.4756971311168834\n",
      "\u001b[2m\u001b[36m(pid=29797)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29798)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=29798)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29798)\u001b[0m ring length: 257\n",
      "\u001b[2m\u001b[36m(pid=29798)\u001b[0m v_max: 4.997304682316517\n",
      "\u001b[2m\u001b[36m(pid=29798)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29794)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=29794)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29794)\u001b[0m ring length: 263\n",
      "\u001b[2m\u001b[36m(pid=29794)\u001b[0m v_max: 5.282212215151455\n",
      "\u001b[2m\u001b[36m(pid=29794)\u001b[0m -----------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=29795)\u001b[0m ./michael_files/emission_collection/singleagent_onelane_ring_20220929-1355051664477705.418794-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=29796)\u001b[0m ./michael_files/emission_collection/singleagent_onelane_ring_20220929-1355051664477705.4181402-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=29797)\u001b[0m ./michael_files/emission_collection/singleagent_onelane_ring_20220929-1355051664477705.4184906-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=29798)\u001b[0m ./michael_files/emission_collection/singleagent_onelane_ring_20220929-1355051664477705.4185667-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=29794)\u001b[0m ./michael_files/emission_collection/singleagent_onelane_ring_20220929-1355051664477705.419419-0_emission.csv ./michael_files/emission_collection/\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_76534_00000:\n",
      "  agent_timesteps_total: 390000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-09-29_14-01-54\n",
      "  done: false\n",
      "  episode_len_mean: 3000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2004.8832844479048\n",
      "  episode_reward_mean: -326.52395224740883\n",
      "  episode_reward_min: -3866.959432251971\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 130\n",
      "  experiment_id: 939d1d5e80b64de89564600c497902cc\n",
      "  hostname: michael-home\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.0007812499999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -0.8137884054648674\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.00842324704837239\n",
      "          policy_loss: -0.00022995810373932897\n",
      "          total_loss: 88.23893245438398\n",
      "          vf_explained_var: 0.752909779548645\n",
      "          vf_loss: 88.23915577258094\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 390000\n",
      "    num_steps_sampled: 390000\n",
      "    num_steps_trained: 390000\n",
      "  iterations_since_restore: 26\n",
      "  node_ip: 192.168.1.76\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 27.365289256198352\n",
      "    ram_util_percent: 46.67107438016529\n",
      "  pid: 29799\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0911348216858544\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 33.68954625967679\n",
      "    mean_inference_ms: 1.8470900836467672\n",
      "    mean_raw_obs_processing_ms: 35.01859577443998\n",
      "  time_since_restore: 6745.345756292343\n",
      "  time_this_iter_s: 254.5600380897522\n",
      "  time_total_s: 6745.345756292343\n",
      "  timers:\n",
      "    learn_throughput: 300.146\n",
      "    learn_time_ms: 49975.67\n",
      "    sample_throughput: 72.951\n",
      "    sample_time_ms: 205616.292\n",
      "    update_time_ms: 3.136\n",
      "  timestamp: 1664478114\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 390000\n",
      "  training_iteration: 26\n",
      "  trial_id: '76534_00000'\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 14.6/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/4.72 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_0_7b3716726cd22aa6f9a7a4f69638d7d8, 0.0/1.0 CPU_group_2_7b3716726cd22aa6f9a7a4f69638d7d8, 0.0/1.0 CPU_group_1_7b3716726cd22aa6f9a7a4f69638d7d8, 0.0/1.0 CPU_group_3_7b3716726cd22aa6f9a7a4f69638d7d8, 0.0/6.0 CPU_group_7b3716726cd22aa6f9a7a4f69638d7d8, 0.0/1.0 CPU_group_4_7b3716726cd22aa6f9a7a4f69638d7d8, 0.0/1.0 CPU_group_5_7b3716726cd22aa6f9a7a4f69638d7d8, 0.0/1.0 accelerator_type:G)\n",
      "Result logdir: /home/michael/ray_results/singleagent_onelane_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_76534_00000 | RUNNING  | 192.168.1.76:29799 |     26 |          6745.35 | 390000 | -326.524 |              2004.88 |             -3866.96 |               3000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=29795)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=29795)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29795)\u001b[0m ring length: 226\n",
      "\u001b[2m\u001b[36m(pid=29795)\u001b[0m v_max: 3.5232840694769565\n",
      "\u001b[2m\u001b[36m(pid=29795)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29796)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=29796)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29796)\u001b[0m ring length: 250\n",
      "\u001b[2m\u001b[36m(pid=29796)\u001b[0m v_max: 4.664717904914125\n",
      "\u001b[2m\u001b[36m(pid=29796)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29797)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=29797)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29797)\u001b[0m ring length: 236\n",
      "\u001b[2m\u001b[36m(pid=29797)\u001b[0m v_max: 3.999052674951912\n",
      "\u001b[2m\u001b[36m(pid=29797)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29798)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=29798)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29798)\u001b[0m ring length: 220\n",
      "\u001b[2m\u001b[36m(pid=29798)\u001b[0m v_max: 3.2377399006821497\n",
      "\u001b[2m\u001b[36m(pid=29798)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29794)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=29794)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29794)\u001b[0m ring length: 230\n",
      "\u001b[2m\u001b[36m(pid=29794)\u001b[0m v_max: 3.7136148111012934\n",
      "\u001b[2m\u001b[36m(pid=29794)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29795)\u001b[0m ./michael_files/emission_collection/singleagent_onelane_ring_20220929-1359201664477960.0160723-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=29796)\u001b[0m ./michael_files/emission_collection/singleagent_onelane_ring_20220929-1359201664477960.0187633-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=29797)\u001b[0m ./michael_files/emission_collection/singleagent_onelane_ring_20220929-1359201664477960.0200648-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=29798)\u001b[0m ./michael_files/emission_collection/singleagent_onelane_ring_20220929-1359201664477960.0186949-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=29794)\u001b[0m ./michael_files/emission_collection/singleagent_onelane_ring_20220929-1359201664477960.0202277-0_emission.csv ./michael_files/emission_collection/\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_76534_00000:\n",
      "  agent_timesteps_total: 405000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-09-29_14-06-09\n",
      "  done: false\n",
      "  episode_len_mean: 3000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2004.8832844479048\n",
      "  episode_reward_mean: -113.0932859907546\n",
      "  episode_reward_min: -2571.0119029487296\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 135\n",
      "  experiment_id: 939d1d5e80b64de89564600c497902cc\n",
      "  hostname: michael-home\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.00039062499999999997\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -0.8790519700717118\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009547555885525368\n",
      "          policy_loss: -0.0011935685342967005\n",
      "          total_loss: 69.14882513628167\n",
      "          vf_explained_var: 0.8150707483291626\n",
      "          vf_loss: 69.15001470921403\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 405000\n",
      "    num_steps_sampled: 405000\n",
      "    num_steps_trained: 405000\n",
      "  iterations_since_restore: 27\n",
      "  node_ip: 192.168.1.76\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 27.2260989010989\n",
      "    ram_util_percent: 46.60576923076923\n",
      "  pid: 29799\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09113674446287323\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 33.50883658105156\n",
      "    mean_inference_ms: 1.847228683439632\n",
      "    mean_raw_obs_processing_ms: 35.01966008735762\n",
      "  time_since_restore: 7000.303592920303\n",
      "  time_this_iter_s: 254.9578366279602\n",
      "  time_total_s: 7000.303592920303\n",
      "  timers:\n",
      "    learn_throughput: 299.924\n",
      "    learn_time_ms: 50012.738\n",
      "    sample_throughput: 72.957\n",
      "    sample_time_ms: 205600.829\n",
      "    update_time_ms: 3.135\n",
      "  timestamp: 1664478369\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 405000\n",
      "  training_iteration: 27\n",
      "  trial_id: '76534_00000'\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 14.6/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/4.72 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_1_7b3716726cd22aa6f9a7a4f69638d7d8, 0.0/1.0 CPU_group_3_7b3716726cd22aa6f9a7a4f69638d7d8, 0.0/6.0 CPU_group_7b3716726cd22aa6f9a7a4f69638d7d8, 0.0/1.0 CPU_group_5_7b3716726cd22aa6f9a7a4f69638d7d8, 0.0/1.0 CPU_group_0_7b3716726cd22aa6f9a7a4f69638d7d8, 0.0/1.0 CPU_group_2_7b3716726cd22aa6f9a7a4f69638d7d8, 0.0/1.0 CPU_group_4_7b3716726cd22aa6f9a7a4f69638d7d8)\n",
      "Result logdir: /home/michael/ray_results/singleagent_onelane_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_76534_00000 | RUNNING  | 192.168.1.76:29799 |     27 |           7000.3 | 405000 | -113.093 |              2004.88 |             -2571.01 |               3000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=29795)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=29795)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29795)\u001b[0m ring length: 225\n",
      "\u001b[2m\u001b[36m(pid=29795)\u001b[0m v_max: 3.4756971311168834\n",
      "\u001b[2m\u001b[36m(pid=29795)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29796)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=29796)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29796)\u001b[0m ring length: 262\n",
      "\u001b[2m\u001b[36m(pid=29796)\u001b[0m v_max: 5.234739483008334\n",
      "\u001b[2m\u001b[36m(pid=29796)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29797)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=29797)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29797)\u001b[0m ring length: 242\n",
      "\u001b[2m\u001b[36m(pid=29797)\u001b[0m v_max: 4.2844067680020546\n",
      "\u001b[2m\u001b[36m(pid=29797)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29798)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=29798)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29798)\u001b[0m ring length: 268\n",
      "\u001b[2m\u001b[36m(pid=29798)\u001b[0m v_max: 5.5194978538368025\n",
      "\u001b[2m\u001b[36m(pid=29798)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29794)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=29794)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29794)\u001b[0m ring length: 220\n",
      "\u001b[2m\u001b[36m(pid=29794)\u001b[0m v_max: 3.2377399006821497\n",
      "\u001b[2m\u001b[36m(pid=29794)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29796)\u001b[0m ./michael_files/emission_collection/singleagent_onelane_ring_20220929-1403341664478214.63602-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=29797)\u001b[0m ./michael_files/emission_collection/singleagent_onelane_ring_20220929-1403341664478214.6286266-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=29798)\u001b[0m ./michael_files/emission_collection/singleagent_onelane_ring_20220929-1403341664478214.629404-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=29794)\u001b[0m ./michael_files/emission_collection/singleagent_onelane_ring_20220929-1403341664478214.6294026-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=29795)\u001b[0m ./michael_files/emission_collection/singleagent_onelane_ring_20220929-1403341664478214.6356957-0_emission.csv ./michael_files/emission_collection/\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_76534_00000:\n",
      "  agent_timesteps_total: 420000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-09-29_14-10-24\n",
      "  done: false\n",
      "  episode_len_mean: 3000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2004.8832844479048\n",
      "  episode_reward_mean: 1.031993368416638\n",
      "  episode_reward_min: -2121.1764033992235\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 140\n",
      "  experiment_id: 939d1d5e80b64de89564600c497902cc\n",
      "  hostname: michael-home\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.00019531249999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -0.9309679905741902\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009628577586580107\n",
      "          policy_loss: 0.0005725849514543\n",
      "          total_loss: 41.90115079556481\n",
      "          vf_explained_var: 0.8767728209495544\n",
      "          vf_loss: 41.90057631670418\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 420000\n",
      "    num_steps_sampled: 420000\n",
      "    num_steps_trained: 420000\n",
      "  iterations_since_restore: 28\n",
      "  node_ip: 192.168.1.76\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 27.33626373626374\n",
      "    ram_util_percent: 46.75027472527472\n",
      "  pid: 29799\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09114210409110651\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 33.35630639572421\n",
      "    mean_inference_ms: 1.8474266807618651\n",
      "    mean_raw_obs_processing_ms: 35.01924876255234\n",
      "  time_since_restore: 7255.760500907898\n",
      "  time_this_iter_s: 255.4569079875946\n",
      "  time_total_s: 7255.760500907898\n",
      "  timers:\n",
      "    learn_throughput: 299.655\n",
      "    learn_time_ms: 50057.631\n",
      "    sample_throughput: 72.955\n",
      "    sample_time_ms: 205607.042\n",
      "    update_time_ms: 3.149\n",
      "  timestamp: 1664478624\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 420000\n",
      "  training_iteration: 28\n",
      "  trial_id: '76534_00000'\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 14.6/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/4.72 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_2_7b3716726cd22aa6f9a7a4f69638d7d8, 0.0/1.0 CPU_group_1_7b3716726cd22aa6f9a7a4f69638d7d8, 0.0/1.0 CPU_group_5_7b3716726cd22aa6f9a7a4f69638d7d8, 0.0/6.0 CPU_group_7b3716726cd22aa6f9a7a4f69638d7d8, 0.0/1.0 CPU_group_0_7b3716726cd22aa6f9a7a4f69638d7d8, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_3_7b3716726cd22aa6f9a7a4f69638d7d8, 0.0/1.0 CPU_group_4_7b3716726cd22aa6f9a7a4f69638d7d8)\n",
      "Result logdir: /home/michael/ray_results/singleagent_onelane_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_76534_00000 | RUNNING  | 192.168.1.76:29799 |     28 |          7255.76 | 420000 |  1.03199 |              2004.88 |             -2121.18 |               3000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=29795)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=29795)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29795)\u001b[0m ring length: 266\n",
      "\u001b[2m\u001b[36m(pid=29795)\u001b[0m v_max: 5.42459972166245\n",
      "\u001b[2m\u001b[36m(pid=29795)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29796)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=29796)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29796)\u001b[0m ring length: 263\n",
      "\u001b[2m\u001b[36m(pid=29796)\u001b[0m v_max: 5.282212215151455\n",
      "\u001b[2m\u001b[36m(pid=29796)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29797)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=29797)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29797)\u001b[0m ring length: 246\n",
      "\u001b[2m\u001b[36m(pid=29797)\u001b[0m v_max: 4.474587708995648\n",
      "\u001b[2m\u001b[36m(pid=29797)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29798)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=29798)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29798)\u001b[0m ring length: 230\n",
      "\u001b[2m\u001b[36m(pid=29798)\u001b[0m v_max: 3.7136148111012934\n",
      "\u001b[2m\u001b[36m(pid=29798)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29794)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=29794)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29794)\u001b[0m ring length: 224\n",
      "\u001b[2m\u001b[36m(pid=29794)\u001b[0m v_max: 3.4281086136538996\n",
      "\u001b[2m\u001b[36m(pid=29794)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29795)\u001b[0m ./michael_files/emission_collection/singleagent_onelane_ring_20220929-1407491664478469.6325467-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=29796)\u001b[0m ./michael_files/emission_collection/singleagent_onelane_ring_20220929-1407491664478469.6303554-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=29797)\u001b[0m ./michael_files/emission_collection/singleagent_onelane_ring_20220929-1407491664478469.6238427-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=29798)\u001b[0m ./michael_files/emission_collection/singleagent_onelane_ring_20220929-1407491664478469.6309376-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=29794)\u001b[0m ./michael_files/emission_collection/singleagent_onelane_ring_20220929-1407491664478469.6235943-0_emission.csv ./michael_files/emission_collection/\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_76534_00000:\n",
      "  agent_timesteps_total: 435000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-09-29_14-14-40\n",
      "  done: false\n",
      "  episode_len_mean: 3000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2004.8832844479048\n",
      "  episode_reward_mean: 126.25327535947294\n",
      "  episode_reward_min: -1552.5354339043179\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 145\n",
      "  experiment_id: 939d1d5e80b64de89564600c497902cc\n",
      "  hostname: michael-home\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 9.765624999999999e-05\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -1.0019301854452844\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.007899142575935676\n",
      "          policy_loss: 0.0004087280047962726\n",
      "          total_loss: 80.38724858235504\n",
      "          vf_explained_var: 0.8655694723129272\n",
      "          vf_loss: 80.38683887417034\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 435000\n",
      "    num_steps_sampled: 435000\n",
      "    num_steps_trained: 435000\n",
      "  iterations_since_restore: 29\n",
      "  node_ip: 192.168.1.76\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 27.19369863013699\n",
      "    ram_util_percent: 46.78684931506851\n",
      "  pid: 29799\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0911496417807801\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 33.22543988021452\n",
      "    mean_inference_ms: 1.847561479672099\n",
      "    mean_raw_obs_processing_ms: 35.01940411946458\n",
      "  time_since_restore: 7511.280612230301\n",
      "  time_this_iter_s: 255.52011132240295\n",
      "  time_total_s: 7511.280612230301\n",
      "  timers:\n",
      "    learn_throughput: 299.306\n",
      "    learn_time_ms: 50115.948\n",
      "    sample_throughput: 72.955\n",
      "    sample_time_ms: 205604.842\n",
      "    update_time_ms: 3.159\n",
      "  timestamp: 1664478880\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 435000\n",
      "  training_iteration: 29\n",
      "  trial_id: '76534_00000'\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Memory usage on this node: 14.7/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/4.72 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_5_7b3716726cd22aa6f9a7a4f69638d7d8, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_1_7b3716726cd22aa6f9a7a4f69638d7d8, 0.0/1.0 CPU_group_2_7b3716726cd22aa6f9a7a4f69638d7d8, 0.0/6.0 CPU_group_7b3716726cd22aa6f9a7a4f69638d7d8, 0.0/1.0 CPU_group_3_7b3716726cd22aa6f9a7a4f69638d7d8, 0.0/1.0 CPU_group_0_7b3716726cd22aa6f9a7a4f69638d7d8, 0.0/1.0 CPU_group_4_7b3716726cd22aa6f9a7a4f69638d7d8)\n",
      "Result logdir: /home/michael/ray_results/singleagent_onelane_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_76534_00000 | RUNNING  | 192.168.1.76:29799 |     29 |          7511.28 | 435000 |  126.253 |              2004.88 |             -1552.54 |               3000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=29795)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=29795)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29795)\u001b[0m ring length: 223\n",
      "\u001b[2m\u001b[36m(pid=29795)\u001b[0m v_max: 3.3805185755587788\n",
      "\u001b[2m\u001b[36m(pid=29795)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29796)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=29796)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29796)\u001b[0m ring length: 270\n",
      "\u001b[2m\u001b[36m(pid=29796)\u001b[0m v_max: 5.6143732387852054\n",
      "\u001b[2m\u001b[36m(pid=29796)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29797)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=29797)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29797)\u001b[0m ring length: 263\n",
      "\u001b[2m\u001b[36m(pid=29797)\u001b[0m v_max: 5.282212215151455\n",
      "\u001b[2m\u001b[36m(pid=29797)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29798)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=29798)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29798)\u001b[0m ring length: 252\n",
      "\u001b[2m\u001b[36m(pid=29798)\u001b[0m v_max: 4.759762049824181\n",
      "\u001b[2m\u001b[36m(pid=29798)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29794)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=29794)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29794)\u001b[0m ring length: 223\n",
      "\u001b[2m\u001b[36m(pid=29794)\u001b[0m v_max: 3.3805185755587788\n",
      "\u001b[2m\u001b[36m(pid=29794)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29795)\u001b[0m ./michael_files/emission_collection/singleagent_onelane_ring_20220929-1412051664478725.1292634-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=29796)\u001b[0m ./michael_files/emission_collection/singleagent_onelane_ring_20220929-1412051664478725.129697-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=29797)\u001b[0m ./michael_files/emission_collection/singleagent_onelane_ring_20220929-1412051664478725.1195881-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=29798)\u001b[0m ./michael_files/emission_collection/singleagent_onelane_ring_20220929-1412051664478725.123071-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=29794)\u001b[0m ./michael_files/emission_collection/singleagent_onelane_ring_20220929-1412051664478725.1206741-0_emission.csv ./michael_files/emission_collection/\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_76534_00000:\n",
      "  agent_timesteps_total: 450000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-09-29_14-18-56\n",
      "  done: false\n",
      "  episode_len_mean: 3000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2004.8832844479048\n",
      "  episode_reward_mean: 246.56557324343737\n",
      "  episode_reward_min: -1443.4461642229949\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 150\n",
      "  experiment_id: 939d1d5e80b64de89564600c497902cc\n",
      "  hostname: michael-home\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 4.8828124999999996e-05\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -1.0829306891409016\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009789024007970892\n",
      "          policy_loss: -0.0007117293210763295\n",
      "          total_loss: 86.56098218368271\n",
      "          vf_explained_var: 0.868148148059845\n",
      "          vf_loss: 86.56169308484611\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 450000\n",
      "    num_steps_sampled: 450000\n",
      "    num_steps_trained: 450000\n",
      "  iterations_since_restore: 30\n",
      "  node_ip: 192.168.1.76\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 27.14109589041096\n",
      "    ram_util_percent: 46.86465753424657\n",
      "  pid: 29799\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09115829067069037\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 33.11159830127078\n",
      "    mean_inference_ms: 1.847703641697625\n",
      "    mean_raw_obs_processing_ms: 35.020087780072\n",
      "  time_since_restore: 7767.04226565361\n",
      "  time_this_iter_s: 255.76165342330933\n",
      "  time_total_s: 7767.04226565361\n",
      "  timers:\n",
      "    learn_throughput: 298.921\n",
      "    learn_time_ms: 50180.418\n",
      "    sample_throughput: 72.952\n",
      "    sample_time_ms: 205615.243\n",
      "    update_time_ms: 3.153\n",
      "  timestamp: 1664479136\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 450000\n",
      "  training_iteration: 30\n",
      "  trial_id: '76534_00000'\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 14.6/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/4.72 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_2_7b3716726cd22aa6f9a7a4f69638d7d8, 0.0/1.0 CPU_group_3_7b3716726cd22aa6f9a7a4f69638d7d8, 0.0/1.0 CPU_group_1_7b3716726cd22aa6f9a7a4f69638d7d8, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_4_7b3716726cd22aa6f9a7a4f69638d7d8, 0.0/6.0 CPU_group_7b3716726cd22aa6f9a7a4f69638d7d8, 0.0/1.0 CPU_group_5_7b3716726cd22aa6f9a7a4f69638d7d8, 0.0/1.0 CPU_group_0_7b3716726cd22aa6f9a7a4f69638d7d8)\n",
      "Result logdir: /home/michael/ray_results/singleagent_onelane_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_76534_00000 | RUNNING  | 192.168.1.76:29799 |     30 |          7767.04 | 450000 |  246.566 |              2004.88 |             -1443.45 |               3000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=29795)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=29795)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29795)\u001b[0m ring length: 265\n",
      "\u001b[2m\u001b[36m(pid=29795)\u001b[0m v_max: 5.37714246265477\n",
      "\u001b[2m\u001b[36m(pid=29795)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29796)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=29796)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29796)\u001b[0m ring length: 233\n",
      "\u001b[2m\u001b[36m(pid=29796)\u001b[0m v_max: 3.856343201216534\n",
      "\u001b[2m\u001b[36m(pid=29796)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29797)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=29797)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29797)\u001b[0m ring length: 254\n",
      "\u001b[2m\u001b[36m(pid=29797)\u001b[0m v_max: 4.854791141191876\n",
      "\u001b[2m\u001b[36m(pid=29797)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29798)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=29798)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29798)\u001b[0m ring length: 246\n",
      "\u001b[2m\u001b[36m(pid=29798)\u001b[0m v_max: 4.474587708995648\n",
      "\u001b[2m\u001b[36m(pid=29798)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29794)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=29794)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29794)\u001b[0m ring length: 226\n",
      "\u001b[2m\u001b[36m(pid=29794)\u001b[0m v_max: 3.5232840694769565\n",
      "\u001b[2m\u001b[36m(pid=29794)\u001b[0m -----------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=29795)\u001b[0m ./michael_files/emission_collection/singleagent_onelane_ring_20220929-1416201664478980.7388568-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=29796)\u001b[0m ./michael_files/emission_collection/singleagent_onelane_ring_20220929-1416201664478980.7388449-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=29797)\u001b[0m ./michael_files/emission_collection/singleagent_onelane_ring_20220929-1416201664478980.735078-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=29798)\u001b[0m ./michael_files/emission_collection/singleagent_onelane_ring_20220929-1416201664478980.7326958-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=29794)\u001b[0m ./michael_files/emission_collection/singleagent_onelane_ring_20220929-1416201664478980.7369082-0_emission.csv ./michael_files/emission_collection/\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_76534_00000:\n",
      "  agent_timesteps_total: 465000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-09-29_14-23-11\n",
      "  done: false\n",
      "  episode_len_mean: 3000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2004.8832844479048\n",
      "  episode_reward_mean: 353.72198103345613\n",
      "  episode_reward_min: -1443.4461642229949\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 155\n",
      "  experiment_id: 939d1d5e80b64de89564600c497902cc\n",
      "  hostname: michael-home\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.4414062499999998e-05\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -1.1436580027564098\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008153537824929157\n",
      "          policy_loss: 0.0005973720591578443\n",
      "          total_loss: 95.06395456346415\n",
      "          vf_explained_var: 0.8889006972312927\n",
      "          vf_loss: 95.06335688768807\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 465000\n",
      "    num_steps_sampled: 465000\n",
      "    num_steps_trained: 465000\n",
      "  iterations_since_restore: 31\n",
      "  node_ip: 192.168.1.76\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 27.220604395604393\n",
      "    ram_util_percent: 46.85686813186813\n",
      "  pid: 29799\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09116779798732538\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 33.01165180547657\n",
      "    mean_inference_ms: 1.847730325090016\n",
      "    mean_raw_obs_processing_ms: 35.020257343280925\n",
      "  time_since_restore: 8022.341928720474\n",
      "  time_this_iter_s: 255.299663066864\n",
      "  time_total_s: 8022.341928720474\n",
      "  timers:\n",
      "    learn_throughput: 298.826\n",
      "    learn_time_ms: 50196.517\n",
      "    sample_throughput: 72.95\n",
      "    sample_time_ms: 205620.364\n",
      "    update_time_ms: 3.155\n",
      "  timestamp: 1664479391\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 465000\n",
      "  training_iteration: 31\n",
      "  trial_id: '76534_00000'\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 14.7/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/4.72 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_1_7b3716726cd22aa6f9a7a4f69638d7d8, 0.0/1.0 CPU_group_4_7b3716726cd22aa6f9a7a4f69638d7d8, 0.0/1.0 CPU_group_0_7b3716726cd22aa6f9a7a4f69638d7d8, 0.0/1.0 CPU_group_3_7b3716726cd22aa6f9a7a4f69638d7d8, 0.0/1.0 CPU_group_5_7b3716726cd22aa6f9a7a4f69638d7d8, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_2_7b3716726cd22aa6f9a7a4f69638d7d8, 0.0/6.0 CPU_group_7b3716726cd22aa6f9a7a4f69638d7d8)\n",
      "Result logdir: /home/michael/ray_results/singleagent_onelane_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_76534_00000 | RUNNING  | 192.168.1.76:29799 |     31 |          8022.34 | 465000 |  353.722 |              2004.88 |             -1443.45 |               3000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=29795)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=29795)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29795)\u001b[0m ring length: 237\n",
      "\u001b[2m\u001b[36m(pid=29795)\u001b[0m v_max: 4.04661795519353\n",
      "\u001b[2m\u001b[36m(pid=29795)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29796)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=29796)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29796)\u001b[0m ring length: 260\n",
      "\u001b[2m\u001b[36m(pid=29796)\u001b[0m v_max: 5.139779427502188\n",
      "\u001b[2m\u001b[36m(pid=29796)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29797)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=29797)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29797)\u001b[0m ring length: 228\n",
      "\u001b[2m\u001b[36m(pid=29797)\u001b[0m v_max: 3.618452967700356\n",
      "\u001b[2m\u001b[36m(pid=29797)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29798)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=29798)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29798)\u001b[0m ring length: 232\n",
      "\u001b[2m\u001b[36m(pid=29798)\u001b[0m v_max: 3.8087690783250063\n",
      "\u001b[2m\u001b[36m(pid=29798)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29794)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=29794)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29794)\u001b[0m ring length: 220\n",
      "\u001b[2m\u001b[36m(pid=29794)\u001b[0m v_max: 3.2377399006821497\n",
      "\u001b[2m\u001b[36m(pid=29794)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29795)\u001b[0m ./michael_files/emission_collection/singleagent_onelane_ring_20220929-1420361664479236.6805124-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=29796)\u001b[0m ./michael_files/emission_collection/singleagent_onelane_ring_20220929-1420361664479236.676082-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=29797)\u001b[0m ./michael_files/emission_collection/singleagent_onelane_ring_20220929-1420361664479236.6812692-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=29798)\u001b[0m ./michael_files/emission_collection/singleagent_onelane_ring_20220929-1420361664479236.6806743-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=29794)\u001b[0m ./michael_files/emission_collection/singleagent_onelane_ring_20220929-1420361664479236.67206-0_emission.csv ./michael_files/emission_collection/\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_76534_00000:\n",
      "  agent_timesteps_total: 480000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-09-29_14-27-26\n",
      "  done: false\n",
      "  episode_len_mean: 3000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2149.298719586105\n",
      "  episode_reward_mean: 436.9322473274205\n",
      "  episode_reward_min: -1443.4461642229949\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 160\n",
      "  experiment_id: 939d1d5e80b64de89564600c497902cc\n",
      "  hostname: michael-home\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.2207031249999999e-05\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -1.1023807461989128\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009269177766649112\n",
      "          policy_loss: -0.0005986335967525335\n",
      "          total_loss: 95.85009174993483\n",
      "          vf_explained_var: 0.8630241751670837\n",
      "          vf_loss: 95.85069049253302\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 480000\n",
      "    num_steps_sampled: 480000\n",
      "    num_steps_trained: 480000\n",
      "  iterations_since_restore: 32\n",
      "  node_ip: 192.168.1.76\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 27.280769230769234\n",
      "    ram_util_percent: 46.93324175824175\n",
      "  pid: 29799\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09117573436148194\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 32.92280592987188\n",
      "    mean_inference_ms: 1.8476782559258231\n",
      "    mean_raw_obs_processing_ms: 35.02076664843189\n",
      "  time_since_restore: 8277.539361000061\n",
      "  time_this_iter_s: 255.1974322795868\n",
      "  time_total_s: 8277.539361000061\n",
      "  timers:\n",
      "    learn_throughput: 298.661\n",
      "    learn_time_ms: 50224.09\n",
      "    sample_throughput: 73.161\n",
      "    sample_time_ms: 205026.704\n",
      "    update_time_ms: 3.118\n",
      "  timestamp: 1664479646\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 480000\n",
      "  training_iteration: 32\n",
      "  trial_id: '76534_00000'\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Memory usage on this node: 14.7/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/4.72 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_2_7b3716726cd22aa6f9a7a4f69638d7d8, 0.0/1.0 CPU_group_5_7b3716726cd22aa6f9a7a4f69638d7d8, 0.0/1.0 CPU_group_3_7b3716726cd22aa6f9a7a4f69638d7d8, 0.0/6.0 CPU_group_7b3716726cd22aa6f9a7a4f69638d7d8, 0.0/1.0 CPU_group_1_7b3716726cd22aa6f9a7a4f69638d7d8, 0.0/1.0 CPU_group_0_7b3716726cd22aa6f9a7a4f69638d7d8, 0.0/1.0 CPU_group_4_7b3716726cd22aa6f9a7a4f69638d7d8, 0.0/1.0 accelerator_type:G)\n",
      "Result logdir: /home/michael/ray_results/singleagent_onelane_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_76534_00000 | RUNNING  | 192.168.1.76:29799 |     32 |          8277.54 | 480000 |  436.932 |               2149.3 |             -1443.45 |               3000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=29798)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=29798)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29798)\u001b[0m ring length: 264\n",
      "\u001b[2m\u001b[36m(pid=29798)\u001b[0m v_max: 5.329679917416892\n",
      "\u001b[2m\u001b[36m(pid=29798)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29794)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=29794)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29794)\u001b[0m ring length: 269\n",
      "\u001b[2m\u001b[36m(pid=29794)\u001b[0m v_max: 5.566938458220921\n",
      "\u001b[2m\u001b[36m(pid=29794)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29795)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=29795)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29795)\u001b[0m ring length: 247\n",
      "\u001b[2m\u001b[36m(pid=29795)\u001b[0m v_max: 4.522125250095652\n",
      "\u001b[2m\u001b[36m(pid=29795)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29796)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=29796)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29796)\u001b[0m ring length: 261\n",
      "\u001b[2m\u001b[36m(pid=29796)\u001b[0m v_max: 5.187261846097525\n",
      "\u001b[2m\u001b[36m(pid=29796)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29797)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=29797)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29797)\u001b[0m ring length: 251\n",
      "\u001b[2m\u001b[36m(pid=29797)\u001b[0m v_max: 4.71224180704279\n",
      "\u001b[2m\u001b[36m(pid=29797)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29795)\u001b[0m ./michael_files/emission_collection/singleagent_onelane_ring_20220929-1424511664479491.8774984-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=29796)\u001b[0m ./michael_files/emission_collection/singleagent_onelane_ring_20220929-1424511664479491.879637-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=29797)\u001b[0m ./michael_files/emission_collection/singleagent_onelane_ring_20220929-1424511664479491.8767006-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=29798)\u001b[0m ./michael_files/emission_collection/singleagent_onelane_ring_20220929-1424511664479491.8766656-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=29794)\u001b[0m ./michael_files/emission_collection/singleagent_onelane_ring_20220929-1424511664479491.877001-0_emission.csv ./michael_files/emission_collection/\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_76534_00000:\n",
      "  agent_timesteps_total: 495000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-09-29_14-31-42\n",
      "  done: false\n",
      "  episode_len_mean: 3000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2149.298719586105\n",
      "  episode_reward_mean: 471.03711175224316\n",
      "  episode_reward_min: -1443.4461642229949\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 165\n",
      "  experiment_id: 939d1d5e80b64de89564600c497902cc\n",
      "  hostname: michael-home\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 6.1035156249999995e-06\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -1.110382144632986\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010618305855696617\n",
      "          policy_loss: 6.656591914657314e-05\n",
      "          total_loss: 44.88157505100056\n",
      "          vf_explained_var: 0.8954672813415527\n",
      "          vf_loss: 44.88150848938247\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 495000\n",
      "    num_steps_sampled: 495000\n",
      "    num_steps_trained: 495000\n",
      "  iterations_since_restore: 33\n",
      "  node_ip: 192.168.1.76\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 27.251232876712326\n",
      "    ram_util_percent: 46.908493150684926\n",
      "  pid: 29799\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09118755673594332\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 32.84326526303156\n",
      "    mean_inference_ms: 1.8476458106764866\n",
      "    mean_raw_obs_processing_ms: 35.02162871158691\n",
      "  time_since_restore: 8533.200510263443\n",
      "  time_this_iter_s: 255.66114926338196\n",
      "  time_total_s: 8533.200510263443\n",
      "  timers:\n",
      "    learn_throughput: 298.271\n",
      "    learn_time_ms: 50289.789\n",
      "    sample_throughput: 73.155\n",
      "    sample_time_ms: 205045.389\n",
      "    update_time_ms: 3.137\n",
      "  timestamp: 1664479902\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 495000\n",
      "  training_iteration: 33\n",
      "  trial_id: '76534_00000'\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 14.7/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/4.72 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_4_7b3716726cd22aa6f9a7a4f69638d7d8, 0.0/1.0 CPU_group_0_7b3716726cd22aa6f9a7a4f69638d7d8, 0.0/1.0 CPU_group_3_7b3716726cd22aa6f9a7a4f69638d7d8, 0.0/1.0 CPU_group_2_7b3716726cd22aa6f9a7a4f69638d7d8, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_5_7b3716726cd22aa6f9a7a4f69638d7d8, 0.0/1.0 CPU_group_1_7b3716726cd22aa6f9a7a4f69638d7d8, 0.0/6.0 CPU_group_7b3716726cd22aa6f9a7a4f69638d7d8)\n",
      "Result logdir: /home/michael/ray_results/singleagent_onelane_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_76534_00000 | RUNNING  | 192.168.1.76:29799 |     33 |           8533.2 | 495000 |  471.037 |               2149.3 |             -1443.45 |               3000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=29795)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=29795)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29795)\u001b[0m ring length: 268\n",
      "\u001b[2m\u001b[36m(pid=29795)\u001b[0m v_max: 5.5194978538368025\n",
      "\u001b[2m\u001b[36m(pid=29795)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29796)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=29796)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29796)\u001b[0m ring length: 258\n",
      "\u001b[2m\u001b[36m(pid=29796)\u001b[0m v_max: 5.044800727535787\n",
      "\u001b[2m\u001b[36m(pid=29796)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29797)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=29797)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29797)\u001b[0m ring length: 230\n",
      "\u001b[2m\u001b[36m(pid=29797)\u001b[0m v_max: 3.7136148111012934\n",
      "\u001b[2m\u001b[36m(pid=29797)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29798)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=29798)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29798)\u001b[0m ring length: 243\n",
      "\u001b[2m\u001b[36m(pid=29798)\u001b[0m v_max: 4.331956438196479\n",
      "\u001b[2m\u001b[36m(pid=29798)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29794)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=29794)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=29794)\u001b[0m ring length: 244\n",
      "\u001b[2m\u001b[36m(pid=29794)\u001b[0m v_max: 4.379503211387676\n",
      "\u001b[2m\u001b[36m(pid=29794)\u001b[0m -----------------------\n"
     ]
    }
   ],
   "source": [
    "!python examples/train.py singleagent_ring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca1e948b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
