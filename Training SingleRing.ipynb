{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1375af72",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'num_workers': 5, 'num_envs_per_worker': 1, 'create_env_on_driver': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'gamma': 0.999, 'lr': 5e-05, 'train_batch_size': 15000, 'model': {'_use_default_native_models': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': [[16, [8, 8], 4], [32, [4, 4], 2], [256, [11, 11], 1]], 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'num_framestacks': 'auto', 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1, 'framestack': True}, 'optimizer': {}, 'horizon': 3000, 'soft_horizon': False, 'no_done_at_end': False, 'env': None, 'observation_space': None, 'action_space': None, 'env_config': {}, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'tf', 'eager_tracing': False, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_num_episodes': 10, 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'collect_metrics_timeout': 180, 'metrics_smoothing_episodes': 100, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'min_iter_time_s': 0, 'timesteps_per_iteration': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {}, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, 'simple_optimizer': -1, 'monitor': -1, 'use_critic': True, 'use_gae': True, 'lambda': 0.97, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 10, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.02, 'vf_share_layers': -1}\n",
      "2022-10-08 11:06:21,535\tINFO services.py:1247 -- View the Ray dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265\u001b[39m\u001b[22m\n",
      "== Status ==\n",
      "Memory usage on this node: 7.9/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 0/6 CPUs, 0/6 GPUs, 0.0/7.23 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_onelane_ring\n",
      "Number of trials: 1/1 (1 PENDING)\n",
      "+-----------------------------------------+----------+-------+\n",
      "| Trial name                              | status   | loc   |\n",
      "|-----------------------------------------+----------+-------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_26f87_00000 | PENDING  |       |\n",
      "+-----------------------------------------+----------+-------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=307)\u001b[0m 2022-10-08 11:06:23,547\tINFO trainer.py:706 -- Tip: set framework=tfe or the --eager flag to enable TensorFlow eager execution\n",
      "\u001b[2m\u001b[36m(pid=307)\u001b[0m 2022-10-08 11:06:23,547\tINFO trainer.py:720 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m WARNING:tensorflow:From /home/michael-lab/anaconda3/envs/flow/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m If using Keras pass *_constraint arguments to layers.\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m WARNING:tensorflow:From /home/michael-lab/anaconda3/envs/flow/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m If using Keras pass *_constraint arguments to layers.\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m WARNING:tensorflow:From /home/michael-lab/anaconda3/envs/flow/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m If using Keras pass *_constraint arguments to layers.\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m WARNING:tensorflow:From /home/michael-lab/anaconda3/envs/flow/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m If using Keras pass *_constraint arguments to layers.\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m WARNING:tensorflow:From /home/michael-lab/anaconda3/envs/flow/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m If using Keras pass *_constraint arguments to layers.\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m WARNING:tensorflow:From /home/michael-lab/anaconda3/envs/flow/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m If using Keras pass *_constraint arguments to layers.\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m WARNING:tensorflow:From /home/michael-lab/anaconda3/envs/flow/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m If using Keras pass *_constraint arguments to layers.\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m WARNING:tensorflow:From /home/michael-lab/anaconda3/envs/flow/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m If using Keras pass *_constraint arguments to layers.\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m WARNING:tensorflow:From /home/michael-lab/anaconda3/envs/flow/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m If using Keras pass *_constraint arguments to layers.\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m WARNING:tensorflow:From /home/michael-lab/anaconda3/envs/flow/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m If using Keras pass *_constraint arguments to layers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m WARNING:tensorflow:From /home/michael-lab/anaconda3/envs/flow/lib/python3.7/site-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m WARNING:tensorflow:From /home/michael-lab/anaconda3/envs/flow/lib/python3.7/site-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m WARNING:tensorflow:From /home/michael-lab/anaconda3/envs/flow/lib/python3.7/site-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m WARNING:tensorflow:From /home/michael-lab/anaconda3/envs/flow/lib/python3.7/site-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m WARNING:tensorflow:From /home/michael-lab/anaconda3/envs/flow/lib/python3.7/site-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m WARNING:tensorflow:From /home/michael-lab/anaconda3/envs/flow/lib/python3.7/site-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m WARNING:tensorflow:From /home/michael-lab/anaconda3/envs/flow/lib/python3.7/site-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m WARNING:tensorflow:From /home/michael-lab/anaconda3/envs/flow/lib/python3.7/site-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m WARNING:tensorflow:From /home/michael-lab/anaconda3/envs/flow/lib/python3.7/site-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m WARNING:tensorflow:From /home/michael-lab/anaconda3/envs/flow/lib/python3.7/site-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "\u001b[2m\u001b[36m(pid=307)\u001b[0m WARNING:tensorflow:From /home/michael-lab/anaconda3/envs/flow/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=307)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=307)\u001b[0m If using Keras pass *_constraint arguments to layers.\n",
      "\u001b[2m\u001b[36m(pid=307)\u001b[0m WARNING:tensorflow:From /home/michael-lab/anaconda3/envs/flow/lib/python3.7/site-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=307)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=307)\u001b[0m Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "\u001b[2m\u001b[36m(pid=307)\u001b[0m 2022-10-08 11:06:28,083\tWARNING util.py:55 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m ring length: 230\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m v_max: 3.7136148111012934\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m ring length: 270\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m v_max: 5.6143732387852054\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m ring length: 268\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m v_max: 5.5194978538368025\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m ring length: 223\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m v_max: 3.3805185755587788\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m ring length: 227\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m v_max: 3.570869368805752\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m ring length: 262\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m v_max: 5.234739483008334\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m ring length: 259\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m v_max: 5.092292348291853\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m ring length: 235\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m v_max: 3.9514850725282584\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m ring length: 232\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m v_max: 3.8087690783250063\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m ring length: 249\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m v_max: 4.617190445131122\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m Could not connect to TraCI server at localhost:57509 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m Could not connect to TraCI server at localhost:54035 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m Could not connect to TraCI server at localhost:36289 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m Could not connect to TraCI server at localhost:34795 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m Could not connect to TraCI server at localhost:34447 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m Could not connect to TraCI server at localhost:34447 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 3 seconds\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m Could not connect to TraCI server at localhost:57509 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 3 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m Could not connect to TraCI server at localhost:54035 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 3 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m Could not connect to TraCI server at localhost:36289 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 3 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m Could not connect to TraCI server at localhost:34795 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 3 seconds\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_26f87_00000:\n",
      "  agent_timesteps_total: 15000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-08_11-07-09\n",
      "  done: false\n",
      "  episode_len_mean: 3000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -4451.277460946896\n",
      "  episode_reward_mean: -4871.770239477724\n",
      "  episode_reward_min: -5227.059352675403\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 5\n",
      "  experiment_id: edcbe001b14143f4bf76a6a2914a7bd0\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.341774344444275\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.007584212347865105\n",
      "          model: {}\n",
      "          policy_loss: -0.004909635987132788\n",
      "          total_loss: 1369.8017578125\n",
      "          vf_explained_var: -0.011073783971369267\n",
      "          vf_loss: 1369.80517578125\n",
      "    num_agent_steps_sampled: 15000\n",
      "    num_agent_steps_trained: 15000\n",
      "    num_steps_sampled: 15000\n",
      "    num_steps_trained: 15000\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 26.32833333333333\n",
      "    ram_util_percent: 30.651666666666678\n",
      "  pid: 307\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07771617847456609\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 6.794011501501656\n",
      "    mean_inference_ms: 0.575144121703288\n",
      "    mean_raw_obs_processing_ms: 5.295433929784025\n",
      "  time_since_restore: 41.61117625236511\n",
      "  time_this_iter_s: 41.61117625236511\n",
      "  time_total_s: 41.61117625236511\n",
      "  timers:\n",
      "    learn_throughput: 6118.003\n",
      "    learn_time_ms: 2451.78\n",
      "    load_throughput: 427492.916\n",
      "    load_time_ms: 35.088\n",
      "    sample_throughput: 383.766\n",
      "    sample_time_ms: 39086.363\n",
      "    update_time_ms: 1.287\n",
      "  timestamp: 1665245229\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 15000\n",
      "  training_iteration: 1\n",
      "  trial_id: 26f87_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 9.6/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.23 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_onelane_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |    ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_26f87_00000 | RUNNING  | 141.225.10.229:307 |      1 |          41.6112 | 15000 | -4871.77 |             -4451.28 |             -5227.06 |               3000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=307)\u001b[0m 2022-10-08 11:07:09,696\tWARNING ppo.py:242 -- The magnitude of your environment rewards are more than 487.0x the scale of `vf_clip_param`. This means that it will take more than 487.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m ring length: 229\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m v_max: 3.666034803266416\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m ring length: 224\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m v_max: 3.4281086136538996\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m ring length: 233\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m v_max: 3.856343201216534\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m ring length: 240\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m v_max: 4.189299083856172\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m ring length: 268\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m v_max: 5.5194978538368025\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 1 seconds\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_26f87_00000:\n",
      "  agent_timesteps_total: 30000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-08_11-07-35\n",
      "  done: false\n",
      "  episode_len_mean: 3000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -4253.512683653694\n",
      "  episode_reward_mean: -4830.491924483691\n",
      "  episode_reward_min: -5227.059352675403\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 10\n",
      "  experiment_id: edcbe001b14143f4bf76a6a2914a7bd0\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.10000000149011612\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.249494194984436\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016148926690220833\n",
      "          model: {}\n",
      "          policy_loss: -0.007274187169969082\n",
      "          total_loss: 1286.61376953125\n",
      "          vf_explained_var: 0.005556122865527868\n",
      "          vf_loss: 1286.619384765625\n",
      "    num_agent_steps_sampled: 30000\n",
      "    num_agent_steps_trained: 30000\n",
      "    num_steps_sampled: 30000\n",
      "    num_steps_trained: 30000\n",
      "  iterations_since_restore: 2\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 27.258333333333333\n",
      "    ram_util_percent: 29.988888888888887\n",
      "  pid: 307\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07661464104117958\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 5.984540309263639\n",
      "    mean_inference_ms: 0.5644528726106277\n",
      "    mean_raw_obs_processing_ms: 4.746694715401001\n",
      "  time_since_restore: 66.91795325279236\n",
      "  time_this_iter_s: 25.306777000427246\n",
      "  time_total_s: 66.91795325279236\n",
      "  timers:\n",
      "    learn_throughput: 6553.791\n",
      "    learn_time_ms: 2288.752\n",
      "    load_throughput: 826138.271\n",
      "    load_time_ms: 18.157\n",
      "    sample_throughput: 481.875\n",
      "    sample_time_ms: 31128.423\n",
      "    update_time_ms: 1.812\n",
      "  timestamp: 1665245255\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 30000\n",
      "  training_iteration: 2\n",
      "  trial_id: 26f87_00000\n",
      "  \n",
      "\u001b[2m\u001b[36m(pid=307)\u001b[0m 2022-10-08 11:07:35,056\tWARNING ppo.py:242 -- The magnitude of your environment rewards are more than 483.0x the scale of `vf_clip_param`. This means that it will take more than 483.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "== Status ==\n",
      "Memory usage on this node: 9.2/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.23 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_2_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_0_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_1_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_3_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_5_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_4_25daa8673b2c24aecf80a7fc79df4e93, 0.0/6.0 CPU_group_25daa8673b2c24aecf80a7fc79df4e93)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_onelane_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |    ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_26f87_00000 | RUNNING  | 141.225.10.229:307 |      2 |           66.918 | 30000 | -4830.49 |             -4253.51 |             -5227.06 |               3000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m ring length: 229\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m v_max: 3.666034803266416\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m ring length: 262\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m v_max: 5.234739483008334\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m ring length: 223\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m v_max: 3.3805185755587788\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m ring length: 252\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m v_max: 4.759762049824181\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m ring length: 234\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m v_max: 3.903915223349057\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 1 seconds\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_26f87_00000:\n",
      "  agent_timesteps_total: 45000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-08_11-08-00\n",
      "  done: false\n",
      "  episode_len_mean: 3000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3278.176325684843\n",
      "  episode_reward_mean: -4494.425412059954\n",
      "  episode_reward_min: -5227.059352675403\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 15\n",
      "  experiment_id: edcbe001b14143f4bf76a6a2914a7bd0\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.10000000149011612\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.0543112754821777\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01967926323413849\n",
      "          model: {}\n",
      "          policy_loss: -0.0078841932117939\n",
      "          total_loss: 754.5277709960938\n",
      "          vf_explained_var: -0.001294097863137722\n",
      "          vf_loss: 754.5335693359375\n",
      "    num_agent_steps_sampled: 45000\n",
      "    num_agent_steps_trained: 45000\n",
      "    num_steps_sampled: 45000\n",
      "    num_steps_trained: 45000\n",
      "  iterations_since_restore: 3\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 24.38611111111111\n",
      "    ram_util_percent: 29.55833333333333\n",
      "  pid: 307\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07517656985763516\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 5.515110906309667\n",
      "    mean_inference_ms: 0.5523494116031133\n",
      "    mean_raw_obs_processing_ms: 4.459282890651863\n",
      "  time_since_restore: 91.86627650260925\n",
      "  time_this_iter_s: 24.948323249816895\n",
      "  time_total_s: 91.86627650260925\n",
      "  timers:\n",
      "    learn_throughput: 6504.678\n",
      "    learn_time_ms: 2306.033\n",
      "    load_throughput: 1202663.982\n",
      "    load_time_ms: 12.472\n",
      "    sample_throughput: 530.291\n",
      "    sample_time_ms: 28286.354\n",
      "    update_time_ms: 1.567\n",
      "  timestamp: 1665245280\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 45000\n",
      "  training_iteration: 3\n",
      "  trial_id: 26f87_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 9.2/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.23 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_5_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_0_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_3_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_1_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 accelerator_type:G, 0.0/6.0 CPU_group_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_2_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_4_25daa8673b2c24aecf80a7fc79df4e93)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_onelane_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |    ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_26f87_00000 | RUNNING  | 141.225.10.229:307 |      3 |          91.8663 | 45000 | -4494.43 |             -3278.18 |             -5227.06 |               3000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=307)\u001b[0m 2022-10-08 11:08:00,033\tWARNING ppo.py:242 -- The magnitude of your environment rewards are more than 449.0x the scale of `vf_clip_param`. This means that it will take more than 449.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m ring length: 269\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m v_max: 5.566938458220921\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m ring length: 259\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m v_max: 5.092292348291853\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m ring length: 221\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m v_max: 3.285334164169417\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m ring length: 229\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m v_max: 3.666034803266416\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m ring length: 227\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m v_max: 3.570869368805752\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 1 seconds\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_26f87_00000:\n",
      "  agent_timesteps_total: 60000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-08_11-08-24\n",
      "  done: false\n",
      "  episode_len_mean: 3000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -1434.4733010522928\n",
      "  episode_reward_mean: -4126.488410151578\n",
      "  episode_reward_min: -5227.059352675403\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 20\n",
      "  experiment_id: edcbe001b14143f4bf76a6a2914a7bd0\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.10000000149011612\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.8779413104057312\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01506874617189169\n",
      "          model: {}\n",
      "          policy_loss: -0.005208146758377552\n",
      "          total_loss: 494.8333435058594\n",
      "          vf_explained_var: -0.0022465260699391365\n",
      "          vf_loss: 494.8370361328125\n",
      "    num_agent_steps_sampled: 60000\n",
      "    num_agent_steps_trained: 60000\n",
      "    num_steps_sampled: 60000\n",
      "    num_steps_trained: 60000\n",
      "  iterations_since_restore: 4\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 24.868571428571432\n",
      "    ram_util_percent: 29.65714285714286\n",
      "  pid: 307\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07404071439738298\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 5.205088118312567\n",
      "    mean_inference_ms: 0.5431260232869615\n",
      "    mean_raw_obs_processing_ms: 4.261580388067466\n",
      "  time_since_restore: 116.5483078956604\n",
      "  time_this_iter_s: 24.682031393051147\n",
      "  time_total_s: 116.5483078956604\n",
      "  timers:\n",
      "    learn_throughput: 6600.493\n",
      "    learn_time_ms: 2272.558\n",
      "    load_throughput: 1496798.568\n",
      "    load_time_ms: 10.021\n",
      "    sample_throughput: 558.88\n",
      "    sample_time_ms: 26839.372\n",
      "    update_time_ms: 1.447\n",
      "  timestamp: 1665245304\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 60000\n",
      "  training_iteration: 4\n",
      "  trial_id: 26f87_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Memory usage on this node: 9.2/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.23 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_3_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_4_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_0_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_5_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_2_25daa8673b2c24aecf80a7fc79df4e93, 0.0/6.0 CPU_group_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_1_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 accelerator_type:G)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_onelane_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |    ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_26f87_00000 | RUNNING  | 141.225.10.229:307 |      4 |          116.548 | 60000 | -4126.49 |             -1434.47 |             -5227.06 |               3000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=307)\u001b[0m 2022-10-08 11:08:24,734\tWARNING ppo.py:242 -- The magnitude of your environment rewards are more than 413.0x the scale of `vf_clip_param`. This means that it will take more than 413.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m ring length: 237\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m v_max: 4.04661795519353\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m ring length: 251\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m v_max: 4.71224180704279\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m ring length: 232\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m v_max: 3.8087690783250063\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m ring length: 250\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m v_max: 4.664717904914125\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m ring length: 229\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m v_max: 3.666034803266416\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 1 seconds\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_26f87_00000:\n",
      "  agent_timesteps_total: 75000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-08_11-08-50\n",
      "  done: false\n",
      "  episode_len_mean: 3000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -380.9965406985417\n",
      "  episode_reward_mean: -3661.2843195558025\n",
      "  episode_reward_min: -5227.059352675403\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 25\n",
      "  experiment_id: edcbe001b14143f4bf76a6a2914a7bd0\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.10000000149011612\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.6273290514945984\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01933143101632595\n",
      "          model: {}\n",
      "          policy_loss: -0.005876941606402397\n",
      "          total_loss: 228.4927978515625\n",
      "          vf_explained_var: -0.0012384632136672735\n",
      "          vf_loss: 228.4967803955078\n",
      "    num_agent_steps_sampled: 75000\n",
      "    num_agent_steps_trained: 75000\n",
      "    num_steps_sampled: 75000\n",
      "    num_steps_trained: 75000\n",
      "  iterations_since_restore: 5\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 24.610810810810808\n",
      "    ram_util_percent: 29.654054054054054\n",
      "  pid: 307\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07319424462662133\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 4.987859114328826\n",
      "    mean_inference_ms: 0.5363613260552949\n",
      "    mean_raw_obs_processing_ms: 4.1272362846246065\n",
      "  time_since_restore: 142.17425179481506\n",
      "  time_this_iter_s: 25.625943899154663\n",
      "  time_total_s: 142.17425179481506\n",
      "  timers:\n",
      "    learn_throughput: 6567.403\n",
      "    learn_time_ms: 2284.008\n",
      "    load_throughput: 1803774.147\n",
      "    load_time_ms: 8.316\n",
      "    sample_throughput: 574.082\n",
      "    sample_time_ms: 26128.658\n",
      "    update_time_ms: 1.417\n",
      "  timestamp: 1665245330\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 75000\n",
      "  training_iteration: 5\n",
      "  trial_id: 26f87_00000\n",
      "  \n",
      "\u001b[2m\u001b[36m(pid=307)\u001b[0m 2022-10-08 11:08:50,401\tWARNING ppo.py:242 -- The magnitude of your environment rewards are more than 366.0x the scale of `vf_clip_param`. This means that it will take more than 366.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "== Status ==\n",
      "Memory usage on this node: 9.2/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.23 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_2_25daa8673b2c24aecf80a7fc79df4e93, 0.0/6.0 CPU_group_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_4_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_3_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_0_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_5_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_1_25daa8673b2c24aecf80a7fc79df4e93)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_onelane_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |    ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_26f87_00000 | RUNNING  | 141.225.10.229:307 |      5 |          142.174 | 75000 | -3661.28 |             -380.997 |             -5227.06 |               3000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m ring length: 254\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m v_max: 4.854791141191876\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m ring length: 257\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m v_max: 4.997304682316517\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m ring length: 258\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m v_max: 5.044800727535787\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m ring length: 226\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m v_max: 3.5232840694769565\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m ring length: 220\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m v_max: 3.2377399006821497\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 1 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 1 seconds\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_26f87_00000:\n",
      "  agent_timesteps_total: 90000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-08_11-09-15\n",
      "  done: false\n",
      "  episode_len_mean: 3000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -380.9965406985417\n",
      "  episode_reward_mean: -3276.8286467113185\n",
      "  episode_reward_min: -5227.059352675403\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 30\n",
      "  experiment_id: edcbe001b14143f4bf76a6a2914a7bd0\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.10000000149011612\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.4705983102321625\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.025821451097726822\n",
      "          model: {}\n",
      "          policy_loss: -0.00890158861875534\n",
      "          total_loss: 148.8064727783203\n",
      "          vf_explained_var: -0.0010976934572681785\n",
      "          vf_loss: 148.81280517578125\n",
      "    num_agent_steps_sampled: 90000\n",
      "    num_agent_steps_trained: 90000\n",
      "    num_steps_sampled: 90000\n",
      "    num_steps_trained: 90000\n",
      "  iterations_since_restore: 6\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 24.70277777777778\n",
      "    ram_util_percent: 29.57222222222222\n",
      "  pid: 307\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07253199647621944\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 4.825929896047159\n",
      "    mean_inference_ms: 0.5310913989683318\n",
      "    mean_raw_obs_processing_ms: 4.029542059040027\n",
      "  time_since_restore: 167.4935610294342\n",
      "  time_this_iter_s: 25.31930923461914\n",
      "  time_total_s: 167.4935610294342\n",
      "  timers:\n",
      "    learn_throughput: 6551.194\n",
      "    learn_time_ms: 2289.659\n",
      "    load_throughput: 2107878.761\n",
      "    load_time_ms: 7.116\n",
      "    sample_throughput: 585.803\n",
      "    sample_time_ms: 25605.896\n",
      "    update_time_ms: 1.355\n",
      "  timestamp: 1665245355\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 90000\n",
      "  training_iteration: 6\n",
      "  trial_id: 26f87_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 9.2/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.23 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_5_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_1_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_0_25daa8673b2c24aecf80a7fc79df4e93, 0.0/6.0 CPU_group_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_4_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_3_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_2_25daa8673b2c24aecf80a7fc79df4e93)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_onelane_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |    ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_26f87_00000 | RUNNING  | 141.225.10.229:307 |      6 |          167.494 | 90000 | -3276.83 |             -380.997 |             -5227.06 |               3000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=307)\u001b[0m 2022-10-08 11:09:15,749\tWARNING ppo.py:242 -- The magnitude of your environment rewards are more than 328.0x the scale of `vf_clip_param`. This means that it will take more than 328.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m ring length: 260\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m v_max: 5.139779427502188\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m ring length: 229\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m v_max: 3.666034803266416\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m ring length: 235\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m v_max: 3.9514850725282584\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m ring length: 246\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m v_max: 4.474587708995648\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m ring length: 250\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m v_max: 4.664717904914125\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 1 seconds\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_26f87_00000:\n",
      "  agent_timesteps_total: 105000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-08_11-09-40\n",
      "  done: false\n",
      "  episode_len_mean: 3000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 394.6188524098183\n",
      "  episode_reward_mean: -2931.113242200976\n",
      "  episode_reward_min: -5227.059352675403\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 35\n",
      "  experiment_id: edcbe001b14143f4bf76a6a2914a7bd0\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.10000000149011612\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.3524196743965149\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010033023543655872\n",
      "          model: {}\n",
      "          policy_loss: -0.002121981931850314\n",
      "          total_loss: 200.10047912597656\n",
      "          vf_explained_var: -0.0003548931854311377\n",
      "          vf_loss: 200.1016082763672\n",
      "    num_agent_steps_sampled: 105000\n",
      "    num_agent_steps_trained: 105000\n",
      "    num_steps_sampled: 105000\n",
      "    num_steps_trained: 105000\n",
      "  iterations_since_restore: 7\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 25.36857142857143\n",
      "    ram_util_percent: 29.52857142857143\n",
      "  pid: 307\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07197192320864576\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 4.699133283837605\n",
      "    mean_inference_ms: 0.5267667014197618\n",
      "    mean_raw_obs_processing_ms: 3.9502795223641947\n",
      "  time_since_restore: 192.12981367111206\n",
      "  time_this_iter_s: 24.636252641677856\n",
      "  time_total_s: 192.12981367111206\n",
      "  timers:\n",
      "    learn_throughput: 6668.363\n",
      "    learn_time_ms: 2249.428\n",
      "    load_throughput: 2392747.464\n",
      "    load_time_ms: 6.269\n",
      "    sample_throughput: 595.725\n",
      "    sample_time_ms: 25179.41\n",
      "    update_time_ms: 1.336\n",
      "  timestamp: 1665245380\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 105000\n",
      "  training_iteration: 7\n",
      "  trial_id: 26f87_00000\n",
      "  \n",
      "\u001b[2m\u001b[36m(pid=307)\u001b[0m 2022-10-08 11:09:40,406\tWARNING ppo.py:242 -- The magnitude of your environment rewards are more than 293.0x the scale of `vf_clip_param`. This means that it will take more than 293.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "== Status ==\n",
      "Memory usage on this node: 9.2/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.23 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_0_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_4_25daa8673b2c24aecf80a7fc79df4e93, 0.0/6.0 CPU_group_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_5_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_3_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_2_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_1_25daa8673b2c24aecf80a7fc79df4e93)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_onelane_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_26f87_00000 | RUNNING  | 141.225.10.229:307 |      7 |           192.13 | 105000 | -2931.11 |              394.619 |             -5227.06 |               3000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m ring length: 224\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m v_max: 3.4281086136538996\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m ring length: 257\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m v_max: 4.997304682316517\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m ring length: 224\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m v_max: 3.4281086136538996\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m ring length: 258\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m v_max: 5.044800727535787\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m ring length: 234\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m v_max: 3.903915223349057\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m Could not connect to TraCI server at localhost:54035 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m Could not connect to TraCI server at localhost:36289 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m Could not connect to TraCI server at localhost:34795 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m Could not connect to TraCI server at localhost:34447 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m Could not connect to TraCI server at localhost:57509 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m Could not connect to TraCI server at localhost:34795 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 3 seconds\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m Could not connect to TraCI server at localhost:34447 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 3 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m Could not connect to TraCI server at localhost:54035 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 3 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m Could not connect to TraCI server at localhost:36289 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 3 seconds\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m Could not connect to TraCI server at localhost:57509 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 3 seconds\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_26f87_00000:\n",
      "  agent_timesteps_total: 120000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-08_11-10-11\n",
      "  done: false\n",
      "  episode_len_mean: 3000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 394.6188524098183\n",
      "  episode_reward_mean: -2669.026640889981\n",
      "  episode_reward_min: -5227.059352675403\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 40\n",
      "  experiment_id: edcbe001b14143f4bf76a6a2914a7bd0\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.10000000149011612\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.27780571579933167\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.02375359833240509\n",
      "          model: {}\n",
      "          policy_loss: -0.006877324543893337\n",
      "          total_loss: 167.58682250976562\n",
      "          vf_explained_var: -0.0003093083796557039\n",
      "          vf_loss: 167.59132385253906\n",
      "    num_agent_steps_sampled: 120000\n",
      "    num_agent_steps_trained: 120000\n",
      "    num_steps_sampled: 120000\n",
      "    num_steps_trained: 120000\n",
      "  iterations_since_restore: 8\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.095454545454547\n",
      "    ram_util_percent: 29.513636363636355\n",
      "  pid: 307\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07150777177227122\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 4.596481665328148\n",
      "    mean_inference_ms: 0.5231757275625231\n",
      "    mean_raw_obs_processing_ms: 3.9143586331720526\n",
      "  time_since_restore: 222.79076099395752\n",
      "  time_this_iter_s: 30.66094732284546\n",
      "  time_total_s: 222.79076099395752\n",
      "  timers:\n",
      "    learn_throughput: 6631.754\n",
      "    learn_time_ms: 2261.845\n",
      "    load_throughput: 2667962.598\n",
      "    load_time_ms: 5.622\n",
      "    sample_throughput: 586.629\n",
      "    sample_time_ms: 25569.828\n",
      "    update_time_ms: 1.308\n",
      "  timestamp: 1665245411\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 120000\n",
      "  training_iteration: 8\n",
      "  trial_id: 26f87_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 9.2/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.23 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_0_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_1_25daa8673b2c24aecf80a7fc79df4e93, 0.0/6.0 CPU_group_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_5_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_4_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_3_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_2_25daa8673b2c24aecf80a7fc79df4e93)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_onelane_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_26f87_00000 | RUNNING  | 141.225.10.229:307 |      8 |          222.791 | 120000 | -2669.03 |              394.619 |             -5227.06 |               3000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=307)\u001b[0m 2022-10-08 11:10:11,096\tWARNING ppo.py:242 -- The magnitude of your environment rewards are more than 267.0x the scale of `vf_clip_param`. This means that it will take more than 267.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m ring length: 234\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m v_max: 3.903915223349057\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m ring length: 265\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m v_max: 5.37714246265477\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m ring length: 236\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m v_max: 3.999052674951912\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m ring length: 226\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m v_max: 3.5232840694769565\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m ring length: 233\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m v_max: 3.856343201216534\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 1 seconds\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_26f87_00000:\n",
      "  agent_timesteps_total: 135000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-08_11-10-36\n",
      "  done: false\n",
      "  episode_len_mean: 3000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 394.6188524098183\n",
      "  episode_reward_mean: -2434.1935187684535\n",
      "  episode_reward_min: -5227.059352675403\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 45\n",
      "  experiment_id: edcbe001b14143f4bf76a6a2914a7bd0\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.10000000149011612\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.1865028738975525\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010962553322315216\n",
      "          model: {}\n",
      "          policy_loss: -0.00290653295814991\n",
      "          total_loss: 182.6697998046875\n",
      "          vf_explained_var: -0.00011125690798508003\n",
      "          vf_loss: 182.67166137695312\n",
      "    num_agent_steps_sampled: 135000\n",
      "    num_agent_steps_trained: 135000\n",
      "    num_steps_sampled: 135000\n",
      "    num_steps_trained: 135000\n",
      "  iterations_since_restore: 9\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 24.902777777777782\n",
      "    ram_util_percent: 29.5\n",
      "  pid: 307\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07112601524655845\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 4.512019086800138\n",
      "    mean_inference_ms: 0.5202145936889301\n",
      "    mean_raw_obs_processing_ms: 3.8810717629773985\n",
      "  time_since_restore: 248.0309615135193\n",
      "  time_this_iter_s: 25.240200519561768\n",
      "  time_total_s: 248.0309615135193\n",
      "  timers:\n",
      "    learn_throughput: 6701.413\n",
      "    learn_time_ms: 2238.334\n",
      "    load_throughput: 2918566.259\n",
      "    load_time_ms: 5.14\n",
      "    sample_throughput: 592.779\n",
      "    sample_time_ms: 25304.56\n",
      "    update_time_ms: 1.277\n",
      "  timestamp: 1665245436\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 135000\n",
      "  training_iteration: 9\n",
      "  trial_id: 26f87_00000\n",
      "  \n",
      "\u001b[2m\u001b[36m(pid=307)\u001b[0m 2022-10-08 11:10:36,370\tWARNING ppo.py:242 -- The magnitude of your environment rewards are more than 243.0x the scale of `vf_clip_param`. This means that it will take more than 243.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "== Status ==\n",
      "Memory usage on this node: 9.2/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.23 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_2_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_3_25daa8673b2c24aecf80a7fc79df4e93, 0.0/6.0 CPU_group_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_5_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_4_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_0_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_1_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 accelerator_type:G)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_onelane_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_26f87_00000 | RUNNING  | 141.225.10.229:307 |      9 |          248.031 | 135000 | -2434.19 |              394.619 |             -5227.06 |               3000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m ring length: 237\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m v_max: 4.04661795519353\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m ring length: 227\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m v_max: 3.570869368805752\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m ring length: 234\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m v_max: 3.903915223349057\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m ring length: 242\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m v_max: 4.2844067680020546\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m ring length: 238\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m v_max: 4.094180836186086\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 1 seconds\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_26f87_00000:\n",
      "  agent_timesteps_total: 150000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-08_11-11-01\n",
      "  done: false\n",
      "  episode_len_mean: 3000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 836.7291370672203\n",
      "  episode_reward_mean: -2244.093677732202\n",
      "  episode_reward_min: -5227.059352675403\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 50\n",
      "  experiment_id: edcbe001b14143f4bf76a6a2914a7bd0\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.10000000149011612\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.09252399951219559\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.022129399701952934\n",
      "          model: {}\n",
      "          policy_loss: -0.005880167242139578\n",
      "          total_loss: 197.5752410888672\n",
      "          vf_explained_var: -7.184143032645807e-05\n",
      "          vf_loss: 197.5789031982422\n",
      "    num_agent_steps_sampled: 150000\n",
      "    num_agent_steps_trained: 150000\n",
      "    num_steps_sampled: 150000\n",
      "    num_steps_trained: 150000\n",
      "  iterations_since_restore: 10\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 24.458333333333332\n",
      "    ram_util_percent: 29.450000000000003\n",
      "  pid: 307\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0707990646023774\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 4.440924521271298\n",
      "    mean_inference_ms: 0.5176797140421295\n",
      "    mean_raw_obs_processing_ms: 3.8505391936020943\n",
      "  time_since_restore: 273.3842542171478\n",
      "  time_this_iter_s: 25.35329270362854\n",
      "  time_total_s: 273.3842542171478\n",
      "  timers:\n",
      "    learn_throughput: 6710.85\n",
      "    learn_time_ms: 2235.186\n",
      "    load_throughput: 3158821.314\n",
      "    load_time_ms: 4.749\n",
      "    sample_throughput: 597.9\n",
      "    sample_time_ms: 25087.818\n",
      "    update_time_ms: 1.258\n",
      "  timestamp: 1665245461\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 150000\n",
      "  training_iteration: 10\n",
      "  trial_id: 26f87_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 9.2/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.23 GiB heap, 0.0/14.65 GiB objects (0.0/6.0 CPU_group_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_2_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_1_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_4_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_3_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_5_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_0_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 accelerator_type:G)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_onelane_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_26f87_00000 | RUNNING  | 141.225.10.229:307 |     10 |          273.384 | 150000 | -2244.09 |              836.729 |             -5227.06 |               3000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=307)\u001b[0m 2022-10-08 11:11:01,750\tWARNING ppo.py:242 -- The magnitude of your environment rewards are more than 224.0x the scale of `vf_clip_param`. This means that it will take more than 224.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m ring length: 225\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m v_max: 3.4756971311168834\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m ring length: 247\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m v_max: 4.522125250095652\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m ring length: 222\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m v_max: 3.3329270738617227\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m ring length: 252\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m v_max: 4.759762049824181\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m ring length: 239\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m v_max: 4.141741239205832\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 1 seconds\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_26f87_00000:\n",
      "  agent_timesteps_total: 165000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-08_11-11-26\n",
      "  done: false\n",
      "  episode_len_mean: 3000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 836.7291370672203\n",
      "  episode_reward_mean: -2079.7323944721384\n",
      "  episode_reward_min: -5227.059352675403\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 55\n",
      "  experiment_id: edcbe001b14143f4bf76a6a2914a7bd0\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.10000000149011612\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.035413019359111786\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01666215807199478\n",
      "          model: {}\n",
      "          policy_loss: -0.0042555551044642925\n",
      "          total_loss: 158.869384765625\n",
      "          vf_explained_var: -3.3262447686865926e-05\n",
      "          vf_loss: 158.87197875976562\n",
      "    num_agent_steps_sampled: 165000\n",
      "    num_agent_steps_trained: 165000\n",
      "    num_steps_sampled: 165000\n",
      "    num_steps_trained: 165000\n",
      "  iterations_since_restore: 11\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 24.41111111111111\n",
      "    ram_util_percent: 29.46944444444445\n",
      "  pid: 307\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07052227246912209\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 4.380167209525007\n",
      "    mean_inference_ms: 0.5155073545381734\n",
      "    mean_raw_obs_processing_ms: 3.8207740109653843\n",
      "  time_since_restore: 298.3936023712158\n",
      "  time_this_iter_s: 25.009348154067993\n",
      "  time_total_s: 298.3936023712158\n",
      "  timers:\n",
      "    learn_throughput: 6828.424\n",
      "    learn_time_ms: 2196.7\n",
      "    load_throughput: 11096629.451\n",
      "    load_time_ms: 1.352\n",
      "    sample_throughput: 639.039\n",
      "    sample_time_ms: 23472.762\n",
      "    update_time_ms: 1.231\n",
      "  timestamp: 1665245486\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 165000\n",
      "  training_iteration: 11\n",
      "  trial_id: 26f87_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 9.2/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.23 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_4_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_0_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_2_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_5_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_1_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_3_25daa8673b2c24aecf80a7fc79df4e93, 0.0/6.0 CPU_group_25daa8673b2c24aecf80a7fc79df4e93)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_onelane_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_26f87_00000 | RUNNING  | 141.225.10.229:307 |     11 |          298.394 | 165000 | -2079.73 |              836.729 |             -5227.06 |               3000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=307)\u001b[0m 2022-10-08 11:11:26,787\tWARNING ppo.py:242 -- The magnitude of your environment rewards are more than 208.0x the scale of `vf_clip_param`. This means that it will take more than 208.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m ring length: 221\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m v_max: 3.285334164169417\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m ring length: 242\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m v_max: 4.2844067680020546\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m ring length: 220\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m v_max: 3.2377399006821497\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m ring length: 270\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m v_max: 5.6143732387852054\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m ring length: 259\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m v_max: 5.092292348291853\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 1 seconds\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_26f87_00000:\n",
      "  agent_timesteps_total: 180000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-08_11-11-51\n",
      "  done: false\n",
      "  episode_len_mean: 3000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 836.7291370672203\n",
      "  episode_reward_mean: -1908.079843682146\n",
      "  episode_reward_min: -5227.059352675403\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 60\n",
      "  experiment_id: edcbe001b14143f4bf76a6a2914a7bd0\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.10000000149011612\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: -0.12950114905834198\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014329571276903152\n",
      "          model: {}\n",
      "          policy_loss: -0.003081752685829997\n",
      "          total_loss: 189.4532928466797\n",
      "          vf_explained_var: -1.8719934814726003e-05\n",
      "          vf_loss: 189.45492553710938\n",
      "    num_agent_steps_sampled: 180000\n",
      "    num_agent_steps_trained: 180000\n",
      "    num_steps_sampled: 180000\n",
      "    num_steps_trained: 180000\n",
      "  iterations_since_restore: 12\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 25.134285714285713\n",
      "    ram_util_percent: 29.485714285714284\n",
      "  pid: 307\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07028168457627393\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 4.327514089482232\n",
      "    mean_inference_ms: 0.513639704297114\n",
      "    mean_raw_obs_processing_ms: 3.7927470405907537\n",
      "  time_since_restore: 322.96194100379944\n",
      "  time_this_iter_s: 24.568338632583618\n",
      "  time_total_s: 322.96194100379944\n",
      "  timers:\n",
      "    learn_throughput: 6907.036\n",
      "    learn_time_ms: 2171.698\n",
      "    load_throughput: 9878092.666\n",
      "    load_time_ms: 1.519\n",
      "    sample_throughput: 640.365\n",
      "    sample_time_ms: 23424.131\n",
      "    update_time_ms: 1.121\n",
      "  timestamp: 1665245511\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 180000\n",
      "  training_iteration: 12\n",
      "  trial_id: 26f87_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 9.2/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.23 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_2_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_3_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_5_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_1_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_4_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 accelerator_type:G, 0.0/6.0 CPU_group_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_0_25daa8673b2c24aecf80a7fc79df4e93)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_onelane_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_26f87_00000 | RUNNING  | 141.225.10.229:307 |     12 |          322.962 | 180000 | -1908.08 |              836.729 |             -5227.06 |               3000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m ring length: 235\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m v_max: 3.9514850725282584\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m ring length: 251\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m v_max: 4.71224180704279\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m ring length: 259\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m v_max: 5.092292348291853\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m ring length: 253\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m v_max: 4.807278529691987\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m ring length: 241\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m v_max: 4.236854288051661\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 1 seconds\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_26f87_00000:\n",
      "  agent_timesteps_total: 195000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-08_11-12-17\n",
      "  done: false\n",
      "  episode_len_mean: 3000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1036.7424025706484\n",
      "  episode_reward_mean: -1766.030130489457\n",
      "  episode_reward_min: -5227.059352675403\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 65\n",
      "  experiment_id: edcbe001b14143f4bf76a6a2914a7bd0\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.10000000149011612\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: -0.1164080947637558\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.005917334463447332\n",
      "          model: {}\n",
      "          policy_loss: 0.0006517797010019422\n",
      "          total_loss: 391.9528503417969\n",
      "          vf_explained_var: -4.2609681258909404e-06\n",
      "          vf_loss: 391.95159912109375\n",
      "    num_agent_steps_sampled: 195000\n",
      "    num_agent_steps_trained: 195000\n",
      "    num_steps_sampled: 195000\n",
      "    num_steps_trained: 195000\n",
      "  iterations_since_restore: 13\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 23.605405405405406\n",
      "    ram_util_percent: 29.451351351351352\n",
      "  pid: 307\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07006302993831949\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 4.281121911981654\n",
      "    mean_inference_ms: 0.5119590291394273\n",
      "    mean_raw_obs_processing_ms: 3.769674755744623\n",
      "  time_since_restore: 349.36421370506287\n",
      "  time_this_iter_s: 26.402272701263428\n",
      "  time_total_s: 349.36421370506287\n",
      "  timers:\n",
      "    learn_throughput: 6935.708\n",
      "    learn_time_ms: 2162.721\n",
      "    load_throughput: 8577893.517\n",
      "    load_time_ms: 1.749\n",
      "    sample_throughput: 636.191\n",
      "    sample_time_ms: 23577.814\n",
      "    update_time_ms: 1.123\n",
      "  timestamp: 1665245537\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 195000\n",
      "  training_iteration: 13\n",
      "  trial_id: 26f87_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Memory usage on this node: 9.2/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.23 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_5_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_3_25daa8673b2c24aecf80a7fc79df4e93, 0.0/6.0 CPU_group_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_2_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_0_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_4_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_1_25daa8673b2c24aecf80a7fc79df4e93)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_onelane_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_26f87_00000 | RUNNING  | 141.225.10.229:307 |     13 |          349.364 | 195000 | -1766.03 |              1036.74 |             -5227.06 |               3000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m ring length: 241\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m v_max: 4.236854288051661\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m ring length: 244\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m v_max: 4.379503211387676\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m ring length: 242\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m v_max: 4.2844067680020546\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m ring length: 226\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m v_max: 3.5232840694769565\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m ring length: 259\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m v_max: 5.092292348291853\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 1 seconds\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_26f87_00000:\n",
      "  agent_timesteps_total: 210000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-08_11-12-44\n",
      "  done: false\n",
      "  episode_len_mean: 3000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1036.7424025706484\n",
      "  episode_reward_mean: -1609.5255244499149\n",
      "  episode_reward_min: -5227.059352675403\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 70\n",
      "  experiment_id: edcbe001b14143f4bf76a6a2914a7bd0\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.05000000074505806\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: -0.32194969058036804\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.00857594609260559\n",
      "          model: {}\n",
      "          policy_loss: -0.0004412175330799073\n",
      "          total_loss: 334.00390625\n",
      "          vf_explained_var: -2.7407947982283076e-06\n",
      "          vf_loss: 334.00396728515625\n",
      "    num_agent_steps_sampled: 210000\n",
      "    num_agent_steps_trained: 210000\n",
      "    num_steps_sampled: 210000\n",
      "    num_steps_trained: 210000\n",
      "  iterations_since_restore: 14\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 23.80263157894737\n",
      "    ram_util_percent: 29.494736842105254\n",
      "  pid: 307\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06986427514558065\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 4.240117817085741\n",
      "    mean_inference_ms: 0.5104417696423387\n",
      "    mean_raw_obs_processing_ms: 3.7504485591227055\n",
      "  time_since_restore: 375.728378534317\n",
      "  time_this_iter_s: 26.36416482925415\n",
      "  time_total_s: 375.728378534317\n",
      "  timers:\n",
      "    learn_throughput: 6895.86\n",
      "    learn_time_ms: 2175.218\n",
      "    load_throughput: 9420462.679\n",
      "    load_time_ms: 1.592\n",
      "    sample_throughput: 632.012\n",
      "    sample_time_ms: 23733.744\n",
      "    update_time_ms: 1.118\n",
      "  timestamp: 1665245564\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 210000\n",
      "  training_iteration: 14\n",
      "  trial_id: 26f87_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 9.2/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.23 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_0_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_2_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_3_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_5_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_1_25daa8673b2c24aecf80a7fc79df4e93, 0.0/6.0 CPU_group_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_4_25daa8673b2c24aecf80a7fc79df4e93)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_onelane_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_26f87_00000 | RUNNING  | 141.225.10.229:307 |     14 |          375.728 | 210000 | -1609.53 |              1036.74 |             -5227.06 |               3000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m ring length: 221\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m v_max: 3.285334164169417\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m ring length: 247\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m v_max: 4.522125250095652\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m ring length: 234\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m v_max: 3.903915223349057\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m ring length: 242\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m v_max: 4.2844067680020546\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m ring length: 254\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m v_max: 4.854791141191876\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m Could not connect to TraCI server at localhost:34795 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m Could not connect to TraCI server at localhost:34447 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m Could not connect to TraCI server at localhost:57509 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m Could not connect to TraCI server at localhost:54035 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m Could not connect to TraCI server at localhost:36289 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 2 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m Could not connect to TraCI server at localhost:34795 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 3 seconds\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m Could not connect to TraCI server at localhost:34447 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 3 seconds\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m Could not connect to TraCI server at localhost:57509 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 3 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m Could not connect to TraCI server at localhost:54035 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 3 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m Could not connect to TraCI server at localhost:36289 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 3 seconds\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_26f87_00000:\n",
      "  agent_timesteps_total: 225000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-08_11-13-14\n",
      "  done: false\n",
      "  episode_len_mean: 3000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1036.7424025706484\n",
      "  episode_reward_mean: -1478.2613789708328\n",
      "  episode_reward_min: -5227.059352675403\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 75\n",
      "  experiment_id: edcbe001b14143f4bf76a6a2914a7bd0\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.02500000037252903\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: -0.42045727372169495\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.027907175943255424\n",
      "          model: {}\n",
      "          policy_loss: -0.003507411340251565\n",
      "          total_loss: 321.8483581542969\n",
      "          vf_explained_var: -1.2868489420725382e-06\n",
      "          vf_loss: 321.8511657714844\n",
      "    num_agent_steps_sampled: 225000\n",
      "    num_agent_steps_trained: 225000\n",
      "    num_steps_sampled: 225000\n",
      "    num_steps_trained: 225000\n",
      "  iterations_since_restore: 15\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.093023255813957\n",
      "    ram_util_percent: 29.54186046511628\n",
      "  pid: 307\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06968139470240482\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 4.2033081532744285\n",
      "    mean_inference_ms: 0.5090489300176968\n",
      "    mean_raw_obs_processing_ms: 3.740202622904748\n",
      "  time_since_restore: 405.76492166519165\n",
      "  time_this_iter_s: 30.036543130874634\n",
      "  time_total_s: 405.76492166519165\n",
      "  timers:\n",
      "    learn_throughput: 6926.476\n",
      "    learn_time_ms: 2165.603\n",
      "    load_throughput: 9654951.429\n",
      "    load_time_ms: 1.554\n",
      "    sample_throughput: 620.236\n",
      "    sample_time_ms: 24184.332\n",
      "    update_time_ms: 1.23\n",
      "  timestamp: 1665245594\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 225000\n",
      "  training_iteration: 15\n",
      "  trial_id: 26f87_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 9.2/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.23 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_1_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_0_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_3_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_5_25daa8673b2c24aecf80a7fc79df4e93, 0.0/6.0 CPU_group_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_2_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_4_25daa8673b2c24aecf80a7fc79df4e93)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_onelane_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_26f87_00000 | RUNNING  | 141.225.10.229:307 |     15 |          405.765 | 225000 | -1478.26 |              1036.74 |             -5227.06 |               3000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m ring length: 270\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m v_max: 5.6143732387852054\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m ring length: 249\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m v_max: 4.617190445131122\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m ring length: 251\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m v_max: 4.71224180704279\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m ring length: 255\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m v_max: 4.902299776966978\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m ring length: 236\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m v_max: 3.999052674951912\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 1 seconds\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_26f87_00000:\n",
      "  agent_timesteps_total: 240000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-08_11-13-39\n",
      "  done: false\n",
      "  episode_len_mean: 3000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1343.0997189489685\n",
      "  episode_reward_mean: -1355.0589879557144\n",
      "  episode_reward_min: -5227.059352675403\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 80\n",
      "  experiment_id: edcbe001b14143f4bf76a6a2914a7bd0\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.02500000037252903\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: -0.5071533918380737\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013239492662250996\n",
      "          model: {}\n",
      "          policy_loss: 0.0003341707633808255\n",
      "          total_loss: 186.49647521972656\n",
      "          vf_explained_var: -1.0311093774362234e-06\n",
      "          vf_loss: 186.49581909179688\n",
      "    num_agent_steps_sampled: 240000\n",
      "    num_agent_steps_trained: 240000\n",
      "    num_steps_sampled: 240000\n",
      "    num_steps_trained: 240000\n",
      "  iterations_since_restore: 16\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 23.881081081081085\n",
      "    ram_util_percent: 29.648648648648656\n",
      "  pid: 307\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0695096936313875\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 4.170044377915031\n",
      "    mean_inference_ms: 0.5077613183389836\n",
      "    mean_raw_obs_processing_ms: 3.73031321478487\n",
      "  time_since_restore: 431.3379261493683\n",
      "  time_this_iter_s: 25.573004484176636\n",
      "  time_total_s: 431.3379261493683\n",
      "  timers:\n",
      "    learn_throughput: 7068.499\n",
      "    learn_time_ms: 2122.091\n",
      "    load_throughput: 9626587.101\n",
      "    load_time_ms: 1.558\n",
      "    sample_throughput: 618.474\n",
      "    sample_time_ms: 24253.239\n",
      "    update_time_ms: 1.226\n",
      "  timestamp: 1665245619\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 240000\n",
      "  training_iteration: 16\n",
      "  trial_id: 26f87_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 9.2/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.23 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_1_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_2_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_3_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_0_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_5_25daa8673b2c24aecf80a7fc79df4e93, 0.0/6.0 CPU_group_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_4_25daa8673b2c24aecf80a7fc79df4e93)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_onelane_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_26f87_00000 | RUNNING  | 141.225.10.229:307 |     16 |          431.338 | 240000 | -1355.06 |               1343.1 |             -5227.06 |               3000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m ring length: 260\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m v_max: 5.139779427502188\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m ring length: 229\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m v_max: 3.666034803266416\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m ring length: 247\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m v_max: 4.522125250095652\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m ring length: 259\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m v_max: 5.092292348291853\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m ring length: 252\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m v_max: 4.759762049824181\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 1 seconds\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_26f87_00000:\n",
      "  agent_timesteps_total: 255000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-08_11-14-04\n",
      "  done: false\n",
      "  episode_len_mean: 3000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1764.6841561171912\n",
      "  episode_reward_mean: -1214.8410185758337\n",
      "  episode_reward_min: -5227.059352675403\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 85\n",
      "  experiment_id: edcbe001b14143f4bf76a6a2914a7bd0\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.02500000037252903\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: -0.740768551826477\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016712147742509842\n",
      "          model: {}\n",
      "          policy_loss: -0.0014623756287619472\n",
      "          total_loss: 183.29299926757812\n",
      "          vf_explained_var: -5.49177855191374e-07\n",
      "          vf_loss: 183.29403686523438\n",
      "    num_agent_steps_sampled: 255000\n",
      "    num_agent_steps_trained: 255000\n",
      "    num_steps_sampled: 255000\n",
      "    num_steps_trained: 255000\n",
      "  iterations_since_restore: 17\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 24.931428571428572\n",
      "    ram_util_percent: 29.625714285714288\n",
      "  pid: 307\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0693536309054961\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 4.14018541345124\n",
      "    mean_inference_ms: 0.5066017305087622\n",
      "    mean_raw_obs_processing_ms: 3.719816666737813\n",
      "  time_since_restore: 456.38084292411804\n",
      "  time_this_iter_s: 25.042916774749756\n",
      "  time_total_s: 456.38084292411804\n",
      "  timers:\n",
      "    learn_throughput: 6979.036\n",
      "    learn_time_ms: 2149.294\n",
      "    load_throughput: 9675292.96\n",
      "    load_time_ms: 1.55\n",
      "    sample_throughput: 618.139\n",
      "    sample_time_ms: 24266.39\n",
      "    update_time_ms: 1.271\n",
      "  timestamp: 1665245644\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 255000\n",
      "  training_iteration: 17\n",
      "  trial_id: 26f87_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 9.2/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.23 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_4_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_0_25daa8673b2c24aecf80a7fc79df4e93, 0.0/6.0 CPU_group_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_1_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_2_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_3_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_5_25daa8673b2c24aecf80a7fc79df4e93)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_onelane_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_26f87_00000 | RUNNING  | 141.225.10.229:307 |     17 |          456.381 | 255000 | -1214.84 |              1764.68 |             -5227.06 |               3000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m ring length: 267\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m v_max: 5.472051563171826\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m ring length: 238\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m v_max: 4.094180836186086\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m ring length: 227\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m v_max: 3.570869368805752\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m ring length: 258\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m v_max: 5.044800727535787\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m ring length: 260\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m v_max: 5.139779427502188\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m Could not connect to TraCI server at localhost:36289 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m Could not connect to TraCI server at localhost:34795 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m Could not connect to TraCI server at localhost:34447 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m Could not connect to TraCI server at localhost:54035 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m Could not connect to TraCI server at localhost:57509 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m Could not connect to TraCI server at localhost:36289 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 3 seconds\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m Could not connect to TraCI server at localhost:34795 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 3 seconds\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m Could not connect to TraCI server at localhost:34447 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 3 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m Could not connect to TraCI server at localhost:54035 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 3 seconds\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m Could not connect to TraCI server at localhost:57509 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 3 seconds\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_26f87_00000:\n",
      "  agent_timesteps_total: 270000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-08_11-14-35\n",
      "  done: false\n",
      "  episode_len_mean: 3000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1764.6841561171912\n",
      "  episode_reward_mean: -1084.2562650727798\n",
      "  episode_reward_min: -5227.059352675403\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 90\n",
      "  experiment_id: edcbe001b14143f4bf76a6a2914a7bd0\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.02500000037252903\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: -0.6398365497589111\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.1154956966638565\n",
      "          model: {}\n",
      "          policy_loss: -0.0255147535353899\n",
      "          total_loss: 224.1824951171875\n",
      "          vf_explained_var: -2.5268295189562195e-07\n",
      "          vf_loss: 224.2051239013672\n",
      "    num_agent_steps_sampled: 270000\n",
      "    num_agent_steps_trained: 270000\n",
      "    num_steps_sampled: 270000\n",
      "    num_steps_trained: 270000\n",
      "  iterations_since_restore: 18\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.57045454545454\n",
      "    ram_util_percent: 29.588636363636365\n",
      "  pid: 307\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06921093372154444\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 4.113248114225494\n",
      "    mean_inference_ms: 0.5055528594931269\n",
      "    mean_raw_obs_processing_ms: 3.715005665288361\n",
      "  time_since_restore: 487.15186500549316\n",
      "  time_this_iter_s: 30.771022081375122\n",
      "  time_total_s: 487.15186500549316\n",
      "  timers:\n",
      "    learn_throughput: 6938.377\n",
      "    learn_time_ms: 2161.889\n",
      "    load_throughput: 9381831.196\n",
      "    load_time_ms: 1.599\n",
      "    sample_throughput: 618.184\n",
      "    sample_time_ms: 24264.638\n",
      "    update_time_ms: 1.292\n",
      "  timestamp: 1665245675\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 270000\n",
      "  training_iteration: 18\n",
      "  trial_id: 26f87_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Memory usage on this node: 9.2/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.23 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_1_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_2_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_4_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_0_25daa8673b2c24aecf80a7fc79df4e93, 0.0/6.0 CPU_group_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_3_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_5_25daa8673b2c24aecf80a7fc79df4e93)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_onelane_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_26f87_00000 | RUNNING  | 141.225.10.229:307 |     18 |          487.152 | 270000 | -1084.26 |              1764.68 |             -5227.06 |               3000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m ring length: 226\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m v_max: 3.5232840694769565\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m ring length: 252\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m v_max: 4.759762049824181\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m ring length: 243\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m v_max: 4.331956438196479\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m ring length: 223\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m v_max: 3.3805185755587788\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m ring length: 238\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m v_max: 4.094180836186086\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 1 seconds\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_26f87_00000:\n",
      "  agent_timesteps_total: 285000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-08_11-15-01\n",
      "  done: false\n",
      "  episode_len_mean: 3000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1979.8174690336782\n",
      "  episode_reward_mean: -1039.7955941213786\n",
      "  episode_reward_min: -5227.059352675403\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 95\n",
      "  experiment_id: edcbe001b14143f4bf76a6a2914a7bd0\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.03750000149011612\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: -0.2704432010650635\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014583451673388481\n",
      "          model: {}\n",
      "          policy_loss: 0.00033210936817340553\n",
      "          total_loss: 1008.1634521484375\n",
      "          vf_explained_var: -2.4453187918993535e-08\n",
      "          vf_loss: 1008.1624755859375\n",
      "    num_agent_steps_sampled: 285000\n",
      "    num_agent_steps_trained: 285000\n",
      "    num_steps_sampled: 285000\n",
      "    num_steps_trained: 285000\n",
      "  iterations_since_restore: 19\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 23.60810810810811\n",
      "    ram_util_percent: 29.681081081081075\n",
      "  pid: 307\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06907644414460958\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 4.088613390315169\n",
      "    mean_inference_ms: 0.5045669059044199\n",
      "    mean_raw_obs_processing_ms: 3.7104999427139433\n",
      "  time_since_restore: 512.8783168792725\n",
      "  time_this_iter_s: 25.726451873779297\n",
      "  time_total_s: 512.8783168792725\n",
      "  timers:\n",
      "    learn_throughput: 6970.06\n",
      "    learn_time_ms: 2152.062\n",
      "    load_throughput: 8917094.465\n",
      "    load_time_ms: 1.682\n",
      "    sample_throughput: 616.709\n",
      "    sample_time_ms: 24322.666\n",
      "    update_time_ms: 1.351\n",
      "  timestamp: 1665245701\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 285000\n",
      "  training_iteration: 19\n",
      "  trial_id: 26f87_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 9.3/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.23 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_3_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_4_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_5_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_0_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_2_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_1_25daa8673b2c24aecf80a7fc79df4e93, 0.0/6.0 CPU_group_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 accelerator_type:G)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_onelane_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_26f87_00000 | RUNNING  | 141.225.10.229:307 |     19 |          512.878 | 285000 |  -1039.8 |              1979.82 |             -5227.06 |               3000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m ring length: 243\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m v_max: 4.331956438196479\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m ring length: 236\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m v_max: 3.999052674951912\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m ring length: 226\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m v_max: 3.5232840694769565\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m ring length: 249\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m v_max: 4.617190445131122\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m ring length: 234\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m v_max: 3.903915223349057\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 1 seconds\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_26f87_00000:\n",
      "  agent_timesteps_total: 300000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-08_11-15-26\n",
      "  done: false\n",
      "  episode_len_mean: 3000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1979.8174690336782\n",
      "  episode_reward_mean: -1010.2761771240278\n",
      "  episode_reward_min: -5227.059352675403\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 100\n",
      "  experiment_id: edcbe001b14143f4bf76a6a2914a7bd0\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.03750000149011612\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: -0.1847551167011261\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009991643019020557\n",
      "          model: {}\n",
      "          policy_loss: -0.001692836987785995\n",
      "          total_loss: 628.7777709960938\n",
      "          vf_explained_var: -4.7887493082043875e-08\n",
      "          vf_loss: 628.7791748046875\n",
      "    num_agent_steps_sampled: 300000\n",
      "    num_agent_steps_trained: 300000\n",
      "    num_steps_sampled: 300000\n",
      "    num_steps_trained: 300000\n",
      "  iterations_since_restore: 20\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 24.754285714285718\n",
      "    ram_util_percent: 29.731428571428577\n",
      "  pid: 307\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06895259163395959\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 4.0659068495374955\n",
      "    mean_inference_ms: 0.5036619202195793\n",
      "    mean_raw_obs_processing_ms: 3.7053744654968943\n",
      "  time_since_restore: 537.5269577503204\n",
      "  time_this_iter_s: 24.648640871047974\n",
      "  time_total_s: 537.5269577503204\n",
      "  timers:\n",
      "    learn_throughput: 6996.859\n",
      "    learn_time_ms: 2143.819\n",
      "    load_throughput: 8991133.85\n",
      "    load_time_ms: 1.668\n",
      "    sample_throughput: 618.284\n",
      "    sample_time_ms: 24260.693\n",
      "    update_time_ms: 1.342\n",
      "  timestamp: 1665245726\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 300000\n",
      "  training_iteration: 20\n",
      "  trial_id: 26f87_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 9.3/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.23 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_0_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_5_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_4_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_3_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_1_25daa8673b2c24aecf80a7fc79df4e93, 0.0/6.0 CPU_group_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_2_25daa8673b2c24aecf80a7fc79df4e93)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_onelane_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_26f87_00000 | RUNNING  | 141.225.10.229:307 |     20 |          537.527 | 300000 | -1010.28 |              1979.82 |             -5227.06 |               3000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m ring length: 226\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m v_max: 3.5232840694769565\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m ring length: 224\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m v_max: 3.4281086136538996\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m ring length: 230\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m v_max: 3.7136148111012934\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m ring length: 225\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m v_max: 3.4756971311168834\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m ring length: 256\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m v_max: 4.949804327743507\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m Could not connect to TraCI server at localhost:34795 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m Could not connect to TraCI server at localhost:57509 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m Could not connect to TraCI server at localhost:54035 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m Could not connect to TraCI server at localhost:36289 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m Could not connect to TraCI server at localhost:34447 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m Could not connect to TraCI server at localhost:34795 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 3 seconds\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m Could not connect to TraCI server at localhost:57509 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 3 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m Could not connect to TraCI server at localhost:54035 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 3 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m Could not connect to TraCI server at localhost:36289 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 3 seconds\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m Could not connect to TraCI server at localhost:34447 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 3 seconds\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_26f87_00000:\n",
      "  agent_timesteps_total: 315000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-08_11-15-56\n",
      "  done: false\n",
      "  episode_len_mean: 3000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1979.8174690336782\n",
      "  episode_reward_mean: -833.3724642534627\n",
      "  episode_reward_min: -5088.450841511557\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 105\n",
      "  experiment_id: edcbe001b14143f4bf76a6a2914a7bd0\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.01875000074505806\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.15268583595752716\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.031605809926986694\n",
      "          model: {}\n",
      "          policy_loss: -0.0011577539844438434\n",
      "          total_loss: 1202.5113525390625\n",
      "          vf_explained_var: -5.094414223805188e-09\n",
      "          vf_loss: 1202.5118408203125\n",
      "    num_agent_steps_sampled: 315000\n",
      "    num_agent_steps_trained: 315000\n",
      "    num_steps_sampled: 315000\n",
      "    num_steps_trained: 315000\n",
      "  iterations_since_restore: 21\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.443181818181817\n",
      "    ram_util_percent: 29.609090909090913\n",
      "  pid: 307\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06839647241832467\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 3.9074973169729117\n",
      "    mean_inference_ms: 0.4992209030848455\n",
      "    mean_raw_obs_processing_ms: 3.624163815960616\n",
      "  time_since_restore: 568.1779489517212\n",
      "  time_this_iter_s: 30.650991201400757\n",
      "  time_total_s: 568.1779489517212\n",
      "  timers:\n",
      "    learn_throughput: 6875.183\n",
      "    learn_time_ms: 2181.76\n",
      "    load_throughput: 8066589.738\n",
      "    load_time_ms: 1.86\n",
      "    sample_throughput: 605.17\n",
      "    sample_time_ms: 24786.411\n",
      "    update_time_ms: 1.342\n",
      "  timestamp: 1665245756\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 315000\n",
      "  training_iteration: 21\n",
      "  trial_id: 26f87_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 9.3/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.23 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_3_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_1_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_5_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_4_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_0_25daa8673b2c24aecf80a7fc79df4e93, 0.0/6.0 CPU_group_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_2_25daa8673b2c24aecf80a7fc79df4e93)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_onelane_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_26f87_00000 | RUNNING  | 141.225.10.229:307 |     21 |          568.178 | 315000 | -833.372 |              1979.82 |             -5088.45 |               3000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m ring length: 270\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m v_max: 5.6143732387852054\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m ring length: 234\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m v_max: 3.903915223349057\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m ring length: 229\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m v_max: 3.666034803266416\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m ring length: 254\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m v_max: 4.854791141191876\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m ring length: 232\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m v_max: 3.8087690783250063\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m Could not connect to TraCI server at localhost:34795 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m Could not connect to TraCI server at localhost:34447 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m Could not connect to TraCI server at localhost:36289 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 2 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m Could not connect to TraCI server at localhost:34795 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 3 seconds\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m Could not connect to TraCI server at localhost:34447 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 3 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m Could not connect to TraCI server at localhost:36289 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 3 seconds\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_26f87_00000:\n",
      "  agent_timesteps_total: 330000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-08_11-16-28\n",
      "  done: false\n",
      "  episode_len_mean: 3000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1979.8174690336782\n",
      "  episode_reward_mean: -624.5173161875059\n",
      "  episode_reward_min: -4326.450438316295\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 110\n",
      "  experiment_id: edcbe001b14143f4bf76a6a2914a7bd0\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.01875000074505806\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: -0.1690373569726944\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0342976450920105\n",
      "          model: {}\n",
      "          policy_loss: -0.0052382368594408035\n",
      "          total_loss: 856.84521484375\n",
      "          vf_explained_var: -1.5283243115504774e-08\n",
      "          vf_loss: 856.849609375\n",
      "    num_agent_steps_sampled: 330000\n",
      "    num_agent_steps_trained: 330000\n",
      "    num_steps_sampled: 330000\n",
      "    num_steps_trained: 330000\n",
      "  iterations_since_restore: 22\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.082608695652176\n",
      "    ram_util_percent: 29.613043478260874\n",
      "  pid: 307\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06794680196810986\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 3.8295311938044114\n",
      "    mean_inference_ms: 0.4958334178134759\n",
      "    mean_raw_obs_processing_ms: 3.600545384918634\n",
      "  time_since_restore: 599.9546086788177\n",
      "  time_this_iter_s: 31.776659727096558\n",
      "  time_total_s: 599.9546086788177\n",
      "  timers:\n",
      "    learn_throughput: 6790.02\n",
      "    learn_time_ms: 2209.125\n",
      "    load_throughput: 8019701.721\n",
      "    load_time_ms: 1.87\n",
      "    sample_throughput: 588.716\n",
      "    sample_time_ms: 25479.16\n",
      "    update_time_ms: 1.637\n",
      "  timestamp: 1665245788\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 330000\n",
      "  training_iteration: 22\n",
      "  trial_id: 26f87_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 9.3/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.23 GiB heap, 0.0/14.65 GiB objects (0.0/6.0 CPU_group_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_1_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_4_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_5_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_3_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_2_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_0_25daa8673b2c24aecf80a7fc79df4e93)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_onelane_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_26f87_00000 | RUNNING  | 141.225.10.229:307 |     22 |          599.955 | 330000 | -624.517 |              1979.82 |             -4326.45 |               3000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m ring length: 266\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m v_max: 5.42459972166245\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m ring length: 252\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m v_max: 4.759762049824181\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m ring length: 256\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m v_max: 4.949804327743507\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m ring length: 244\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m v_max: 4.379503211387676\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m ring length: 246\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m v_max: 4.474587708995648\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m Could not connect to TraCI server at localhost:57509 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m Could not connect to TraCI server at localhost:36289 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m Could not connect to TraCI server at localhost:57509 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 3 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m Could not connect to TraCI server at localhost:36289 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 3 seconds\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_26f87_00000:\n",
      "  agent_timesteps_total: 345000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-08_11-17-00\n",
      "  done: false\n",
      "  episode_len_mean: 3000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1979.8174690336782\n",
      "  episode_reward_mean: -432.5003114742191\n",
      "  episode_reward_min: -3804.402633930361\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 115\n",
      "  experiment_id: edcbe001b14143f4bf76a6a2914a7bd0\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.01875000074505806\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: -0.5304223299026489\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.03891260549426079\n",
      "          model: {}\n",
      "          policy_loss: -0.007116771303117275\n",
      "          total_loss: 912.8582763671875\n",
      "          vf_explained_var: -8.151062935723985e-09\n",
      "          vf_loss: 912.8646240234375\n",
      "    num_agent_steps_sampled: 345000\n",
      "    num_agent_steps_trained: 345000\n",
      "    num_steps_sampled: 345000\n",
      "    num_steps_trained: 345000\n",
      "  iterations_since_restore: 23\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.172727272727272\n",
      "    ram_util_percent: 29.627272727272725\n",
      "  pid: 307\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06765621996815599\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 3.7812139577264747\n",
      "    mean_inference_ms: 0.4937142605628986\n",
      "    mean_raw_obs_processing_ms: 3.5951797743674208\n",
      "  time_since_restore: 631.2270705699921\n",
      "  time_this_iter_s: 31.272461891174316\n",
      "  time_total_s: 631.2270705699921\n",
      "  timers:\n",
      "    learn_throughput: 6848.118\n",
      "    learn_time_ms: 2190.383\n",
      "    load_throughput: 9070988.206\n",
      "    load_time_ms: 1.654\n",
      "    sample_throughput: 577.251\n",
      "    sample_time_ms: 25985.226\n",
      "    update_time_ms: 1.627\n",
      "  timestamp: 1665245820\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 345000\n",
      "  training_iteration: 23\n",
      "  trial_id: 26f87_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 9.3/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.23 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_0_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_1_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_3_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_2_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_4_25daa8673b2c24aecf80a7fc79df4e93, 0.0/6.0 CPU_group_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_5_25daa8673b2c24aecf80a7fc79df4e93)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_onelane_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_26f87_00000 | RUNNING  | 141.225.10.229:307 |     23 |          631.227 | 345000 |   -432.5 |              1979.82 |              -3804.4 |               3000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m ring length: 265\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m v_max: 5.37714246265477\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m ring length: 255\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m v_max: 4.902299776966978\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m ring length: 252\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m v_max: 4.759762049824181\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m ring length: 244\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m v_max: 4.379503211387676\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m ring length: 246\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m v_max: 4.474587708995648\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 1 seconds\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_26f87_00000:\n",
      "  agent_timesteps_total: 360000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-08_11-17-25\n",
      "  done: false\n",
      "  episode_len_mean: 3000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1979.8174690336782\n",
      "  episode_reward_mean: -281.10882836726114\n",
      "  episode_reward_min: -3426.338972530315\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 120\n",
      "  experiment_id: edcbe001b14143f4bf76a6a2914a7bd0\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.01875000074505806\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: -0.6562395691871643\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.02252059057354927\n",
      "          model: {}\n",
      "          policy_loss: -0.0017917229561135173\n",
      "          total_loss: 1199.7799072265625\n",
      "          vf_explained_var: 2.547207111902594e-09\n",
      "          vf_loss: 1199.78125\n",
      "    num_agent_steps_sampled: 360000\n",
      "    num_agent_steps_trained: 360000\n",
      "    num_steps_sampled: 360000\n",
      "    num_steps_trained: 360000\n",
      "  iterations_since_restore: 24\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 24.294444444444444\n",
      "    ram_util_percent: 29.669444444444448\n",
      "  pid: 307\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06744684472048959\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 3.747718808153633\n",
      "    mean_inference_ms: 0.4922166772926986\n",
      "    mean_raw_obs_processing_ms: 3.5993028955329716\n",
      "  time_since_restore: 656.5639405250549\n",
      "  time_this_iter_s: 25.336869955062866\n",
      "  time_total_s: 656.5639405250549\n",
      "  timers:\n",
      "    learn_throughput: 6911.2\n",
      "    learn_time_ms: 2170.39\n",
      "    load_throughput: 9043476.261\n",
      "    load_time_ms: 1.659\n",
      "    sample_throughput: 579.089\n",
      "    sample_time_ms: 25902.739\n",
      "    update_time_ms: 1.626\n",
      "  timestamp: 1665245845\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 360000\n",
      "  training_iteration: 24\n",
      "  trial_id: 26f87_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 9.3/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.23 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_1_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_5_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_0_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_2_25daa8673b2c24aecf80a7fc79df4e93, 0.0/6.0 CPU_group_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_4_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_3_25daa8673b2c24aecf80a7fc79df4e93)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_onelane_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_26f87_00000 | RUNNING  | 141.225.10.229:307 |     24 |          656.564 | 360000 | -281.109 |              1979.82 |             -3426.34 |               3000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m ring length: 261\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m v_max: 5.187261846097525\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m ring length: 269\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m v_max: 5.566938458220921\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m ring length: 257\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m v_max: 4.997304682316517\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m ring length: 264\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m v_max: 5.329679917416892\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m ring length: 262\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m v_max: 5.234739483008334\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m Could not connect to TraCI server at localhost:54035 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m Could not connect to TraCI server at localhost:36289 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m Could not connect to TraCI server at localhost:34795 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m Could not connect to TraCI server at localhost:34447 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m Could not connect to TraCI server at localhost:57509 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m Could not connect to TraCI server at localhost:54035 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 3 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m Could not connect to TraCI server at localhost:36289 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 3 seconds\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m Could not connect to TraCI server at localhost:34795 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 3 seconds\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m Could not connect to TraCI server at localhost:34447 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 3 seconds\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m Could not connect to TraCI server at localhost:57509 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 3 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m Could not connect to TraCI server at localhost:54035 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m Could not connect to TraCI server at localhost:36289 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m Could not connect to TraCI server at localhost:54035 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 3 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m Could not connect to TraCI server at localhost:36289 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 3 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_WaveAttenuationPOEnv-v0_26f87_00000:\n",
      "  agent_timesteps_total: 375000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-08_11-18-02\n",
      "  done: false\n",
      "  episode_len_mean: 3000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1979.8174690336782\n",
      "  episode_reward_mean: -252.0180719301339\n",
      "  episode_reward_min: -6471.328111909182\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 125\n",
      "  experiment_id: edcbe001b14143f4bf76a6a2914a7bd0\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.01875000074505806\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.20292820036411285\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.042317215353250504\n",
      "          model: {}\n",
      "          policy_loss: -0.003234497271478176\n",
      "          total_loss: 1894.1748046875\n",
      "          vf_explained_var: -1.9868215517249155e-08\n",
      "          vf_loss: 1894.1771240234375\n",
      "    num_agent_steps_sampled: 375000\n",
      "    num_agent_steps_trained: 375000\n",
      "    num_steps_sampled: 375000\n",
      "    num_steps_trained: 375000\n",
      "  iterations_since_restore: 25\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 18.91698113207547\n",
      "    ram_util_percent: 29.654716981132076\n",
      "  pid: 307\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0672773172551496\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 3.721751515350894\n",
      "    mean_inference_ms: 0.4910219032854707\n",
      "    mean_raw_obs_processing_ms: 3.6134327625391642\n",
      "  time_since_restore: 693.2920804023743\n",
      "  time_this_iter_s: 36.728139877319336\n",
      "  time_total_s: 693.2920804023743\n",
      "  timers:\n",
      "    learn_throughput: 6759.928\n",
      "    learn_time_ms: 2218.959\n",
      "    load_throughput: 8909265.474\n",
      "    load_time_ms: 1.684\n",
      "    sample_throughput: 565.532\n",
      "    sample_time_ms: 26523.688\n",
      "    update_time_ms: 1.531\n",
      "  timestamp: 1665245882\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 375000\n",
      "  training_iteration: 25\n",
      "  trial_id: 26f87_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 9.3/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.23 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_4_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_1_25daa8673b2c24aecf80a7fc79df4e93, 0.0/6.0 CPU_group_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_2_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_3_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_0_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_5_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 accelerator_type:G)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_onelane_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_26f87_00000 | RUNNING  | 141.225.10.229:307 |     25 |          693.292 | 375000 | -252.018 |              1979.82 |             -6471.33 |               3000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m ring length: 247\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m v_max: 4.522125250095652\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m ring length: 265\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m v_max: 5.37714246265477\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m ring length: 247\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m v_max: 4.522125250095652\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m ring length: 231\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m v_max: 3.761192925277308\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m ring length: 256\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m v_max: 4.949804327743507\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 1 seconds\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_26f87_00000:\n",
      "  agent_timesteps_total: 390000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-08_11-18-28\n",
      "  done: false\n",
      "  episode_len_mean: 3000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1979.8174690336782\n",
      "  episode_reward_mean: -164.82259319169034\n",
      "  episode_reward_min: -6471.328111909182\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 130\n",
      "  experiment_id: edcbe001b14143f4bf76a6a2914a7bd0\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.02812499925494194\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: -1.0052398443222046\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.03852811083197594\n",
      "          model: {}\n",
      "          policy_loss: 0.00047436929889954627\n",
      "          total_loss: 1419.784423828125\n",
      "          vf_explained_var: -1.0188828669654981e-09\n",
      "          vf_loss: 1419.7828369140625\n",
      "    num_agent_steps_sampled: 390000\n",
      "    num_agent_steps_trained: 390000\n",
      "    num_steps_sampled: 390000\n",
      "    num_steps_trained: 390000\n",
      "  iterations_since_restore: 26\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 24.22972972972973\n",
      "    ram_util_percent: 29.751351351351342\n",
      "  pid: 307\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06713624340422257\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 3.700873290301983\n",
      "    mean_inference_ms: 0.49004704467009175\n",
      "    mean_raw_obs_processing_ms: 3.62898774253499\n",
      "  time_since_restore: 719.3613746166229\n",
      "  time_this_iter_s: 26.069294214248657\n",
      "  time_total_s: 719.3613746166229\n",
      "  timers:\n",
      "    learn_throughput: 6570.81\n",
      "    learn_time_ms: 2282.824\n",
      "    load_throughput: 8933808.557\n",
      "    load_time_ms: 1.679\n",
      "    sample_throughput: 565.859\n",
      "    sample_time_ms: 26508.364\n",
      "    update_time_ms: 1.716\n",
      "  timestamp: 1665245908\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 390000\n",
      "  training_iteration: 26\n",
      "  trial_id: 26f87_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 9.3/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.23 GiB heap, 0.0/14.65 GiB objects (0.0/6.0 CPU_group_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_2_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_0_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_1_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_3_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_4_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_5_25daa8673b2c24aecf80a7fc79df4e93)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_onelane_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_26f87_00000 | RUNNING  | 141.225.10.229:307 |     26 |          719.361 | 390000 | -164.823 |              1979.82 |             -6471.33 |               3000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m ring length: 261\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m v_max: 5.187261846097525\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m ring length: 243\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m v_max: 4.331956438196479\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m ring length: 231\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m v_max: 3.761192925277308\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m ring length: 259\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m v_max: 5.092292348291853\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m ring length: 258\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m v_max: 5.044800727535787\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m Could not connect to TraCI server at localhost:57509 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m Could not connect to TraCI server at localhost:54035 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m Could not connect to TraCI server at localhost:36289 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m Could not connect to TraCI server at localhost:57509 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 3 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m Could not connect to TraCI server at localhost:54035 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 3 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m Could not connect to TraCI server at localhost:36289 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 3 seconds\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_26f87_00000:\n",
      "  agent_timesteps_total: 405000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-08_11-19-00\n",
      "  done: false\n",
      "  episode_len_mean: 3000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1979.8174690336782\n",
      "  episode_reward_mean: -187.70155488887926\n",
      "  episode_reward_min: -6471.328111909182\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 135\n",
      "  experiment_id: edcbe001b14143f4bf76a6a2914a7bd0\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.02812499925494194\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.19849418103694916\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011470411904156208\n",
      "          model: {}\n",
      "          policy_loss: 0.001363390707410872\n",
      "          total_loss: 1897.8197021484375\n",
      "          vf_explained_var: -1.7830450005362763e-08\n",
      "          vf_loss: 1897.818115234375\n",
      "    num_agent_steps_sampled: 405000\n",
      "    num_agent_steps_trained: 405000\n",
      "    num_steps_sampled: 405000\n",
      "    num_steps_trained: 405000\n",
      "  iterations_since_restore: 27\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.57173913043478\n",
      "    ram_util_percent: 29.69782608695652\n",
      "  pid: 307\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06702440298163988\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 3.6836410697180724\n",
      "    mean_inference_ms: 0.48926597936976196\n",
      "    mean_raw_obs_processing_ms: 3.6499410030146633\n",
      "  time_since_restore: 751.3305974006653\n",
      "  time_this_iter_s: 31.96922278404236\n",
      "  time_total_s: 751.3305974006653\n",
      "  timers:\n",
      "    learn_throughput: 6432.015\n",
      "    learn_time_ms: 2332.084\n",
      "    load_throughput: 8055538.341\n",
      "    load_time_ms: 1.862\n",
      "    sample_throughput: 552.464\n",
      "    sample_time_ms: 27151.076\n",
      "    update_time_ms: 1.665\n",
      "  timestamp: 1665245940\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 405000\n",
      "  training_iteration: 27\n",
      "  trial_id: 26f87_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 9.3/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.23 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_0_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_2_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_5_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_3_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_4_25daa8673b2c24aecf80a7fc79df4e93, 0.0/6.0 CPU_group_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_1_25daa8673b2c24aecf80a7fc79df4e93)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_onelane_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_26f87_00000 | RUNNING  | 141.225.10.229:307 |     27 |          751.331 | 405000 | -187.702 |              1979.82 |             -6471.33 |               3000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m ring length: 243\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m v_max: 4.331956438196479\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m ring length: 221\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m v_max: 3.285334164169417\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m ring length: 239\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m v_max: 4.141741239205832\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m ring length: 232\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m v_max: 3.8087690783250063\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m ring length: 262\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m v_max: 5.234739483008334\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 1 seconds\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_26f87_00000:\n",
      "  agent_timesteps_total: 420000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-08_11-19-25\n",
      "  done: false\n",
      "  episode_len_mean: 3000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1979.8174690336782\n",
      "  episode_reward_mean: -251.63524983528433\n",
      "  episode_reward_min: -7307.768976759082\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 140\n",
      "  experiment_id: edcbe001b14143f4bf76a6a2914a7bd0\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.02812499925494194\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.7098292708396912\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.034076135605573654\n",
      "          model: {}\n",
      "          policy_loss: 0.0001882843644125387\n",
      "          total_loss: 2217.23779296875\n",
      "          vf_explained_var: -6.113296979748384e-09\n",
      "          vf_loss: 2217.236572265625\n",
      "    num_agent_steps_sampled: 420000\n",
      "    num_agent_steps_trained: 420000\n",
      "    num_steps_sampled: 420000\n",
      "    num_steps_trained: 420000\n",
      "  iterations_since_restore: 28\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 24.358333333333334\n",
      "    ram_util_percent: 29.752777777777776\n",
      "  pid: 307\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0669284439588703\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 3.6691612081890796\n",
      "    mean_inference_ms: 0.48861661412477053\n",
      "    mean_raw_obs_processing_ms: 3.6604273041509248\n",
      "  time_since_restore: 776.5812032222748\n",
      "  time_this_iter_s: 25.250605821609497\n",
      "  time_total_s: 776.5812032222748\n",
      "  timers:\n",
      "    learn_throughput: 6479.047\n",
      "    learn_time_ms: 2315.155\n",
      "    load_throughput: 8203429.257\n",
      "    load_time_ms: 1.829\n",
      "    sample_throughput: 563.569\n",
      "    sample_time_ms: 26616.077\n",
      "    update_time_ms: 1.686\n",
      "  timestamp: 1665245965\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 420000\n",
      "  training_iteration: 28\n",
      "  trial_id: 26f87_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Memory usage on this node: 9.3/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.23 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_3_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_5_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_0_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_1_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_2_25daa8673b2c24aecf80a7fc79df4e93, 0.0/6.0 CPU_group_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_4_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 accelerator_type:G)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_onelane_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_26f87_00000 | RUNNING  | 141.225.10.229:307 |     28 |          776.581 | 420000 | -251.635 |              1979.82 |             -7307.77 |               3000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m ring length: 226\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m v_max: 3.5232840694769565\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m ring length: 235\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m v_max: 3.9514850725282584\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m ring length: 230\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m v_max: 3.7136148111012934\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m ring length: 228\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m v_max: 3.618452967700356\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m ring length: 228\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m v_max: 3.618452967700356\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m Could not connect to TraCI server at localhost:54035 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m Could not connect to TraCI server at localhost:34447 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m Could not connect to TraCI server at localhost:34795 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m Could not connect to TraCI server at localhost:57509 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m Could not connect to TraCI server at localhost:54035 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 3 seconds\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m Could not connect to TraCI server at localhost:34447 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 3 seconds\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m Could not connect to TraCI server at localhost:57509 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 3 seconds\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m Could not connect to TraCI server at localhost:34795 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 3 seconds\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_26f87_00000:\n",
      "  agent_timesteps_total: 435000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-08_11-19-55\n",
      "  done: false\n",
      "  episode_len_mean: 3000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1979.8174690336782\n",
      "  episode_reward_mean: -261.4224791555314\n",
      "  episode_reward_min: -7307.768976759082\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 145\n",
      "  experiment_id: edcbe001b14143f4bf76a6a2914a7bd0\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.02812499925494194\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: -0.22507379949092865\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.03294586390256882\n",
      "          model: {}\n",
      "          policy_loss: -0.0016913700383156538\n",
      "          total_loss: 1505.9193115234375\n",
      "          vf_explained_var: 1.0188828669654981e-09\n",
      "          vf_loss: 1505.9200439453125\n",
      "    num_agent_steps_sampled: 435000\n",
      "    num_agent_steps_trained: 435000\n",
      "    num_steps_sampled: 435000\n",
      "    num_steps_trained: 435000\n",
      "  iterations_since_restore: 29\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.56744186046512\n",
      "    ram_util_percent: 29.706976744186054\n",
      "  pid: 307\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06684093060098237\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 3.6564766096851566\n",
      "    mean_inference_ms: 0.4880413474318112\n",
      "    mean_raw_obs_processing_ms: 3.6748997532678978\n",
      "  time_since_restore: 806.6433324813843\n",
      "  time_this_iter_s: 30.062129259109497\n",
      "  time_total_s: 806.6433324813843\n",
      "  timers:\n",
      "    learn_throughput: 6411.136\n",
      "    learn_time_ms: 2339.679\n",
      "    load_throughput: 8488775.552\n",
      "    load_time_ms: 1.767\n",
      "    sample_throughput: 555.044\n",
      "    sample_time_ms: 27024.87\n",
      "    update_time_ms: 1.9\n",
      "  timestamp: 1665245995\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 435000\n",
      "  training_iteration: 29\n",
      "  trial_id: 26f87_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 9.3/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.23 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_0_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_3_25daa8673b2c24aecf80a7fc79df4e93, 0.0/6.0 CPU_group_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_4_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_1_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_5_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_2_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 accelerator_type:G)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_onelane_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_26f87_00000 | RUNNING  | 141.225.10.229:307 |     29 |          806.643 | 435000 | -261.422 |              1979.82 |             -7307.77 |               3000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m ring length: 227\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m v_max: 3.570869368805752\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m ring length: 259\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m v_max: 5.092292348291853\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m ring length: 255\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m v_max: 4.902299776966978\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m ring length: 250\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m v_max: 4.664717904914125\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m ring length: 230\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m v_max: 3.7136148111012934\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 1 seconds\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_26f87_00000:\n",
      "  agent_timesteps_total: 450000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-08_11-20-20\n",
      "  done: false\n",
      "  episode_len_mean: 3000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1979.8174690336782\n",
      "  episode_reward_mean: -310.66458969509824\n",
      "  episode_reward_min: -7307.768976759082\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 150\n",
      "  experiment_id: edcbe001b14143f4bf76a6a2914a7bd0\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.02812499925494194\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.375104159116745\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.03263954445719719\n",
      "          model: {}\n",
      "          policy_loss: -0.0026295389980077744\n",
      "          total_loss: 1545.81787109375\n",
      "          vf_explained_var: -6.113296979748384e-09\n",
      "          vf_loss: 1545.8194580078125\n",
      "    num_agent_steps_sampled: 450000\n",
      "    num_agent_steps_trained: 450000\n",
      "    num_steps_sampled: 450000\n",
      "    num_steps_trained: 450000\n",
      "  iterations_since_restore: 30\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 24.391428571428573\n",
      "    ram_util_percent: 29.84285714285714\n",
      "  pid: 307\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06676273645708478\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 3.645204409086932\n",
      "    mean_inference_ms: 0.4875428442571173\n",
      "    mean_raw_obs_processing_ms: 3.6903148609709904\n",
      "  time_since_restore: 831.446578502655\n",
      "  time_this_iter_s: 24.803246021270752\n",
      "  time_total_s: 831.446578502655\n",
      "  timers:\n",
      "    learn_throughput: 6397.014\n",
      "    learn_time_ms: 2344.844\n",
      "    load_throughput: 8442187.752\n",
      "    load_time_ms: 1.777\n",
      "    sample_throughput: 554.843\n",
      "    sample_time_ms: 27034.696\n",
      "    update_time_ms: 2.05\n",
      "  timestamp: 1665246020\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 450000\n",
      "  training_iteration: 30\n",
      "  trial_id: 26f87_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 9.3/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.23 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_5_25daa8673b2c24aecf80a7fc79df4e93, 0.0/6.0 CPU_group_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_1_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_2_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_3_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_4_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_0_25daa8673b2c24aecf80a7fc79df4e93)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_onelane_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_26f87_00000 | RUNNING  | 141.225.10.229:307 |     30 |          831.447 | 450000 | -310.665 |              1979.82 |             -7307.77 |               3000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m ring length: 252\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m v_max: 4.759762049824181\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m ring length: 243\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m v_max: 4.331956438196479\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m ring length: 253\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m v_max: 4.807278529691987\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m ring length: 270\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m v_max: 5.6143732387852054\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m ring length: 243\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m v_max: 4.331956438196479\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 1 seconds\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_26f87_00000:\n",
      "  agent_timesteps_total: 465000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-08_11-20-45\n",
      "  done: false\n",
      "  episode_len_mean: 3000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1979.8174690336782\n",
      "  episode_reward_mean: -338.7343819399349\n",
      "  episode_reward_min: -7307.768976759082\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 155\n",
      "  experiment_id: edcbe001b14143f4bf76a6a2914a7bd0\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.02812499925494194\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: -0.27902987599372864\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.02392977476119995\n",
      "          model: {}\n",
      "          policy_loss: 0.0010287296026945114\n",
      "          total_loss: 1826.441162109375\n",
      "          vf_explained_var: -1.1207711203553572e-08\n",
      "          vf_loss: 1826.439453125\n",
      "    num_agent_steps_sampled: 465000\n",
      "    num_agent_steps_trained: 465000\n",
      "    num_steps_sampled: 465000\n",
      "    num_steps_trained: 465000\n",
      "  iterations_since_restore: 31\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 24.394594594594594\n",
      "    ram_util_percent: 29.767567567567564\n",
      "  pid: 307\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06668936326698667\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 3.6352056175508114\n",
      "    mean_inference_ms: 0.48710019538436156\n",
      "    mean_raw_obs_processing_ms: 3.707128217554143\n",
      "  time_since_restore: 856.7197952270508\n",
      "  time_this_iter_s: 25.273216724395752\n",
      "  time_total_s: 856.7197952270508\n",
      "  timers:\n",
      "    learn_throughput: 6445.069\n",
      "    learn_time_ms: 2327.361\n",
      "    load_throughput: 9422437.885\n",
      "    load_time_ms: 1.592\n",
      "    sample_throughput: 565.721\n",
      "    sample_time_ms: 26514.854\n",
      "    update_time_ms: 2.059\n",
      "  timestamp: 1665246045\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 465000\n",
      "  training_iteration: 31\n",
      "  trial_id: 26f87_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 9.3/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.23 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_1_25daa8673b2c24aecf80a7fc79df4e93, 0.0/6.0 CPU_group_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_0_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_5_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_3_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_4_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_2_25daa8673b2c24aecf80a7fc79df4e93)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_onelane_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_26f87_00000 | RUNNING  | 141.225.10.229:307 |     31 |           856.72 | 465000 | -338.734 |              1979.82 |             -7307.77 |               3000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m ring length: 264\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m v_max: 5.329679917416892\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m ring length: 230\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m v_max: 3.7136148111012934\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m ring length: 245\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m v_max: 4.427046998576354\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m ring length: 235\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m v_max: 3.9514850725282584\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m ring length: 268\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m v_max: 5.5194978538368025\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 1 seconds\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_26f87_00000:\n",
      "  agent_timesteps_total: 480000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-08_11-21-11\n",
      "  done: false\n",
      "  episode_len_mean: 3000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1979.8174690336782\n",
      "  episode_reward_mean: -409.8345745236721\n",
      "  episode_reward_min: -7307.768976759082\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 160\n",
      "  experiment_id: edcbe001b14143f4bf76a6a2914a7bd0\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.02812499925494194\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.1628149151802063\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.018877895548939705\n",
      "          model: {}\n",
      "          policy_loss: 0.0006154166767373681\n",
      "          total_loss: 2151.952392578125\n",
      "          vf_explained_var: 4.0755314678619925e-09\n",
      "          vf_loss: 2151.951171875\n",
      "    num_agent_steps_sampled: 480000\n",
      "    num_agent_steps_trained: 480000\n",
      "    num_steps_sampled: 480000\n",
      "    num_steps_trained: 480000\n",
      "  iterations_since_restore: 32\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 24.386111111111113\n",
      "    ram_util_percent: 29.761111111111106\n",
      "  pid: 307\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06662085345045855\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 3.6262898356382185\n",
      "    mean_inference_ms: 0.486692284433771\n",
      "    mean_raw_obs_processing_ms: 3.724965695910047\n",
      "  time_since_restore: 882.3072845935822\n",
      "  time_this_iter_s: 25.587489366531372\n",
      "  time_total_s: 882.3072845935822\n",
      "  timers:\n",
      "    learn_throughput: 6423.41\n",
      "    learn_time_ms: 2335.208\n",
      "    load_throughput: 10595779.511\n",
      "    load_time_ms: 1.416\n",
      "    sample_throughput: 579.403\n",
      "    sample_time_ms: 25888.709\n",
      "    update_time_ms: 1.739\n",
      "  timestamp: 1665246071\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 480000\n",
      "  training_iteration: 32\n",
      "  trial_id: 26f87_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 9.3/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.23 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_5_25daa8673b2c24aecf80a7fc79df4e93, 0.0/6.0 CPU_group_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_4_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_2_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_0_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_3_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_1_25daa8673b2c24aecf80a7fc79df4e93)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_onelane_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_26f87_00000 | RUNNING  | 141.225.10.229:307 |     32 |          882.307 | 480000 | -409.835 |              1979.82 |             -7307.77 |               3000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m ring length: 262\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m v_max: 5.234739483008334\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m ring length: 240\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m v_max: 4.189299083856172\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m ring length: 233\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m v_max: 3.856343201216534\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m ring length: 264\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m v_max: 5.329679917416892\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m ring length: 230\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m v_max: 3.7136148111012934\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 1 seconds\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_26f87_00000:\n",
      "  agent_timesteps_total: 495000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-08_11-21-36\n",
      "  done: false\n",
      "  episode_len_mean: 3000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1979.8174690336782\n",
      "  episode_reward_mean: -402.1378313091908\n",
      "  episode_reward_min: -7307.768976759082\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 165\n",
      "  experiment_id: edcbe001b14143f4bf76a6a2914a7bd0\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.02812499925494194\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: -0.9996374845504761\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.026634713634848595\n",
      "          model: {}\n",
      "          policy_loss: -0.0011769250268116593\n",
      "          total_loss: 1361.2147216796875\n",
      "          vf_explained_var: -7.641621557752387e-09\n",
      "          vf_loss: 1361.215087890625\n",
      "    num_agent_steps_sampled: 495000\n",
      "    num_agent_steps_trained: 495000\n",
      "    num_steps_sampled: 495000\n",
      "    num_steps_trained: 495000\n",
      "  iterations_since_restore: 33\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 25.061111111111114\n",
      "    ram_util_percent: 29.763888888888882\n",
      "  pid: 307\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06656129650010612\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 3.6184578250844783\n",
      "    mean_inference_ms: 0.486344558614959\n",
      "    mean_raw_obs_processing_ms: 3.7415665349882845\n",
      "  time_since_restore: 907.4149901866913\n",
      "  time_this_iter_s: 25.10770559310913\n",
      "  time_total_s: 907.4149901866913\n",
      "  timers:\n",
      "    learn_throughput: 6392.918\n",
      "    learn_time_ms: 2346.346\n",
      "    load_throughput: 10737372.427\n",
      "    load_time_ms: 1.397\n",
      "    sample_throughput: 593.799\n",
      "    sample_time_ms: 25261.07\n",
      "    update_time_ms: 1.81\n",
      "  timestamp: 1665246096\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 495000\n",
      "  training_iteration: 33\n",
      "  trial_id: 26f87_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 9.3/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.23 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_2_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_5_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 accelerator_type:G, 0.0/6.0 CPU_group_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_0_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_1_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_3_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_4_25daa8673b2c24aecf80a7fc79df4e93)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_onelane_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_26f87_00000 | RUNNING  | 141.225.10.229:307 |     33 |          907.415 | 495000 | -402.138 |              1979.82 |             -7307.77 |               3000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m ring length: 263\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m v_max: 5.282212215151455\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m ring length: 221\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m v_max: 3.285334164169417\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m ring length: 264\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m v_max: 5.329679917416892\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m ring length: 261\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m v_max: 5.187261846097525\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m ring length: 257\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m v_max: 4.997304682316517\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 1 seconds\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_26f87_00000:\n",
      "  agent_timesteps_total: 510000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-08_11-22-02\n",
      "  done: false\n",
      "  episode_len_mean: 3000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1979.8174690336782\n",
      "  episode_reward_mean: -557.4930000033139\n",
      "  episode_reward_min: -8324.169161384283\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 170\n",
      "  experiment_id: edcbe001b14143f4bf76a6a2914a7bd0\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.02812499925494194\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.117516040802002\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.020257214084267616\n",
      "          model: {}\n",
      "          policy_loss: 0.00627477839589119\n",
      "          total_loss: 2544.312255859375\n",
      "          vf_explained_var: 3.5660898678457897e-09\n",
      "          vf_loss: 2544.304931640625\n",
      "    num_agent_steps_sampled: 510000\n",
      "    num_agent_steps_trained: 510000\n",
      "    num_steps_sampled: 510000\n",
      "    num_steps_trained: 510000\n",
      "  iterations_since_restore: 34\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 23.602702702702704\n",
      "    ram_util_percent: 29.748648648648643\n",
      "  pid: 307\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06650740560987267\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 3.611232277089232\n",
      "    mean_inference_ms: 0.4860400496441904\n",
      "    mean_raw_obs_processing_ms: 3.7575136965862015\n",
      "  time_since_restore: 933.3561308383942\n",
      "  time_this_iter_s: 25.94114065170288\n",
      "  time_total_s: 933.3561308383942\n",
      "  timers:\n",
      "    learn_throughput: 6391.168\n",
      "    learn_time_ms: 2346.989\n",
      "    load_throughput: 9462685.939\n",
      "    load_time_ms: 1.585\n",
      "    sample_throughput: 592.414\n",
      "    sample_time_ms: 25320.134\n",
      "    update_time_ms: 1.918\n",
      "  timestamp: 1665246122\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 510000\n",
      "  training_iteration: 34\n",
      "  trial_id: 26f87_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 9.3/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.23 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_5_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_1_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_3_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_2_25daa8673b2c24aecf80a7fc79df4e93, 0.0/6.0 CPU_group_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_4_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_0_25daa8673b2c24aecf80a7fc79df4e93)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_onelane_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_26f87_00000 | RUNNING  | 141.225.10.229:307 |     34 |          933.356 | 510000 | -557.493 |              1979.82 |             -8324.17 |               3000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m ring length: 242\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m v_max: 4.2844067680020546\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m ring length: 247\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m v_max: 4.522125250095652\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m ring length: 241\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m v_max: 4.236854288051661\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m ring length: 261\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m v_max: 5.187261846097525\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m ring length: 231\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m v_max: 3.761192925277308\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 1 seconds\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_26f87_00000:\n",
      "  agent_timesteps_total: 525000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-08_11-22-28\n",
      "  done: false\n",
      "  episode_len_mean: 3000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2530.805184574418\n",
      "  episode_reward_mean: -543.7378684784742\n",
      "  episode_reward_min: -8324.169161384283\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 175\n",
      "  experiment_id: edcbe001b14143f4bf76a6a2914a7bd0\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.02812499925494194\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: -1.5641729831695557\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.05120677128434181\n",
      "          model: {}\n",
      "          policy_loss: -0.004104150924831629\n",
      "          total_loss: 1299.706787109375\n",
      "          vf_explained_var: -1.3245476715439963e-08\n",
      "          vf_loss: 1299.7095947265625\n",
      "    num_agent_steps_sampled: 525000\n",
      "    num_agent_steps_trained: 525000\n",
      "    num_steps_sampled: 525000\n",
      "    num_steps_trained: 525000\n",
      "  iterations_since_restore: 35\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 24.370270270270264\n",
      "    ram_util_percent: 29.75945945945945\n",
      "  pid: 307\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06646152121454012\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 3.604960639355324\n",
      "    mean_inference_ms: 0.485798424315493\n",
      "    mean_raw_obs_processing_ms: 3.767966165063234\n",
      "  time_since_restore: 959.4018466472626\n",
      "  time_this_iter_s: 26.045715808868408\n",
      "  time_total_s: 959.4018466472626\n",
      "  timers:\n",
      "    learn_throughput: 6484.219\n",
      "    learn_time_ms: 2313.309\n",
      "    load_throughput: 9638680.618\n",
      "    load_time_ms: 1.556\n",
      "    sample_throughput: 617.648\n",
      "    sample_time_ms: 24285.667\n",
      "    update_time_ms: 1.875\n",
      "  timestamp: 1665246148\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 525000\n",
      "  training_iteration: 35\n",
      "  trial_id: 26f87_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Memory usage on this node: 9.3/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.23 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_2_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_3_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_0_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_5_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_4_25daa8673b2c24aecf80a7fc79df4e93, 0.0/6.0 CPU_group_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_1_25daa8673b2c24aecf80a7fc79df4e93)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_onelane_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_26f87_00000 | RUNNING  | 141.225.10.229:307 |     35 |          959.402 | 525000 | -543.738 |              2530.81 |             -8324.17 |               3000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m ring length: 260\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m v_max: 5.139779427502188\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m ring length: 238\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m v_max: 4.094180836186086\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m ring length: 245\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m v_max: 4.427046998576354\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m ring length: 251\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m v_max: 4.71224180704279\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m ring length: 237\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m v_max: 4.04661795519353\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 1 seconds\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_26f87_00000:\n",
      "  agent_timesteps_total: 540000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-08_11-22-54\n",
      "  done: false\n",
      "  episode_len_mean: 3000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2530.805184574418\n",
      "  episode_reward_mean: -605.3536748391249\n",
      "  episode_reward_min: -8324.169161384283\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 180\n",
      "  experiment_id: edcbe001b14143f4bf76a6a2914a7bd0\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.04218750074505806\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: -0.53215092420578\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015026749111711979\n",
      "          model: {}\n",
      "          policy_loss: 0.0006222102674655616\n",
      "          total_loss: 1738.21630859375\n",
      "          vf_explained_var: -5.603855601776786e-09\n",
      "          vf_loss: 1738.21533203125\n",
      "    num_agent_steps_sampled: 540000\n",
      "    num_agent_steps_trained: 540000\n",
      "    num_steps_sampled: 540000\n",
      "    num_steps_trained: 540000\n",
      "  iterations_since_restore: 36\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 23.784210526315793\n",
      "    ram_util_percent: 29.768421052631574\n",
      "  pid: 307\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06642440372354008\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 3.599383544492815\n",
      "    mean_inference_ms: 0.4856109283408002\n",
      "    mean_raw_obs_processing_ms: 3.7789680471356437\n",
      "  time_since_restore: 985.429559469223\n",
      "  time_this_iter_s: 26.02771282196045\n",
      "  time_total_s: 985.429559469223\n",
      "  timers:\n",
      "    learn_throughput: 6655.173\n",
      "    learn_time_ms: 2253.886\n",
      "    load_throughput: 9572102.789\n",
      "    load_time_ms: 1.567\n",
      "    sample_throughput: 616.214\n",
      "    sample_time_ms: 24342.211\n",
      "    update_time_ms: 1.698\n",
      "  timestamp: 1665246174\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 540000\n",
      "  training_iteration: 36\n",
      "  trial_id: 26f87_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 9.3/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.23 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 accelerator_type:G, 0.0/6.0 CPU_group_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_0_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_5_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_2_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_4_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_1_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_3_25daa8673b2c24aecf80a7fc79df4e93)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_onelane_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_26f87_00000 | RUNNING  | 141.225.10.229:307 |     36 |           985.43 | 540000 | -605.354 |              2530.81 |             -8324.17 |               3000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m ring length: 239\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m v_max: 4.141741239205832\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m ring length: 230\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m v_max: 3.7136148111012934\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m ring length: 268\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m v_max: 5.5194978538368025\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m ring length: 237\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m v_max: 4.04661795519353\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m ring length: 266\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m v_max: 5.42459972166245\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 1 seconds\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_26f87_00000:\n",
      "  agent_timesteps_total: 555000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-08_11-23-19\n",
      "  done: false\n",
      "  episode_len_mean: 3000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2530.805184574418\n",
      "  episode_reward_mean: -701.7527792559529\n",
      "  episode_reward_min: -8324.169161384283\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 185\n",
      "  experiment_id: edcbe001b14143f4bf76a6a2914a7bd0\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.04218750074505806\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: -0.40416282415390015\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.07278131693601608\n",
      "          model: {}\n",
      "          policy_loss: 0.00245867483317852\n",
      "          total_loss: 1828.0992431640625\n",
      "          vf_explained_var: -5.603855601776786e-09\n",
      "          vf_loss: 1828.0936279296875\n",
      "    num_agent_steps_sampled: 555000\n",
      "    num_agent_steps_trained: 555000\n",
      "    num_steps_sampled: 555000\n",
      "    num_steps_trained: 555000\n",
      "  iterations_since_restore: 37\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 24.73714285714286\n",
      "    ram_util_percent: 29.811428571428568\n",
      "  pid: 307\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06639087675050287\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 3.5941168385269675\n",
      "    mean_inference_ms: 0.48544399796621723\n",
      "    mean_raw_obs_processing_ms: 3.7906145871787045\n",
      "  time_since_restore: 1010.2706711292267\n",
      "  time_this_iter_s: 24.841111660003662\n",
      "  time_total_s: 1010.2706711292267\n",
      "  timers:\n",
      "    learn_throughput: 6903.362\n",
      "    learn_time_ms: 2172.854\n",
      "    load_throughput: 9899074.832\n",
      "    load_time_ms: 1.515\n",
      "    sample_throughput: 632.615\n",
      "    sample_time_ms: 23711.087\n",
      "    update_time_ms: 1.687\n",
      "  timestamp: 1665246199\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 555000\n",
      "  training_iteration: 37\n",
      "  trial_id: 26f87_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Memory usage on this node: 9.3/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.23 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_2_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_3_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_4_25daa8673b2c24aecf80a7fc79df4e93, 0.0/6.0 CPU_group_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_5_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_1_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_0_25daa8673b2c24aecf80a7fc79df4e93)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_onelane_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_26f87_00000 | RUNNING  | 141.225.10.229:307 |     37 |          1010.27 | 555000 | -701.753 |              2530.81 |             -8324.17 |               3000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m ring length: 244\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m v_max: 4.379503211387676\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m ring length: 257\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m v_max: 4.997304682316517\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m ring length: 232\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m v_max: 3.8087690783250063\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m ring length: 227\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m v_max: 3.570869368805752\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m ring length: 261\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m v_max: 5.187261846097525\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m Could not connect to TraCI server at localhost:34795 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m Could not connect to TraCI server at localhost:34447 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m Could not connect to TraCI server at localhost:57509 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m Could not connect to TraCI server at localhost:54035 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m Could not connect to TraCI server at localhost:36289 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m Could not connect to TraCI server at localhost:34795 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 3 seconds\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m Could not connect to TraCI server at localhost:34447 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 3 seconds\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m Could not connect to TraCI server at localhost:57509 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 3 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m Could not connect to TraCI server at localhost:54035 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 3 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m Could not connect to TraCI server at localhost:36289 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 3 seconds\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_26f87_00000:\n",
      "  agent_timesteps_total: 570000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-08_11-23-51\n",
      "  done: false\n",
      "  episode_len_mean: 3000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2530.805184574418\n",
      "  episode_reward_mean: -779.2775203911734\n",
      "  episode_reward_min: -8324.169161384283\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 190\n",
      "  experiment_id: edcbe001b14143f4bf76a6a2914a7bd0\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.06328125298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: -0.7493948936462402\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.047352008521556854\n",
      "          model: {}\n",
      "          policy_loss: 0.0035044958349317312\n",
      "          total_loss: 1885.765380859375\n",
      "          vf_explained_var: -1.4773800849354757e-08\n",
      "          vf_loss: 1885.7586669921875\n",
      "    num_agent_steps_sampled: 570000\n",
      "    num_agent_steps_trained: 570000\n",
      "    num_steps_sampled: 570000\n",
      "    num_steps_trained: 570000\n",
      "  iterations_since_restore: 38\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.74666666666667\n",
      "    ram_util_percent: 29.78666666666668\n",
      "  pid: 307\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06635896291630246\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 3.589065835096905\n",
      "    mean_inference_ms: 0.4852868625871909\n",
      "    mean_raw_obs_processing_ms: 3.800212954517346\n",
      "  time_since_restore: 1042.0340704917908\n",
      "  time_this_iter_s: 31.763399362564087\n",
      "  time_total_s: 1042.0340704917908\n",
      "  timers:\n",
      "    learn_throughput: 6886.295\n",
      "    learn_time_ms: 2178.24\n",
      "    load_throughput: 9431053.815\n",
      "    load_time_ms: 1.59\n",
      "    sample_throughput: 615.867\n",
      "    sample_time_ms: 24355.922\n",
      "    update_time_ms: 1.781\n",
      "  timestamp: 1665246231\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 570000\n",
      "  training_iteration: 38\n",
      "  trial_id: 26f87_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 9.3/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.23 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_0_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_3_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_2_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_1_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_5_25daa8673b2c24aecf80a7fc79df4e93, 0.0/6.0 CPU_group_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_4_25daa8673b2c24aecf80a7fc79df4e93)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_onelane_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_26f87_00000 | RUNNING  | 141.225.10.229:307 |     38 |          1042.03 | 570000 | -779.278 |              2530.81 |             -8324.17 |               3000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m ring length: 220\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m v_max: 3.2377399006821497\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m ring length: 224\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m v_max: 3.4281086136538996\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m ring length: 256\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m v_max: 4.949804327743507\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m ring length: 220\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m v_max: 3.2377399006821497\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m ring length: 231\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m v_max: 3.761192925277308\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 1 seconds\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_26f87_00000:\n",
      "  agent_timesteps_total: 585000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-08_11-24-17\n",
      "  done: false\n",
      "  episode_len_mean: 3000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2530.805184574418\n",
      "  episode_reward_mean: -896.626217554708\n",
      "  episode_reward_min: -8324.169161384283\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 195\n",
      "  experiment_id: edcbe001b14143f4bf76a6a2914a7bd0\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.09492187201976776\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.2045072317123413\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.034119922667741776\n",
      "          model: {}\n",
      "          policy_loss: 0.006482519209384918\n",
      "          total_loss: 2254.073486328125\n",
      "          vf_explained_var: -1.528324244937096e-09\n",
      "          vf_loss: 2254.0634765625\n",
      "    num_agent_steps_sampled: 585000\n",
      "    num_agent_steps_trained: 585000\n",
      "    num_steps_sampled: 585000\n",
      "    num_steps_trained: 585000\n",
      "  iterations_since_restore: 39\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 23.762162162162163\n",
      "    ram_util_percent: 29.84594594594595\n",
      "  pid: 307\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06633237912809652\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 3.5843268573881266\n",
      "    mean_inference_ms: 0.4851694164088758\n",
      "    mean_raw_obs_processing_ms: 3.809320430321831\n",
      "  time_since_restore: 1067.3953568935394\n",
      "  time_this_iter_s: 25.361286401748657\n",
      "  time_total_s: 1067.3953568935394\n",
      "  timers:\n",
      "    learn_throughput: 6838.916\n",
      "    learn_time_ms: 2193.33\n",
      "    load_throughput: 9608356.878\n",
      "    load_time_ms: 1.561\n",
      "    sample_throughput: 628.364\n",
      "    sample_time_ms: 23871.509\n",
      "    update_time_ms: 1.521\n",
      "  timestamp: 1665246257\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 585000\n",
      "  training_iteration: 39\n",
      "  trial_id: 26f87_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 9.3/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.23 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_1_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_5_25daa8673b2c24aecf80a7fc79df4e93, 0.0/6.0 CPU_group_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_4_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_2_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_3_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_0_25daa8673b2c24aecf80a7fc79df4e93)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_onelane_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_26f87_00000 | RUNNING  | 141.225.10.229:307 |     39 |           1067.4 | 585000 | -896.626 |              2530.81 |             -8324.17 |               3000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m ring length: 231\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m v_max: 3.761192925277308\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m ring length: 247\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m v_max: 4.522125250095652\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m ring length: 253\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m v_max: 4.807278529691987\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m ring length: 262\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m v_max: 5.234739483008334\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m ring length: 236\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m v_max: 3.999052674951912\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 1 seconds\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_26f87_00000:\n",
      "  agent_timesteps_total: 600000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-08_11-24-42\n",
      "  done: false\n",
      "  episode_len_mean: 3000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2530.805184574418\n",
      "  episode_reward_mean: -969.2625074352809\n",
      "  episode_reward_min: -8324.169161384283\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 200\n",
      "  experiment_id: edcbe001b14143f4bf76a6a2914a7bd0\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.09492187201976776\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.15661288797855377\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0233453381806612\n",
      "          model: {}\n",
      "          policy_loss: 0.0008890045573934913\n",
      "          total_loss: 1810.4990234375\n",
      "          vf_explained_var: -2.0377657339309962e-09\n",
      "          vf_loss: 1810.4959716796875\n",
      "    num_agent_steps_sampled: 600000\n",
      "    num_agent_steps_trained: 600000\n",
      "    num_steps_sampled: 600000\n",
      "    num_steps_trained: 600000\n",
      "  iterations_since_restore: 40\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 24.650000000000002\n",
      "    ram_util_percent: 29.84722222222222\n",
      "  pid: 307\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06630680876361153\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 3.5798966904635443\n",
      "    mean_inference_ms: 0.4850609309378369\n",
      "    mean_raw_obs_processing_ms: 3.8189187647973792\n",
      "  time_since_restore: 1092.5483605861664\n",
      "  time_this_iter_s: 25.153003692626953\n",
      "  time_total_s: 1092.5483605861664\n",
      "  timers:\n",
      "    learn_throughput: 6817.889\n",
      "    learn_time_ms: 2200.094\n",
      "    load_throughput: 9354212.138\n",
      "    load_time_ms: 1.604\n",
      "    sample_throughput: 627.619\n",
      "    sample_time_ms: 23899.853\n",
      "    update_time_ms: 1.388\n",
      "  timestamp: 1665246282\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 600000\n",
      "  training_iteration: 40\n",
      "  trial_id: 26f87_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 9.3/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.23 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_2_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_0_25daa8673b2c24aecf80a7fc79df4e93, 0.0/6.0 CPU_group_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_3_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_4_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_1_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_5_25daa8673b2c24aecf80a7fc79df4e93)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_onelane_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_26f87_00000 | RUNNING  | 141.225.10.229:307 |     40 |          1092.55 | 600000 | -969.263 |              2530.81 |             -8324.17 |               3000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m ring length: 227\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m v_max: 3.570869368805752\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m ring length: 245\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m v_max: 4.427046998576354\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m ring length: 240\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m v_max: 4.189299083856172\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m ring length: 222\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m v_max: 3.3329270738617227\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m ring length: 244\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m v_max: 4.379503211387676\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m Could not connect to TraCI server at localhost:34795 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m Could not connect to TraCI server at localhost:34447 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m Could not connect to TraCI server at localhost:57509 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m Could not connect to TraCI server at localhost:54035 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m Could not connect to TraCI server at localhost:36289 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m Could not connect to TraCI server at localhost:36289 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 3 seconds\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m Could not connect to TraCI server at localhost:34795 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 3 seconds\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m Could not connect to TraCI server at localhost:34447 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 3 seconds\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m Could not connect to TraCI server at localhost:57509 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 3 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m Could not connect to TraCI server at localhost:54035 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 3 seconds\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_26f87_00000:\n",
      "  agent_timesteps_total: 615000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-08_11-25-12\n",
      "  done: false\n",
      "  episode_len_mean: 3000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2530.805184574418\n",
      "  episode_reward_mean: -945.2783820944851\n",
      "  episode_reward_min: -8324.169161384283\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 205\n",
      "  experiment_id: edcbe001b14143f4bf76a6a2914a7bd0\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.09492187201976776\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: -0.47183477878570557\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.044227395206689835\n",
      "          model: {}\n",
      "          policy_loss: 0.004643541295081377\n",
      "          total_loss: 1941.15576171875\n",
      "          vf_explained_var: -7.641621557752387e-09\n",
      "          vf_loss: 1941.1466064453125\n",
      "    num_agent_steps_sampled: 615000\n",
      "    num_agent_steps_trained: 615000\n",
      "    num_steps_sampled: 615000\n",
      "    num_steps_trained: 615000\n",
      "  iterations_since_restore: 41\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.467441860465115\n",
      "    ram_util_percent: 29.888372093023257\n",
      "  pid: 307\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06628072136658196\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 3.575809865944525\n",
      "    mean_inference_ms: 0.4849565651489528\n",
      "    mean_raw_obs_processing_ms: 3.826820387211896\n",
      "  time_since_restore: 1122.8908605575562\n",
      "  time_this_iter_s: 30.34249997138977\n",
      "  time_total_s: 1122.8908605575562\n",
      "  timers:\n",
      "    learn_throughput: 6846.355\n",
      "    learn_time_ms: 2190.947\n",
      "    load_throughput: 8331950.735\n",
      "    load_time_ms: 1.8\n",
      "    sample_throughput: 614.372\n",
      "    sample_time_ms: 24415.16\n",
      "    update_time_ms: 1.638\n",
      "  timestamp: 1665246312\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 615000\n",
      "  training_iteration: 41\n",
      "  trial_id: 26f87_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 9.4/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.23 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_0_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_4_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_2_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_3_25daa8673b2c24aecf80a7fc79df4e93, 0.0/6.0 CPU_group_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_1_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_5_25daa8673b2c24aecf80a7fc79df4e93)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_onelane_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_26f87_00000 | RUNNING  | 141.225.10.229:307 |     41 |          1122.89 | 615000 | -945.278 |              2530.81 |             -8324.17 |               3000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m ring length: 236\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m v_max: 3.999052674951912\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m ring length: 252\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m v_max: 4.759762049824181\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m ring length: 265\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m v_max: 5.37714246265477\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m ring length: 251\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m v_max: 4.71224180704279\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m ring length: 242\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m v_max: 4.2844067680020546\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 1 seconds\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_26f87_00000:\n",
      "  agent_timesteps_total: 630000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-08_11-25-38\n",
      "  done: false\n",
      "  episode_len_mean: 3000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2530.805184574418\n",
      "  episode_reward_mean: -1000.8671805019396\n",
      "  episode_reward_min: -8324.169161384283\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 210\n",
      "  experiment_id: edcbe001b14143f4bf76a6a2914a7bd0\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.14238281548023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.3165137469768524\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008419708348810673\n",
      "          model: {}\n",
      "          policy_loss: 0.0009574752184562385\n",
      "          total_loss: 2037.0953369140625\n",
      "          vf_explained_var: -1.4773800849354757e-08\n",
      "          vf_loss: 2037.09326171875\n",
      "    num_agent_steps_sampled: 630000\n",
      "    num_agent_steps_trained: 630000\n",
      "    num_steps_sampled: 630000\n",
      "    num_steps_trained: 630000\n",
      "  iterations_since_restore: 42\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 24.786486486486485\n",
      "    ram_util_percent: 29.900000000000006\n",
      "  pid: 307\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06625777890068817\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 3.572074583163265\n",
      "    mean_inference_ms: 0.48486578355578275\n",
      "    mean_raw_obs_processing_ms: 3.831415444542888\n",
      "  time_since_restore: 1148.760927438736\n",
      "  time_this_iter_s: 25.87006688117981\n",
      "  time_total_s: 1148.760927438736\n",
      "  timers:\n",
      "    learn_throughput: 6700.104\n",
      "    learn_time_ms: 2238.771\n",
      "    load_throughput: 8391964.786\n",
      "    load_time_ms: 1.787\n",
      "    sample_throughput: 614.875\n",
      "    sample_time_ms: 24395.19\n",
      "    update_time_ms: 1.909\n",
      "  timestamp: 1665246338\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 630000\n",
      "  training_iteration: 42\n",
      "  trial_id: 26f87_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Memory usage on this node: 9.3/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.23 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_2_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_3_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_0_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_5_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_4_25daa8673b2c24aecf80a7fc79df4e93, 0.0/6.0 CPU_group_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_1_25daa8673b2c24aecf80a7fc79df4e93)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_onelane_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_26f87_00000 | RUNNING  | 141.225.10.229:307 |     42 |          1148.76 | 630000 | -1000.87 |              2530.81 |             -8324.17 |               3000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m ring length: 233\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m v_max: 3.856343201216534\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m ring length: 238\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m v_max: 4.094180836186086\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m ring length: 235\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m v_max: 3.9514850725282584\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m ring length: 226\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m v_max: 3.5232840694769565\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m ring length: 255\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m v_max: 4.902299776966978\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 1 seconds\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_26f87_00000:\n",
      "  agent_timesteps_total: 645000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-08_11-26-03\n",
      "  done: false\n",
      "  episode_len_mean: 3000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2530.805184574418\n",
      "  episode_reward_mean: -1153.7186941679718\n",
      "  episode_reward_min: -8324.169161384283\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 215\n",
      "  experiment_id: edcbe001b14143f4bf76a6a2914a7bd0\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.07119140774011612\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.662552833557129\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.051863737404346466\n",
      "          model: {}\n",
      "          policy_loss: -0.0005233330884948373\n",
      "          total_loss: 2574.174560546875\n",
      "          vf_explained_var: 9.16994569166718e-09\n",
      "          vf_loss: 2574.171142578125\n",
      "    num_agent_steps_sampled: 645000\n",
      "    num_agent_steps_trained: 645000\n",
      "    num_steps_sampled: 645000\n",
      "    num_steps_trained: 645000\n",
      "  iterations_since_restore: 43\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 24.41666666666667\n",
      "    ram_util_percent: 29.94166666666667\n",
      "  pid: 307\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06623563757877324\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 3.568524535905829\n",
      "    mean_inference_ms: 0.48478461188880645\n",
      "    mean_raw_obs_processing_ms: 3.8327153751348355\n",
      "  time_since_restore: 1173.8651118278503\n",
      "  time_this_iter_s: 25.10418438911438\n",
      "  time_total_s: 1173.8651118278503\n",
      "  timers:\n",
      "    learn_throughput: 6664.035\n",
      "    learn_time_ms: 2250.888\n",
      "    load_throughput: 8352081.563\n",
      "    load_time_ms: 1.796\n",
      "    sample_throughput: 615.181\n",
      "    sample_time_ms: 24383.074\n",
      "    update_time_ms: 1.837\n",
      "  timestamp: 1665246363\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 645000\n",
      "  training_iteration: 43\n",
      "  trial_id: 26f87_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 9.3/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.23 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_5_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_3_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_0_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_2_25daa8673b2c24aecf80a7fc79df4e93, 0.0/6.0 CPU_group_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_4_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_1_25daa8673b2c24aecf80a7fc79df4e93)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_onelane_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_26f87_00000 | RUNNING  | 141.225.10.229:307 |     43 |          1173.87 | 645000 | -1153.72 |              2530.81 |             -8324.17 |               3000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m ring length: 243\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m v_max: 4.331956438196479\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m ring length: 236\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m v_max: 3.999052674951912\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m ring length: 239\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m v_max: 4.141741239205832\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m ring length: 244\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m v_max: 4.379503211387676\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m ring length: 235\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m v_max: 3.9514850725282584\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 1 seconds\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_26f87_00000:\n",
      "  agent_timesteps_total: 660000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-08_11-26-28\n",
      "  done: false\n",
      "  episode_len_mean: 3000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2530.805184574418\n",
      "  episode_reward_mean: -1241.4017124174989\n",
      "  episode_reward_min: -8324.169161384283\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 220\n",
      "  experiment_id: edcbe001b14143f4bf76a6a2914a7bd0\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.10678710788488388\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.2582852840423584\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.02585858479142189\n",
      "          model: {}\n",
      "          policy_loss: 0.0017101162811741233\n",
      "          total_loss: 2049.490478515625\n",
      "          vf_explained_var: 6.622738357719982e-09\n",
      "          vf_loss: 2049.485595703125\n",
      "    num_agent_steps_sampled: 660000\n",
      "    num_agent_steps_trained: 660000\n",
      "    num_steps_sampled: 660000\n",
      "    num_steps_trained: 660000\n",
      "  iterations_since_restore: 44\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 24.35833333333333\n",
      "    ram_util_percent: 29.950000000000003\n",
      "  pid: 307\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06621416851060528\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 3.565008960483427\n",
      "    mean_inference_ms: 0.4847093579367368\n",
      "    mean_raw_obs_processing_ms: 3.834715966231754\n",
      "  time_since_restore: 1198.8811838626862\n",
      "  time_this_iter_s: 25.016072034835815\n",
      "  time_total_s: 1198.8811838626862\n",
      "  timers:\n",
      "    learn_throughput: 6604.498\n",
      "    learn_time_ms: 2271.179\n",
      "    load_throughput: 9106042.755\n",
      "    load_time_ms: 1.647\n",
      "    sample_throughput: 618.029\n",
      "    sample_time_ms: 24270.697\n",
      "    update_time_ms: 1.76\n",
      "  timestamp: 1665246388\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 660000\n",
      "  training_iteration: 44\n",
      "  trial_id: 26f87_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Memory usage on this node: 9.3/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.23 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_5_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_2_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_4_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_3_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_1_25daa8673b2c24aecf80a7fc79df4e93, 0.0/6.0 CPU_group_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_0_25daa8673b2c24aecf80a7fc79df4e93)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_onelane_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_26f87_00000 | RUNNING  | 141.225.10.229:307 |     44 |          1198.88 | 660000 |  -1241.4 |              2530.81 |             -8324.17 |               3000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m ring length: 231\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m v_max: 3.761192925277308\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m ring length: 245\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m v_max: 4.427046998576354\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m ring length: 255\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m v_max: 4.902299776966978\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m ring length: 270\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m v_max: 5.6143732387852054\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m ring length: 265\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m v_max: 5.37714246265477\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_26f87_00000:\n",
      "  agent_timesteps_total: 675000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-08_11-26-53\n",
      "  done: false\n",
      "  episode_len_mean: 3000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2530.805184574418\n",
      "  episode_reward_mean: -1310.4335233457134\n",
      "  episode_reward_min: -8324.169161384283\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 225\n",
      "  experiment_id: edcbe001b14143f4bf76a6a2914a7bd0\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.10678710788488388\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.1086300611495972\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.07039123773574829\n",
      "          model: {}\n",
      "          policy_loss: 0.009093414060771465\n",
      "          total_loss: 2466.316162109375\n",
      "          vf_explained_var: -5.094414223805188e-09\n",
      "          vf_loss: 2466.299560546875\n",
      "    num_agent_steps_sampled: 675000\n",
      "    num_agent_steps_trained: 675000\n",
      "    num_steps_sampled: 675000\n",
      "    num_steps_trained: 675000\n",
      "  iterations_since_restore: 45\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 25.245714285714282\n",
      "    ram_util_percent: 29.92\n",
      "  pid: 307\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06619404751450812\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 3.561629539334993\n",
      "    mean_inference_ms: 0.48463761873763966\n",
      "    mean_raw_obs_processing_ms: 3.82975151778491\n",
      "  time_since_restore: 1223.4750275611877\n",
      "  time_this_iter_s: 24.593843698501587\n",
      "  time_total_s: 1223.4750275611877\n",
      "  timers:\n",
      "    learn_throughput: 6629.765\n",
      "    learn_time_ms: 2262.524\n",
      "    load_throughput: 9051152.352\n",
      "    load_time_ms: 1.657\n",
      "    sample_throughput: 621.531\n",
      "    sample_time_ms: 24133.943\n",
      "    update_time_ms: 1.763\n",
      "  timestamp: 1665246413\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 675000\n",
      "  training_iteration: 45\n",
      "  trial_id: 26f87_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 9.3/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.23 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_0_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_1_25daa8673b2c24aecf80a7fc79df4e93, 0.0/6.0 CPU_group_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_4_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_2_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_3_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_5_25daa8673b2c24aecf80a7fc79df4e93)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_onelane_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_26f87_00000 | RUNNING  | 141.225.10.229:307 |     45 |          1223.48 | 675000 | -1310.43 |              2530.81 |             -8324.17 |               3000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m ring length: 222\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m v_max: 3.3329270738617227\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m ring length: 229\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m v_max: 3.666034803266416\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m ring length: 269\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m v_max: 5.566938458220921\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m ring length: 221\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m v_max: 3.285334164169417\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m ring length: 240\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m v_max: 4.189299083856172\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 1 seconds\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_26f87_00000:\n",
      "  agent_timesteps_total: 690000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-08_11-27-18\n",
      "  done: false\n",
      "  episode_len_mean: 3000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2530.805184574418\n",
      "  episode_reward_mean: -1436.2369824765974\n",
      "  episode_reward_min: -8324.169161384283\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 230\n",
      "  experiment_id: edcbe001b14143f4bf76a6a2914a7bd0\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.16018065810203552\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.743571400642395\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.028430551290512085\n",
      "          model: {}\n",
      "          policy_loss: 0.013742971234023571\n",
      "          total_loss: 2531.99951171875\n",
      "          vf_explained_var: -7.641621557752387e-09\n",
      "          vf_loss: 2531.981201171875\n",
      "    num_agent_steps_sampled: 690000\n",
      "    num_agent_steps_trained: 690000\n",
      "    num_steps_sampled: 690000\n",
      "    num_steps_trained: 690000\n",
      "  iterations_since_restore: 46\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 25.10857142857143\n",
      "    ram_util_percent: 29.962857142857143\n",
      "  pid: 307\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06617384429370106\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 3.558231155622902\n",
      "    mean_inference_ms: 0.4845686712104642\n",
      "    mean_raw_obs_processing_ms: 3.824989118370532\n",
      "  time_since_restore: 1248.1178138256073\n",
      "  time_this_iter_s: 24.642786264419556\n",
      "  time_total_s: 1248.1178138256073\n",
      "  timers:\n",
      "    learn_throughput: 6563.096\n",
      "    learn_time_ms: 2285.507\n",
      "    load_throughput: 8293618.424\n",
      "    load_time_ms: 1.809\n",
      "    sample_throughput: 625.729\n",
      "    sample_time_ms: 23972.053\n",
      "    update_time_ms: 1.755\n",
      "  timestamp: 1665246438\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 690000\n",
      "  training_iteration: 46\n",
      "  trial_id: 26f87_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Memory usage on this node: 9.3/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.23 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_5_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_3_25daa8673b2c24aecf80a7fc79df4e93, 0.0/6.0 CPU_group_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_4_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_0_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_2_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_1_25daa8673b2c24aecf80a7fc79df4e93)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_onelane_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_26f87_00000 | RUNNING  | 141.225.10.229:307 |     46 |          1248.12 | 690000 | -1436.24 |              2530.81 |             -8324.17 |               3000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m ring length: 265\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m v_max: 5.37714246265477\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m ring length: 253\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m v_max: 4.807278529691987\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m ring length: 262\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m v_max: 5.234739483008334\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m ring length: 226\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m v_max: 3.5232840694769565\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m ring length: 232\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m v_max: 3.8087690783250063\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 1 seconds\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_26f87_00000:\n",
      "  agent_timesteps_total: 705000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-08_11-27-42\n",
      "  done: false\n",
      "  episode_len_mean: 3000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2530.805184574418\n",
      "  episode_reward_mean: -1439.9156290296457\n",
      "  episode_reward_min: -8324.169161384283\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 235\n",
      "  experiment_id: edcbe001b14143f4bf76a6a2914a7bd0\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.16018065810203552\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: -0.19514359533786774\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01812095381319523\n",
      "          model: {}\n",
      "          policy_loss: 0.008307283744215965\n",
      "          total_loss: 1888.0706787109375\n",
      "          vf_explained_var: 2.0377657339309962e-09\n",
      "          vf_loss: 1888.0595703125\n",
      "    num_agent_steps_sampled: 705000\n",
      "    num_agent_steps_trained: 705000\n",
      "    num_steps_sampled: 705000\n",
      "    num_steps_trained: 705000\n",
      "  iterations_since_restore: 47\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 24.737142857142857\n",
      "    ram_util_percent: 29.954285714285717\n",
      "  pid: 307\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06615328970339411\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 3.554930249062177\n",
      "    mean_inference_ms: 0.48449358519737046\n",
      "    mean_raw_obs_processing_ms: 3.817514018496291\n",
      "  time_since_restore: 1272.8203501701355\n",
      "  time_this_iter_s: 24.7025363445282\n",
      "  time_total_s: 1272.8203501701355\n",
      "  timers:\n",
      "    learn_throughput: 6569.587\n",
      "    learn_time_ms: 2283.249\n",
      "    load_throughput: 8951858.966\n",
      "    load_time_ms: 1.676\n",
      "    sample_throughput: 626.03\n",
      "    sample_time_ms: 23960.508\n",
      "    update_time_ms: 1.823\n",
      "  timestamp: 1665246462\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 705000\n",
      "  training_iteration: 47\n",
      "  trial_id: 26f87_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 9.3/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.23 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_1_25daa8673b2c24aecf80a7fc79df4e93, 0.0/6.0 CPU_group_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_0_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_3_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_4_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_2_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_5_25daa8673b2c24aecf80a7fc79df4e93)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_onelane_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_26f87_00000 | RUNNING  | 141.225.10.229:307 |     47 |          1272.82 | 705000 | -1439.92 |              2530.81 |             -8324.17 |               3000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m ring length: 266\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m v_max: 5.42459972166245\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m ring length: 236\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m v_max: 3.999052674951912\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m ring length: 266\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m v_max: 5.42459972166245\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m ring length: 246\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m v_max: 4.474587708995648\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m ring length: 239\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m v_max: 4.141741239205832\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 1 seconds\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_26f87_00000:\n",
      "  agent_timesteps_total: 720000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-08_11-28-07\n",
      "  done: false\n",
      "  episode_len_mean: 3000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2530.805184574418\n",
      "  episode_reward_mean: -1316.4807107592815\n",
      "  episode_reward_min: -8324.169161384283\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 240\n",
      "  experiment_id: edcbe001b14143f4bf76a6a2914a7bd0\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.16018065810203552\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: -1.6789898872375488\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.044577572494745255\n",
      "          model: {}\n",
      "          policy_loss: 0.002835844876244664\n",
      "          total_loss: 1459.60693359375\n",
      "          vf_explained_var: -3.5660898678457897e-09\n",
      "          vf_loss: 1459.5970458984375\n",
      "    num_agent_steps_sampled: 720000\n",
      "    num_agent_steps_trained: 720000\n",
      "    num_steps_sampled: 720000\n",
      "    num_steps_trained: 720000\n",
      "  iterations_since_restore: 48\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 24.769444444444446\n",
      "    ram_util_percent: 29.958333333333332\n",
      "  pid: 307\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06613372050193286\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 3.5518366453542285\n",
      "    mean_inference_ms: 0.48442155083612415\n",
      "    mean_raw_obs_processing_ms: 3.8103118899532906\n",
      "  time_since_restore: 1297.5121099948883\n",
      "  time_this_iter_s: 24.691759824752808\n",
      "  time_total_s: 1297.5121099948883\n",
      "  timers:\n",
      "    learn_throughput: 6571.472\n",
      "    learn_time_ms: 2282.594\n",
      "    load_throughput: 9287652.79\n",
      "    load_time_ms: 1.615\n",
      "    sample_throughput: 645.018\n",
      "    sample_time_ms: 23255.178\n",
      "    update_time_ms: 1.68\n",
      "  timestamp: 1665246487\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 720000\n",
      "  training_iteration: 48\n",
      "  trial_id: 26f87_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Memory usage on this node: 9.3/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.23 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_4_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_5_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_1_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_2_25daa8673b2c24aecf80a7fc79df4e93, 0.0/6.0 CPU_group_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_0_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_3_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 accelerator_type:G)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_onelane_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_26f87_00000 | RUNNING  | 141.225.10.229:307 |     48 |          1297.51 | 720000 | -1316.48 |              2530.81 |             -8324.17 |               3000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m ring length: 226\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m v_max: 3.5232840694769565\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m ring length: 241\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m v_max: 4.236854288051661\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m ring length: 263\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m v_max: 5.282212215151455\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m ring length: 241\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m v_max: 4.236854288051661\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m ring length: 246\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m v_max: 4.474587708995648\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 1 seconds\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_26f87_00000:\n",
      "  agent_timesteps_total: 735000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-08_11-28-33\n",
      "  done: false\n",
      "  episode_len_mean: 3000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2530.805184574418\n",
      "  episode_reward_mean: -1314.2569757268682\n",
      "  episode_reward_min: -8324.169161384283\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 245\n",
      "  experiment_id: edcbe001b14143f4bf76a6a2914a7bd0\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.24027100205421448\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: -0.6086551547050476\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.02731219492852688\n",
      "          model: {}\n",
      "          policy_loss: 0.00933505967259407\n",
      "          total_loss: 1973.837890625\n",
      "          vf_explained_var: -5.094414223805188e-09\n",
      "          vf_loss: 1973.8221435546875\n",
      "    num_agent_steps_sampled: 735000\n",
      "    num_agent_steps_trained: 735000\n",
      "    num_steps_sampled: 735000\n",
      "    num_steps_trained: 735000\n",
      "  iterations_since_restore: 49\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 23.85135135135135\n",
      "    ram_util_percent: 29.945945945945947\n",
      "  pid: 307\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06611391657905244\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 3.548937690374755\n",
      "    mean_inference_ms: 0.4843460373180516\n",
      "    mean_raw_obs_processing_ms: 3.801273076735894\n",
      "  time_since_restore: 1323.247163772583\n",
      "  time_this_iter_s: 25.735053777694702\n",
      "  time_total_s: 1323.247163772583\n",
      "  timers:\n",
      "    learn_throughput: 6703.097\n",
      "    learn_time_ms: 2237.772\n",
      "    load_throughput: 9299733.932\n",
      "    load_time_ms: 1.613\n",
      "    sample_throughput: 642.754\n",
      "    sample_time_ms: 23337.094\n",
      "    update_time_ms: 1.667\n",
      "  timestamp: 1665246513\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 735000\n",
      "  training_iteration: 49\n",
      "  trial_id: 26f87_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 9.3/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.23 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_1_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_4_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_3_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_2_25daa8673b2c24aecf80a7fc79df4e93, 0.0/6.0 CPU_group_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_0_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_5_25daa8673b2c24aecf80a7fc79df4e93)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_onelane_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_26f87_00000 | RUNNING  | 141.225.10.229:307 |     49 |          1323.25 | 735000 | -1314.26 |              2530.81 |             -8324.17 |               3000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m ring length: 222\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m v_max: 3.3329270738617227\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m ring length: 239\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m v_max: 4.141741239205832\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m ring length: 225\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m v_max: 3.4756971311168834\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m ring length: 246\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m v_max: 4.474587708995648\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m ring length: 227\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m v_max: 3.570869368805752\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 1 seconds\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_26f87_00000:\n",
      "  agent_timesteps_total: 750000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-08_11-28-59\n",
      "  done: false\n",
      "  episode_len_mean: 3000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2530.805184574418\n",
      "  episode_reward_mean: -1301.016964118617\n",
      "  episode_reward_min: -8324.169161384283\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 250\n",
      "  experiment_id: edcbe001b14143f4bf76a6a2914a7bd0\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.24027100205421448\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.09809242188930511\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.019569048658013344\n",
      "          model: {}\n",
      "          policy_loss: 0.00019594721379689872\n",
      "          total_loss: 2082.056396484375\n",
      "          vf_explained_var: 2.547207111902594e-09\n",
      "          vf_loss: 2082.05126953125\n",
      "    num_agent_steps_sampled: 750000\n",
      "    num_agent_steps_trained: 750000\n",
      "    num_steps_sampled: 750000\n",
      "    num_steps_trained: 750000\n",
      "  iterations_since_restore: 50\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 24.499999999999993\n",
      "    ram_util_percent: 29.958333333333332\n",
      "  pid: 307\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0660954323033027\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 3.5463318398354766\n",
      "    mean_inference_ms: 0.4842770159704235\n",
      "    mean_raw_obs_processing_ms: 3.792733657258995\n",
      "  time_since_restore: 1348.822538614273\n",
      "  time_this_iter_s: 25.575374841690063\n",
      "  time_total_s: 1348.822538614273\n",
      "  timers:\n",
      "    learn_throughput: 6615.119\n",
      "    learn_time_ms: 2267.533\n",
      "    load_throughput: 8503921.171\n",
      "    load_time_ms: 1.764\n",
      "    sample_throughput: 642.422\n",
      "    sample_time_ms: 23349.131\n",
      "    update_time_ms: 1.948\n",
      "  timestamp: 1665246539\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 750000\n",
      "  training_iteration: 50\n",
      "  trial_id: 26f87_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Memory usage on this node: 9.4/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.23 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_1_25daa8673b2c24aecf80a7fc79df4e93, 0.0/6.0 CPU_group_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_3_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_2_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_5_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_0_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_4_25daa8673b2c24aecf80a7fc79df4e93)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_onelane_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_26f87_00000 | RUNNING  | 141.225.10.229:307 |     50 |          1348.82 | 750000 | -1301.02 |              2530.81 |             -8324.17 |               3000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m ring length: 238\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m v_max: 4.094180836186086\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m ring length: 223\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m v_max: 3.3805185755587788\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m ring length: 223\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m v_max: 3.3805185755587788\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m ring length: 236\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m v_max: 3.999052674951912\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m ring length: 244\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m v_max: 4.379503211387676\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m Could not connect to TraCI server at localhost:34795 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m Could not connect to TraCI server at localhost:54035 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m Could not connect to TraCI server at localhost:36289 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m Could not connect to TraCI server at localhost:34447 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m Could not connect to TraCI server at localhost:57509 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m Could not connect to TraCI server at localhost:34795 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 3 seconds\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m Could not connect to TraCI server at localhost:34447 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 3 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m Could not connect to TraCI server at localhost:54035 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 3 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m Could not connect to TraCI server at localhost:36289 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 3 seconds\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m Could not connect to TraCI server at localhost:57509 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 3 seconds\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_26f87_00000:\n",
      "  agent_timesteps_total: 765000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-08_11-29-29\n",
      "  done: false\n",
      "  episode_len_mean: 3000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2530.805184574418\n",
      "  episode_reward_mean: -1312.3080264658918\n",
      "  episode_reward_min: -8324.169161384283\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 255\n",
      "  experiment_id: edcbe001b14143f4bf76a6a2914a7bd0\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.24027100205421448\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: -0.0023126222658902407\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0073457094840705395\n",
      "          model: {}\n",
      "          policy_loss: 0.0011287028901278973\n",
      "          total_loss: 1904.5986328125\n",
      "          vf_explained_var: -1.7830450005362763e-08\n",
      "          vf_loss: 1904.595947265625\n",
      "    num_agent_steps_sampled: 765000\n",
      "    num_agent_steps_trained: 765000\n",
      "    num_steps_sampled: 765000\n",
      "    num_steps_trained: 765000\n",
      "  iterations_since_restore: 51\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.367441860465114\n",
      "    ram_util_percent: 29.92093023255814\n",
      "  pid: 307\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06607699462672952\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 3.54375210461898\n",
      "    mean_inference_ms: 0.48420490015633155\n",
      "    mean_raw_obs_processing_ms: 3.7866926214941445\n",
      "  time_since_restore: 1378.8991639614105\n",
      "  time_this_iter_s: 30.07662534713745\n",
      "  time_total_s: 1378.8991639614105\n",
      "  timers:\n",
      "    learn_throughput: 6633.378\n",
      "    learn_time_ms: 2261.291\n",
      "    load_throughput: 9577931.706\n",
      "    load_time_ms: 1.566\n",
      "    sample_throughput: 642.972\n",
      "    sample_time_ms: 23329.166\n",
      "    update_time_ms: 1.876\n",
      "  timestamp: 1665246569\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 765000\n",
      "  training_iteration: 51\n",
      "  trial_id: 26f87_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 9.4/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.23 GiB heap, 0.0/14.65 GiB objects (0.0/6.0 CPU_group_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_3_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_5_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_1_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_4_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_2_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_0_25daa8673b2c24aecf80a7fc79df4e93)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_onelane_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_26f87_00000 | RUNNING  | 141.225.10.229:307 |     51 |           1378.9 | 765000 | -1312.31 |              2530.81 |             -8324.17 |               3000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m ring length: 246\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m v_max: 4.474587708995648\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m ring length: 248\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m v_max: 4.569659527528101\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m ring length: 263\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m v_max: 5.282212215151455\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m ring length: 257\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m v_max: 4.997304682316517\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m ring length: 252\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m v_max: 4.759762049824181\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 1 seconds\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_26f87_00000:\n",
      "  agent_timesteps_total: 780000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-08_11-29-53\n",
      "  done: false\n",
      "  episode_len_mean: 3000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2530.805184574418\n",
      "  episode_reward_mean: -1366.5867708731218\n",
      "  episode_reward_min: -8324.169161384283\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 260\n",
      "  experiment_id: edcbe001b14143f4bf76a6a2914a7bd0\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.12013550102710724\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.1953285932540894\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.09249120950698853\n",
      "          model: {}\n",
      "          policy_loss: 0.01079965103417635\n",
      "          total_loss: 2194.1552734375\n",
      "          vf_explained_var: -1.171715258152517e-08\n",
      "          vf_loss: 2194.133056640625\n",
      "    num_agent_steps_sampled: 780000\n",
      "    num_agent_steps_trained: 780000\n",
      "    num_steps_sampled: 780000\n",
      "    num_steps_trained: 780000\n",
      "  iterations_since_restore: 52\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 24.93714285714286\n",
      "    ram_util_percent: 30.011428571428574\n",
      "  pid: 307\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06605945843633435\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 3.5411421790443245\n",
      "    mean_inference_ms: 0.4841298833132809\n",
      "    mean_raw_obs_processing_ms: 3.7811497488539607\n",
      "  time_since_restore: 1403.5665836334229\n",
      "  time_this_iter_s: 24.66741967201233\n",
      "  time_total_s: 1403.5665836334229\n",
      "  timers:\n",
      "    learn_throughput: 6864.265\n",
      "    learn_time_ms: 2185.23\n",
      "    load_throughput: 9337552.317\n",
      "    load_time_ms: 1.606\n",
      "    sample_throughput: 644.183\n",
      "    sample_time_ms: 23285.291\n",
      "    update_time_ms: 1.835\n",
      "  timestamp: 1665246593\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 780000\n",
      "  training_iteration: 52\n",
      "  trial_id: 26f87_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 9.4/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.23 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_5_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_4_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_1_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_3_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_0_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_2_25daa8673b2c24aecf80a7fc79df4e93, 0.0/6.0 CPU_group_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 accelerator_type:G)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_onelane_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_26f87_00000 | RUNNING  | 141.225.10.229:307 |     52 |          1403.57 | 780000 | -1366.59 |              2530.81 |             -8324.17 |               3000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m ring length: 246\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m v_max: 4.474587708995648\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m ring length: 224\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m v_max: 3.4281086136538996\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m ring length: 268\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m v_max: 5.5194978538368025\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m ring length: 258\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m v_max: 5.044800727535787\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m ring length: 266\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m v_max: 5.42459972166245\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 1 seconds\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_26f87_00000:\n",
      "  agent_timesteps_total: 795000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-08_11-30-19\n",
      "  done: false\n",
      "  episode_len_mean: 3000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2530.805184574418\n",
      "  episode_reward_mean: -1579.0757126543308\n",
      "  episode_reward_min: -8324.169161384283\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 265\n",
      "  experiment_id: edcbe001b14143f4bf76a6a2914a7bd0\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.18020324409008026\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 3.1270041465759277\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017975520342588425\n",
      "          model: {}\n",
      "          policy_loss: 0.010701111517846584\n",
      "          total_loss: 3022.4658203125\n",
      "          vf_explained_var: 1.528324244937096e-09\n",
      "          vf_loss: 3022.45166015625\n",
      "    num_agent_steps_sampled: 795000\n",
      "    num_agent_steps_trained: 795000\n",
      "    num_steps_sampled: 795000\n",
      "    num_steps_trained: 795000\n",
      "  iterations_since_restore: 53\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 23.521621621621623\n",
      "    ram_util_percent: 30.143243243243248\n",
      "  pid: 307\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0660422272571874\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 3.5385525912496183\n",
      "    mean_inference_ms: 0.4840589793897659\n",
      "    mean_raw_obs_processing_ms: 3.776135865009756\n",
      "  time_since_restore: 1429.4148030281067\n",
      "  time_this_iter_s: 25.848219394683838\n",
      "  time_total_s: 1429.4148030281067\n",
      "  timers:\n",
      "    learn_throughput: 7057.171\n",
      "    learn_time_ms: 2125.497\n",
      "    load_throughput: 9399070.768\n",
      "    load_time_ms: 1.596\n",
      "    sample_throughput: 640.501\n",
      "    sample_time_ms: 23419.157\n",
      "    update_time_ms: 1.836\n",
      "  timestamp: 1665246619\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 795000\n",
      "  training_iteration: 53\n",
      "  trial_id: 26f87_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 9.4/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.23 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_0_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_3_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_2_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_1_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_5_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_4_25daa8673b2c24aecf80a7fc79df4e93, 0.0/6.0 CPU_group_25daa8673b2c24aecf80a7fc79df4e93)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_onelane_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_26f87_00000 | RUNNING  | 141.225.10.229:307 |     53 |          1429.41 | 795000 | -1579.08 |              2530.81 |             -8324.17 |               3000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m ring length: 264\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m v_max: 5.329679917416892\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m ring length: 253\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m v_max: 4.807278529691987\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m ring length: 225\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m v_max: 3.4756971311168834\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m ring length: 240\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m v_max: 4.189299083856172\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m ring length: 266\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m v_max: 5.42459972166245\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 1 seconds\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_26f87_00000:\n",
      "  agent_timesteps_total: 810000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-08_11-30-46\n",
      "  done: false\n",
      "  episode_len_mean: 3000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2530.805184574418\n",
      "  episode_reward_mean: -1573.5630241435988\n",
      "  episode_reward_min: -7392.237639384751\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 270\n",
      "  experiment_id: edcbe001b14143f4bf76a6a2914a7bd0\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.18020324409008026\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.2372386455535889\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01712283492088318\n",
      "          model: {}\n",
      "          policy_loss: 0.007951853796839714\n",
      "          total_loss: 2621.585693359375\n",
      "          vf_explained_var: 0.0\n",
      "          vf_loss: 2621.57421875\n",
      "    num_agent_steps_sampled: 810000\n",
      "    num_agent_steps_trained: 810000\n",
      "    num_steps_sampled: 810000\n",
      "    num_steps_trained: 810000\n",
      "  iterations_since_restore: 54\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 23.325641025641026\n",
      "    ram_util_percent: 30.053846153846145\n",
      "  pid: 307\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06602671646323435\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 3.5361776250331207\n",
      "    mean_inference_ms: 0.483995903213305\n",
      "    mean_raw_obs_processing_ms: 3.7711273074445817\n",
      "  time_since_restore: 1456.1588382720947\n",
      "  time_this_iter_s: 26.744035243988037\n",
      "  time_total_s: 1456.1588382720947\n",
      "  timers:\n",
      "    learn_throughput: 7014.911\n",
      "    learn_time_ms: 2138.302\n",
      "    load_throughput: 8592302.86\n",
      "    load_time_ms: 1.746\n",
      "    sample_throughput: 636.16\n",
      "    sample_time_ms: 23578.971\n",
      "    update_time_ms: 1.813\n",
      "  timestamp: 1665246646\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 810000\n",
      "  training_iteration: 54\n",
      "  trial_id: 26f87_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 9.4/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.23 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_0_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_1_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_3_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_5_25daa8673b2c24aecf80a7fc79df4e93, 0.0/6.0 CPU_group_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_4_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_2_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 accelerator_type:G)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_onelane_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_26f87_00000 | RUNNING  | 141.225.10.229:307 |     54 |          1456.16 | 810000 | -1573.56 |              2530.81 |             -7392.24 |               3000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m ring length: 238\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m v_max: 4.094180836186086\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m ring length: 220\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m v_max: 3.2377399006821497\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m ring length: 243\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m v_max: 4.331956438196479\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m ring length: 249\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m v_max: 4.617190445131122\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m ring length: 225\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m v_max: 3.4756971311168834\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 1 seconds\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_26f87_00000:\n",
      "  agent_timesteps_total: 825000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-08_11-31-11\n",
      "  done: false\n",
      "  episode_len_mean: 3000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2314.450685613813\n",
      "  episode_reward_mean: -1659.0327647555462\n",
      "  episode_reward_min: -7392.237639384751\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 275\n",
      "  experiment_id: edcbe001b14143f4bf76a6a2914a7bd0\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.18020324409008026\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: -0.4852977991104126\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015464474447071552\n",
      "          model: {}\n",
      "          policy_loss: 0.003783897962421179\n",
      "          total_loss: 2259.993896484375\n",
      "          vf_explained_var: -4.0755314678619925e-09\n",
      "          vf_loss: 2259.98681640625\n",
      "    num_agent_steps_sampled: 825000\n",
      "    num_agent_steps_trained: 825000\n",
      "    num_steps_sampled: 825000\n",
      "    num_steps_trained: 825000\n",
      "  iterations_since_restore: 55\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 24.608333333333334\n",
      "    ram_util_percent: 30.05833333333333\n",
      "  pid: 307\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06601063853346734\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 3.5337624322403047\n",
      "    mean_inference_ms: 0.48392701817844785\n",
      "    mean_raw_obs_processing_ms: 3.766160421746232\n",
      "  time_since_restore: 1481.2898523807526\n",
      "  time_this_iter_s: 25.131014108657837\n",
      "  time_total_s: 1481.2898523807526\n",
      "  timers:\n",
      "    learn_throughput: 6930.48\n",
      "    learn_time_ms: 2164.352\n",
      "    load_throughput: 8557126.341\n",
      "    load_time_ms: 1.753\n",
      "    sample_throughput: 635.428\n",
      "    sample_time_ms: 23606.154\n",
      "    update_time_ms: 2.045\n",
      "  timestamp: 1665246671\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 825000\n",
      "  training_iteration: 55\n",
      "  trial_id: 26f87_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 9.4/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.23 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_4_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_2_25daa8673b2c24aecf80a7fc79df4e93, 0.0/6.0 CPU_group_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_5_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_3_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_1_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_0_25daa8673b2c24aecf80a7fc79df4e93)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_onelane_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_26f87_00000 | RUNNING  | 141.225.10.229:307 |     55 |          1481.29 | 825000 | -1659.03 |              2314.45 |             -7392.24 |               3000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m ring length: 246\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m v_max: 4.474587708995648\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m ring length: 246\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m v_max: 4.474587708995648\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m ring length: 259\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m v_max: 5.092292348291853\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m ring length: 229\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m v_max: 3.666034803266416\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m ring length: 251\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m v_max: 4.71224180704279\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 1 seconds\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_26f87_00000:\n",
      "  agent_timesteps_total: 840000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-08_11-31-36\n",
      "  done: false\n",
      "  episode_len_mean: 3000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2314.450685613813\n",
      "  episode_reward_mean: -1772.9961188808586\n",
      "  episode_reward_min: -7392.237639384751\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 280\n",
      "  experiment_id: edcbe001b14143f4bf76a6a2914a7bd0\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.18020324409008026\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.668113350868225\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010119098238646984\n",
      "          model: {}\n",
      "          policy_loss: 0.003140100045129657\n",
      "          total_loss: 2555.914794921875\n",
      "          vf_explained_var: 4.0755314678619925e-09\n",
      "          vf_loss: 2555.909912109375\n",
      "    num_agent_steps_sampled: 840000\n",
      "    num_agent_steps_trained: 840000\n",
      "    num_steps_sampled: 840000\n",
      "    num_steps_trained: 840000\n",
      "  iterations_since_restore: 56\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 24.705714285714286\n",
      "    ram_util_percent: 30.05142857142857\n",
      "  pid: 307\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0659942077258813\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 3.531345602210022\n",
      "    mean_inference_ms: 0.483854319688773\n",
      "    mean_raw_obs_processing_ms: 3.7610286123651986\n",
      "  time_since_restore: 1506.0928206443787\n",
      "  time_this_iter_s: 24.8029682636261\n",
      "  time_total_s: 1506.0928206443787\n",
      "  timers:\n",
      "    learn_throughput: 7001.247\n",
      "    learn_time_ms: 2142.476\n",
      "    load_throughput: 9191584.853\n",
      "    load_time_ms: 1.632\n",
      "    sample_throughput: 634.405\n",
      "    sample_time_ms: 23644.201\n",
      "    update_time_ms: 2.048\n",
      "  timestamp: 1665246696\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 840000\n",
      "  training_iteration: 56\n",
      "  trial_id: 26f87_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 9.4/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.23 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_2_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_0_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_1_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_4_25daa8673b2c24aecf80a7fc79df4e93, 0.0/6.0 CPU_group_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_5_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_3_25daa8673b2c24aecf80a7fc79df4e93)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_onelane_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_26f87_00000 | RUNNING  | 141.225.10.229:307 |     56 |          1506.09 | 840000 |    -1773 |              2314.45 |             -7392.24 |               3000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m ring length: 264\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m v_max: 5.329679917416892\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m ring length: 265\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m v_max: 5.37714246265477\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m ring length: 221\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m v_max: 3.285334164169417\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m ring length: 265\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m v_max: 5.37714246265477\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m ring length: 231\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m v_max: 3.761192925277308\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_26f87_00000:\n",
      "  agent_timesteps_total: 855000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-08_11-32-01\n",
      "  done: false\n",
      "  episode_len_mean: 3000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2314.450685613813\n",
      "  episode_reward_mean: -1866.9593611574614\n",
      "  episode_reward_min: -7392.237639384751\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 285\n",
      "  experiment_id: edcbe001b14143f4bf76a6a2914a7bd0\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.18020324409008026\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.2936674356460571\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.022197339683771133\n",
      "          model: {}\n",
      "          policy_loss: 0.004499238450080156\n",
      "          total_loss: 2717.63134765625\n",
      "          vf_explained_var: 1.0188828669654981e-09\n",
      "          vf_loss: 2717.62255859375\n",
      "    num_agent_steps_sampled: 855000\n",
      "    num_agent_steps_trained: 855000\n",
      "    num_steps_sampled: 855000\n",
      "    num_steps_trained: 855000\n",
      "  iterations_since_restore: 57\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 25.140000000000004\n",
      "    ram_util_percent: 30.071428571428573\n",
      "  pid: 307\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06597782071491766\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 3.528974664368204\n",
      "    mean_inference_ms: 0.4837796170962065\n",
      "    mean_raw_obs_processing_ms: 3.756152164897228\n",
      "  time_since_restore: 1530.4614255428314\n",
      "  time_this_iter_s: 24.36860489845276\n",
      "  time_total_s: 1530.4614255428314\n",
      "  timers:\n",
      "    learn_throughput: 6933.967\n",
      "    learn_time_ms: 2163.264\n",
      "    load_throughput: 9098668.055\n",
      "    load_time_ms: 1.649\n",
      "    sample_throughput: 635.864\n",
      "    sample_time_ms: 23589.941\n",
      "    update_time_ms: 2.067\n",
      "  timestamp: 1665246721\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 855000\n",
      "  training_iteration: 57\n",
      "  trial_id: 26f87_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 9.4/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.23 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_1_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_5_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_0_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 accelerator_type:G, 0.0/6.0 CPU_group_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_2_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_4_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_3_25daa8673b2c24aecf80a7fc79df4e93)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_onelane_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_26f87_00000 | RUNNING  | 141.225.10.229:307 |     57 |          1530.46 | 855000 | -1866.96 |              2314.45 |             -7392.24 |               3000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m ring length: 223\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m v_max: 3.3805185755587788\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m ring length: 237\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m v_max: 4.04661795519353\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m ring length: 257\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m v_max: 4.997304682316517\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m ring length: 261\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m v_max: 5.187261846097525\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m ring length: 240\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m v_max: 4.189299083856172\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 1 seconds\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_26f87_00000:\n",
      "  agent_timesteps_total: 870000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-08_11-32-26\n",
      "  done: false\n",
      "  episode_len_mean: 3000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2314.450685613813\n",
      "  episode_reward_mean: -1874.6494453706175\n",
      "  episode_reward_min: -7392.237639384751\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 290\n",
      "  experiment_id: edcbe001b14143f4bf76a6a2914a7bd0\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.18020324409008026\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: -0.8911212086677551\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.03219042718410492\n",
      "          model: {}\n",
      "          policy_loss: 0.00859933439642191\n",
      "          total_loss: 2230.451171875\n",
      "          vf_explained_var: -3.5660898678457897e-09\n",
      "          vf_loss: 2230.436767578125\n",
      "    num_agent_steps_sampled: 870000\n",
      "    num_agent_steps_trained: 870000\n",
      "    num_steps_sampled: 870000\n",
      "    num_steps_trained: 870000\n",
      "  iterations_since_restore: 58\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 24.46111111111111\n",
      "    ram_util_percent: 30.063888888888886\n",
      "  pid: 307\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06596239373036808\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 3.5266873830112098\n",
      "    mean_inference_ms: 0.48370782786495753\n",
      "    mean_raw_obs_processing_ms: 3.7486975459055634\n",
      "  time_since_restore: 1555.359935760498\n",
      "  time_this_iter_s: 24.898510217666626\n",
      "  time_total_s: 1555.359935760498\n",
      "  timers:\n",
      "    learn_throughput: 6953.228\n",
      "    learn_time_ms: 2157.271\n",
      "    load_throughput: 9214470.254\n",
      "    load_time_ms: 1.628\n",
      "    sample_throughput: 635.154\n",
      "    sample_time_ms: 23616.302\n",
      "    update_time_ms: 2.141\n",
      "  timestamp: 1665246746\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 870000\n",
      "  training_iteration: 58\n",
      "  trial_id: 26f87_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 9.4/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.23 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_2_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_5_25daa8673b2c24aecf80a7fc79df4e93, 0.0/6.0 CPU_group_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_0_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_3_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_4_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_1_25daa8673b2c24aecf80a7fc79df4e93)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_onelane_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_26f87_00000 | RUNNING  | 141.225.10.229:307 |     58 |          1555.36 | 870000 | -1874.65 |              2314.45 |             -7392.24 |               3000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m ring length: 257\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m v_max: 4.997304682316517\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m ring length: 232\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m v_max: 3.8087690783250063\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m ring length: 241\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m v_max: 4.236854288051661\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m ring length: 236\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m v_max: 3.999052674951912\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m ring length: 241\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m v_max: 4.236854288051661\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m Could not connect to TraCI server at localhost:36289 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m Could not connect to TraCI server at localhost:34795 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m Could not connect to TraCI server at localhost:34447 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m Could not connect to TraCI server at localhost:57509 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m Could not connect to TraCI server at localhost:54035 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m Could not connect to TraCI server at localhost:34795 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 3 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m Could not connect to TraCI server at localhost:36289 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 3 seconds\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m Could not connect to TraCI server at localhost:34447 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 3 seconds\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m Could not connect to TraCI server at localhost:57509 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 3 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m Could not connect to TraCI server at localhost:54035 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 3 seconds\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_26f87_00000:\n",
      "  agent_timesteps_total: 885000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-08_11-32-56\n",
      "  done: false\n",
      "  episode_len_mean: 3000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2314.450685613813\n",
      "  episode_reward_mean: -1891.0693748230394\n",
      "  episode_reward_min: -7392.237639384751\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 295\n",
      "  experiment_id: edcbe001b14143f4bf76a6a2914a7bd0\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.18020324409008026\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.7963279485702515\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.03079260140657425\n",
      "          model: {}\n",
      "          policy_loss: 0.009731899946928024\n",
      "          total_loss: 2727.724365234375\n",
      "          vf_explained_var: -6.113296979748384e-09\n",
      "          vf_loss: 2727.70947265625\n",
      "    num_agent_steps_sampled: 885000\n",
      "    num_agent_steps_trained: 885000\n",
      "    num_steps_sampled: 885000\n",
      "    num_steps_trained: 885000\n",
      "  iterations_since_restore: 59\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.609302325581396\n",
      "    ram_util_percent: 29.999999999999996\n",
      "  pid: 307\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06594802588712512\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 3.5244921674904277\n",
      "    mean_inference_ms: 0.4836427295399963\n",
      "    mean_raw_obs_processing_ms: 3.743023540452685\n",
      "  time_since_restore: 1585.9987597465515\n",
      "  time_this_iter_s: 30.638823986053467\n",
      "  time_total_s: 1585.9987597465515\n",
      "  timers:\n",
      "    learn_throughput: 6684.531\n",
      "    learn_time_ms: 2243.987\n",
      "    load_throughput: 9279023.052\n",
      "    load_time_ms: 1.617\n",
      "    sample_throughput: 624.481\n",
      "    sample_time_ms: 24019.95\n",
      "    update_time_ms: 2.164\n",
      "  timestamp: 1665246776\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 885000\n",
      "  training_iteration: 59\n",
      "  trial_id: 26f87_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Memory usage on this node: 9.4/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.23 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_5_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_4_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_2_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_0_25daa8673b2c24aecf80a7fc79df4e93, 0.0/6.0 CPU_group_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_3_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_1_25daa8673b2c24aecf80a7fc79df4e93)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_onelane_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_26f87_00000 | RUNNING  | 141.225.10.229:307 |     59 |             1586 | 885000 | -1891.07 |              2314.45 |             -7392.24 |               3000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m ring length: 261\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m v_max: 5.187261846097525\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m ring length: 237\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m v_max: 4.04661795519353\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m ring length: 247\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m v_max: 4.522125250095652\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m ring length: 260\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m v_max: 5.139779427502188\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m ring length: 227\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m v_max: 3.570869368805752\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 1 seconds\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_26f87_00000:\n",
      "  agent_timesteps_total: 900000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-08_11-33-21\n",
      "  done: false\n",
      "  episode_len_mean: 3000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2314.450685613813\n",
      "  episode_reward_mean: -1888.9605817735128\n",
      "  episode_reward_min: -7830.841784933431\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 300\n",
      "  experiment_id: edcbe001b14143f4bf76a6a2914a7bd0\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.18020324409008026\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.5835145711898804\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012178235687315464\n",
      "          model: {}\n",
      "          policy_loss: -0.0007285999017767608\n",
      "          total_loss: 2310.578369140625\n",
      "          vf_explained_var: -6.622738357719982e-09\n",
      "          vf_loss: 2310.57666015625\n",
      "    num_agent_steps_sampled: 900000\n",
      "    num_agent_steps_trained: 900000\n",
      "    num_steps_sampled: 900000\n",
      "    num_steps_trained: 900000\n",
      "  iterations_since_restore: 60\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 24.625714285714285\n",
      "    ram_util_percent: 30.06857142857143\n",
      "  pid: 307\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.065934718084772\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 3.522426218530711\n",
      "    mean_inference_ms: 0.4835820253708333\n",
      "    mean_raw_obs_processing_ms: 3.7373230170750382\n",
      "  time_since_restore: 1610.1737430095673\n",
      "  time_this_iter_s: 24.174983263015747\n",
      "  time_total_s: 1610.1737430095673\n",
      "  timers:\n",
      "    learn_throughput: 6896.163\n",
      "    learn_time_ms: 2175.123\n",
      "    load_throughput: 10616520.14\n",
      "    load_time_ms: 1.413\n",
      "    sample_throughput: 626.324\n",
      "    sample_time_ms: 23949.276\n",
      "    update_time_ms: 1.873\n",
      "  timestamp: 1665246801\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 900000\n",
      "  training_iteration: 60\n",
      "  trial_id: 26f87_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 9.4/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.23 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_5_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_2_25daa8673b2c24aecf80a7fc79df4e93, 0.0/6.0 CPU_group_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_4_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_1_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_3_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_0_25daa8673b2c24aecf80a7fc79df4e93)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_onelane_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_26f87_00000 | RUNNING  | 141.225.10.229:307 |     60 |          1610.17 | 900000 | -1888.96 |              2314.45 |             -7830.84 |               3000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m ring length: 269\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m v_max: 5.566938458220921\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m ring length: 256\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m v_max: 4.949804327743507\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m ring length: 237\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m v_max: 4.04661795519353\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m ring length: 258\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m v_max: 5.044800727535787\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m ring length: 263\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m v_max: 5.282212215151455\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 1 seconds\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_26f87_00000:\n",
      "  agent_timesteps_total: 915000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-08_11-33-46\n",
      "  done: false\n",
      "  episode_len_mean: 3000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2314.450685613813\n",
      "  episode_reward_mean: -1909.8782944471545\n",
      "  episode_reward_min: -7830.841784933431\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 305\n",
      "  experiment_id: edcbe001b14143f4bf76a6a2914a7bd0\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.18020324409008026\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: -0.4192226529121399\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.049653731286525726\n",
      "          model: {}\n",
      "          policy_loss: 0.011204802431166172\n",
      "          total_loss: 2400.2421875\n",
      "          vf_explained_var: -4.0755314678619925e-09\n",
      "          vf_loss: 2400.22216796875\n",
      "    num_agent_steps_sampled: 915000\n",
      "    num_agent_steps_trained: 915000\n",
      "    num_steps_sampled: 915000\n",
      "    num_steps_trained: 915000\n",
      "  iterations_since_restore: 61\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 24.680555555555557\n",
      "    ram_util_percent: 30.063888888888886\n",
      "  pid: 307\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06592250296028176\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 3.5204306708525688\n",
      "    mean_inference_ms: 0.48352772953094425\n",
      "    mean_raw_obs_processing_ms: 3.729832422673806\n",
      "  time_since_restore: 1635.3706846237183\n",
      "  time_this_iter_s: 25.196941614151\n",
      "  time_total_s: 1635.3706846237183\n",
      "  timers:\n",
      "    learn_throughput: 6945.878\n",
      "    learn_time_ms: 2159.554\n",
      "    load_throughput: 10508879.535\n",
      "    load_time_ms: 1.427\n",
      "    sample_throughput: 638.924\n",
      "    sample_time_ms: 23476.96\n",
      "    update_time_ms: 1.714\n",
      "  timestamp: 1665246826\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 915000\n",
      "  training_iteration: 61\n",
      "  trial_id: 26f87_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Memory usage on this node: 9.4/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.23 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_5_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_0_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_2_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 accelerator_type:G, 0.0/6.0 CPU_group_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_1_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_4_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_3_25daa8673b2c24aecf80a7fc79df4e93)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_onelane_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_26f87_00000 | RUNNING  | 141.225.10.229:307 |     61 |          1635.37 | 915000 | -1909.88 |              2314.45 |             -7830.84 |               3000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m ring length: 259\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m v_max: 5.092292348291853\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m ring length: 227\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m v_max: 3.570869368805752\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m ring length: 230\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m v_max: 3.7136148111012934\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m ring length: 256\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m v_max: 4.949804327743507\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m ring length: 253\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m v_max: 4.807278529691987\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 1 seconds\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_26f87_00000:\n",
      "  agent_timesteps_total: 930000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-08_11-34-12\n",
      "  done: false\n",
      "  episode_len_mean: 3000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2314.450685613813\n",
      "  episode_reward_mean: -1965.8576839286052\n",
      "  episode_reward_min: -7932.104803961036\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 310\n",
      "  experiment_id: edcbe001b14143f4bf76a6a2914a7bd0\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2703048586845398\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.8697011470794678\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.032859474420547485\n",
      "          model: {}\n",
      "          policy_loss: 0.021845409646630287\n",
      "          total_loss: 2955.571044921875\n",
      "          vf_explained_var: -7.1321797356915795e-09\n",
      "          vf_loss: 2955.5400390625\n",
      "    num_agent_steps_sampled: 930000\n",
      "    num_agent_steps_trained: 930000\n",
      "    num_steps_sampled: 930000\n",
      "    num_steps_trained: 930000\n",
      "  iterations_since_restore: 62\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 23.456756756756757\n",
      "    ram_util_percent: 30.05405405405405\n",
      "  pid: 307\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06591031905079947\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 3.5185413959344136\n",
      "    mean_inference_ms: 0.483472968358785\n",
      "    mean_raw_obs_processing_ms: 3.7227598727720412\n",
      "  time_since_restore: 1661.4893848896027\n",
      "  time_this_iter_s: 26.1187002658844\n",
      "  time_total_s: 1661.4893848896027\n",
      "  timers:\n",
      "    learn_throughput: 6852.775\n",
      "    learn_time_ms: 2188.894\n",
      "    load_throughput: 10823079.305\n",
      "    load_time_ms: 1.386\n",
      "    sample_throughput: 635.787\n",
      "    sample_time_ms: 23592.801\n",
      "    update_time_ms: 1.527\n",
      "  timestamp: 1665246852\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 930000\n",
      "  training_iteration: 62\n",
      "  trial_id: 26f87_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 9.4/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.23 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_3_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_0_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_4_25daa8673b2c24aecf80a7fc79df4e93, 0.0/6.0 CPU_group_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_5_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_2_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_1_25daa8673b2c24aecf80a7fc79df4e93)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_onelane_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_26f87_00000 | RUNNING  | 141.225.10.229:307 |     62 |          1661.49 | 930000 | -1965.86 |              2314.45 |              -7932.1 |               3000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m ring length: 267\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m v_max: 5.472051563171826\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m ring length: 262\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m v_max: 5.234739483008334\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m ring length: 221\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m v_max: 3.285334164169417\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m ring length: 252\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m v_max: 4.759762049824181\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m ring length: 252\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m v_max: 4.759762049824181\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m Could not connect to TraCI server at localhost:34447 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m Could not connect to TraCI server at localhost:34795 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m Could not connect to TraCI server at localhost:54035 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 2 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m Could not connect to TraCI server at localhost:34447 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 3 seconds\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m Could not connect to TraCI server at localhost:34795 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 3 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m Could not connect to TraCI server at localhost:54035 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 3 seconds\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_26f87_00000:\n",
      "  agent_timesteps_total: 945000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-08_11-34-43\n",
      "  done: false\n",
      "  episode_len_mean: 3000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2314.450685613813\n",
      "  episode_reward_mean: -1898.9270329881242\n",
      "  episode_reward_min: -7932.104803961036\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 315\n",
      "  experiment_id: edcbe001b14143f4bf76a6a2914a7bd0\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2703048586845398\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.12453088909387589\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.02456216886639595\n",
      "          model: {}\n",
      "          policy_loss: 0.003911240957677364\n",
      "          total_loss: 2628.7255859375\n",
      "          vf_explained_var: -4.58497284583359e-09\n",
      "          vf_loss: 2628.715576171875\n",
      "    num_agent_steps_sampled: 945000\n",
      "    num_agent_steps_trained: 945000\n",
      "    num_steps_sampled: 945000\n",
      "    num_steps_trained: 945000\n",
      "  iterations_since_restore: 63\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.96888888888889\n",
      "    ram_util_percent: 30.015555555555547\n",
      "  pid: 307\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06589828559153536\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 3.516700206743082\n",
      "    mean_inference_ms: 0.4834197580196239\n",
      "    mean_raw_obs_processing_ms: 3.7176042008530636\n",
      "  time_since_restore: 1692.632432937622\n",
      "  time_this_iter_s: 31.14304804801941\n",
      "  time_total_s: 1692.632432937622\n",
      "  timers:\n",
      "    learn_throughput: 6705.251\n",
      "    learn_time_ms: 2237.053\n",
      "    load_throughput: 9306612.23\n",
      "    load_time_ms: 1.612\n",
      "    sample_throughput: 623.084\n",
      "    sample_time_ms: 24073.807\n",
      "    update_time_ms: 1.525\n",
      "  timestamp: 1665246883\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 945000\n",
      "  training_iteration: 63\n",
      "  trial_id: 26f87_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 9.4/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.23 GiB heap, 0.0/14.65 GiB objects (0.0/6.0 CPU_group_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_5_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_0_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_4_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_1_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_2_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_3_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 accelerator_type:G)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_onelane_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_26f87_00000 | RUNNING  | 141.225.10.229:307 |     63 |          1692.63 | 945000 | -1898.93 |              2314.45 |              -7932.1 |               3000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m ring length: 245\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m v_max: 4.427046998576354\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m ring length: 245\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m v_max: 4.427046998576354\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m ring length: 253\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m v_max: 4.807278529691987\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m ring length: 249\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m v_max: 4.617190445131122\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m ring length: 261\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m v_max: 5.187261846097525\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m Could not connect to TraCI server at localhost:54035 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m Could not connect to TraCI server at localhost:34795 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m Could not connect to TraCI server at localhost:34447 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m Could not connect to TraCI server at localhost:36289 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m Could not connect to TraCI server at localhost:57509 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m Could not connect to TraCI server at localhost:54035 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 3 seconds\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m Could not connect to TraCI server at localhost:34795 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 3 seconds\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m Could not connect to TraCI server at localhost:34447 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 3 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m Could not connect to TraCI server at localhost:36289 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 3 seconds\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m Could not connect to TraCI server at localhost:57509 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 3 seconds\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 1 seconds\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_26f87_00000:\n",
      "  agent_timesteps_total: 960000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-08_11-35-15\n",
      "  done: false\n",
      "  episode_len_mean: 3000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2314.450685613813\n",
      "  episode_reward_mean: -1951.6888279532004\n",
      "  episode_reward_min: -7932.104803961036\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 320\n",
      "  experiment_id: edcbe001b14143f4bf76a6a2914a7bd0\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2703048586845398\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.883129358291626\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.027466384693980217\n",
      "          model: {}\n",
      "          policy_loss: 0.012448156252503395\n",
      "          total_loss: 2747.301513671875\n",
      "          vf_explained_var: -9.679387069638778e-09\n",
      "          vf_loss: 2747.28125\n",
      "    num_agent_steps_sampled: 960000\n",
      "    num_agent_steps_trained: 960000\n",
      "    num_steps_sampled: 960000\n",
      "    num_steps_trained: 960000\n",
      "  iterations_since_restore: 64\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.680000000000003\n",
      "    ram_util_percent: 30.108888888888895\n",
      "  pid: 307\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06588734802042726\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 3.515022757760426\n",
      "    mean_inference_ms: 0.48337301229200386\n",
      "    mean_raw_obs_processing_ms: 3.7143031813929235\n",
      "  time_since_restore: 1724.3730759620667\n",
      "  time_this_iter_s: 31.74064302444458\n",
      "  time_total_s: 1724.3730759620667\n",
      "  timers:\n",
      "    learn_throughput: 6697.003\n",
      "    learn_time_ms: 2239.808\n",
      "    load_throughput: 9360196.385\n",
      "    load_time_ms: 1.603\n",
      "    sample_throughput: 610.486\n",
      "    sample_time_ms: 24570.585\n",
      "    update_time_ms: 1.601\n",
      "  timestamp: 1665246915\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 960000\n",
      "  training_iteration: 64\n",
      "  trial_id: 26f87_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Memory usage on this node: 9.4/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.23 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_2_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_1_25daa8673b2c24aecf80a7fc79df4e93, 0.0/6.0 CPU_group_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_4_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_5_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_0_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_3_25daa8673b2c24aecf80a7fc79df4e93)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_onelane_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_26f87_00000 | RUNNING  | 141.225.10.229:307 |     64 |          1724.37 | 960000 | -1951.69 |              2314.45 |              -7932.1 |               3000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m ring length: 245\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m v_max: 4.427046998576354\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m ring length: 258\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m v_max: 5.044800727535787\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m ring length: 259\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m v_max: 5.092292348291853\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m ring length: 226\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m v_max: 3.5232840694769565\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m ring length: 248\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m v_max: 4.569659527528101\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 1 seconds\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_26f87_00000:\n",
      "  agent_timesteps_total: 975000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-08_11-35-40\n",
      "  done: false\n",
      "  episode_len_mean: 3000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2314.450685613813\n",
      "  episode_reward_mean: -1954.6830499868797\n",
      "  episode_reward_min: -8214.95284352449\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 325\n",
      "  experiment_id: edcbe001b14143f4bf76a6a2914a7bd0\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2703048586845398\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.4364289045333862\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.02098269574344158\n",
      "          model: {}\n",
      "          policy_loss: 0.00747524993494153\n",
      "          total_loss: 2990.74169921875\n",
      "          vf_explained_var: -1.884933276130596e-08\n",
      "          vf_loss: 2990.72802734375\n",
      "    num_agent_steps_sampled: 975000\n",
      "    num_agent_steps_trained: 975000\n",
      "    num_steps_sampled: 975000\n",
      "    num_steps_trained: 975000\n",
      "  iterations_since_restore: 65\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 24.666666666666668\n",
      "    ram_util_percent: 30.158333333333342\n",
      "  pid: 307\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06587622774283562\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 3.5134441841711697\n",
      "    mean_inference_ms: 0.48332718412146164\n",
      "    mean_raw_obs_processing_ms: 3.7115032448575045\n",
      "  time_since_restore: 1749.4466454982758\n",
      "  time_this_iter_s: 25.073569536209106\n",
      "  time_total_s: 1749.4466454982758\n",
      "  timers:\n",
      "    learn_throughput: 6890.444\n",
      "    learn_time_ms: 2176.928\n",
      "    load_throughput: 9080545.573\n",
      "    load_time_ms: 1.652\n",
      "    sample_throughput: 609.062\n",
      "    sample_time_ms: 24628.049\n",
      "    update_time_ms: 1.385\n",
      "  timestamp: 1665246940\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 975000\n",
      "  training_iteration: 65\n",
      "  trial_id: 26f87_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 9.4/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.23 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_0_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_1_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_5_25daa8673b2c24aecf80a7fc79df4e93, 0.0/6.0 CPU_group_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_2_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_3_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_4_25daa8673b2c24aecf80a7fc79df4e93)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_onelane_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_26f87_00000 | RUNNING  | 141.225.10.229:307 |     65 |          1749.45 | 975000 | -1954.68 |              2314.45 |             -8214.95 |               3000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m ring length: 223\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m v_max: 3.3805185755587788\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m ring length: 241\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m v_max: 4.236854288051661\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m ring length: 224\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m v_max: 3.4281086136538996\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m ring length: 258\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m v_max: 5.044800727535787\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m ring length: 253\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m v_max: 4.807278529691987\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m Could not connect to TraCI server at localhost:34795 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m Could not connect to TraCI server at localhost:57509 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m Could not connect to TraCI server at localhost:54035 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m Could not connect to TraCI server at localhost:36289 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 2 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m Could not connect to TraCI server at localhost:34447 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m Could not connect to TraCI server at localhost:34795 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 3 seconds\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m Could not connect to TraCI server at localhost:57509 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 3 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m Could not connect to TraCI server at localhost:54035 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 3 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m Could not connect to TraCI server at localhost:36289 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 3 seconds\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m Could not connect to TraCI server at localhost:34447 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 3 seconds\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m Could not connect to TraCI server at localhost:34795 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m Could not connect to TraCI server at localhost:34447 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m Could not connect to TraCI server at localhost:57509 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m Could not connect to TraCI server at localhost:54035 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m Could not connect to TraCI server at localhost:36289 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m Could not connect to TraCI server at localhost:34795 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 3 seconds\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m Could not connect to TraCI server at localhost:34447 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 3 seconds\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m Could not connect to TraCI server at localhost:57509 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 3 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m Could not connect to TraCI server at localhost:54035 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 3 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m Could not connect to TraCI server at localhost:36289 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 3 seconds\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_26f87_00000:\n",
      "  agent_timesteps_total: 990000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-08_11-36-17\n",
      "  done: false\n",
      "  episode_len_mean: 3000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2314.450685613813\n",
      "  episode_reward_mean: -1985.9314252171293\n",
      "  episode_reward_min: -8214.95284352449\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 330\n",
      "  experiment_id: edcbe001b14143f4bf76a6a2914a7bd0\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2703048586845398\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.32407546043396\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.03701285272836685\n",
      "          model: {}\n",
      "          policy_loss: 0.014432821422815323\n",
      "          total_loss: 2911.4169921875\n",
      "          vf_explained_var: -1.3245476715439963e-08\n",
      "          vf_loss: 2911.392578125\n",
      "    num_agent_steps_sampled: 990000\n",
      "    num_agent_steps_trained: 990000\n",
      "    num_steps_sampled: 990000\n",
      "    num_steps_trained: 990000\n",
      "  iterations_since_restore: 66\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 18.459615384615386\n",
      "    ram_util_percent: 30.04615384615385\n",
      "  pid: 307\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06586481714286849\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 3.511831681734114\n",
      "    mean_inference_ms: 0.48328080946570295\n",
      "    mean_raw_obs_processing_ms: 3.711965271183764\n",
      "  time_since_restore: 1785.7313122749329\n",
      "  time_this_iter_s: 36.284666776657104\n",
      "  time_total_s: 1785.7313122749329\n",
      "  timers:\n",
      "    learn_throughput: 6715.919\n",
      "    learn_time_ms: 2233.499\n",
      "    load_throughput: 9343098.993\n",
      "    load_time_ms: 1.605\n",
      "    sample_throughput: 583.217\n",
      "    sample_time_ms: 25719.406\n",
      "    update_time_ms: 1.523\n",
      "  timestamp: 1665246977\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 990000\n",
      "  training_iteration: 66\n",
      "  trial_id: 26f87_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 9.4/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.23 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_3_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_1_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_0_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_2_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 accelerator_type:G, 0.0/6.0 CPU_group_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_4_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_5_25daa8673b2c24aecf80a7fc79df4e93)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_onelane_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_26f87_00000 | RUNNING  | 141.225.10.229:307 |     66 |          1785.73 | 990000 | -1985.93 |              2314.45 |             -8214.95 |               3000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m ring length: 264\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m v_max: 5.329679917416892\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m ring length: 249\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m v_max: 4.617190445131122\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m ring length: 224\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m v_max: 3.4281086136538996\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m ring length: 269\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m v_max: 5.566938458220921\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m ring length: 225\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m v_max: 3.4756971311168834\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 1 seconds\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_26f87_00000:\n",
      "  agent_timesteps_total: 1005000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-08_11-36-41\n",
      "  done: false\n",
      "  episode_len_mean: 3000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2314.450685613813\n",
      "  episode_reward_mean: -2043.8400416283737\n",
      "  episode_reward_min: -8214.95284352449\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 335\n",
      "  experiment_id: edcbe001b14143f4bf76a6a2914a7bd0\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2703048586845398\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.0395911931991577\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01844828948378563\n",
      "          model: {}\n",
      "          policy_loss: 0.002526217373088002\n",
      "          total_loss: 2723.46826171875\n",
      "          vf_explained_var: -7.1321797356915795e-09\n",
      "          vf_loss: 2723.460693359375\n",
      "    num_agent_steps_sampled: 1005000\n",
      "    num_agent_steps_trained: 1005000\n",
      "    num_steps_sampled: 1005000\n",
      "    num_steps_trained: 1005000\n",
      "  iterations_since_restore: 67\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 25.417142857142856\n",
      "    ram_util_percent: 30.15714285714286\n",
      "  pid: 307\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06585471326944331\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 3.5103220897629925\n",
      "    mean_inference_ms: 0.48324135078969915\n",
      "    mean_raw_obs_processing_ms: 3.7125105447280395\n",
      "  time_since_restore: 1810.3288288116455\n",
      "  time_this_iter_s: 24.597516536712646\n",
      "  time_total_s: 1810.3288288116455\n",
      "  timers:\n",
      "    learn_throughput: 6637.004\n",
      "    learn_time_ms: 2260.056\n",
      "    load_throughput: 9210153.711\n",
      "    load_time_ms: 1.629\n",
      "    sample_throughput: 583.304\n",
      "    sample_time_ms: 25715.581\n",
      "    update_time_ms: 1.633\n",
      "  timestamp: 1665247001\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1005000\n",
      "  training_iteration: 67\n",
      "  trial_id: 26f87_00000\n",
      "  \n",
      "\u001b[2m\u001b[36m(pid=307)\u001b[0m 2022-10-08 11:36:41,736\tWARNING ppo.py:242 -- The magnitude of your environment rewards are more than 204.0x the scale of `vf_clip_param`. This means that it will take more than 204.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Memory usage on this node: 9.4/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.23 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_3_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_5_25daa8673b2c24aecf80a7fc79df4e93, 0.0/6.0 CPU_group_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_4_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_0_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_1_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_2_25daa8673b2c24aecf80a7fc79df4e93)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_onelane_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_26f87_00000 | RUNNING  | 141.225.10.229:307 |     67 |          1810.33 | 1005000 | -2043.84 |              2314.45 |             -8214.95 |               3000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m ring length: 263\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m v_max: 5.282212215151455\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m ring length: 249\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m v_max: 4.617190445131122\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m ring length: 228\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m v_max: 3.618452967700356\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m ring length: 264\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m v_max: 5.329679917416892\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m ring length: 256\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m v_max: 4.949804327743507\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 1 seconds\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_26f87_00000:\n",
      "  agent_timesteps_total: 1020000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-08_11-37-07\n",
      "  done: false\n",
      "  episode_len_mean: 3000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2314.450685613813\n",
      "  episode_reward_mean: -2246.4703637211614\n",
      "  episode_reward_min: -8214.95284352449\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 340\n",
      "  experiment_id: edcbe001b14143f4bf76a6a2914a7bd0\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2703048586845398\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 2.7605440616607666\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.030259544029831886\n",
      "          model: {}\n",
      "          policy_loss: 0.01683422178030014\n",
      "          total_loss: 2974.739013671875\n",
      "          vf_explained_var: -4.0755314678619925e-09\n",
      "          vf_loss: 2974.71337890625\n",
      "    num_agent_steps_sampled: 1020000\n",
      "    num_agent_steps_trained: 1020000\n",
      "    num_steps_sampled: 1020000\n",
      "    num_steps_trained: 1020000\n",
      "  iterations_since_restore: 68\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 24.024324324324326\n",
      "    ram_util_percent: 30.16216216216217\n",
      "  pid: 307\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06584495943414184\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 3.5087977324323623\n",
      "    mean_inference_ms: 0.4832026289635269\n",
      "    mean_raw_obs_processing_ms: 3.7134837205734077\n",
      "  time_since_restore: 1835.7030725479126\n",
      "  time_this_iter_s: 25.37424373626709\n",
      "  time_total_s: 1835.7030725479126\n",
      "  timers:\n",
      "    learn_throughput: 6615.518\n",
      "    learn_time_ms: 2267.396\n",
      "    load_throughput: 9290670.132\n",
      "    load_time_ms: 1.615\n",
      "    sample_throughput: 582.392\n",
      "    sample_time_ms: 25755.83\n",
      "    update_time_ms: 1.687\n",
      "  timestamp: 1665247027\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1020000\n",
      "  training_iteration: 68\n",
      "  trial_id: 26f87_00000\n",
      "  \n",
      "\u001b[2m\u001b[36m(pid=307)\u001b[0m 2022-10-08 11:37:07,178\tWARNING ppo.py:242 -- The magnitude of your environment rewards are more than 225.0x the scale of `vf_clip_param`. This means that it will take more than 225.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "== Status ==\n",
      "Memory usage on this node: 9.4/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.23 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_3_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_1_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_4_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_2_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_0_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_5_25daa8673b2c24aecf80a7fc79df4e93, 0.0/6.0 CPU_group_25daa8673b2c24aecf80a7fc79df4e93)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_onelane_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_26f87_00000 | RUNNING  | 141.225.10.229:307 |     68 |           1835.7 | 1020000 | -2246.47 |              2314.45 |             -8214.95 |               3000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m ring length: 266\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m v_max: 5.42459972166245\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m ring length: 227\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m v_max: 3.570869368805752\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m ring length: 220\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m v_max: 3.2377399006821497\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m ring length: 266\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m v_max: 5.42459972166245\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m ring length: 260\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m v_max: 5.139779427502188\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m Could not connect to TraCI server at localhost:57509 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 2 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m Could not connect to TraCI server at localhost:34795 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m Could not connect to TraCI server at localhost:34447 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m Could not connect to TraCI server at localhost:54035 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m Could not connect to TraCI server at localhost:36289 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m Could not connect to TraCI server at localhost:57509 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 3 seconds\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m Could not connect to TraCI server at localhost:34795 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 3 seconds\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m Could not connect to TraCI server at localhost:34447 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 3 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m Could not connect to TraCI server at localhost:54035 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 3 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m Could not connect to TraCI server at localhost:36289 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 3 seconds\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 1 seconds\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_26f87_00000:\n",
      "  agent_timesteps_total: 1035000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-08_11-37-38\n",
      "  done: false\n",
      "  episode_len_mean: 3000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2314.450685613813\n",
      "  episode_reward_mean: -2363.300444502345\n",
      "  episode_reward_min: -8214.95284352449\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 345\n",
      "  experiment_id: edcbe001b14143f4bf76a6a2914a7bd0\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2703048586845398\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.9251625537872314\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.029114270582795143\n",
      "          model: {}\n",
      "          policy_loss: 0.014757885597646236\n",
      "          total_loss: 3115.55810546875\n",
      "          vf_explained_var: -4.58497284583359e-09\n",
      "          vf_loss: 3115.5361328125\n",
      "    num_agent_steps_sampled: 1035000\n",
      "    num_agent_steps_trained: 1035000\n",
      "    num_steps_sampled: 1035000\n",
      "    num_steps_trained: 1035000\n",
      "  iterations_since_restore: 69\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.89772727272727\n",
      "    ram_util_percent: 30.09545454545455\n",
      "  pid: 307\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06583594610060706\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 3.507317289742281\n",
      "    mean_inference_ms: 0.48316676193485486\n",
      "    mean_raw_obs_processing_ms: 3.7158206105327944\n",
      "  time_since_restore: 1866.9861783981323\n",
      "  time_this_iter_s: 31.283105850219727\n",
      "  time_total_s: 1866.9861783981323\n",
      "  timers:\n",
      "    learn_throughput: 6745.24\n",
      "    learn_time_ms: 2223.79\n",
      "    load_throughput: 9241404.838\n",
      "    load_time_ms: 1.623\n",
      "    sample_throughput: 579.96\n",
      "    sample_time_ms: 25863.852\n",
      "    update_time_ms: 1.666\n",
      "  timestamp: 1665247058\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1035000\n",
      "  training_iteration: 69\n",
      "  trial_id: 26f87_00000\n",
      "  \n",
      "\u001b[2m\u001b[36m(pid=307)\u001b[0m 2022-10-08 11:37:38,534\tWARNING ppo.py:242 -- The magnitude of your environment rewards are more than 236.0x the scale of `vf_clip_param`. This means that it will take more than 236.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "== Status ==\n",
      "Memory usage on this node: 9.4/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.23 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_5_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_4_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_0_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_3_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_1_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_2_25daa8673b2c24aecf80a7fc79df4e93, 0.0/6.0 CPU_group_25daa8673b2c24aecf80a7fc79df4e93)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_onelane_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_26f87_00000 | RUNNING  | 141.225.10.229:307 |     69 |          1866.99 | 1035000 |  -2363.3 |              2314.45 |             -8214.95 |               3000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m ring length: 230\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m v_max: 3.7136148111012934\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m ring length: 262\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m v_max: 5.234739483008334\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m ring length: 270\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m v_max: 5.6143732387852054\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m ring length: 226\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m v_max: 3.5232840694769565\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m ring length: 246\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m v_max: 4.474587708995648\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 1 seconds\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_26f87_00000:\n",
      "  agent_timesteps_total: 1050000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-08_11-38-03\n",
      "  done: false\n",
      "  episode_len_mean: 3000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2314.450685613813\n",
      "  episode_reward_mean: -2432.6892792683684\n",
      "  episode_reward_min: -8214.95284352449\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 350\n",
      "  experiment_id: edcbe001b14143f4bf76a6a2914a7bd0\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2703048586845398\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.2251412868499756\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.06674806028604507\n",
      "          model: {}\n",
      "          policy_loss: 0.029865840449929237\n",
      "          total_loss: 3126.378662109375\n",
      "          vf_explained_var: 3.056648489874192e-09\n",
      "          vf_loss: 3126.331298828125\n",
      "    num_agent_steps_sampled: 1050000\n",
      "    num_agent_steps_trained: 1050000\n",
      "    num_steps_sampled: 1050000\n",
      "    num_steps_trained: 1050000\n",
      "  iterations_since_restore: 70\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 24.717142857142857\n",
      "    ram_util_percent: 30.168571428571433\n",
      "  pid: 307\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0658269015895755\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 3.505851046588201\n",
      "    mean_inference_ms: 0.48313087930826043\n",
      "    mean_raw_obs_processing_ms: 3.7181912664163574\n",
      "  time_since_restore: 1891.591097831726\n",
      "  time_this_iter_s: 24.60491943359375\n",
      "  time_total_s: 1891.591097831726\n",
      "  timers:\n",
      "    learn_throughput: 6738.681\n",
      "    learn_time_ms: 2225.955\n",
      "    load_throughput: 9031273.416\n",
      "    load_time_ms: 1.661\n",
      "    sample_throughput: 579.046\n",
      "    sample_time_ms: 25904.687\n",
      "    update_time_ms: 1.679\n",
      "  timestamp: 1665247083\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1050000\n",
      "  training_iteration: 70\n",
      "  trial_id: 26f87_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Memory usage on this node: 9.4/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.23 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_2_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_1_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_0_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_5_25daa8673b2c24aecf80a7fc79df4e93, 0.0/6.0 CPU_group_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_3_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_4_25daa8673b2c24aecf80a7fc79df4e93)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_onelane_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_26f87_00000 | RUNNING  | 141.225.10.229:307 |     70 |          1891.59 | 1050000 | -2432.69 |              2314.45 |             -8214.95 |               3000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=307)\u001b[0m 2022-10-08 11:38:03,197\tWARNING ppo.py:242 -- The magnitude of your environment rewards are more than 243.0x the scale of `vf_clip_param`. This means that it will take more than 243.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m ring length: 242\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m v_max: 4.2844067680020546\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m ring length: 244\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m v_max: 4.379503211387676\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m ring length: 248\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m v_max: 4.569659527528101\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m ring length: 248\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m v_max: 4.569659527528101\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m ring length: 225\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m v_max: 3.4756971311168834\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 1 seconds\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_26f87_00000:\n",
      "  agent_timesteps_total: 1065000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-08_11-38-28\n",
      "  done: false\n",
      "  episode_len_mean: 3000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2314.450685613813\n",
      "  episode_reward_mean: -2506.019650224459\n",
      "  episode_reward_min: -8214.95284352449\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 355\n",
      "  experiment_id: edcbe001b14143f4bf76a6a2914a7bd0\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4054573178291321\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.327120065689087\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011056452989578247\n",
      "          model: {}\n",
      "          policy_loss: 0.008409051224589348\n",
      "          total_loss: 2881.114501953125\n",
      "          vf_explained_var: -7.641621557752387e-09\n",
      "          vf_loss: 2881.1015625\n",
      "    num_agent_steps_sampled: 1065000\n",
      "    num_agent_steps_trained: 1065000\n",
      "    num_steps_sampled: 1065000\n",
      "    num_steps_trained: 1065000\n",
      "  iterations_since_restore: 71\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 24.170270270270265\n",
      "    ram_util_percent: 30.164864864864867\n",
      "  pid: 307\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06581797019522663\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 3.5044896492259245\n",
      "    mean_inference_ms: 0.4830961395027785\n",
      "    mean_raw_obs_processing_ms: 3.7188533739635066\n",
      "  time_since_restore: 1916.93190407753\n",
      "  time_this_iter_s: 25.340806245803833\n",
      "  time_total_s: 1916.93190407753\n",
      "  timers:\n",
      "    learn_throughput: 6559.755\n",
      "    learn_time_ms: 2286.671\n",
      "    load_throughput: 8870951.186\n",
      "    load_time_ms: 1.691\n",
      "    sample_throughput: 580.082\n",
      "    sample_time_ms: 25858.407\n",
      "    update_time_ms: 1.653\n",
      "  timestamp: 1665247108\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1065000\n",
      "  training_iteration: 71\n",
      "  trial_id: 26f87_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 9.4/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.23 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_2_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_0_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_4_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_5_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_1_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_3_25daa8673b2c24aecf80a7fc79df4e93, 0.0/6.0 CPU_group_25daa8673b2c24aecf80a7fc79df4e93)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_onelane_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_26f87_00000 | RUNNING  | 141.225.10.229:307 |     71 |          1916.93 | 1065000 | -2506.02 |              2314.45 |             -8214.95 |               3000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=307)\u001b[0m 2022-10-08 11:38:28,592\tWARNING ppo.py:242 -- The magnitude of your environment rewards are more than 251.0x the scale of `vf_clip_param`. This means that it will take more than 251.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m ring length: 235\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m v_max: 3.9514850725282584\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m ring length: 235\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m v_max: 3.9514850725282584\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m ring length: 229\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m v_max: 3.666034803266416\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m ring length: 241\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m v_max: 4.236854288051661\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m ring length: 246\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m v_max: 4.474587708995648\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 1 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 1 seconds\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_26f87_00000:\n",
      "  agent_timesteps_total: 1080000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-08_11-38-55\n",
      "  done: false\n",
      "  episode_len_mean: 3000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2314.450685613813\n",
      "  episode_reward_mean: -2471.2310548615787\n",
      "  episode_reward_min: -8214.95284352449\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 360\n",
      "  experiment_id: edcbe001b14143f4bf76a6a2914a7bd0\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4054573178291321\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.3965274393558502\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008040125481784344\n",
      "          model: {}\n",
      "          policy_loss: 0.0024205308873206377\n",
      "          total_loss: 2822.23193359375\n",
      "          vf_explained_var: 4.0755314678619925e-09\n",
      "          vf_loss: 2822.22607421875\n",
      "    num_agent_steps_sampled: 1080000\n",
      "    num_agent_steps_trained: 1080000\n",
      "    num_steps_sampled: 1080000\n",
      "    num_steps_trained: 1080000\n",
      "  iterations_since_restore: 72\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 23.53684210526316\n",
      "    ram_util_percent: 30.173684210526318\n",
      "  pid: 307\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06580855380161683\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 3.5032155970345946\n",
      "    mean_inference_ms: 0.48306275208759625\n",
      "    mean_raw_obs_processing_ms: 3.71978430601712\n",
      "  time_since_restore: 1943.4632453918457\n",
      "  time_this_iter_s: 26.531341314315796\n",
      "  time_total_s: 1943.4632453918457\n",
      "  timers:\n",
      "    learn_throughput: 6513.173\n",
      "    learn_time_ms: 2303.025\n",
      "    load_throughput: 8044208.615\n",
      "    load_time_ms: 1.865\n",
      "    sample_throughput: 579.523\n",
      "    sample_time_ms: 25883.351\n",
      "    update_time_ms: 1.628\n",
      "  timestamp: 1665247135\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1080000\n",
      "  training_iteration: 72\n",
      "  trial_id: 26f87_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 9.4/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.23 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_2_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_3_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_5_25daa8673b2c24aecf80a7fc79df4e93, 0.0/6.0 CPU_group_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_1_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_4_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_0_25daa8673b2c24aecf80a7fc79df4e93)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_onelane_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_26f87_00000 | RUNNING  | 141.225.10.229:307 |     72 |          1943.46 | 1080000 | -2471.23 |              2314.45 |             -8214.95 |               3000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=307)\u001b[0m 2022-10-08 11:38:55,181\tWARNING ppo.py:242 -- The magnitude of your environment rewards are more than 247.0x the scale of `vf_clip_param`. This means that it will take more than 247.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m ring length: 267\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m v_max: 5.472051563171826\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m ring length: 222\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m v_max: 3.3329270738617227\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m ring length: 267\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m v_max: 5.472051563171826\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m ring length: 233\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m v_max: 3.856343201216534\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m ring length: 236\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m v_max: 3.999052674951912\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 1 seconds\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_26f87_00000:\n",
      "  agent_timesteps_total: 1095000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-08_11-39-19\n",
      "  done: false\n",
      "  episode_len_mean: 3000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2314.450685613813\n",
      "  episode_reward_mean: -2399.984304814212\n",
      "  episode_reward_min: -8214.95284352449\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 365\n",
      "  experiment_id: edcbe001b14143f4bf76a6a2914a7bd0\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20272865891456604\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.3360072374343872\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.029448306187987328\n",
      "          model: {}\n",
      "          policy_loss: 0.004347336944192648\n",
      "          total_loss: 2930.564697265625\n",
      "          vf_explained_var: -2.547207111902594e-09\n",
      "          vf_loss: 2930.55419921875\n",
      "    num_agent_steps_sampled: 1095000\n",
      "    num_agent_steps_trained: 1095000\n",
      "    num_steps_sampled: 1095000\n",
      "    num_steps_trained: 1095000\n",
      "  iterations_since_restore: 73\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 24.574285714285715\n",
      "    ram_util_percent: 30.21714285714286\n",
      "  pid: 307\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06579922768252632\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 3.5019452505134363\n",
      "    mean_inference_ms: 0.483026625974484\n",
      "    mean_raw_obs_processing_ms: 3.7205888357433965\n",
      "  time_since_restore: 1967.9825539588928\n",
      "  time_this_iter_s: 24.51930856704712\n",
      "  time_total_s: 1967.9825539588928\n",
      "  timers:\n",
      "    learn_throughput: 6576.58\n",
      "    learn_time_ms: 2280.821\n",
      "    load_throughput: 9001425.015\n",
      "    load_time_ms: 1.666\n",
      "    sample_throughput: 594.222\n",
      "    sample_time_ms: 25243.108\n",
      "    update_time_ms: 1.798\n",
      "  timestamp: 1665247159\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1095000\n",
      "  training_iteration: 73\n",
      "  trial_id: 26f87_00000\n",
      "  \n",
      "\u001b[2m\u001b[36m(pid=307)\u001b[0m 2022-10-08 11:39:19,749\tWARNING ppo.py:242 -- The magnitude of your environment rewards are more than 240.0x the scale of `vf_clip_param`. This means that it will take more than 240.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "== Status ==\n",
      "Memory usage on this node: 9.4/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.23 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_0_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_4_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_5_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_1_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_2_25daa8673b2c24aecf80a7fc79df4e93, 0.0/6.0 CPU_group_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_3_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 accelerator_type:G)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_onelane_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_26f87_00000 | RUNNING  | 141.225.10.229:307 |     73 |          1967.98 | 1095000 | -2399.98 |              2314.45 |             -8214.95 |               3000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m ring length: 270\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m v_max: 5.6143732387852054\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m ring length: 234\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m v_max: 3.903915223349057\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m ring length: 228\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m v_max: 3.618452967700356\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m ring length: 243\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m v_max: 4.331956438196479\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m ring length: 221\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m v_max: 3.285334164169417\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m Could not connect to TraCI server at localhost:34447 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m Could not connect to TraCI server at localhost:54035 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m Could not connect to TraCI server at localhost:36289 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m Could not connect to TraCI server at localhost:34795 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m Could not connect to TraCI server at localhost:57509 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m Could not connect to TraCI server at localhost:34447 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 3 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m Could not connect to TraCI server at localhost:54035 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 3 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m Could not connect to TraCI server at localhost:36289 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 3 seconds\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m Could not connect to TraCI server at localhost:34795 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 3 seconds\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m Could not connect to TraCI server at localhost:57509 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 3 seconds\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_26f87_00000:\n",
      "  agent_timesteps_total: 1110000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-08_11-39-49\n",
      "  done: false\n",
      "  episode_len_mean: 3000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1780.2421550047243\n",
      "  episode_reward_mean: -2410.3017858791227\n",
      "  episode_reward_min: -8214.95284352449\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 370\n",
      "  experiment_id: edcbe001b14143f4bf76a6a2914a7bd0\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20272865891456604\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.2846462726593018\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.04428035393357277\n",
      "          model: {}\n",
      "          policy_loss: 0.02262950874865055\n",
      "          total_loss: 3260.82958984375\n",
      "          vf_explained_var: -6.113296979748384e-09\n",
      "          vf_loss: 3260.798095703125\n",
      "    num_agent_steps_sampled: 1110000\n",
      "    num_agent_steps_trained: 1110000\n",
      "    num_steps_sampled: 1110000\n",
      "    num_steps_trained: 1110000\n",
      "  iterations_since_restore: 74\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.569767441860467\n",
      "    ram_util_percent: 30.211627906976748\n",
      "  pid: 307\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.065789676583257\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 3.5006527387399853\n",
      "    mean_inference_ms: 0.4829897073548011\n",
      "    mean_raw_obs_processing_ms: 3.722474251698604\n",
      "  time_since_restore: 1998.018642425537\n",
      "  time_this_iter_s: 30.036088466644287\n",
      "  time_total_s: 1998.018642425537\n",
      "  timers:\n",
      "    learn_throughput: 6660.452\n",
      "    learn_time_ms: 2252.099\n",
      "    load_throughput: 9096694.717\n",
      "    load_time_ms: 1.649\n",
      "    sample_throughput: 597.568\n",
      "    sample_time_ms: 25101.766\n",
      "    update_time_ms: 1.744\n",
      "  timestamp: 1665247189\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1110000\n",
      "  training_iteration: 74\n",
      "  trial_id: 26f87_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 9.4/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.23 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_3_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_0_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_5_25daa8673b2c24aecf80a7fc79df4e93, 0.0/6.0 CPU_group_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_1_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_2_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_4_25daa8673b2c24aecf80a7fc79df4e93)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_onelane_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_26f87_00000 | RUNNING  | 141.225.10.229:307 |     74 |          1998.02 | 1110000 |  -2410.3 |              1780.24 |             -8214.95 |               3000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=307)\u001b[0m 2022-10-08 11:39:49,857\tWARNING ppo.py:242 -- The magnitude of your environment rewards are more than 241.0x the scale of `vf_clip_param`. This means that it will take more than 241.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m ring length: 222\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m v_max: 3.3329270738617227\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m ring length: 256\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m v_max: 4.949804327743507\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m ring length: 244\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m v_max: 4.379503211387676\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m ring length: 269\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m v_max: 5.566938458220921\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m ring length: 252\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m v_max: 4.759762049824181\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m Could not connect to TraCI server at localhost:57509 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 2 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m Could not connect to TraCI server at localhost:34795 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m Could not connect to TraCI server at localhost:57509 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 3 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m Could not connect to TraCI server at localhost:54035 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m Could not connect to TraCI server at localhost:34795 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 3 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m Could not connect to TraCI server at localhost:54035 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 3 seconds\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_26f87_00000:\n",
      "  agent_timesteps_total: 1125000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-08_11-40-22\n",
      "  done: false\n",
      "  episode_len_mean: 3000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1731.8509332843425\n",
      "  episode_reward_mean: -2475.7758186987207\n",
      "  episode_reward_min: -8214.95284352449\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 375\n",
      "  experiment_id: edcbe001b14143f4bf76a6a2914a7bd0\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30409297347068787\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.9577838182449341\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009824109263718128\n",
      "          model: {}\n",
      "          policy_loss: 0.005330823827534914\n",
      "          total_loss: 2733.501953125\n",
      "          vf_explained_var: -1.171715258152517e-08\n",
      "          vf_loss: 2733.493896484375\n",
      "    num_agent_steps_sampled: 1125000\n",
      "    num_agent_steps_trained: 1125000\n",
      "    num_steps_sampled: 1125000\n",
      "    num_steps_trained: 1125000\n",
      "  iterations_since_restore: 75\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.808695652173913\n",
      "    ram_util_percent: 30.300000000000015\n",
      "  pid: 307\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06578004042679575\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 3.499341335621475\n",
      "    mean_inference_ms: 0.48295339759519385\n",
      "    mean_raw_obs_processing_ms: 3.7260187768372277\n",
      "  time_since_restore: 2030.6181509494781\n",
      "  time_this_iter_s: 32.59950852394104\n",
      "  time_total_s: 2030.6181509494781\n",
      "  timers:\n",
      "    learn_throughput: 6542.695\n",
      "    learn_time_ms: 2292.633\n",
      "    load_throughput: 9096431.67\n",
      "    load_time_ms: 1.649\n",
      "    sample_throughput: 581.093\n",
      "    sample_time_ms: 25813.432\n",
      "    update_time_ms: 1.893\n",
      "  timestamp: 1665247222\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1125000\n",
      "  training_iteration: 75\n",
      "  trial_id: 26f87_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 9.4/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.23 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_3_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_1_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_4_25daa8673b2c24aecf80a7fc79df4e93, 0.0/6.0 CPU_group_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_0_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_2_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_5_25daa8673b2c24aecf80a7fc79df4e93)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_onelane_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_26f87_00000 | RUNNING  | 141.225.10.229:307 |     75 |          2030.62 | 1125000 | -2475.78 |              1731.85 |             -8214.95 |               3000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=307)\u001b[0m 2022-10-08 11:40:22,537\tWARNING ppo.py:242 -- The magnitude of your environment rewards are more than 248.0x the scale of `vf_clip_param`. This means that it will take more than 248.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m ring length: 260\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m v_max: 5.139779427502188\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m ring length: 263\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m v_max: 5.282212215151455\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m ring length: 269\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m v_max: 5.566938458220921\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m ring length: 266\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m v_max: 5.42459972166245\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m ring length: 250\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m v_max: 4.664717904914125\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 1 seconds\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_26f87_00000:\n",
      "  agent_timesteps_total: 1140000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-08_11-40-47\n",
      "  done: false\n",
      "  episode_len_mean: 3000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1967.6986690688911\n",
      "  episode_reward_mean: -2423.065064659257\n",
      "  episode_reward_min: -8214.95284352449\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 380\n",
      "  experiment_id: edcbe001b14143f4bf76a6a2914a7bd0\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.15204648673534393\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.6506679058074951\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.05402948334813118\n",
      "          model: {}\n",
      "          policy_loss: 0.013153285719454288\n",
      "          total_loss: 2863.762451171875\n",
      "          vf_explained_var: -6.622738357719982e-09\n",
      "          vf_loss: 2863.740966796875\n",
      "    num_agent_steps_sampled: 1140000\n",
      "    num_agent_steps_trained: 1140000\n",
      "    num_steps_sampled: 1140000\n",
      "    num_steps_trained: 1140000\n",
      "  iterations_since_restore: 76\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 24.581081081081074\n",
      "    ram_util_percent: 30.267567567567564\n",
      "  pid: 307\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06577131057480311\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 3.498146351137304\n",
      "    mean_inference_ms: 0.48292298234833\n",
      "    mean_raw_obs_processing_ms: 3.7295346650268733\n",
      "  time_since_restore: 2056.003403186798\n",
      "  time_this_iter_s: 25.385252237319946\n",
      "  time_total_s: 2056.003403186798\n",
      "  timers:\n",
      "    learn_throughput: 6587.805\n",
      "    learn_time_ms: 2276.934\n",
      "    load_throughput: 8978559.196\n",
      "    load_time_ms: 1.671\n",
      "    sample_throughput: 606.311\n",
      "    sample_time_ms: 24739.791\n",
      "    update_time_ms: 1.749\n",
      "  timestamp: 1665247247\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1140000\n",
      "  training_iteration: 76\n",
      "  trial_id: 26f87_00000\n",
      "  \n",
      "\u001b[2m\u001b[36m(pid=307)\u001b[0m 2022-10-08 11:40:47,994\tWARNING ppo.py:242 -- The magnitude of your environment rewards are more than 242.0x the scale of `vf_clip_param`. This means that it will take more than 242.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "== Status ==\n",
      "Memory usage on this node: 9.4/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.23 GiB heap, 0.0/14.65 GiB objects (0.0/6.0 CPU_group_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_5_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_0_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_2_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_1_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_3_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_4_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 accelerator_type:G)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_onelane_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_26f87_00000 | RUNNING  | 141.225.10.229:307 |     76 |             2056 | 1140000 | -2423.07 |               1967.7 |             -8214.95 |               3000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m ring length: 264\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m v_max: 5.329679917416892\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m ring length: 244\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m v_max: 4.379503211387676\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m ring length: 246\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m v_max: 4.474587708995648\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m ring length: 235\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m v_max: 3.9514850725282584\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m ring length: 257\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m v_max: 4.997304682316517\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 1 seconds\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_26f87_00000:\n",
      "  agent_timesteps_total: 1155000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-08_11-41-13\n",
      "  done: false\n",
      "  episode_len_mean: 3000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1967.6986690688911\n",
      "  episode_reward_mean: -2317.1942442550016\n",
      "  episode_reward_min: -8214.95284352449\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 385\n",
      "  experiment_id: edcbe001b14143f4bf76a6a2914a7bd0\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2280697375535965\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: -0.8173703551292419\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.03773948922753334\n",
      "          model: {}\n",
      "          policy_loss: 0.013218413107097149\n",
      "          total_loss: 2862.716064453125\n",
      "          vf_explained_var: -8.660504313695583e-09\n",
      "          vf_loss: 2862.693603515625\n",
      "    num_agent_steps_sampled: 1155000\n",
      "    num_agent_steps_trained: 1155000\n",
      "    num_steps_sampled: 1155000\n",
      "    num_steps_trained: 1155000\n",
      "  iterations_since_restore: 77\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 24.294444444444444\n",
      "    ram_util_percent: 30.263888888888882\n",
      "  pid: 307\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06576224468827643\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 3.4970394203738113\n",
      "    mean_inference_ms: 0.48289183591263357\n",
      "    mean_raw_obs_processing_ms: 3.7333272446685326\n",
      "  time_since_restore: 2081.7675971984863\n",
      "  time_this_iter_s: 25.764194011688232\n",
      "  time_total_s: 2081.7675971984863\n",
      "  timers:\n",
      "    learn_throughput: 6613.339\n",
      "    learn_time_ms: 2268.143\n",
      "    load_throughput: 8657093.321\n",
      "    load_time_ms: 1.733\n",
      "    sample_throughput: 603.246\n",
      "    sample_time_ms: 24865.479\n",
      "    update_time_ms: 1.572\n",
      "  timestamp: 1665247273\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1155000\n",
      "  training_iteration: 77\n",
      "  trial_id: 26f87_00000\n",
      "  \n",
      "\u001b[2m\u001b[36m(pid=307)\u001b[0m 2022-10-08 11:41:13,807\tWARNING ppo.py:242 -- The magnitude of your environment rewards are more than 232.0x the scale of `vf_clip_param`. This means that it will take more than 232.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "== Status ==\n",
      "Memory usage on this node: 9.4/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.23 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_0_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_2_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_1_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_3_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_5_25daa8673b2c24aecf80a7fc79df4e93, 0.0/6.0 CPU_group_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_4_25daa8673b2c24aecf80a7fc79df4e93)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_onelane_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_26f87_00000 | RUNNING  | 141.225.10.229:307 |     77 |          2081.77 | 1155000 | -2317.19 |               1967.7 |             -8214.95 |               3000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m ring length: 233\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m v_max: 3.856343201216534\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m ring length: 250\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m v_max: 4.664717904914125\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m ring length: 256\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m v_max: 4.949804327743507\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m ring length: 255\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m v_max: 4.902299776966978\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m ring length: 222\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m v_max: 3.3329270738617227\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 1 seconds\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_26f87_00000:\n",
      "  agent_timesteps_total: 1170000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-08_11-41-39\n",
      "  done: false\n",
      "  episode_len_mean: 3000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1967.6986690688911\n",
      "  episode_reward_mean: -2511.7634551305323\n",
      "  episode_reward_min: -8214.95284352449\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 390\n",
      "  experiment_id: edcbe001b14143f4bf76a6a2914a7bd0\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2280697375535965\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 3.7719063758850098\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01670040935277939\n",
      "          model: {}\n",
      "          policy_loss: 0.010970847681164742\n",
      "          total_loss: 3326.358642578125\n",
      "          vf_explained_var: -5.094414223805188e-09\n",
      "          vf_loss: 3326.343505859375\n",
      "    num_agent_steps_sampled: 1170000\n",
      "    num_agent_steps_trained: 1170000\n",
      "    num_steps_sampled: 1170000\n",
      "    num_steps_trained: 1170000\n",
      "  iterations_since_restore: 78\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 24.751351351351357\n",
      "    ram_util_percent: 30.264864864864858\n",
      "  pid: 307\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06575387224046851\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 3.4959370857854712\n",
      "    mean_inference_ms: 0.482862449879409\n",
      "    mean_raw_obs_processing_ms: 3.737361993600772\n",
      "  time_since_restore: 2107.0613543987274\n",
      "  time_this_iter_s: 25.29375720024109\n",
      "  time_total_s: 2107.0613543987274\n",
      "  timers:\n",
      "    learn_throughput: 6592.616\n",
      "    learn_time_ms: 2275.273\n",
      "    load_throughput: 8481337.288\n",
      "    load_time_ms: 1.769\n",
      "    sample_throughput: 603.612\n",
      "    sample_time_ms: 24850.392\n",
      "    update_time_ms: 1.472\n",
      "  timestamp: 1665247299\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1170000\n",
      "  training_iteration: 78\n",
      "  trial_id: 26f87_00000\n",
      "  \n",
      "\u001b[2m\u001b[36m(pid=307)\u001b[0m 2022-10-08 11:41:39,150\tWARNING ppo.py:242 -- The magnitude of your environment rewards are more than 251.0x the scale of `vf_clip_param`. This means that it will take more than 251.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Memory usage on this node: 9.4/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.23 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_2_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_5_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_4_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_1_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_3_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_0_25daa8673b2c24aecf80a7fc79df4e93, 0.0/6.0 CPU_group_25daa8673b2c24aecf80a7fc79df4e93)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_onelane_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_26f87_00000 | RUNNING  | 141.225.10.229:307 |     78 |          2107.06 | 1170000 | -2511.76 |               1967.7 |             -8214.95 |               3000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m ring length: 239\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m v_max: 4.141741239205832\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m ring length: 256\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m v_max: 4.949804327743507\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m ring length: 238\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m v_max: 4.094180836186086\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m ring length: 243\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m v_max: 4.331956438196479\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m ring length: 229\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m v_max: 3.666034803266416\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m Could not connect to TraCI server at localhost:36289 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m Could not connect to TraCI server at localhost:34795 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m Could not connect to TraCI server at localhost:34447 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m Could not connect to TraCI server at localhost:57509 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m Could not connect to TraCI server at localhost:54035 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m Could not connect to TraCI server at localhost:36289 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 3 seconds\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m Could not connect to TraCI server at localhost:34795 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 3 seconds\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m Could not connect to TraCI server at localhost:34447 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 3 seconds\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m Could not connect to TraCI server at localhost:57509 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 3 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m Could not connect to TraCI server at localhost:54035 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 3 seconds\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_26f87_00000:\n",
      "  agent_timesteps_total: 1185000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-08_11-42-09\n",
      "  done: false\n",
      "  episode_len_mean: 3000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1967.6986690688911\n",
      "  episode_reward_mean: -2479.5144850316397\n",
      "  episode_reward_min: -8214.95284352449\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 395\n",
      "  experiment_id: edcbe001b14143f4bf76a6a2914a7bd0\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2280697375535965\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.9770799279212952\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.021527936682105064\n",
      "          model: {}\n",
      "          policy_loss: 0.007460635155439377\n",
      "          total_loss: 3162.7783203125\n",
      "          vf_explained_var: -6.622738357719982e-09\n",
      "          vf_loss: 3162.765869140625\n",
      "    num_agent_steps_sampled: 1185000\n",
      "    num_agent_steps_trained: 1185000\n",
      "    num_steps_sampled: 1185000\n",
      "    num_steps_trained: 1185000\n",
      "  iterations_since_restore: 79\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.3\n",
      "    ram_util_percent: 30.21627906976744\n",
      "  pid: 307\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06574554256097033\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 3.4948883630316434\n",
      "    mean_inference_ms: 0.48283141152656256\n",
      "    mean_raw_obs_processing_ms: 3.7410646708130573\n",
      "  time_since_restore: 2137.1019451618195\n",
      "  time_this_iter_s: 30.04059076309204\n",
      "  time_total_s: 2137.1019451618195\n",
      "  timers:\n",
      "    learn_throughput: 6611.024\n",
      "    learn_time_ms: 2268.938\n",
      "    load_throughput: 8474368.61\n",
      "    load_time_ms: 1.77\n",
      "    sample_throughput: 606.491\n",
      "    sample_time_ms: 24732.455\n",
      "    update_time_ms: 1.485\n",
      "  timestamp: 1665247329\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1185000\n",
      "  training_iteration: 79\n",
      "  trial_id: 26f87_00000\n",
      "  \n",
      "\u001b[2m\u001b[36m(pid=307)\u001b[0m 2022-10-08 11:42:09,260\tWARNING ppo.py:242 -- The magnitude of your environment rewards are more than 248.0x the scale of `vf_clip_param`. This means that it will take more than 248.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "== Status ==\n",
      "Memory usage on this node: 9.4/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.23 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_3_25daa8673b2c24aecf80a7fc79df4e93, 0.0/6.0 CPU_group_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_1_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_5_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_0_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_4_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_2_25daa8673b2c24aecf80a7fc79df4e93)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_onelane_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_26f87_00000 | RUNNING  | 141.225.10.229:307 |     79 |           2137.1 | 1185000 | -2479.51 |               1967.7 |             -8214.95 |               3000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m ring length: 260\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m v_max: 5.139779427502188\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m ring length: 259\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m v_max: 5.092292348291853\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m ring length: 263\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m v_max: 5.282212215151455\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m ring length: 224\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m v_max: 3.4281086136538996\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m ring length: 253\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m v_max: 4.807278529691987\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m Could not connect to TraCI server at localhost:34795 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m Could not connect to TraCI server at localhost:34447 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m Could not connect to TraCI server at localhost:57509 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m Could not connect to TraCI server at localhost:54035 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m Could not connect to TraCI server at localhost:36289 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m Could not connect to TraCI server at localhost:34795 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 3 seconds\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m Could not connect to TraCI server at localhost:34447 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 3 seconds\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m Could not connect to TraCI server at localhost:57509 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 3 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m Could not connect to TraCI server at localhost:54035 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 3 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m Could not connect to TraCI server at localhost:36289 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 3 seconds\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 1 seconds\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_26f87_00000:\n",
      "  agent_timesteps_total: 1200000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-08_11-42-40\n",
      "  done: false\n",
      "  episode_len_mean: 3000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1967.6986690688911\n",
      "  episode_reward_mean: -2601.9825600974414\n",
      "  episode_reward_min: -8214.95284352449\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 400\n",
      "  experiment_id: edcbe001b14143f4bf76a6a2914a7bd0\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2280697375535965\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 3.1478912830352783\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014247545972466469\n",
      "          model: {}\n",
      "          policy_loss: 0.007178671192377806\n",
      "          total_loss: 3322.775146484375\n",
      "          vf_explained_var: 0.0\n",
      "          vf_loss: 3322.764404296875\n",
      "    num_agent_steps_sampled: 1200000\n",
      "    num_agent_steps_trained: 1200000\n",
      "    num_steps_sampled: 1200000\n",
      "    num_steps_trained: 1200000\n",
      "  iterations_since_restore: 80\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.89090909090909\n",
      "    ram_util_percent: 30.211363636363636\n",
      "  pid: 307\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06573815231909215\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 3.493887863577421\n",
      "    mean_inference_ms: 0.4828056405193579\n",
      "    mean_raw_obs_processing_ms: 3.746342637890828\n",
      "  time_since_restore: 2168.4670996665955\n",
      "  time_this_iter_s: 31.365154504776\n",
      "  time_total_s: 2168.4670996665955\n",
      "  timers:\n",
      "    learn_throughput: 6479.708\n",
      "    learn_time_ms: 2314.919\n",
      "    load_throughput: 8489233.717\n",
      "    load_time_ms: 1.767\n",
      "    sample_throughput: 591.425\n",
      "    sample_time_ms: 25362.484\n",
      "    update_time_ms: 1.469\n",
      "  timestamp: 1665247360\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1200000\n",
      "  training_iteration: 80\n",
      "  trial_id: 26f87_00000\n",
      "  \n",
      "\u001b[2m\u001b[36m(pid=307)\u001b[0m 2022-10-08 11:42:40,702\tWARNING ppo.py:242 -- The magnitude of your environment rewards are more than 260.0x the scale of `vf_clip_param`. This means that it will take more than 260.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "== Status ==\n",
      "Memory usage on this node: 9.5/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.23 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_3_25daa8673b2c24aecf80a7fc79df4e93, 0.0/6.0 CPU_group_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_5_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_4_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_1_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_0_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_2_25daa8673b2c24aecf80a7fc79df4e93)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_onelane_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_26f87_00000 | RUNNING  | 141.225.10.229:307 |     80 |          2168.47 | 1200000 | -2601.98 |               1967.7 |             -8214.95 |               3000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m ring length: 251\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m v_max: 4.71224180704279\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m ring length: 240\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m v_max: 4.189299083856172\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m ring length: 224\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m v_max: 3.4281086136538996\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m ring length: 235\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m v_max: 3.9514850725282584\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m ring length: 229\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m v_max: 3.666034803266416\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 1 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m Could not connect to TraCI server at localhost:54035 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m Could not connect to TraCI server at localhost:36289 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m Could not connect to TraCI server at localhost:34795 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m Could not connect to TraCI server at localhost:34447 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m Could not connect to TraCI server at localhost:57509 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m Could not connect to TraCI server at localhost:57509 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 3 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m Could not connect to TraCI server at localhost:54035 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 3 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m Could not connect to TraCI server at localhost:36289 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 3 seconds\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m Could not connect to TraCI server at localhost:34795 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 3 seconds\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m Could not connect to TraCI server at localhost:34447 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 3 seconds\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_26f87_00000:\n",
      "  agent_timesteps_total: 1215000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-08_11-43-11\n",
      "  done: false\n",
      "  episode_len_mean: 3000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1967.6986690688911\n",
      "  episode_reward_mean: -2621.9404069696916\n",
      "  episode_reward_min: -8214.95284352449\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 405\n",
      "  experiment_id: edcbe001b14143f4bf76a6a2914a7bd0\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2280697375535965\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.001972187776118517\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0867113247513771\n",
      "          model: {}\n",
      "          policy_loss: 0.031148793175816536\n",
      "          total_loss: 3067.628662109375\n",
      "          vf_explained_var: -1.1207711203553572e-08\n",
      "          vf_loss: 3067.5771484375\n",
      "    num_agent_steps_sampled: 1215000\n",
      "    num_agent_steps_trained: 1215000\n",
      "    num_steps_sampled: 1215000\n",
      "    num_steps_trained: 1215000\n",
      "  iterations_since_restore: 81\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.324999999999996\n",
      "    ram_util_percent: 30.218181818181815\n",
      "  pid: 307\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0657306777345706\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 3.4929150998274\n",
      "    mean_inference_ms: 0.48277811880010646\n",
      "    mean_raw_obs_processing_ms: 3.752710332355862\n",
      "  time_since_restore: 2198.716404438019\n",
      "  time_this_iter_s: 30.24930477142334\n",
      "  time_total_s: 2198.716404438019\n",
      "  timers:\n",
      "    learn_throughput: 6687.439\n",
      "    learn_time_ms: 2243.011\n",
      "    load_throughput: 8790878.605\n",
      "    load_time_ms: 1.706\n",
      "    sample_throughput: 578.588\n",
      "    sample_time_ms: 25925.199\n",
      "    update_time_ms: 1.517\n",
      "  timestamp: 1665247391\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1215000\n",
      "  training_iteration: 81\n",
      "  trial_id: 26f87_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 9.5/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.23 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 accelerator_type:G, 0.0/6.0 CPU_group_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_4_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_0_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_3_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_5_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_2_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_1_25daa8673b2c24aecf80a7fc79df4e93)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_onelane_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_26f87_00000 | RUNNING  | 141.225.10.229:307 |     81 |          2198.72 | 1215000 | -2621.94 |               1967.7 |             -8214.95 |               3000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=307)\u001b[0m 2022-10-08 11:43:11,004\tWARNING ppo.py:242 -- The magnitude of your environment rewards are more than 262.0x the scale of `vf_clip_param`. This means that it will take more than 262.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m ring length: 235\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m v_max: 3.9514850725282584\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m ring length: 236\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m v_max: 3.999052674951912\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m ring length: 229\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m v_max: 3.666034803266416\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m ring length: 231\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m v_max: 3.761192925277308\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m ring length: 226\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m v_max: 3.5232840694769565\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m Could not connect to TraCI server at localhost:34795 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m Could not connect to TraCI server at localhost:34447 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m Could not connect to TraCI server at localhost:54035 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m Could not connect to TraCI server at localhost:36289 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m Could not connect to TraCI server at localhost:34795 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 3 seconds\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m Could not connect to TraCI server at localhost:34447 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 3 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m Could not connect to TraCI server at localhost:54035 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 3 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m Could not connect to TraCI server at localhost:36289 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 3 seconds\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_26f87_00000:\n",
      "  agent_timesteps_total: 1230000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-08_11-43-42\n",
      "  done: false\n",
      "  episode_len_mean: 3000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1967.6986690688911\n",
      "  episode_reward_mean: -2483.809652641133\n",
      "  episode_reward_min: -8214.95284352449\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 410\n",
      "  experiment_id: edcbe001b14143f4bf76a6a2914a7bd0\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.34210461378097534\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: -1.6593831777572632\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012420108541846275\n",
      "          model: {}\n",
      "          policy_loss: 0.005444633774459362\n",
      "          total_loss: 2169.603271484375\n",
      "          vf_explained_var: -2.0377657339309962e-09\n",
      "          vf_loss: 2169.59375\n",
      "    num_agent_steps_sampled: 1230000\n",
      "    num_agent_steps_trained: 1230000\n",
      "    num_steps_sampled: 1230000\n",
      "    num_steps_trained: 1230000\n",
      "  iterations_since_restore: 82\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.408888888888892\n",
      "    ram_util_percent: 30.26222222222223\n",
      "  pid: 307\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06572317386821167\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 3.4919365062905894\n",
      "    mean_inference_ms: 0.48275119689669893\n",
      "    mean_raw_obs_processing_ms: 3.7601322900254583\n",
      "  time_since_restore: 2230.5799584388733\n",
      "  time_this_iter_s: 31.863554000854492\n",
      "  time_total_s: 2230.5799584388733\n",
      "  timers:\n",
      "    learn_throughput: 6734.128\n",
      "    learn_time_ms: 2227.46\n",
      "    load_throughput: 8675237.859\n",
      "    load_time_ms: 1.729\n",
      "    sample_throughput: 566.597\n",
      "    sample_time_ms: 26473.824\n",
      "    update_time_ms: 1.516\n",
      "  timestamp: 1665247422\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1230000\n",
      "  training_iteration: 82\n",
      "  trial_id: 26f87_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=307)\u001b[0m 2022-10-08 11:43:42,937\tWARNING ppo.py:242 -- The magnitude of your environment rewards are more than 248.0x the scale of `vf_clip_param`. This means that it will take more than 248.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "== Status ==\n",
      "Memory usage on this node: 9.5/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.23 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_1_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_4_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_3_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_2_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_0_25daa8673b2c24aecf80a7fc79df4e93, 0.0/6.0 CPU_group_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_5_25daa8673b2c24aecf80a7fc79df4e93)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_onelane_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_26f87_00000 | RUNNING  | 141.225.10.229:307 |     82 |          2230.58 | 1230000 | -2483.81 |               1967.7 |             -8214.95 |               3000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m ring length: 225\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m v_max: 3.4756971311168834\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m ring length: 222\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m v_max: 3.3329270738617227\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m ring length: 267\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m v_max: 5.472051563171826\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m ring length: 254\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m v_max: 4.854791141191876\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m ring length: 253\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m v_max: 4.807278529691987\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 1 seconds\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_26f87_00000:\n",
      "  agent_timesteps_total: 1245000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-08_11-44-07\n",
      "  done: false\n",
      "  episode_len_mean: 3000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1967.6986690688911\n",
      "  episode_reward_mean: -2514.495551162443\n",
      "  episode_reward_min: -8214.95284352449\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 415\n",
      "  experiment_id: edcbe001b14143f4bf76a6a2914a7bd0\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.34210461378097534\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.7665794491767883\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.07770765572786331\n",
      "          model: {}\n",
      "          policy_loss: 0.007711911108344793\n",
      "          total_loss: 2831.888916015625\n",
      "          vf_explained_var: -2.547207111902594e-09\n",
      "          vf_loss: 2831.8544921875\n",
      "    num_agent_steps_sampled: 1245000\n",
      "    num_agent_steps_trained: 1245000\n",
      "    num_steps_sampled: 1245000\n",
      "    num_steps_trained: 1245000\n",
      "  iterations_since_restore: 83\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 24.202777777777776\n",
      "    ram_util_percent: 30.327777777777783\n",
      "  pid: 307\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06571598365549654\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 3.4909396480223216\n",
      "    mean_inference_ms: 0.4827243334958311\n",
      "    mean_raw_obs_processing_ms: 3.766039670125433\n",
      "  time_since_restore: 2255.595456123352\n",
      "  time_this_iter_s: 25.01549768447876\n",
      "  time_total_s: 2255.595456123352\n",
      "  timers:\n",
      "    learn_throughput: 6633.986\n",
      "    learn_time_ms: 2261.084\n",
      "    load_throughput: 7866975.104\n",
      "    load_time_ms: 1.907\n",
      "    sample_throughput: 566.255\n",
      "    sample_time_ms: 26489.811\n",
      "    update_time_ms: 1.393\n",
      "  timestamp: 1665247447\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1245000\n",
      "  training_iteration: 83\n",
      "  trial_id: 26f87_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 9.5/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.23 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_4_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_2_25daa8673b2c24aecf80a7fc79df4e93, 0.0/6.0 CPU_group_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_5_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_1_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_3_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_0_25daa8673b2c24aecf80a7fc79df4e93)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_onelane_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_26f87_00000 | RUNNING  | 141.225.10.229:307 |     83 |           2255.6 | 1245000 |  -2514.5 |               1967.7 |             -8214.95 |               3000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=307)\u001b[0m 2022-10-08 11:44:07,999\tWARNING ppo.py:242 -- The magnitude of your environment rewards are more than 251.0x the scale of `vf_clip_param`. This means that it will take more than 251.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m ring length: 231\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m v_max: 3.761192925277308\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m ring length: 263\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m v_max: 5.282212215151455\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m ring length: 237\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m v_max: 4.04661795519353\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m ring length: 255\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m v_max: 4.902299776966978\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m ring length: 241\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m v_max: 4.236854288051661\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 1 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_WaveAttenuationPOEnv-v0_26f87_00000:\n",
      "  agent_timesteps_total: 1260000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-08_11-44-33\n",
      "  done: false\n",
      "  episode_len_mean: 3000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1967.6986690688911\n",
      "  episode_reward_mean: -2479.859996851745\n",
      "  episode_reward_min: -8214.95284352449\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 420\n",
      "  experiment_id: edcbe001b14143f4bf76a6a2914a7bd0\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5131568908691406\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.0757734775543213\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.020667489618062973\n",
      "          model: {}\n",
      "          policy_loss: 0.010034331120550632\n",
      "          total_loss: 3146.4638671875\n",
      "          vf_explained_var: -1.5792684493476372e-08\n",
      "          vf_loss: 3146.443115234375\n",
      "    num_agent_steps_sampled: 1260000\n",
      "    num_agent_steps_trained: 1260000\n",
      "    num_steps_sampled: 1260000\n",
      "    num_steps_trained: 1260000\n",
      "  iterations_since_restore: 84\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 24.255555555555553\n",
      "    ram_util_percent: 30.350000000000005\n",
      "  pid: 307\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06570924073022871\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 3.489949613782416\n",
      "    mean_inference_ms: 0.4826972707475341\n",
      "    mean_raw_obs_processing_ms: 3.77029489722402\n",
      "  time_since_restore: 2280.7554171085358\n",
      "  time_this_iter_s: 25.159960985183716\n",
      "  time_total_s: 2280.7554171085358\n",
      "  timers:\n",
      "    learn_throughput: 6594.472\n",
      "    learn_time_ms: 2274.632\n",
      "    load_throughput: 7663815.429\n",
      "    load_time_ms: 1.957\n",
      "    sample_throughput: 577.18\n",
      "    sample_time_ms: 25988.436\n",
      "    update_time_ms: 1.459\n",
      "  timestamp: 1665247473\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1260000\n",
      "  training_iteration: 84\n",
      "  trial_id: 26f87_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 9.5/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.23 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_5_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_1_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_4_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_3_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_0_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_2_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 accelerator_type:G, 0.0/6.0 CPU_group_25daa8673b2c24aecf80a7fc79df4e93)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_onelane_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_26f87_00000 | RUNNING  | 141.225.10.229:307 |     84 |          2280.76 | 1260000 | -2479.86 |               1967.7 |             -8214.95 |               3000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=307)\u001b[0m 2022-10-08 11:44:33,229\tWARNING ppo.py:242 -- The magnitude of your environment rewards are more than 248.0x the scale of `vf_clip_param`. This means that it will take more than 248.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m ring length: 230\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m v_max: 3.7136148111012934\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m ring length: 241\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m v_max: 4.236854288051661\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m ring length: 257\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m v_max: 4.997304682316517\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m ring length: 239\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m v_max: 4.141741239205832\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m ring length: 251\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m v_max: 4.71224180704279\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m Could not connect to TraCI server at localhost:34795 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m Could not connect to TraCI server at localhost:34447 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m Could not connect to TraCI server at localhost:54035 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m Could not connect to TraCI server at localhost:57509 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m Could not connect to TraCI server at localhost:36289 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m Could not connect to TraCI server at localhost:34795 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 3 seconds\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m Could not connect to TraCI server at localhost:34447 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 3 seconds\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m Could not connect to TraCI server at localhost:57509 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 3 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m Could not connect to TraCI server at localhost:54035 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 3 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m Could not connect to TraCI server at localhost:36289 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 3 seconds\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 1 seconds\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_26f87_00000:\n",
      "  agent_timesteps_total: 1275000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-08_11-45-04\n",
      "  done: false\n",
      "  episode_len_mean: 3000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2299.497932707432\n",
      "  episode_reward_mean: -2437.929157962197\n",
      "  episode_reward_min: -8063.785226228408\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 425\n",
      "  experiment_id: edcbe001b14143f4bf76a6a2914a7bd0\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5131568908691406\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.627829909324646\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014728059992194176\n",
      "          model: {}\n",
      "          policy_loss: 0.0050888583064079285\n",
      "          total_loss: 2987.627197265625\n",
      "          vf_explained_var: 9.679387069638778e-09\n",
      "          vf_loss: 2987.614501953125\n",
      "    num_agent_steps_sampled: 1275000\n",
      "    num_agent_steps_trained: 1275000\n",
      "    num_steps_sampled: 1275000\n",
      "    num_steps_trained: 1275000\n",
      "  iterations_since_restore: 85\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.922222222222224\n",
      "    ram_util_percent: 30.33111111111111\n",
      "  pid: 307\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06570281953254768\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 3.488988390119801\n",
      "    mean_inference_ms: 0.48267091738424384\n",
      "    mean_raw_obs_processing_ms: 3.7758055544002973\n",
      "  time_since_restore: 2312.21204161644\n",
      "  time_this_iter_s: 31.456624507904053\n",
      "  time_total_s: 2312.21204161644\n",
      "  timers:\n",
      "    learn_throughput: 6619.433\n",
      "    learn_time_ms: 2266.055\n",
      "    load_throughput: 7907117.272\n",
      "    load_time_ms: 1.897\n",
      "    sample_throughput: 579.528\n",
      "    sample_time_ms: 25883.114\n",
      "    update_time_ms: 1.315\n",
      "  timestamp: 1665247504\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1275000\n",
      "  training_iteration: 85\n",
      "  trial_id: 26f87_00000\n",
      "  \n",
      "\u001b[2m\u001b[36m(pid=307)\u001b[0m 2022-10-08 11:45:04,740\tWARNING ppo.py:242 -- The magnitude of your environment rewards are more than 244.0x the scale of `vf_clip_param`. This means that it will take more than 244.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Memory usage on this node: 9.5/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.23 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_0_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_3_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_4_25daa8673b2c24aecf80a7fc79df4e93, 0.0/6.0 CPU_group_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_2_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_1_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_5_25daa8673b2c24aecf80a7fc79df4e93)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_onelane_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_26f87_00000 | RUNNING  | 141.225.10.229:307 |     85 |          2312.21 | 1275000 | -2437.93 |               2299.5 |             -8063.79 |               3000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m ring length: 226\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m v_max: 3.5232840694769565\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m ring length: 246\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m v_max: 4.474587708995648\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m ring length: 257\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m v_max: 4.997304682316517\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m ring length: 266\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m v_max: 5.42459972166245\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m ring length: 234\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m v_max: 3.903915223349057\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 1 seconds\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_26f87_00000:\n",
      "  agent_timesteps_total: 1290000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-08_11-45-30\n",
      "  done: false\n",
      "  episode_len_mean: 3000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2299.497932707432\n",
      "  episode_reward_mean: -2367.0494469490404\n",
      "  episode_reward_min: -8063.785226228408\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 430\n",
      "  experiment_id: edcbe001b14143f4bf76a6a2914a7bd0\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5131568908691406\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: -0.4438342750072479\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.007171756122261286\n",
      "          model: {}\n",
      "          policy_loss: 0.0015316794160753489\n",
      "          total_loss: 2919.3125\n",
      "          vf_explained_var: -5.094414223805188e-09\n",
      "          vf_loss: 2919.307373046875\n",
      "    num_agent_steps_sampled: 1290000\n",
      "    num_agent_steps_trained: 1290000\n",
      "    num_steps_sampled: 1290000\n",
      "    num_steps_trained: 1290000\n",
      "  iterations_since_restore: 86\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 23.924324324324324\n",
      "    ram_util_percent: 30.42432432432433\n",
      "  pid: 307\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06569743701157513\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 3.488100844256115\n",
      "    mean_inference_ms: 0.4826490902920248\n",
      "    mean_raw_obs_processing_ms: 3.778562398289397\n",
      "  time_since_restore: 2337.9710919857025\n",
      "  time_this_iter_s: 25.759050369262695\n",
      "  time_total_s: 2337.9710919857025\n",
      "  timers:\n",
      "    learn_throughput: 6602.032\n",
      "    learn_time_ms: 2272.028\n",
      "    load_throughput: 7913781.132\n",
      "    load_time_ms: 1.895\n",
      "    sample_throughput: 578.844\n",
      "    sample_time_ms: 25913.724\n",
      "    update_time_ms: 1.579\n",
      "  timestamp: 1665247530\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1290000\n",
      "  training_iteration: 86\n",
      "  trial_id: 26f87_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 9.5/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.23 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_2_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_1_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_3_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_5_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_4_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 accelerator_type:G, 0.0/6.0 CPU_group_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_0_25daa8673b2c24aecf80a7fc79df4e93)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_onelane_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_26f87_00000 | RUNNING  | 141.225.10.229:307 |     86 |          2337.97 | 1290000 | -2367.05 |               2299.5 |             -8063.79 |               3000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=307)\u001b[0m 2022-10-08 11:45:30,573\tWARNING ppo.py:242 -- The magnitude of your environment rewards are more than 237.0x the scale of `vf_clip_param`. This means that it will take more than 237.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m ring length: 224\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m v_max: 3.4281086136538996\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m ring length: 252\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m v_max: 4.759762049824181\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m ring length: 234\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m v_max: 3.903915223349057\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m ring length: 225\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m v_max: 3.4756971311168834\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m ring length: 249\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m v_max: 4.617190445131122\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 1 seconds\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_26f87_00000:\n",
      "  agent_timesteps_total: 1305000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-08_11-45-56\n",
      "  done: false\n",
      "  episode_len_mean: 3000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2299.497932707432\n",
      "  episode_reward_mean: -2259.318091501576\n",
      "  episode_reward_min: -8063.785226228408\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 435\n",
      "  experiment_id: edcbe001b14143f4bf76a6a2914a7bd0\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2565784454345703\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: -1.1427640914916992\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.048304516822099686\n",
      "          model: {}\n",
      "          policy_loss: 0.019814984872937202\n",
      "          total_loss: 2578.951904296875\n",
      "          vf_explained_var: -5.603855601776786e-09\n",
      "          vf_loss: 2578.919677734375\n",
      "    num_agent_steps_sampled: 1305000\n",
      "    num_agent_steps_trained: 1305000\n",
      "    num_steps_sampled: 1305000\n",
      "    num_steps_trained: 1305000\n",
      "  iterations_since_restore: 87\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 24.29189189189189\n",
      "    ram_util_percent: 30.367567567567576\n",
      "  pid: 307\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06569268447838318\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 3.4872575746390417\n",
      "    mean_inference_ms: 0.4826315301890072\n",
      "    mean_raw_obs_processing_ms: 3.7814583794902306\n",
      "  time_since_restore: 2363.8330397605896\n",
      "  time_this_iter_s: 25.861947774887085\n",
      "  time_total_s: 2363.8330397605896\n",
      "  timers:\n",
      "    learn_throughput: 6447.902\n",
      "    learn_time_ms: 2326.338\n",
      "    load_throughput: 8138169.4\n",
      "    load_time_ms: 1.843\n",
      "    sample_throughput: 579.841\n",
      "    sample_time_ms: 25869.179\n",
      "    update_time_ms: 1.565\n",
      "  timestamp: 1665247556\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1305000\n",
      "  training_iteration: 87\n",
      "  trial_id: 26f87_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Memory usage on this node: 9.5/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.23 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_0_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_5_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_2_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_4_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_1_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_3_25daa8673b2c24aecf80a7fc79df4e93, 0.0/6.0 CPU_group_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 accelerator_type:G)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_onelane_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_26f87_00000 | RUNNING  | 141.225.10.229:307 |     87 |          2363.83 | 1305000 | -2259.32 |               2299.5 |             -8063.79 |               3000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=307)\u001b[0m 2022-10-08 11:45:56,507\tWARNING ppo.py:242 -- The magnitude of your environment rewards are more than 226.0x the scale of `vf_clip_param`. This means that it will take more than 226.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m ring length: 227\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m v_max: 3.570869368805752\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m ring length: 245\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m v_max: 4.427046998576354\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m ring length: 255\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m v_max: 4.902299776966978\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m ring length: 247\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m v_max: 4.522125250095652\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m ring length: 229\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m v_max: 3.666034803266416\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 1 seconds\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_26f87_00000:\n",
      "  agent_timesteps_total: 1320000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-08_11-46-22\n",
      "  done: false\n",
      "  episode_len_mean: 3000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2299.497932707432\n",
      "  episode_reward_mean: -2229.8106458750144\n",
      "  episode_reward_min: -8063.785226228408\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 440\n",
      "  experiment_id: edcbe001b14143f4bf76a6a2914a7bd0\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.38486766815185547\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.6431587934494019\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016317183151841164\n",
      "          model: {}\n",
      "          policy_loss: 0.005996534135192633\n",
      "          total_loss: 3131.74365234375\n",
      "          vf_explained_var: -1.4773800849354757e-08\n",
      "          vf_loss: 3131.73193359375\n",
      "    num_agent_steps_sampled: 1320000\n",
      "    num_agent_steps_trained: 1320000\n",
      "    num_steps_sampled: 1320000\n",
      "    num_steps_trained: 1320000\n",
      "  iterations_since_restore: 88\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 24.249999999999996\n",
      "    ram_util_percent: 30.35555555555556\n",
      "  pid: 307\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06568851712690255\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 3.4864765146273005\n",
      "    mean_inference_ms: 0.48261877612365367\n",
      "    mean_raw_obs_processing_ms: 3.784424109667607\n",
      "  time_since_restore: 2389.385491847992\n",
      "  time_this_iter_s: 25.552452087402344\n",
      "  time_total_s: 2389.385491847992\n",
      "  timers:\n",
      "    learn_throughput: 6444.152\n",
      "    learn_time_ms: 2327.692\n",
      "    load_throughput: 7658404.645\n",
      "    load_time_ms: 1.959\n",
      "    sample_throughput: 579.295\n",
      "    sample_time_ms: 25893.547\n",
      "    update_time_ms: 1.593\n",
      "  timestamp: 1665247582\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1320000\n",
      "  training_iteration: 88\n",
      "  trial_id: 26f87_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 9.5/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.23 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_0_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_5_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_1_25daa8673b2c24aecf80a7fc79df4e93, 0.0/6.0 CPU_group_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_4_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_3_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_2_25daa8673b2c24aecf80a7fc79df4e93)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_onelane_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_26f87_00000 | RUNNING  | 141.225.10.229:307 |     88 |          2389.39 | 1320000 | -2229.81 |               2299.5 |             -8063.79 |               3000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=307)\u001b[0m 2022-10-08 11:46:22,121\tWARNING ppo.py:242 -- The magnitude of your environment rewards are more than 223.0x the scale of `vf_clip_param`. This means that it will take more than 223.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m ring length: 221\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m v_max: 3.285334164169417\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m ring length: 239\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m v_max: 4.141741239205832\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m ring length: 244\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m v_max: 4.379503211387676\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m ring length: 226\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m v_max: 3.5232840694769565\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m ring length: 269\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m v_max: 5.566938458220921\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 1 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 1 seconds\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_26f87_00000:\n",
      "  agent_timesteps_total: 1335000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-08_11-46-48\n",
      "  done: false\n",
      "  episode_len_mean: 3000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2299.497932707432\n",
      "  episode_reward_mean: -2165.0194655815103\n",
      "  episode_reward_min: -8063.785226228408\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 445\n",
      "  experiment_id: edcbe001b14143f4bf76a6a2914a7bd0\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.38486766815185547\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.08754120022058487\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.018369169905781746\n",
      "          model: {}\n",
      "          policy_loss: 0.0066942693665623665\n",
      "          total_loss: 3004.977783203125\n",
      "          vf_explained_var: 0.0\n",
      "          vf_loss: 3004.9638671875\n",
      "    num_agent_steps_sampled: 1335000\n",
      "    num_agent_steps_trained: 1335000\n",
      "    num_steps_sampled: 1335000\n",
      "    num_steps_trained: 1335000\n",
      "  iterations_since_restore: 89\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 24.36216216216216\n",
      "    ram_util_percent: 30.35945945945947\n",
      "  pid: 307\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0656846790817084\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 3.485716701433806\n",
      "    mean_inference_ms: 0.48260880582458365\n",
      "    mean_raw_obs_processing_ms: 3.786006303560461\n",
      "  time_since_restore: 2415.2462215423584\n",
      "  time_this_iter_s: 25.860729694366455\n",
      "  time_total_s: 2415.2462215423584\n",
      "  timers:\n",
      "    learn_throughput: 6375.911\n",
      "    learn_time_ms: 2352.605\n",
      "    load_throughput: 7698795.888\n",
      "    load_time_ms: 1.948\n",
      "    sample_throughput: 589.373\n",
      "    sample_time_ms: 25450.757\n",
      "    update_time_ms: 1.588\n",
      "  timestamp: 1665247608\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1335000\n",
      "  training_iteration: 89\n",
      "  trial_id: 26f87_00000\n",
      "  \n",
      "\u001b[2m\u001b[36m(pid=307)\u001b[0m 2022-10-08 11:46:48,046\tWARNING ppo.py:242 -- The magnitude of your environment rewards are more than 217.0x the scale of `vf_clip_param`. This means that it will take more than 217.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "== Status ==\n",
      "Memory usage on this node: 9.5/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.23 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_2_25daa8673b2c24aecf80a7fc79df4e93, 0.0/6.0 CPU_group_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_1_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_5_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_3_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_0_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_4_25daa8673b2c24aecf80a7fc79df4e93)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_onelane_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_26f87_00000 | RUNNING  | 141.225.10.229:307 |     89 |          2415.25 | 1335000 | -2165.02 |               2299.5 |             -8063.79 |               3000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m ring length: 248\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m v_max: 4.569659527528101\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m ring length: 253\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m v_max: 4.807278529691987\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m ring length: 241\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m v_max: 4.236854288051661\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m ring length: 266\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m v_max: 5.42459972166245\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m ring length: 235\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m v_max: 3.9514850725282584\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 1 seconds\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_26f87_00000:\n",
      "  agent_timesteps_total: 1350000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-08_11-47-13\n",
      "  done: false\n",
      "  episode_len_mean: 3000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2299.497932707432\n",
      "  episode_reward_mean: -2290.421226881044\n",
      "  episode_reward_min: -8063.785226228408\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 450\n",
      "  experiment_id: edcbe001b14143f4bf76a6a2914a7bd0\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.38486766815185547\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 4.307645797729492\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.09815486520528793\n",
      "          model: {}\n",
      "          policy_loss: 0.01916634477674961\n",
      "          total_loss: 3219.3349609375\n",
      "          vf_explained_var: -1.6811567249419568e-08\n",
      "          vf_loss: 3219.27783203125\n",
      "    num_agent_steps_sampled: 1350000\n",
      "    num_agent_steps_trained: 1350000\n",
      "    num_steps_sampled: 1350000\n",
      "    num_steps_trained: 1350000\n",
      "  iterations_since_restore: 90\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 24.319444444444443\n",
      "    ram_util_percent: 30.39166666666667\n",
      "  pid: 307\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06568080255809378\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 3.4849366106895316\n",
      "    mean_inference_ms: 0.48259737070511494\n",
      "    mean_raw_obs_processing_ms: 3.787624457569184\n",
      "  time_since_restore: 2440.3177206516266\n",
      "  time_this_iter_s: 25.07149910926819\n",
      "  time_total_s: 2440.3177206516266\n",
      "  timers:\n",
      "    learn_throughput: 6322.686\n",
      "    learn_time_ms: 2372.409\n",
      "    load_throughput: 7077640.283\n",
      "    load_time_ms: 2.119\n",
      "    sample_throughput: 604.805\n",
      "    sample_time_ms: 24801.376\n",
      "    update_time_ms: 1.597\n",
      "  timestamp: 1665247633\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1350000\n",
      "  training_iteration: 90\n",
      "  trial_id: 26f87_00000\n",
      "  \n",
      "\u001b[2m\u001b[36m(pid=307)\u001b[0m 2022-10-08 11:47:13,208\tWARNING ppo.py:242 -- The magnitude of your environment rewards are more than 229.0x the scale of `vf_clip_param`. This means that it will take more than 229.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "== Status ==\n",
      "Memory usage on this node: 9.5/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.23 GiB heap, 0.0/14.65 GiB objects (0.0/6.0 CPU_group_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_5_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_3_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_4_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_2_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_0_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_1_25daa8673b2c24aecf80a7fc79df4e93)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_onelane_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_26f87_00000 | RUNNING  | 141.225.10.229:307 |     90 |          2440.32 | 1350000 | -2290.42 |               2299.5 |             -8063.79 |               3000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m ring length: 244\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m v_max: 4.379503211387676\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m ring length: 239\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m v_max: 4.141741239205832\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m ring length: 242\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m v_max: 4.2844067680020546\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m ring length: 236\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m v_max: 3.999052674951912\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m ring length: 262\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m v_max: 5.234739483008334\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m Could not connect to TraCI server at localhost:34795 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m Could not connect to TraCI server at localhost:34447 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m Could not connect to TraCI server at localhost:57509 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m Could not connect to TraCI server at localhost:54035 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m Could not connect to TraCI server at localhost:36289 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m Could not connect to TraCI server at localhost:34795 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 3 seconds\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m Could not connect to TraCI server at localhost:34447 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 3 seconds\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m Could not connect to TraCI server at localhost:57509 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 3 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m Could not connect to TraCI server at localhost:54035 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 3 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m Could not connect to TraCI server at localhost:36289 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 3 seconds\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m Could not connect to TraCI server at localhost:34795 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m Could not connect to TraCI server at localhost:57509 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m Could not connect to TraCI server at localhost:54035 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m Could not connect to TraCI server at localhost:34447 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m Could not connect to TraCI server at localhost:34795 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 3 seconds\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m Could not connect to TraCI server at localhost:34447 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 3 seconds\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m Could not connect to TraCI server at localhost:57509 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 3 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m Could not connect to TraCI server at localhost:54035 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 3 seconds\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_26f87_00000:\n",
      "  agent_timesteps_total: 1365000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-08_11-47-50\n",
      "  done: false\n",
      "  episode_len_mean: 3000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2299.497932707432\n",
      "  episode_reward_mean: -2212.594740917076\n",
      "  episode_reward_min: -8063.785226228408\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 455\n",
      "  experiment_id: edcbe001b14143f4bf76a6a2914a7bd0\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5773015022277832\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: -0.14121504127979279\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.006207666825503111\n",
      "          model: {}\n",
      "          policy_loss: 0.0018437012331560254\n",
      "          total_loss: 2730.298828125\n",
      "          vf_explained_var: 6.622738357719982e-09\n",
      "          vf_loss: 2730.292724609375\n",
      "    num_agent_steps_sampled: 1365000\n",
      "    num_agent_steps_trained: 1365000\n",
      "    num_steps_sampled: 1365000\n",
      "    num_steps_trained: 1365000\n",
      "  iterations_since_restore: 91\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 18.61132075471698\n",
      "    ram_util_percent: 30.286792452830195\n",
      "  pid: 307\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06567722058282527\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 3.4841903099071097\n",
      "    mean_inference_ms: 0.4825867245690937\n",
      "    mean_raw_obs_processing_ms: 3.791393584388991\n",
      "  time_since_restore: 2477.1437635421753\n",
      "  time_this_iter_s: 36.826042890548706\n",
      "  time_total_s: 2477.1437635421753\n",
      "  timers:\n",
      "    learn_throughput: 6123.701\n",
      "    learn_time_ms: 2449.499\n",
      "    load_throughput: 6909132.44\n",
      "    load_time_ms: 2.171\n",
      "    sample_throughput: 590.971\n",
      "    sample_time_ms: 25381.941\n",
      "    update_time_ms: 1.556\n",
      "  timestamp: 1665247670\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1365000\n",
      "  training_iteration: 91\n",
      "  trial_id: 26f87_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 9.5/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.23 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_5_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_3_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_4_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_0_25daa8673b2c24aecf80a7fc79df4e93, 0.0/6.0 CPU_group_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_2_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_1_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 accelerator_type:G)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_onelane_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_26f87_00000 | RUNNING  | 141.225.10.229:307 |     91 |          2477.14 | 1365000 | -2212.59 |               2299.5 |             -8063.79 |               3000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=307)\u001b[0m 2022-10-08 11:47:50,100\tWARNING ppo.py:242 -- The magnitude of your environment rewards are more than 221.0x the scale of `vf_clip_param`. This means that it will take more than 221.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m ring length: 230\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m v_max: 3.7136148111012934\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m ring length: 262\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m v_max: 5.234739483008334\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m ring length: 262\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m v_max: 5.234739483008334\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m ring length: 262\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m v_max: 5.234739483008334\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m ring length: 229\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m v_max: 3.666034803266416\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 1 seconds\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_26f87_00000:\n",
      "  agent_timesteps_total: 1380000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-08_11-48-14\n",
      "  done: false\n",
      "  episode_len_mean: 3000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2299.497932707432\n",
      "  episode_reward_mean: -2252.0437007000787\n",
      "  episode_reward_min: -8063.785226228408\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 460\n",
      "  experiment_id: edcbe001b14143f4bf76a6a2914a7bd0\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2886507511138916\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.5200713872909546\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01039374340325594\n",
      "          model: {}\n",
      "          policy_loss: 0.002303231041878462\n",
      "          total_loss: 3156.128662109375\n",
      "          vf_explained_var: -6.622738357719982e-09\n",
      "          vf_loss: 3156.123779296875\n",
      "    num_agent_steps_sampled: 1380000\n",
      "    num_agent_steps_trained: 1380000\n",
      "    num_steps_sampled: 1380000\n",
      "    num_steps_trained: 1380000\n",
      "  iterations_since_restore: 92\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 24.591428571428565\n",
      "    ram_util_percent: 30.45142857142857\n",
      "  pid: 307\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06567392920604627\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 3.483470515700866\n",
      "    mean_inference_ms: 0.4825753582660793\n",
      "    mean_raw_obs_processing_ms: 3.794897675350361\n",
      "  time_since_restore: 2501.676574230194\n",
      "  time_this_iter_s: 24.5328106880188\n",
      "  time_total_s: 2501.676574230194\n",
      "  timers:\n",
      "    learn_throughput: 6150.226\n",
      "    learn_time_ms: 2438.935\n",
      "    load_throughput: 7589668.858\n",
      "    load_time_ms: 1.976\n",
      "    sample_throughput: 608.275\n",
      "    sample_time_ms: 24659.909\n",
      "    update_time_ms: 1.545\n",
      "  timestamp: 1665247694\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1380000\n",
      "  training_iteration: 92\n",
      "  trial_id: 26f87_00000\n",
      "  \n",
      "\u001b[2m\u001b[36m(pid=307)\u001b[0m 2022-10-08 11:48:14,688\tWARNING ppo.py:242 -- The magnitude of your environment rewards are more than 225.0x the scale of `vf_clip_param`. This means that it will take more than 225.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "== Status ==\n",
      "Memory usage on this node: 9.5/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.23 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_3_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_1_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_5_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_2_25daa8673b2c24aecf80a7fc79df4e93, 0.0/6.0 CPU_group_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_0_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_4_25daa8673b2c24aecf80a7fc79df4e93)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_onelane_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_26f87_00000 | RUNNING  | 141.225.10.229:307 |     92 |          2501.68 | 1380000 | -2252.04 |               2299.5 |             -8063.79 |               3000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m ring length: 265\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m v_max: 5.37714246265477\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m ring length: 268\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m v_max: 5.5194978538368025\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m ring length: 246\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m v_max: 4.474587708995648\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m ring length: 223\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m v_max: 3.3805185755587788\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m ring length: 220\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m v_max: 3.2377399006821497\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 1 seconds\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_26f87_00000:\n",
      "  agent_timesteps_total: 1395000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-08_11-48-40\n",
      "  done: false\n",
      "  episode_len_mean: 3000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2299.497932707432\n",
      "  episode_reward_mean: -2299.357529373259\n",
      "  episode_reward_min: -8063.785226228408\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 465\n",
      "  experiment_id: edcbe001b14143f4bf76a6a2914a7bd0\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2886507511138916\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 2.9399662017822266\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.027416901662945747\n",
      "          model: {}\n",
      "          policy_loss: 0.017113078385591507\n",
      "          total_loss: 3456.958984375\n",
      "          vf_explained_var: -4.58497284583359e-09\n",
      "          vf_loss: 3456.93359375\n",
      "    num_agent_steps_sampled: 1395000\n",
      "    num_agent_steps_trained: 1395000\n",
      "    num_steps_sampled: 1395000\n",
      "    num_steps_trained: 1395000\n",
      "  iterations_since_restore: 93\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 24.322222222222223\n",
      "    ram_util_percent: 30.438888888888894\n",
      "  pid: 307\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06567077027791565\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 3.482787384104623\n",
      "    mean_inference_ms: 0.4825657150452687\n",
      "    mean_raw_obs_processing_ms: 3.7984486049705173\n",
      "  time_since_restore: 2527.0701332092285\n",
      "  time_this_iter_s: 25.393558979034424\n",
      "  time_total_s: 2527.0701332092285\n",
      "  timers:\n",
      "    learn_throughput: 6019.604\n",
      "    learn_time_ms: 2491.858\n",
      "    load_throughput: 7399362.555\n",
      "    load_time_ms: 2.027\n",
      "    sample_throughput: 608.65\n",
      "    sample_time_ms: 24644.705\n",
      "    update_time_ms: 1.558\n",
      "  timestamp: 1665247720\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1395000\n",
      "  training_iteration: 93\n",
      "  trial_id: 26f87_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 9.5/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.23 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_4_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_1_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_0_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_2_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_3_25daa8673b2c24aecf80a7fc79df4e93, 0.0/6.0 CPU_group_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_5_25daa8673b2c24aecf80a7fc79df4e93)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_onelane_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_26f87_00000 | RUNNING  | 141.225.10.229:307 |     93 |          2527.07 | 1395000 | -2299.36 |               2299.5 |             -8063.79 |               3000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=307)\u001b[0m 2022-10-08 11:48:40,140\tWARNING ppo.py:242 -- The magnitude of your environment rewards are more than 230.0x the scale of `vf_clip_param`. This means that it will take more than 230.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m ring length: 266\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m v_max: 5.42459972166245\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m ring length: 249\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m v_max: 4.617190445131122\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m ring length: 243\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m v_max: 4.331956438196479\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m ring length: 267\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m v_max: 5.472051563171826\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m ring length: 240\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m v_max: 4.189299083856172\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 1 seconds\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_26f87_00000:\n",
      "  agent_timesteps_total: 1410000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-08_11-49-04\n",
      "  done: false\n",
      "  episode_len_mean: 3000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2299.497932707432\n",
      "  episode_reward_mean: -2292.401332484245\n",
      "  episode_reward_min: -8059.548538429305\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 470\n",
      "  experiment_id: edcbe001b14143f4bf76a6a2914a7bd0\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2886507511138916\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.5357040166854858\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.038006577640771866\n",
      "          model: {}\n",
      "          policy_loss: 0.02134809084236622\n",
      "          total_loss: 3351.501220703125\n",
      "          vf_explained_var: -1.0188828447610376e-08\n",
      "          vf_loss: 3351.468994140625\n",
      "    num_agent_steps_sampled: 1410000\n",
      "    num_agent_steps_trained: 1410000\n",
      "    num_steps_sampled: 1410000\n",
      "    num_steps_trained: 1410000\n",
      "  iterations_since_restore: 94\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 24.36666666666666\n",
      "    ram_util_percent: 30.44166666666667\n",
      "  pid: 307\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0656679495976928\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 3.482125714364372\n",
      "    mean_inference_ms: 0.48255756026517643\n",
      "    mean_raw_obs_processing_ms: 3.8008804152809246\n",
      "  time_since_restore: 2551.811856985092\n",
      "  time_this_iter_s: 24.741723775863647\n",
      "  time_total_s: 2551.811856985092\n",
      "  timers:\n",
      "    learn_throughput: 6071.102\n",
      "    learn_time_ms: 2470.721\n",
      "    load_throughput: 8244710.323\n",
      "    load_time_ms: 1.819\n",
      "    sample_throughput: 609.151\n",
      "    sample_time_ms: 24624.455\n",
      "    update_time_ms: 1.465\n",
      "  timestamp: 1665247744\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1410000\n",
      "  training_iteration: 94\n",
      "  trial_id: 26f87_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 9.5/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.23 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_3_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_0_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_5_25daa8673b2c24aecf80a7fc79df4e93, 0.0/6.0 CPU_group_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_4_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_2_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_1_25daa8673b2c24aecf80a7fc79df4e93)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_onelane_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_26f87_00000 | RUNNING  | 141.225.10.229:307 |     94 |          2551.81 | 1410000 |  -2292.4 |               2299.5 |             -8059.55 |               3000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=307)\u001b[0m 2022-10-08 11:49:04,948\tWARNING ppo.py:242 -- The magnitude of your environment rewards are more than 229.0x the scale of `vf_clip_param`. This means that it will take more than 229.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m ring length: 270\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m v_max: 5.6143732387852054\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m ring length: 268\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m v_max: 5.5194978538368025\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m ring length: 225\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m v_max: 3.4756971311168834\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m ring length: 253\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m v_max: 4.807278529691987\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m ring length: 224\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m v_max: 3.4281086136538996\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m Could not connect to TraCI server at localhost:36289 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m Could not connect to TraCI server at localhost:34795 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m Could not connect to TraCI server at localhost:34447 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m Could not connect to TraCI server at localhost:54035 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m Could not connect to TraCI server at localhost:36289 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 3 seconds\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m Could not connect to TraCI server at localhost:34795 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 3 seconds\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m Could not connect to TraCI server at localhost:34447 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 3 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m Could not connect to TraCI server at localhost:54035 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 3 seconds\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_26f87_00000:\n",
      "  agent_timesteps_total: 1425000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-08_11-49-35\n",
      "  done: false\n",
      "  episode_len_mean: 3000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2299.497932707432\n",
      "  episode_reward_mean: -2284.749127440857\n",
      "  episode_reward_min: -8059.548538429305\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 475\n",
      "  experiment_id: edcbe001b14143f4bf76a6a2914a7bd0\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2886507511138916\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.1247962713241577\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.033538393676280975\n",
      "          model: {}\n",
      "          policy_loss: 0.0132094481959939\n",
      "          total_loss: 3493.22119140625\n",
      "          vf_explained_var: -1.2226593959496768e-08\n",
      "          vf_loss: 3493.197998046875\n",
      "    num_agent_steps_sampled: 1425000\n",
      "    num_agent_steps_trained: 1425000\n",
      "    num_steps_sampled: 1425000\n",
      "    num_steps_trained: 1425000\n",
      "  iterations_since_restore: 95\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.632558139534886\n",
      "    ram_util_percent: 30.393023255813954\n",
      "  pid: 307\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06566524895175077\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 3.4814853606959195\n",
      "    mean_inference_ms: 0.482547603530054\n",
      "    mean_raw_obs_processing_ms: 3.802836045942125\n",
      "  time_since_restore: 2581.912500143051\n",
      "  time_this_iter_s: 30.100643157958984\n",
      "  time_total_s: 2581.912500143051\n",
      "  timers:\n",
      "    learn_throughput: 6085.19\n",
      "    learn_time_ms: 2465.001\n",
      "    load_throughput: 8242873.988\n",
      "    load_time_ms: 1.82\n",
      "    sample_throughput: 612.39\n",
      "    sample_time_ms: 24494.212\n",
      "    update_time_ms: 1.702\n",
      "  timestamp: 1665247775\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1425000\n",
      "  training_iteration: 95\n",
      "  trial_id: 26f87_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=307)\u001b[0m 2022-10-08 11:49:35,126\tWARNING ppo.py:242 -- The magnitude of your environment rewards are more than 228.0x the scale of `vf_clip_param`. This means that it will take more than 228.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "== Status ==\n",
      "Memory usage on this node: 9.5/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.23 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_0_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_2_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_1_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_4_25daa8673b2c24aecf80a7fc79df4e93, 0.0/6.0 CPU_group_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_3_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_5_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 accelerator_type:G)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_onelane_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_26f87_00000 | RUNNING  | 141.225.10.229:307 |     95 |          2581.91 | 1425000 | -2284.75 |               2299.5 |             -8059.55 |               3000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m ring length: 243\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m v_max: 4.331956438196479\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m ring length: 242\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m v_max: 4.2844067680020546\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m ring length: 252\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m v_max: 4.759762049824181\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m ring length: 246\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m v_max: 4.474587708995648\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m ring length: 236\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m v_max: 3.999052674951912\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 1 seconds\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_26f87_00000:\n",
      "  agent_timesteps_total: 1440000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-08_11-50-01\n",
      "  done: false\n",
      "  episode_len_mean: 3000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2299.497932707432\n",
      "  episode_reward_mean: -2334.0966712847353\n",
      "  episode_reward_min: -8098.169144697511\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 480\n",
      "  experiment_id: edcbe001b14143f4bf76a6a2914a7bd0\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2886507511138916\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.845853328704834\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.022962773218750954\n",
      "          model: {}\n",
      "          policy_loss: 0.011777202598750591\n",
      "          total_loss: 3591.561767578125\n",
      "          vf_explained_var: -8.151062935723985e-09\n",
      "          vf_loss: 3591.543212890625\n",
      "    num_agent_steps_sampled: 1440000\n",
      "    num_agent_steps_trained: 1440000\n",
      "    num_steps_sampled: 1440000\n",
      "    num_steps_trained: 1440000\n",
      "  iterations_since_restore: 96\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 23.555263157894736\n",
      "    ram_util_percent: 30.52894736842106\n",
      "  pid: 307\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06566257154826763\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 3.480871469076808\n",
      "    mean_inference_ms: 0.4825368744658779\n",
      "    mean_raw_obs_processing_ms: 3.805002183452022\n",
      "  time_since_restore: 2608.379366159439\n",
      "  time_this_iter_s: 26.46686601638794\n",
      "  time_total_s: 2608.379366159439\n",
      "  timers:\n",
      "    learn_throughput: 6116.951\n",
      "    learn_time_ms: 2452.202\n",
      "    load_throughput: 8244602.28\n",
      "    load_time_ms: 1.819\n",
      "    sample_throughput: 610.302\n",
      "    sample_time_ms: 24577.984\n",
      "    update_time_ms: 1.537\n",
      "  timestamp: 1665247801\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1440000\n",
      "  training_iteration: 96\n",
      "  trial_id: 26f87_00000\n",
      "  \n",
      "\u001b[2m\u001b[36m(pid=307)\u001b[0m 2022-10-08 11:50:01,681\tWARNING ppo.py:242 -- The magnitude of your environment rewards are more than 233.0x the scale of `vf_clip_param`. This means that it will take more than 233.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "== Status ==\n",
      "Memory usage on this node: 9.6/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.23 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_2_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_4_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_3_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_5_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_1_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_0_25daa8673b2c24aecf80a7fc79df4e93, 0.0/6.0 CPU_group_25daa8673b2c24aecf80a7fc79df4e93)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_onelane_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_26f87_00000 | RUNNING  | 141.225.10.229:307 |     96 |          2608.38 | 1440000 |  -2334.1 |               2299.5 |             -8098.17 |               3000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m ring length: 233\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m v_max: 3.856343201216534\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m ring length: 261\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m v_max: 5.187261846097525\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m ring length: 229\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m v_max: 3.666034803266416\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m ring length: 225\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m v_max: 3.4756971311168834\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m ring length: 261\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m v_max: 5.187261846097525\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 1 seconds\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_26f87_00000:\n",
      "  agent_timesteps_total: 1455000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-08_11-50-27\n",
      "  done: false\n",
      "  episode_len_mean: 3000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2299.497932707432\n",
      "  episode_reward_mean: -2406.5079024794672\n",
      "  episode_reward_min: -8098.169144697511\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 485\n",
      "  experiment_id: edcbe001b14143f4bf76a6a2914a7bd0\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2886507511138916\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.9013785719871521\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013860725797712803\n",
      "          model: {}\n",
      "          policy_loss: 0.0021538890432566404\n",
      "          total_loss: 3315.369873046875\n",
      "          vf_explained_var: 4.0755314678619925e-09\n",
      "          vf_loss: 3315.36328125\n",
      "    num_agent_steps_sampled: 1455000\n",
      "    num_agent_steps_trained: 1455000\n",
      "    num_steps_sampled: 1455000\n",
      "    num_steps_trained: 1455000\n",
      "  iterations_since_restore: 97\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 23.637837837837836\n",
      "    ram_util_percent: 30.564864864864866\n",
      "  pid: 307\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06566033692053631\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 3.4802190358875866\n",
      "    mean_inference_ms: 0.48252803981498227\n",
      "    mean_raw_obs_processing_ms: 3.8073081518010974\n",
      "  time_since_restore: 2634.3775947093964\n",
      "  time_this_iter_s: 25.998228549957275\n",
      "  time_total_s: 2634.3775947093964\n",
      "  timers:\n",
      "    learn_throughput: 6315.118\n",
      "    learn_time_ms: 2375.253\n",
      "    load_throughput: 8296133.762\n",
      "    load_time_ms: 1.808\n",
      "    sample_throughput: 608.06\n",
      "    sample_time_ms: 24668.633\n",
      "    update_time_ms: 1.539\n",
      "  timestamp: 1665247827\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1455000\n",
      "  training_iteration: 97\n",
      "  trial_id: 26f87_00000\n",
      "  \n",
      "\u001b[2m\u001b[36m(pid=307)\u001b[0m 2022-10-08 11:50:27,789\tWARNING ppo.py:242 -- The magnitude of your environment rewards are more than 241.0x the scale of `vf_clip_param`. This means that it will take more than 241.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "== Status ==\n",
      "Memory usage on this node: 9.5/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.23 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_2_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_1_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_0_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_3_25daa8673b2c24aecf80a7fc79df4e93, 0.0/6.0 CPU_group_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_5_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_4_25daa8673b2c24aecf80a7fc79df4e93)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_onelane_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_26f87_00000 | RUNNING  | 141.225.10.229:307 |     97 |          2634.38 | 1455000 | -2406.51 |               2299.5 |             -8098.17 |               3000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m ring length: 223\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m v_max: 3.3805185755587788\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m ring length: 256\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m v_max: 4.949804327743507\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m ring length: 229\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m v_max: 3.666034803266416\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m ring length: 260\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m v_max: 5.139779427502188\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m ring length: 249\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m v_max: 4.617190445131122\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m Could not connect to TraCI server at localhost:34795 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m Could not connect to TraCI server at localhost:34447 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m Could not connect to TraCI server at localhost:54035 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m Could not connect to TraCI server at localhost:36289 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m Could not connect to TraCI server at localhost:34795 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 3 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m Could not connect to TraCI server at localhost:36289 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 3 seconds\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m Could not connect to TraCI server at localhost:34447 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 3 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m Could not connect to TraCI server at localhost:54035 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 3 seconds\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_26f87_00000:\n",
      "  agent_timesteps_total: 1470000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-08_11-50-58\n",
      "  done: false\n",
      "  episode_len_mean: 3000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2299.497932707432\n",
      "  episode_reward_mean: -2206.156435469608\n",
      "  episode_reward_min: -8098.169144697511\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 490\n",
      "  experiment_id: edcbe001b14143f4bf76a6a2914a7bd0\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2886507511138916\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: -1.0879443883895874\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.027066823095083237\n",
      "          model: {}\n",
      "          policy_loss: 0.010124475695192814\n",
      "          total_loss: 2753.283203125\n",
      "          vf_explained_var: -9.16994569166718e-09\n",
      "          vf_loss: 2753.265380859375\n",
      "    num_agent_steps_sampled: 1470000\n",
      "    num_agent_steps_trained: 1470000\n",
      "    num_steps_sampled: 1470000\n",
      "    num_steps_trained: 1470000\n",
      "  iterations_since_restore: 98\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.623255813953488\n",
      "    ram_util_percent: 30.416279069767445\n",
      "  pid: 307\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06565888071345855\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 3.479626704059647\n",
      "    mean_inference_ms: 0.4825249068662537\n",
      "    mean_raw_obs_processing_ms: 3.810469247958963\n",
      "  time_since_restore: 2664.7474615573883\n",
      "  time_this_iter_s: 30.369866847991943\n",
      "  time_total_s: 2664.7474615573883\n",
      "  timers:\n",
      "    learn_throughput: 6381.748\n",
      "    learn_time_ms: 2350.453\n",
      "    load_throughput: 8844015.857\n",
      "    load_time_ms: 1.696\n",
      "    sample_throughput: 595.821\n",
      "    sample_time_ms: 25175.357\n",
      "    update_time_ms: 1.501\n",
      "  timestamp: 1665247858\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1470000\n",
      "  training_iteration: 98\n",
      "  trial_id: 26f87_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=307)\u001b[0m 2022-10-08 11:50:58,234\tWARNING ppo.py:242 -- The magnitude of your environment rewards are more than 221.0x the scale of `vf_clip_param`. This means that it will take more than 221.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "== Status ==\n",
      "Memory usage on this node: 9.5/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.23 GiB heap, 0.0/14.65 GiB objects (0.0/6.0 CPU_group_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_0_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_4_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_5_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_3_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_1_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_2_25daa8673b2c24aecf80a7fc79df4e93)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_onelane_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_26f87_00000 | RUNNING  | 141.225.10.229:307 |     98 |          2664.75 | 1470000 | -2206.16 |               2299.5 |             -8098.17 |               3000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m ring length: 240\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m v_max: 4.189299083856172\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m ring length: 258\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m v_max: 5.044800727535787\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m ring length: 221\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m v_max: 3.285334164169417\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m ring length: 225\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m v_max: 3.4756971311168834\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m ring length: 264\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m v_max: 5.329679917416892\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 1 seconds\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_26f87_00000:\n",
      "  agent_timesteps_total: 1485000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-08_11-51-23\n",
      "  done: false\n",
      "  episode_len_mean: 3000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2299.497932707432\n",
      "  episode_reward_mean: -2119.642309816424\n",
      "  episode_reward_min: -8098.169144697511\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 495\n",
      "  experiment_id: edcbe001b14143f4bf76a6a2914a7bd0\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2886507511138916\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: -1.077091932296753\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.018547195941209793\n",
      "          model: {}\n",
      "          policy_loss: 0.007992634549736977\n",
      "          total_loss: 2715.85009765625\n",
      "          vf_explained_var: -3.056648489874192e-09\n",
      "          vf_loss: 2715.83642578125\n",
      "    num_agent_steps_sampled: 1485000\n",
      "    num_agent_steps_trained: 1485000\n",
      "    num_steps_sampled: 1485000\n",
      "    num_steps_trained: 1485000\n",
      "  iterations_since_restore: 99\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 24.078378378378382\n",
      "    ram_util_percent: 30.45945945945946\n",
      "  pid: 307\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06565774367679876\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 3.4790900384650865\n",
      "    mean_inference_ms: 0.48252555754684023\n",
      "    mean_raw_obs_processing_ms: 3.8125952791562456\n",
      "  time_since_restore: 2690.10622048378\n",
      "  time_this_iter_s: 25.3587589263916\n",
      "  time_total_s: 2690.10622048378\n",
      "  timers:\n",
      "    learn_throughput: 6431.473\n",
      "    learn_time_ms: 2332.28\n",
      "    load_throughput: 8685897.312\n",
      "    load_time_ms: 1.727\n",
      "    sample_throughput: 596.574\n",
      "    sample_time_ms: 25143.582\n",
      "    update_time_ms: 1.495\n",
      "  timestamp: 1665247883\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1485000\n",
      "  training_iteration: 99\n",
      "  trial_id: 26f87_00000\n",
      "  \n",
      "\u001b[2m\u001b[36m(pid=307)\u001b[0m 2022-10-08 11:51:23,681\tWARNING ppo.py:242 -- The magnitude of your environment rewards are more than 212.0x the scale of `vf_clip_param`. This means that it will take more than 212.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "== Status ==\n",
      "Memory usage on this node: 9.5/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.23 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_1_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_4_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_2_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_5_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_0_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_3_25daa8673b2c24aecf80a7fc79df4e93, 0.0/6.0 CPU_group_25daa8673b2c24aecf80a7fc79df4e93)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_onelane_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_26f87_00000 | RUNNING  | 141.225.10.229:307 |     99 |          2690.11 | 1485000 | -2119.64 |               2299.5 |             -8098.17 |               3000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m ring length: 227\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m v_max: 3.570869368805752\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m ring length: 232\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m v_max: 3.8087690783250063\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m ring length: 233\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m v_max: 3.856343201216534\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m ring length: 243\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m v_max: 4.331956438196479\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m ring length: 270\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m v_max: 5.6143732387852054\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 1 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 1 seconds\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_26f87_00000:\n",
      "  agent_timesteps_total: 1500000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-08_11-51-49\n",
      "  done: false\n",
      "  episode_len_mean: 3000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2299.497932707432\n",
      "  episode_reward_mean: -2044.122912103021\n",
      "  episode_reward_min: -8098.169144697511\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 500\n",
      "  experiment_id: edcbe001b14143f4bf76a6a2914a7bd0\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2886507511138916\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.8770887851715088\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.029206251725554466\n",
      "          model: {}\n",
      "          policy_loss: 0.012474428862333298\n",
      "          total_loss: 3423.184326171875\n",
      "          vf_explained_var: 5.094414334827491e-10\n",
      "          vf_loss: 3423.163818359375\n",
      "    num_agent_steps_sampled: 1500000\n",
      "    num_agent_steps_trained: 1500000\n",
      "    num_steps_sampled: 1500000\n",
      "    num_steps_trained: 1500000\n",
      "  iterations_since_restore: 100\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 24.71111111111111\n",
      "    ram_util_percent: 30.46111111111111\n",
      "  pid: 307\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06565611549251157\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 3.4785530702501757\n",
      "    mean_inference_ms: 0.4825262727548622\n",
      "    mean_raw_obs_processing_ms: 3.8134919346309575\n",
      "  time_since_restore: 2715.4189195632935\n",
      "  time_this_iter_s: 25.31269907951355\n",
      "  time_total_s: 2715.4189195632935\n",
      "  timers:\n",
      "    learn_throughput: 6474.951\n",
      "    learn_time_ms: 2316.62\n",
      "    load_throughput: 8740682.699\n",
      "    load_time_ms: 1.716\n",
      "    sample_throughput: 595.625\n",
      "    sample_time_ms: 25183.648\n",
      "    update_time_ms: 1.516\n",
      "  timestamp: 1665247909\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1500000\n",
      "  training_iteration: 100\n",
      "  trial_id: 26f87_00000\n",
      "  \n",
      "\u001b[2m\u001b[36m(pid=307)\u001b[0m 2022-10-08 11:51:49,068\tWARNING ppo.py:242 -- The magnitude of your environment rewards are more than 204.0x the scale of `vf_clip_param`. This means that it will take more than 204.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "== Status ==\n",
      "Memory usage on this node: 9.5/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.23 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_3_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_4_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_5_25daa8673b2c24aecf80a7fc79df4e93, 0.0/6.0 CPU_group_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_1_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_0_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_2_25daa8673b2c24aecf80a7fc79df4e93)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_onelane_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_26f87_00000 | RUNNING  | 141.225.10.229:307 |    100 |          2715.42 | 1500000 | -2044.12 |               2299.5 |             -8098.17 |               3000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m ring length: 247\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m v_max: 4.522125250095652\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m ring length: 241\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m v_max: 4.236854288051661\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m ring length: 242\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m v_max: 4.2844067680020546\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m ring length: 265\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m v_max: 5.37714246265477\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m ring length: 266\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m v_max: 5.42459972166245\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 1 seconds\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_26f87_00000:\n",
      "  agent_timesteps_total: 1515000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-08_11-52-14\n",
      "  done: false\n",
      "  episode_len_mean: 3000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2299.497932707432\n",
      "  episode_reward_mean: -2135.6271557639757\n",
      "  episode_reward_min: -8098.169144697511\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 505\n",
      "  experiment_id: edcbe001b14143f4bf76a6a2914a7bd0\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2886507511138916\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 2.4691519737243652\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.007646686397492886\n",
      "          model: {}\n",
      "          policy_loss: 0.001995906000956893\n",
      "          total_loss: 3043.877685546875\n",
      "          vf_explained_var: 1.0188828669654981e-09\n",
      "          vf_loss: 3043.873291015625\n",
      "    num_agent_steps_sampled: 1515000\n",
      "    num_agent_steps_trained: 1515000\n",
      "    num_steps_sampled: 1515000\n",
      "    num_steps_trained: 1515000\n",
      "  iterations_since_restore: 101\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 24.237837837837837\n",
      "    ram_util_percent: 30.470270270270273\n",
      "  pid: 307\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06565450254834843\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 3.4779997702079597\n",
      "    mean_inference_ms: 0.4825273254905191\n",
      "    mean_raw_obs_processing_ms: 3.8133176090358365\n",
      "  time_since_restore: 2741.224457502365\n",
      "  time_this_iter_s: 25.805537939071655\n",
      "  time_total_s: 2741.224457502365\n",
      "  timers:\n",
      "    learn_throughput: 6449.839\n",
      "    learn_time_ms: 2325.64\n",
      "    load_throughput: 8958487.235\n",
      "    load_time_ms: 1.674\n",
      "    sample_throughput: 623.111\n",
      "    sample_time_ms: 24072.742\n",
      "    update_time_ms: 1.53\n",
      "  timestamp: 1665247934\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1515000\n",
      "  training_iteration: 101\n",
      "  trial_id: 26f87_00000\n",
      "  \n",
      "\u001b[2m\u001b[36m(pid=307)\u001b[0m 2022-10-08 11:52:14,983\tWARNING ppo.py:242 -- The magnitude of your environment rewards are more than 214.0x the scale of `vf_clip_param`. This means that it will take more than 214.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "== Status ==\n",
      "Memory usage on this node: 9.5/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.23 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_4_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_1_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_5_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_3_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_0_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_2_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 accelerator_type:G, 0.0/6.0 CPU_group_25daa8673b2c24aecf80a7fc79df4e93)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_onelane_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_26f87_00000 | RUNNING  | 141.225.10.229:307 |    101 |          2741.22 | 1515000 | -2135.63 |               2299.5 |             -8098.17 |               3000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m ring length: 232\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m v_max: 3.8087690783250063\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m ring length: 220\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m v_max: 3.2377399006821497\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m ring length: 242\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m v_max: 4.2844067680020546\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m ring length: 261\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m v_max: 5.187261846097525\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m ring length: 237\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m v_max: 4.04661795519353\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 1 seconds\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_26f87_00000:\n",
      "  agent_timesteps_total: 1530000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-08_11-52-40\n",
      "  done: false\n",
      "  episode_len_mean: 3000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2776.093748767079\n",
      "  episode_reward_mean: -2196.3880287761594\n",
      "  episode_reward_min: -8098.169144697511\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 510\n",
      "  experiment_id: edcbe001b14143f4bf76a6a2914a7bd0\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.1443253755569458\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.05382072925567627\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.02295498736202717\n",
      "          model: {}\n",
      "          policy_loss: 0.007689173799008131\n",
      "          total_loss: 3172.052734375\n",
      "          vf_explained_var: -1.5792684493476372e-08\n",
      "          vf_loss: 3172.041748046875\n",
      "    num_agent_steps_sampled: 1530000\n",
      "    num_agent_steps_trained: 1530000\n",
      "    num_steps_sampled: 1530000\n",
      "    num_steps_trained: 1530000\n",
      "  iterations_since_restore: 102\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 24.597222222222218\n",
      "    ram_util_percent: 30.55833333333333\n",
      "  pid: 307\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06565370346549167\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 3.4775203242458197\n",
      "    mean_inference_ms: 0.4825329231109293\n",
      "    mean_raw_obs_processing_ms: 3.8119453117267854\n",
      "  time_since_restore: 2766.8327326774597\n",
      "  time_this_iter_s: 25.608275175094604\n",
      "  time_total_s: 2766.8327326774597\n",
      "  timers:\n",
      "    learn_throughput: 6428.218\n",
      "    learn_time_ms: 2333.462\n",
      "    load_throughput: 8945749.264\n",
      "    load_time_ms: 1.677\n",
      "    sample_throughput: 620.552\n",
      "    sample_time_ms: 24172.023\n",
      "    update_time_ms: 1.574\n",
      "  timestamp: 1665247960\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1530000\n",
      "  training_iteration: 102\n",
      "  trial_id: 26f87_00000\n",
      "  \n",
      "\u001b[2m\u001b[36m(pid=307)\u001b[0m 2022-10-08 11:52:40,687\tWARNING ppo.py:242 -- The magnitude of your environment rewards are more than 220.0x the scale of `vf_clip_param`. This means that it will take more than 220.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "== Status ==\n",
      "Memory usage on this node: 9.5/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.23 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_3_25daa8673b2c24aecf80a7fc79df4e93, 0.0/6.0 CPU_group_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_1_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_2_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_4_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_5_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_0_25daa8673b2c24aecf80a7fc79df4e93)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_onelane_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_26f87_00000 | RUNNING  | 141.225.10.229:307 |    102 |          2766.83 | 1530000 | -2196.39 |              2776.09 |             -8098.17 |               3000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m ring length: 232\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m v_max: 3.8087690783250063\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m ring length: 231\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m v_max: 3.761192925277308\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m ring length: 235\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m v_max: 3.9514850725282584\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m ring length: 267\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m v_max: 5.472051563171826\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m ring length: 236\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m v_max: 3.999052674951912\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 1 seconds\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_26f87_00000:\n",
      "  agent_timesteps_total: 1545000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-08_11-53-06\n",
      "  done: false\n",
      "  episode_len_mean: 3000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2776.093748767079\n",
      "  episode_reward_mean: -2099.1870619867077\n",
      "  episode_reward_min: -8098.169144697511\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 515\n",
      "  experiment_id: edcbe001b14143f4bf76a6a2914a7bd0\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.1443253755569458\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: -1.6053440570831299\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.02700924500823021\n",
      "          model: {}\n",
      "          policy_loss: 0.0033592230174690485\n",
      "          total_loss: 2437.7412109375\n",
      "          vf_explained_var: -3.5660898678457897e-09\n",
      "          vf_loss: 2437.734130859375\n",
      "    num_agent_steps_sampled: 1545000\n",
      "    num_agent_steps_trained: 1545000\n",
      "    num_steps_sampled: 1545000\n",
      "    num_steps_trained: 1545000\n",
      "  iterations_since_restore: 103\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 24.28108108108108\n",
      "    ram_util_percent: 30.54864864864864\n",
      "  pid: 307\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06565288456557458\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 3.477101337700536\n",
      "    mean_inference_ms: 0.48254013369823295\n",
      "    mean_raw_obs_processing_ms: 3.8106595837296204\n",
      "  time_since_restore: 2792.320882797241\n",
      "  time_this_iter_s: 25.488150119781494\n",
      "  time_total_s: 2792.320882797241\n",
      "  timers:\n",
      "    learn_throughput: 6572.14\n",
      "    learn_time_ms: 2282.362\n",
      "    load_throughput: 9805273.985\n",
      "    load_time_ms: 1.53\n",
      "    sample_throughput: 619.0\n",
      "    sample_time_ms: 24232.629\n",
      "    update_time_ms: 1.637\n",
      "  timestamp: 1665247986\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1545000\n",
      "  training_iteration: 103\n",
      "  trial_id: 26f87_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=307)\u001b[0m 2022-10-08 11:53:06,252\tWARNING ppo.py:242 -- The magnitude of your environment rewards are more than 210.0x the scale of `vf_clip_param`. This means that it will take more than 210.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "== Status ==\n",
      "Memory usage on this node: 9.5/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.23 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_0_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_4_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_3_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_1_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_2_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_5_25daa8673b2c24aecf80a7fc79df4e93, 0.0/6.0 CPU_group_25daa8673b2c24aecf80a7fc79df4e93)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_onelane_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_26f87_00000 | RUNNING  | 141.225.10.229:307 |    103 |          2792.32 | 1545000 | -2099.19 |              2776.09 |             -8098.17 |               3000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m ring length: 267\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m v_max: 5.472051563171826\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m ring length: 258\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m v_max: 5.044800727535787\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m ring length: 269\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m v_max: 5.566938458220921\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m ring length: 244\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m v_max: 4.379503211387676\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m ring length: 260\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m v_max: 5.139779427502188\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 1 seconds\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_26f87_00000:\n",
      "  agent_timesteps_total: 1560000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-08_11-53-31\n",
      "  done: false\n",
      "  episode_len_mean: 3000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2776.093748767079\n",
      "  episode_reward_mean: -2104.0479402351784\n",
      "  episode_reward_min: -8098.169144697511\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 520\n",
      "  experiment_id: edcbe001b14143f4bf76a6a2914a7bd0\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.1443253755569458\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.0331465005874634\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.11125216633081436\n",
      "          model: {}\n",
      "          policy_loss: 0.017396410927176476\n",
      "          total_loss: 3077.451904296875\n",
      "          vf_explained_var: -9.679387069638778e-09\n",
      "          vf_loss: 3077.418212890625\n",
      "    num_agent_steps_sampled: 1560000\n",
      "    num_agent_steps_trained: 1560000\n",
      "    num_steps_sampled: 1560000\n",
      "    num_steps_trained: 1560000\n",
      "  iterations_since_restore: 104\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 24.65\n",
      "    ram_util_percent: 30.55555555555555\n",
      "  pid: 307\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06565215276124221\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 3.476690863942105\n",
      "    mean_inference_ms: 0.4825499252832584\n",
      "    mean_raw_obs_processing_ms: 3.809479745217505\n",
      "  time_since_restore: 2817.394424676895\n",
      "  time_this_iter_s: 25.07354187965393\n",
      "  time_total_s: 2817.394424676895\n",
      "  timers:\n",
      "    learn_throughput: 6522.802\n",
      "    learn_time_ms: 2299.625\n",
      "    load_throughput: 9860443.539\n",
      "    load_time_ms: 1.521\n",
      "    sample_throughput: 618.615\n",
      "    sample_time_ms: 24247.725\n",
      "    update_time_ms: 1.642\n",
      "  timestamp: 1665248011\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1560000\n",
      "  training_iteration: 104\n",
      "  trial_id: 26f87_00000\n",
      "  \n",
      "\u001b[2m\u001b[36m(pid=307)\u001b[0m 2022-10-08 11:53:31,417\tWARNING ppo.py:242 -- The magnitude of your environment rewards are more than 210.0x the scale of `vf_clip_param`. This means that it will take more than 210.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "== Status ==\n",
      "Memory usage on this node: 9.5/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.23 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_1_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_3_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_5_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_2_25daa8673b2c24aecf80a7fc79df4e93, 0.0/6.0 CPU_group_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_0_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_4_25daa8673b2c24aecf80a7fc79df4e93)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_onelane_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_26f87_00000 | RUNNING  | 141.225.10.229:307 |    104 |          2817.39 | 1560000 | -2104.05 |              2776.09 |             -8098.17 |               3000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m ring length: 248\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m v_max: 4.569659527528101\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m ring length: 240\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m v_max: 4.189299083856172\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m ring length: 254\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m v_max: 4.854791141191876\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m ring length: 262\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m v_max: 5.234739483008334\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m ring length: 267\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m v_max: 5.472051563171826\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 1 seconds\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_26f87_00000:\n",
      "  agent_timesteps_total: 1575000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-08_11-53-56\n",
      "  done: false\n",
      "  episode_len_mean: 3000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2776.093748767079\n",
      "  episode_reward_mean: -2094.962983886084\n",
      "  episode_reward_min: -8098.169144697511\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 525\n",
      "  experiment_id: edcbe001b14143f4bf76a6a2914a7bd0\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2164880633354187\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.5232368111610413\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.05643728747963905\n",
      "          model: {}\n",
      "          policy_loss: 0.0191804189234972\n",
      "          total_loss: 3374.81005859375\n",
      "          vf_explained_var: -9.16994569166718e-09\n",
      "          vf_loss: 3374.77880859375\n",
      "    num_agent_steps_sampled: 1575000\n",
      "    num_agent_steps_trained: 1575000\n",
      "    num_steps_sampled: 1575000\n",
      "    num_steps_trained: 1575000\n",
      "  iterations_since_restore: 105\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 24.783333333333335\n",
      "    ram_util_percent: 30.561111111111106\n",
      "  pid: 307\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06565130588926937\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 3.476298817144916\n",
      "    mean_inference_ms: 0.4825602755505225\n",
      "    mean_raw_obs_processing_ms: 3.8070631476464722\n",
      "  time_since_restore: 2842.6551656723022\n",
      "  time_this_iter_s: 25.260740995407104\n",
      "  time_total_s: 2842.6551656723022\n",
      "  timers:\n",
      "    learn_throughput: 6528.25\n",
      "    learn_time_ms: 2297.706\n",
      "    load_throughput: 8739711.337\n",
      "    load_time_ms: 1.716\n",
      "    sample_throughput: 631.158\n",
      "    sample_time_ms: 23765.829\n",
      "    update_time_ms: 1.385\n",
      "  timestamp: 1665248036\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1575000\n",
      "  training_iteration: 105\n",
      "  trial_id: 26f87_00000\n",
      "  \n",
      "\u001b[2m\u001b[36m(pid=307)\u001b[0m 2022-10-08 11:53:56,772\tWARNING ppo.py:242 -- The magnitude of your environment rewards are more than 209.0x the scale of `vf_clip_param`. This means that it will take more than 209.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Memory usage on this node: 9.5/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.23 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_0_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_3_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_1_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_2_25daa8673b2c24aecf80a7fc79df4e93, 0.0/6.0 CPU_group_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_4_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_5_25daa8673b2c24aecf80a7fc79df4e93)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_onelane_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_26f87_00000 | RUNNING  | 141.225.10.229:307 |    105 |          2842.66 | 1575000 | -2094.96 |              2776.09 |             -8098.17 |               3000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m ring length: 251\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m v_max: 4.71224180704279\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m ring length: 266\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m v_max: 5.42459972166245\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m ring length: 234\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m v_max: 3.903915223349057\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m ring length: 246\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m v_max: 4.474587708995648\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m ring length: 224\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m v_max: 3.4281086136538996\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 1 seconds\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_26f87_00000:\n",
      "  agent_timesteps_total: 1590000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-08_11-54-23\n",
      "  done: false\n",
      "  episode_len_mean: 3000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2776.093748767079\n",
      "  episode_reward_mean: -2011.146422206972\n",
      "  episode_reward_min: -8098.169144697511\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 530\n",
      "  experiment_id: edcbe001b14143f4bf76a6a2914a7bd0\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.32473209500312805\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: -2.2890515327453613\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.03751737251877785\n",
      "          model: {}\n",
      "          policy_loss: 0.011115022003650665\n",
      "          total_loss: 2573.27294921875\n",
      "          vf_explained_var: -5.603855601776786e-09\n",
      "          vf_loss: 2573.249755859375\n",
      "    num_agent_steps_sampled: 1590000\n",
      "    num_agent_steps_trained: 1590000\n",
      "    num_steps_sampled: 1590000\n",
      "    num_steps_trained: 1590000\n",
      "  iterations_since_restore: 106\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 23.857894736842105\n",
      "    ram_util_percent: 30.560526315789467\n",
      "  pid: 307\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06565048303544087\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 3.475945707015561\n",
      "    mean_inference_ms: 0.48257377884311414\n",
      "    mean_raw_obs_processing_ms: 3.8046531888210926\n",
      "  time_since_restore: 2869.0049340724945\n",
      "  time_this_iter_s: 26.34976840019226\n",
      "  time_total_s: 2869.0049340724945\n",
      "  timers:\n",
      "    learn_throughput: 6376.818\n",
      "    learn_time_ms: 2352.27\n",
      "    load_throughput: 8595237.51\n",
      "    load_time_ms: 1.745\n",
      "    sample_throughput: 632.909\n",
      "    sample_time_ms: 23700.106\n",
      "    update_time_ms: 1.287\n",
      "  timestamp: 1665248063\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1590000\n",
      "  training_iteration: 106\n",
      "  trial_id: 26f87_00000\n",
      "  \n",
      "\u001b[2m\u001b[36m(pid=307)\u001b[0m 2022-10-08 11:54:23,222\tWARNING ppo.py:242 -- The magnitude of your environment rewards are more than 201.0x the scale of `vf_clip_param`. This means that it will take more than 201.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "== Status ==\n",
      "Memory usage on this node: 9.5/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.23 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_0_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_4_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_1_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_2_25daa8673b2c24aecf80a7fc79df4e93, 0.0/6.0 CPU_group_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_3_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_5_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 accelerator_type:G)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_onelane_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_26f87_00000 | RUNNING  | 141.225.10.229:307 |    106 |             2869 | 1590000 | -2011.15 |              2776.09 |             -8098.17 |               3000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m ring length: 242\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m v_max: 4.2844067680020546\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m ring length: 270\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m v_max: 5.6143732387852054\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m ring length: 248\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m v_max: 4.569659527528101\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m ring length: 261\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m v_max: 5.187261846097525\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m ring length: 252\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m v_max: 4.759762049824181\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m Could not connect to TraCI server at localhost:34795 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m Could not connect to TraCI server at localhost:36289 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 2 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m Could not connect to TraCI server at localhost:36289 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 3 seconds\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m Could not connect to TraCI server at localhost:34795 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 3 seconds\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_26f87_00000:\n",
      "  agent_timesteps_total: 1605000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-08_11-54-54\n",
      "  done: false\n",
      "  episode_len_mean: 3000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2776.093748767079\n",
      "  episode_reward_mean: -2071.666265168457\n",
      "  episode_reward_min: -8098.169144697511\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 535\n",
      "  experiment_id: edcbe001b14143f4bf76a6a2914a7bd0\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.32473209500312805\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.07838207483291626\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.04487745091319084\n",
      "          model: {}\n",
      "          policy_loss: 0.01949945092201233\n",
      "          total_loss: 3120.62451171875\n",
      "          vf_explained_var: -3.5660898678457897e-09\n",
      "          vf_loss: 3120.590576171875\n",
      "    num_agent_steps_sampled: 1605000\n",
      "    num_agent_steps_trained: 1605000\n",
      "    num_steps_sampled: 1605000\n",
      "    num_steps_trained: 1605000\n",
      "  iterations_since_restore: 107\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.96818181818182\n",
      "    ram_util_percent: 30.518181818181816\n",
      "  pid: 307\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0656495840833671\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 3.4755819703088093\n",
      "    mean_inference_ms: 0.4825886514976839\n",
      "    mean_raw_obs_processing_ms: 3.803196435065915\n",
      "  time_since_restore: 2899.834666967392\n",
      "  time_this_iter_s: 30.82973289489746\n",
      "  time_total_s: 2899.834666967392\n",
      "  timers:\n",
      "    learn_throughput: 6319.985\n",
      "    learn_time_ms: 2373.424\n",
      "    load_throughput: 8746880.214\n",
      "    load_time_ms: 1.715\n",
      "    sample_throughput: 620.814\n",
      "    sample_time_ms: 24161.823\n",
      "    update_time_ms: 1.39\n",
      "  timestamp: 1665248094\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1605000\n",
      "  training_iteration: 107\n",
      "  trial_id: 26f87_00000\n",
      "  \n",
      "\u001b[2m\u001b[36m(pid=307)\u001b[0m 2022-10-08 11:54:54,172\tWARNING ppo.py:242 -- The magnitude of your environment rewards are more than 207.0x the scale of `vf_clip_param`. This means that it will take more than 207.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "== Status ==\n",
      "Memory usage on this node: 9.6/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.23 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_5_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_2_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_4_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_3_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_1_25daa8673b2c24aecf80a7fc79df4e93, 0.0/6.0 CPU_group_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_0_25daa8673b2c24aecf80a7fc79df4e93)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_onelane_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_26f87_00000 | RUNNING  | 141.225.10.229:307 |    107 |          2899.83 | 1605000 | -2071.67 |              2776.09 |             -8098.17 |               3000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m ring length: 248\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m v_max: 4.569659527528101\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m ring length: 245\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m v_max: 4.427046998576354\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m ring length: 229\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m v_max: 3.666034803266416\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m ring length: 243\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m v_max: 4.331956438196479\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m ring length: 228\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m v_max: 3.618452967700356\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 1 seconds\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_26f87_00000:\n",
      "  agent_timesteps_total: 1620000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-08_11-55-19\n",
      "  done: false\n",
      "  episode_len_mean: 3000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2776.093748767079\n",
      "  episode_reward_mean: -2039.7851041178264\n",
      "  episode_reward_min: -8098.169144697511\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 540\n",
      "  experiment_id: edcbe001b14143f4bf76a6a2914a7bd0\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.48709815740585327\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.3563929796218872\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.031696874648332596\n",
      "          model: {}\n",
      "          policy_loss: 0.020659007132053375\n",
      "          total_loss: 3436.533447265625\n",
      "          vf_explained_var: 1.528324244937096e-09\n",
      "          vf_loss: 3436.497314453125\n",
      "    num_agent_steps_sampled: 1620000\n",
      "    num_agent_steps_trained: 1620000\n",
      "    num_steps_sampled: 1620000\n",
      "    num_steps_trained: 1620000\n",
      "  iterations_since_restore: 108\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 25.011111111111113\n",
      "    ram_util_percent: 30.850000000000005\n",
      "  pid: 307\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06564888263317673\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 3.4752226383745053\n",
      "    mean_inference_ms: 0.4826072931999751\n",
      "    mean_raw_obs_processing_ms: 3.801810505214403\n",
      "  time_since_restore: 2925.0452041625977\n",
      "  time_this_iter_s: 25.21053719520569\n",
      "  time_total_s: 2925.0452041625977\n",
      "  timers:\n",
      "    learn_throughput: 6334.874\n",
      "    learn_time_ms: 2367.845\n",
      "    load_throughput: 8917473.636\n",
      "    load_time_ms: 1.682\n",
      "    sample_throughput: 634.2\n",
      "    sample_time_ms: 23651.837\n",
      "    update_time_ms: 1.38\n",
      "  timestamp: 1665248119\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1620000\n",
      "  training_iteration: 108\n",
      "  trial_id: 26f87_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 9.6/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.23 GiB heap, 0.0/14.65 GiB objects (0.0/6.0 CPU_group_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_5_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_4_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_2_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_1_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_3_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_0_25daa8673b2c24aecf80a7fc79df4e93)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_onelane_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_26f87_00000 | RUNNING  | 141.225.10.229:307 |    108 |          2925.05 | 1620000 | -2039.79 |              2776.09 |             -8098.17 |               3000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=307)\u001b[0m 2022-10-08 11:55:19,465\tWARNING ppo.py:242 -- The magnitude of your environment rewards are more than 204.0x the scale of `vf_clip_param`. This means that it will take more than 204.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m ring length: 224\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m v_max: 3.4281086136538996\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m ring length: 262\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m v_max: 5.234739483008334\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m ring length: 258\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m v_max: 5.044800727535787\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m ring length: 227\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m v_max: 3.570869368805752\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m ring length: 235\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m v_max: 3.9514850725282584\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m Could not connect to TraCI server at localhost:34795 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m Could not connect to TraCI server at localhost:34447 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m Could not connect to TraCI server at localhost:57509 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m Could not connect to TraCI server at localhost:54035 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m Could not connect to TraCI server at localhost:36289 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m Could not connect to TraCI server at localhost:34795 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 3 seconds\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m Could not connect to TraCI server at localhost:34447 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 3 seconds\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m Could not connect to TraCI server at localhost:57509 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 3 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m Could not connect to TraCI server at localhost:54035 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 3 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m Could not connect to TraCI server at localhost:36289 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 3 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 1 seconds\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_26f87_00000:\n",
      "  agent_timesteps_total: 1635000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-08_11-55-50\n",
      "  done: false\n",
      "  episode_len_mean: 3000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2776.093748767079\n",
      "  episode_reward_mean: -2012.761816447191\n",
      "  episode_reward_min: -8098.169144697511\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 545\n",
      "  experiment_id: edcbe001b14143f4bf76a6a2914a7bd0\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.48709815740585327\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: -0.43078866600990295\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.00366612384095788\n",
      "          model: {}\n",
      "          policy_loss: 0.00017028441652655602\n",
      "          total_loss: 2851.950439453125\n",
      "          vf_explained_var: -5.094414223805188e-09\n",
      "          vf_loss: 2851.948486328125\n",
      "    num_agent_steps_sampled: 1635000\n",
      "    num_agent_steps_trained: 1635000\n",
      "    num_steps_sampled: 1635000\n",
      "    num_steps_trained: 1635000\n",
      "  iterations_since_restore: 109\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.236363636363635\n",
      "    ram_util_percent: 30.62499999999999\n",
      "  pid: 307\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06564785035879607\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 3.474852880678495\n",
      "    mean_inference_ms: 0.48262452929826827\n",
      "    mean_raw_obs_processing_ms: 3.801291524621564\n",
      "  time_since_restore: 2955.675479888916\n",
      "  time_this_iter_s: 30.63027572631836\n",
      "  time_total_s: 2955.675479888916\n",
      "  timers:\n",
      "    learn_throughput: 6344.627\n",
      "    learn_time_ms: 2364.205\n",
      "    load_throughput: 9054017.96\n",
      "    load_time_ms: 1.657\n",
      "    sample_throughput: 620.286\n",
      "    sample_time_ms: 24182.406\n",
      "    update_time_ms: 1.393\n",
      "  timestamp: 1665248150\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1635000\n",
      "  training_iteration: 109\n",
      "  trial_id: 26f87_00000\n",
      "  \n",
      "\u001b[2m\u001b[36m(pid=307)\u001b[0m 2022-10-08 11:55:50,157\tWARNING ppo.py:242 -- The magnitude of your environment rewards are more than 201.0x the scale of `vf_clip_param`. This means that it will take more than 201.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "== Status ==\n",
      "Memory usage on this node: 9.5/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.23 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_2_25daa8673b2c24aecf80a7fc79df4e93, 0.0/6.0 CPU_group_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_5_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_1_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_4_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_0_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_3_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 accelerator_type:G)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_onelane_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_26f87_00000 | RUNNING  | 141.225.10.229:307 |    109 |          2955.68 | 1635000 | -2012.76 |              2776.09 |             -8098.17 |               3000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m ring length: 244\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m v_max: 4.379503211387676\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m ring length: 263\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m v_max: 5.282212215151455\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m ring length: 231\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m v_max: 3.761192925277308\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m ring length: 262\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m v_max: 5.234739483008334\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m ring length: 248\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m v_max: 4.569659527528101\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 1 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m Could not connect to TraCI server at localhost:34795 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m Could not connect to TraCI server at localhost:34447 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m Could not connect to TraCI server at localhost:57509 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m Could not connect to TraCI server at localhost:54035 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m Could not connect to TraCI server at localhost:36289 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m Could not connect to TraCI server at localhost:34795 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 3 seconds\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m Could not connect to TraCI server at localhost:34447 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 3 seconds\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m Could not connect to TraCI server at localhost:57509 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 3 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m Could not connect to TraCI server at localhost:54035 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 3 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m Could not connect to TraCI server at localhost:36289 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 3 seconds\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_26f87_00000:\n",
      "  agent_timesteps_total: 1650000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-08_11-56-20\n",
      "  done: false\n",
      "  episode_len_mean: 3000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2776.093748767079\n",
      "  episode_reward_mean: -1869.5066966532725\n",
      "  episode_reward_min: -8098.169144697511\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 550\n",
      "  experiment_id: edcbe001b14143f4bf76a6a2914a7bd0\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.24354907870292664\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.434446096420288\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.032167598605155945\n",
      "          model: {}\n",
      "          policy_loss: 0.010984468273818493\n",
      "          total_loss: 3173.3115234375\n",
      "          vf_explained_var: -9.16994569166718e-09\n",
      "          vf_loss: 3173.292724609375\n",
      "    num_agent_steps_sampled: 1650000\n",
      "    num_agent_steps_trained: 1650000\n",
      "    num_steps_sampled: 1650000\n",
      "    num_steps_trained: 1650000\n",
      "  iterations_since_restore: 110\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.2046511627907\n",
      "    ram_util_percent: 30.520930232558133\n",
      "  pid: 307\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06564691569350291\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 3.4745114834413533\n",
      "    mean_inference_ms: 0.4826445148881577\n",
      "    mean_raw_obs_processing_ms: 3.801754342167966\n",
      "  time_since_restore: 2985.8812232017517\n",
      "  time_this_iter_s: 30.205743312835693\n",
      "  time_total_s: 2985.8812232017517\n",
      "  timers:\n",
      "    learn_throughput: 6453.446\n",
      "    learn_time_ms: 2324.34\n",
      "    load_throughput: 10009475.778\n",
      "    load_time_ms: 1.499\n",
      "    sample_throughput: 606.998\n",
      "    sample_time_ms: 24711.762\n",
      "    update_time_ms: 1.437\n",
      "  timestamp: 1665248180\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1650000\n",
      "  training_iteration: 110\n",
      "  trial_id: 26f87_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 9.5/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.23 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_2_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_0_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_1_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_5_25daa8673b2c24aecf80a7fc79df4e93, 0.0/6.0 CPU_group_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_4_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_3_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 accelerator_type:G)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_onelane_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_26f87_00000 | RUNNING  | 141.225.10.229:307 |    110 |          2985.88 | 1650000 | -1869.51 |              2776.09 |             -8098.17 |               3000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m ring length: 236\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m v_max: 3.999052674951912\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m ring length: 235\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m v_max: 3.9514850725282584\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m ring length: 251\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m v_max: 4.71224180704279\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m ring length: 270\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m v_max: 5.6143732387852054\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m ring length: 230\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m v_max: 3.7136148111012934\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 1 seconds\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_26f87_00000:\n",
      "  agent_timesteps_total: 1665000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-08_11-56-46\n",
      "  done: false\n",
      "  episode_len_mean: 3000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2776.093748767079\n",
      "  episode_reward_mean: -1868.6668392121585\n",
      "  episode_reward_min: -8098.169144697511\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 555\n",
      "  experiment_id: edcbe001b14143f4bf76a6a2914a7bd0\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.24354907870292664\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: -0.21718455851078033\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.03925231471657753\n",
      "          model: {}\n",
      "          policy_loss: 0.016694650053977966\n",
      "          total_loss: 3034.69482421875\n",
      "          vf_explained_var: 0.0\n",
      "          vf_loss: 3034.668212890625\n",
      "    num_agent_steps_sampled: 1665000\n",
      "    num_agent_steps_trained: 1665000\n",
      "    num_steps_sampled: 1665000\n",
      "    num_steps_trained: 1665000\n",
      "  iterations_since_restore: 111\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 23.813513513513513\n",
      "    ram_util_percent: 30.559459459459454\n",
      "  pid: 307\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06564566199194609\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 3.4741528425314523\n",
      "    mean_inference_ms: 0.4826671540736233\n",
      "    mean_raw_obs_processing_ms: 3.8003688263489273\n",
      "  time_since_restore: 3012.065018415451\n",
      "  time_this_iter_s: 26.18379521369934\n",
      "  time_total_s: 3012.065018415451\n",
      "  timers:\n",
      "    learn_throughput: 6585.23\n",
      "    learn_time_ms: 2277.825\n",
      "    load_throughput: 8819221.172\n",
      "    load_time_ms: 1.701\n",
      "    sample_throughput: 604.946\n",
      "    sample_time_ms: 24795.61\n",
      "    update_time_ms: 1.502\n",
      "  timestamp: 1665248206\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1665000\n",
      "  training_iteration: 111\n",
      "  trial_id: 26f87_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Memory usage on this node: 9.5/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.23 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_2_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_4_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_3_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_1_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_0_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_5_25daa8673b2c24aecf80a7fc79df4e93, 0.0/6.0 CPU_group_25daa8673b2c24aecf80a7fc79df4e93)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_onelane_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_26f87_00000 | RUNNING  | 141.225.10.229:307 |    111 |          3012.07 | 1665000 | -1868.67 |              2776.09 |             -8098.17 |               3000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m ring length: 258\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m v_max: 5.044800727535787\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m ring length: 235\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m v_max: 3.9514850725282584\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m ring length: 222\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m v_max: 3.3329270738617227\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m ring length: 224\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m v_max: 3.4281086136538996\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m ring length: 221\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m v_max: 3.285334164169417\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m Could not connect to TraCI server at localhost:34447 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m Could not connect to TraCI server at localhost:57509 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m Could not connect to TraCI server at localhost:54035 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m Could not connect to TraCI server at localhost:34795 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m Could not connect to TraCI server at localhost:36289 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m Could not connect to TraCI server at localhost:54035 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 3 seconds\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m Could not connect to TraCI server at localhost:34447 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 3 seconds\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m Could not connect to TraCI server at localhost:57509 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 3 seconds\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m Could not connect to TraCI server at localhost:34795 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 3 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m Could not connect to TraCI server at localhost:36289 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 3 seconds\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_26f87_00000:\n",
      "  agent_timesteps_total: 1680000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-08_11-57-16\n",
      "  done: false\n",
      "  episode_len_mean: 3000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2776.093748767079\n",
      "  episode_reward_mean: -1823.5522686712343\n",
      "  episode_reward_min: -8098.169144697511\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 560\n",
      "  experiment_id: edcbe001b14143f4bf76a6a2914a7bd0\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.24354907870292664\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.13041596114635468\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.027217721566557884\n",
      "          model: {}\n",
      "          policy_loss: 0.010697387158870697\n",
      "          total_loss: 3112.44287109375\n",
      "          vf_explained_var: -4.0755314678619925e-09\n",
      "          vf_loss: 3112.42529296875\n",
      "    num_agent_steps_sampled: 1680000\n",
      "    num_agent_steps_trained: 1680000\n",
      "    num_steps_sampled: 1680000\n",
      "    num_steps_trained: 1680000\n",
      "  iterations_since_restore: 112\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.168181818181814\n",
      "    ram_util_percent: 30.5340909090909\n",
      "  pid: 307\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06564462505067153\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 3.473829152989406\n",
      "    mean_inference_ms: 0.4826915603905253\n",
      "    mean_raw_obs_processing_ms: 3.799902682372753\n",
      "  time_since_restore: 3042.233295440674\n",
      "  time_this_iter_s: 30.16827702522278\n",
      "  time_total_s: 3042.233295440674\n",
      "  timers:\n",
      "    learn_throughput: 6636.772\n",
      "    learn_time_ms: 2260.135\n",
      "    load_throughput: 8802193.743\n",
      "    load_time_ms: 1.704\n",
      "    sample_throughput: 593.607\n",
      "    sample_time_ms: 25269.223\n",
      "    update_time_ms: 1.615\n",
      "  timestamp: 1665248236\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1680000\n",
      "  training_iteration: 112\n",
      "  trial_id: 26f87_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 9.5/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.23 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_0_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_5_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_2_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_4_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_1_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 accelerator_type:G, 0.0/6.0 CPU_group_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_3_25daa8673b2c24aecf80a7fc79df4e93)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_onelane_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_26f87_00000 | RUNNING  | 141.225.10.229:307 |    112 |          3042.23 | 1680000 | -1823.55 |              2776.09 |             -8098.17 |               3000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m ring length: 266\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m v_max: 5.42459972166245\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m ring length: 264\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m v_max: 5.329679917416892\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m ring length: 228\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m v_max: 3.618452967700356\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m ring length: 225\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m v_max: 3.4756971311168834\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m ring length: 223\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m v_max: 3.3805185755587788\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 1 seconds\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_26f87_00000:\n",
      "  agent_timesteps_total: 1695000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-08_11-57-43\n",
      "  done: false\n",
      "  episode_len_mean: 3000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2776.093748767079\n",
      "  episode_reward_mean: -1839.8849024818928\n",
      "  episode_reward_min: -8098.169144697511\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 565\n",
      "  experiment_id: edcbe001b14143f4bf76a6a2914a7bd0\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.24354907870292664\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 2.7621235847473145\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013078893534839153\n",
      "          model: {}\n",
      "          policy_loss: 0.006986022926867008\n",
      "          total_loss: 3448.93701171875\n",
      "          vf_explained_var: -5.603855601776786e-09\n",
      "          vf_loss: 3448.926513671875\n",
      "    num_agent_steps_sampled: 1695000\n",
      "    num_agent_steps_trained: 1695000\n",
      "    num_steps_sampled: 1695000\n",
      "    num_steps_trained: 1695000\n",
      "  iterations_since_restore: 113\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 23.349999999999998\n",
      "    ram_util_percent: 30.59210526315789\n",
      "  pid: 307\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06564401957137084\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 3.4735122869026145\n",
      "    mean_inference_ms: 0.48271938727609914\n",
      "    mean_raw_obs_processing_ms: 3.799718783486916\n",
      "  time_since_restore: 3068.787879228592\n",
      "  time_this_iter_s: 26.55458378791809\n",
      "  time_total_s: 3068.787879228592\n",
      "  timers:\n",
      "    learn_throughput: 6518.748\n",
      "    learn_time_ms: 2301.055\n",
      "    load_throughput: 9360753.448\n",
      "    load_time_ms: 1.602\n",
      "    sample_throughput: 592.065\n",
      "    sample_time_ms: 25335.059\n",
      "    update_time_ms: 1.622\n",
      "  timestamp: 1665248263\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1695000\n",
      "  training_iteration: 113\n",
      "  trial_id: 26f87_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 9.6/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.23 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_4_25daa8673b2c24aecf80a7fc79df4e93, 0.0/6.0 CPU_group_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_3_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_5_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_1_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_2_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_0_25daa8673b2c24aecf80a7fc79df4e93)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_onelane_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_26f87_00000 | RUNNING  | 141.225.10.229:307 |    113 |          3068.79 | 1695000 | -1839.88 |              2776.09 |             -8098.17 |               3000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m ring length: 247\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m v_max: 4.522125250095652\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m ring length: 252\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m v_max: 4.759762049824181\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m ring length: 267\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m v_max: 5.472051563171826\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m ring length: 266\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m v_max: 5.42459972166245\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m ring length: 264\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m v_max: 5.329679917416892\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_26f87_00000:\n",
      "  agent_timesteps_total: 1710000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-08_11-58-08\n",
      "  done: false\n",
      "  episode_len_mean: 3000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2776.093748767079\n",
      "  episode_reward_mean: -1870.270818581281\n",
      "  episode_reward_min: -8098.169144697511\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 570\n",
      "  experiment_id: edcbe001b14143f4bf76a6a2914a7bd0\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.24354907870292664\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 2.2542319297790527\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.019097229465842247\n",
      "          model: {}\n",
      "          policy_loss: 0.009684158489108086\n",
      "          total_loss: 3234.145751953125\n",
      "          vf_explained_var: -8.660504313695583e-09\n",
      "          vf_loss: 3234.13134765625\n",
      "    num_agent_steps_sampled: 1710000\n",
      "    num_agent_steps_trained: 1710000\n",
      "    num_steps_sampled: 1710000\n",
      "    num_steps_trained: 1710000\n",
      "  iterations_since_restore: 114\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 24.81142857142857\n",
      "    ram_util_percent: 30.660000000000004\n",
      "  pid: 307\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0656436212690026\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 3.473208161608868\n",
      "    mean_inference_ms: 0.482749495579016\n",
      "    mean_raw_obs_processing_ms: 3.799473750498541\n",
      "  time_since_restore: 3093.135819911957\n",
      "  time_this_iter_s: 24.347940683364868\n",
      "  time_total_s: 3093.135819911957\n",
      "  timers:\n",
      "    learn_throughput: 6511.704\n",
      "    learn_time_ms: 2303.545\n",
      "    load_throughput: 9288475.507\n",
      "    load_time_ms: 1.615\n",
      "    sample_throughput: 593.821\n",
      "    sample_time_ms: 25260.129\n",
      "    update_time_ms: 1.903\n",
      "  timestamp: 1665248288\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1710000\n",
      "  training_iteration: 114\n",
      "  trial_id: 26f87_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 9.6/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.23 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_2_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_0_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_4_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_1_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_3_25daa8673b2c24aecf80a7fc79df4e93, 0.0/6.0 CPU_group_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_5_25daa8673b2c24aecf80a7fc79df4e93)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_onelane_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_26f87_00000 | RUNNING  | 141.225.10.229:307 |    114 |          3093.14 | 1710000 | -1870.27 |              2776.09 |             -8098.17 |               3000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m ring length: 229\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m v_max: 3.666034803266416\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m ring length: 244\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m v_max: 4.379503211387676\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m ring length: 260\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m v_max: 5.139779427502188\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m ring length: 222\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m v_max: 3.3329270738617227\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m ring length: 270\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m v_max: 5.6143732387852054\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 1 seconds\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_26f87_00000:\n",
      "  agent_timesteps_total: 1725000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-08_11-58-33\n",
      "  done: false\n",
      "  episode_len_mean: 3000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2776.093748767079\n",
      "  episode_reward_mean: -1816.7485750005917\n",
      "  episode_reward_min: -8098.169144697511\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 575\n",
      "  experiment_id: edcbe001b14143f4bf76a6a2914a7bd0\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.24354907870292664\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: -0.32837238907814026\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0631071925163269\n",
      "          model: {}\n",
      "          policy_loss: 0.022640632465481758\n",
      "          total_loss: 3244.010498046875\n",
      "          vf_explained_var: -1.171715258152517e-08\n",
      "          vf_loss: 3243.972412109375\n",
      "    num_agent_steps_sampled: 1725000\n",
      "    num_agent_steps_trained: 1725000\n",
      "    num_steps_sampled: 1725000\n",
      "    num_steps_trained: 1725000\n",
      "  iterations_since_restore: 115\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 24.574285714285715\n",
      "    ram_util_percent: 30.64857142857143\n",
      "  pid: 307\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06564334348756472\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 3.4729278706046607\n",
      "    mean_inference_ms: 0.4827829422598686\n",
      "    mean_raw_obs_processing_ms: 3.7984307606334333\n",
      "  time_since_restore: 3118.076587200165\n",
      "  time_this_iter_s: 24.940767288208008\n",
      "  time_total_s: 3118.076587200165\n",
      "  timers:\n",
      "    learn_throughput: 6519.316\n",
      "    learn_time_ms: 2300.855\n",
      "    load_throughput: 10428749.503\n",
      "    load_time_ms: 1.438\n",
      "    sample_throughput: 594.505\n",
      "    sample_time_ms: 25231.064\n",
      "    update_time_ms: 1.937\n",
      "  timestamp: 1665248313\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1725000\n",
      "  training_iteration: 115\n",
      "  trial_id: 26f87_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 9.6/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.23 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_3_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_1_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_2_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_5_25daa8673b2c24aecf80a7fc79df4e93, 0.0/6.0 CPU_group_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_0_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_4_25daa8673b2c24aecf80a7fc79df4e93)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_onelane_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_26f87_00000 | RUNNING  | 141.225.10.229:307 |    115 |          3118.08 | 1725000 | -1816.75 |              2776.09 |             -8098.17 |               3000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m ring length: 251\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m v_max: 4.71224180704279\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m ring length: 260\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m v_max: 5.139779427502188\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m ring length: 228\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m v_max: 3.618452967700356\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m ring length: 232\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m v_max: 3.8087690783250063\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m ring length: 269\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m v_max: 5.566938458220921\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 1 seconds\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_26f87_00000:\n",
      "  agent_timesteps_total: 1740000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-08_11-58-59\n",
      "  done: false\n",
      "  episode_len_mean: 3000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2776.093748767079\n",
      "  episode_reward_mean: -1776.1379852159441\n",
      "  episode_reward_min: -7708.287380663002\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 580\n",
      "  experiment_id: edcbe001b14143f4bf76a6a2914a7bd0\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.36532360315322876\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.8276717066764832\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.02616354450583458\n",
      "          model: {}\n",
      "          policy_loss: 0.00838781800121069\n",
      "          total_loss: 3359.880859375\n",
      "          vf_explained_var: -1.630212587144797e-08\n",
      "          vf_loss: 3359.863037109375\n",
      "    num_agent_steps_sampled: 1740000\n",
      "    num_agent_steps_trained: 1740000\n",
      "    num_steps_sampled: 1740000\n",
      "    num_steps_trained: 1740000\n",
      "  iterations_since_restore: 116\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 23.433333333333337\n",
      "    ram_util_percent: 30.646153846153855\n",
      "  pid: 307\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06564299838061359\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 3.472630356227853\n",
      "    mean_inference_ms: 0.48281844843019023\n",
      "    mean_raw_obs_processing_ms: 3.7974904717951232\n",
      "  time_since_restore: 3144.8682055473328\n",
      "  time_this_iter_s: 26.79161834716797\n",
      "  time_total_s: 3144.8682055473328\n",
      "  timers:\n",
      "    learn_throughput: 6595.89\n",
      "    learn_time_ms: 2274.143\n",
      "    load_throughput: 10157995.35\n",
      "    load_time_ms: 1.477\n",
      "    sample_throughput: 592.855\n",
      "    sample_time_ms: 25301.288\n",
      "    update_time_ms: 2.131\n",
      "  timestamp: 1665248339\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1740000\n",
      "  training_iteration: 116\n",
      "  trial_id: 26f87_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Memory usage on this node: 9.6/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.23 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_4_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_1_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_2_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_0_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_3_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_5_25daa8673b2c24aecf80a7fc79df4e93, 0.0/6.0 CPU_group_25daa8673b2c24aecf80a7fc79df4e93)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_onelane_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_26f87_00000 | RUNNING  | 141.225.10.229:307 |    116 |          3144.87 | 1740000 | -1776.14 |              2776.09 |             -7708.29 |               3000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m ring length: 249\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m v_max: 4.617190445131122\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m ring length: 223\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m v_max: 3.3805185755587788\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m ring length: 257\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m v_max: 4.997304682316517\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m ring length: 251\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m v_max: 4.71224180704279\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m ring length: 266\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m v_max: 5.42459972166245\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m Could not connect to TraCI server at localhost:34447 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m Could not connect to TraCI server at localhost:54035 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m Could not connect to TraCI server at localhost:36289 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m Could not connect to TraCI server at localhost:34795 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m Could not connect to TraCI server at localhost:57509 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m Could not connect to TraCI server at localhost:34447 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 3 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m Could not connect to TraCI server at localhost:54035 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 3 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m Could not connect to TraCI server at localhost:36289 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 3 seconds\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m Could not connect to TraCI server at localhost:34795 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 3 seconds\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m Could not connect to TraCI server at localhost:57509 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 3 seconds\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_26f87_00000:\n",
      "  agent_timesteps_total: 1755000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-08_11-59-30\n",
      "  done: false\n",
      "  episode_len_mean: 3000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2776.093748767079\n",
      "  episode_reward_mean: -1816.034743991307\n",
      "  episode_reward_min: -7708.287380663002\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 585\n",
      "  experiment_id: edcbe001b14143f4bf76a6a2914a7bd0\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.36532360315322876\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.7755322456359863\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.02525383234024048\n",
      "          model: {}\n",
      "          policy_loss: 0.011666839942336082\n",
      "          total_loss: 3380.44482421875\n",
      "          vf_explained_var: 7.1321797356915795e-09\n",
      "          vf_loss: 3380.423583984375\n",
      "    num_agent_steps_sampled: 1755000\n",
      "    num_agent_steps_trained: 1755000\n",
      "    num_steps_sampled: 1755000\n",
      "    num_steps_trained: 1755000\n",
      "  iterations_since_restore: 117\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.544186046511626\n",
      "    ram_util_percent: 30.588372093023256\n",
      "  pid: 307\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06564286430625456\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 3.4723410055666477\n",
      "    mean_inference_ms: 0.48285535708952837\n",
      "    mean_raw_obs_processing_ms: 3.7972562627974487\n",
      "  time_since_restore: 3174.914978981018\n",
      "  time_this_iter_s: 30.046773433685303\n",
      "  time_total_s: 3174.914978981018\n",
      "  timers:\n",
      "    learn_throughput: 6637.218\n",
      "    learn_time_ms: 2259.983\n",
      "    load_throughput: 10054103.81\n",
      "    load_time_ms: 1.492\n",
      "    sample_throughput: 594.358\n",
      "    sample_time_ms: 25237.317\n",
      "    update_time_ms: 2.09\n",
      "  timestamp: 1665248370\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1755000\n",
      "  training_iteration: 117\n",
      "  trial_id: 26f87_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 9.6/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.23 GiB heap, 0.0/14.65 GiB objects (0.0/6.0 CPU_group_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_2_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_4_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_3_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_5_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_0_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_1_25daa8673b2c24aecf80a7fc79df4e93)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_onelane_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_26f87_00000 | RUNNING  | 141.225.10.229:307 |    117 |          3174.91 | 1755000 | -1816.03 |              2776.09 |             -7708.29 |               3000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m ring length: 236\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m v_max: 3.999052674951912\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m ring length: 234\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m v_max: 3.903915223349057\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m ring length: 270\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m v_max: 5.6143732387852054\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m ring length: 258\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m v_max: 5.044800727535787\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m ring length: 257\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m v_max: 4.997304682316517\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 1 seconds\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_26f87_00000:\n",
      "  agent_timesteps_total: 1770000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-08_11-59-57\n",
      "  done: false\n",
      "  episode_len_mean: 3000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2776.093748767079\n",
      "  episode_reward_mean: -1898.8765548316305\n",
      "  episode_reward_min: -7708.287380663002\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 590\n",
      "  experiment_id: edcbe001b14143f4bf76a6a2914a7bd0\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.36532360315322876\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.7046069502830505\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.021365268155932426\n",
      "          model: {}\n",
      "          policy_loss: 0.00842498429119587\n",
      "          total_loss: 3559.097412109375\n",
      "          vf_explained_var: -1.3754918093411561e-08\n",
      "          vf_loss: 3559.081298828125\n",
      "    num_agent_steps_sampled: 1770000\n",
      "    num_agent_steps_trained: 1770000\n",
      "    num_steps_sampled: 1770000\n",
      "    num_steps_trained: 1770000\n",
      "  iterations_since_restore: 118\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 23.34358974358974\n",
      "    ram_util_percent: 30.70000000000001\n",
      "  pid: 307\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06564275328332138\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 3.472056331302611\n",
      "    mean_inference_ms: 0.4828921959699825\n",
      "    mean_raw_obs_processing_ms: 3.796381539325375\n",
      "  time_since_restore: 3202.18635058403\n",
      "  time_this_iter_s: 27.271371603012085\n",
      "  time_total_s: 3202.18635058403\n",
      "  timers:\n",
      "    learn_throughput: 6496.689\n",
      "    learn_time_ms: 2308.868\n",
      "    load_throughput: 9893315.302\n",
      "    load_time_ms: 1.516\n",
      "    sample_throughput: 590.689\n",
      "    sample_time_ms: 25394.058\n",
      "    update_time_ms: 2.153\n",
      "  timestamp: 1665248397\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1770000\n",
      "  training_iteration: 118\n",
      "  trial_id: 26f87_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 9.6/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.23 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_1_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_3_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_0_25daa8673b2c24aecf80a7fc79df4e93, 0.0/6.0 CPU_group_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_2_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_5_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_4_25daa8673b2c24aecf80a7fc79df4e93)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_onelane_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_26f87_00000 | RUNNING  | 141.225.10.229:307 |    118 |          3202.19 | 1770000 | -1898.88 |              2776.09 |             -7708.29 |               3000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m ring length: 245\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m v_max: 4.427046998576354\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m ring length: 264\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m v_max: 5.329679917416892\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m ring length: 223\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m v_max: 3.3805185755587788\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m ring length: 221\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m v_max: 3.285334164169417\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m ring length: 261\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m v_max: 5.187261846097525\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 1 seconds\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_26f87_00000:\n",
      "  agent_timesteps_total: 1785000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-08_12-00-23\n",
      "  done: false\n",
      "  episode_len_mean: 3000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2776.093748767079\n",
      "  episode_reward_mean: -2048.5193167199877\n",
      "  episode_reward_min: -7708.287380663002\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 595\n",
      "  experiment_id: edcbe001b14143f4bf76a6a2914a7bd0\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.36532360315322876\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 2.782514810562134\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.027392076328396797\n",
      "          model: {}\n",
      "          policy_loss: 0.01534468587487936\n",
      "          total_loss: 3688.652587890625\n",
      "          vf_explained_var: 3.5660898678457897e-09\n",
      "          vf_loss: 3688.627197265625\n",
      "    num_agent_steps_sampled: 1785000\n",
      "    num_agent_steps_trained: 1785000\n",
      "    num_steps_sampled: 1785000\n",
      "    num_steps_trained: 1785000\n",
      "  iterations_since_restore: 119\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 23.824324324324323\n",
      "    ram_util_percent: 30.8054054054054\n",
      "  pid: 307\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06564227238296413\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 3.4717444430146833\n",
      "    mean_inference_ms: 0.48292822128488555\n",
      "    mean_raw_obs_processing_ms: 3.7954696603172358\n",
      "  time_since_restore: 3227.94052362442\n",
      "  time_this_iter_s: 25.754173040390015\n",
      "  time_total_s: 3227.94052362442\n",
      "  timers:\n",
      "    learn_throughput: 6415.73\n",
      "    learn_time_ms: 2338.003\n",
      "    load_throughput: 9886940.944\n",
      "    load_time_ms: 1.517\n",
      "    sample_throughput: 602.967\n",
      "    sample_time_ms: 24877.003\n",
      "    update_time_ms: 2.251\n",
      "  timestamp: 1665248423\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1785000\n",
      "  training_iteration: 119\n",
      "  trial_id: 26f87_00000\n",
      "  \n",
      "\u001b[2m\u001b[36m(pid=307)\u001b[0m 2022-10-08 12:00:23,287\tWARNING ppo.py:242 -- The magnitude of your environment rewards are more than 205.0x the scale of `vf_clip_param`. This means that it will take more than 205.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "== Status ==\n",
      "Memory usage on this node: 9.6/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.23 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_3_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_4_25daa8673b2c24aecf80a7fc79df4e93, 0.0/6.0 CPU_group_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_0_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_5_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_1_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_2_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 accelerator_type:G)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_onelane_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_26f87_00000 | RUNNING  | 141.225.10.229:307 |    119 |          3227.94 | 1785000 | -2048.52 |              2776.09 |             -7708.29 |               3000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m ring length: 220\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m v_max: 3.2377399006821497\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m ring length: 239\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m v_max: 4.141741239205832\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m ring length: 266\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m v_max: 5.42459972166245\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m ring length: 257\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m v_max: 4.997304682316517\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m ring length: 221\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m v_max: 3.285334164169417\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 1 seconds\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_26f87_00000:\n",
      "  agent_timesteps_total: 1800000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-08_12-00-48\n",
      "  done: false\n",
      "  episode_len_mean: 3000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2776.093748767079\n",
      "  episode_reward_mean: -2035.3816199045643\n",
      "  episode_reward_min: -7708.287380663002\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 600\n",
      "  experiment_id: edcbe001b14143f4bf76a6a2914a7bd0\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.36532360315322876\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.3797941207885742\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.019345572218298912\n",
      "          model: {}\n",
      "          policy_loss: 0.009479121305048466\n",
      "          total_loss: 3479.766845703125\n",
      "          vf_explained_var: 5.094414334827491e-10\n",
      "          vf_loss: 3479.750244140625\n",
      "    num_agent_steps_sampled: 1800000\n",
      "    num_agent_steps_trained: 1800000\n",
      "    num_steps_sampled: 1800000\n",
      "    num_steps_trained: 1800000\n",
      "  iterations_since_restore: 120\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 24.641666666666666\n",
      "    ram_util_percent: 30.702777777777783\n",
      "  pid: 307\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06564219864507247\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 3.471457478980845\n",
      "    mean_inference_ms: 0.4829658097205026\n",
      "    mean_raw_obs_processing_ms: 3.794532841674327\n",
      "  time_since_restore: 3253.4798550605774\n",
      "  time_this_iter_s: 25.539331436157227\n",
      "  time_total_s: 3253.4798550605774\n",
      "  timers:\n",
      "    learn_throughput: 6225.726\n",
      "    learn_time_ms: 2409.358\n",
      "    load_throughput: 8755766.474\n",
      "    load_time_ms: 1.713\n",
      "    sample_throughput: 616.298\n",
      "    sample_time_ms: 24338.856\n",
      "    update_time_ms: 2.182\n",
      "  timestamp: 1665248448\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1800000\n",
      "  training_iteration: 120\n",
      "  trial_id: 26f87_00000\n",
      "  \n",
      "\u001b[2m\u001b[36m(pid=307)\u001b[0m 2022-10-08 12:00:48,902\tWARNING ppo.py:242 -- The magnitude of your environment rewards are more than 204.0x the scale of `vf_clip_param`. This means that it will take more than 204.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "== Status ==\n",
      "Memory usage on this node: 9.6/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.23 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_3_25daa8673b2c24aecf80a7fc79df4e93, 0.0/6.0 CPU_group_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_4_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_0_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_5_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_2_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_1_25daa8673b2c24aecf80a7fc79df4e93)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_onelane_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_26f87_00000 | RUNNING  | 141.225.10.229:307 |    120 |          3253.48 | 1800000 | -2035.38 |              2776.09 |             -7708.29 |               3000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m ring length: 257\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m v_max: 4.997304682316517\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m ring length: 252\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m v_max: 4.759762049824181\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m ring length: 237\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m v_max: 4.04661795519353\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m ring length: 248\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m v_max: 4.569659527528101\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m ring length: 264\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m v_max: 5.329679917416892\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m Could not connect to TraCI server at localhost:54035 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m Could not connect to TraCI server at localhost:36289 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m Could not connect to TraCI server at localhost:34795 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m Could not connect to TraCI server at localhost:34447 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m Could not connect to TraCI server at localhost:57509 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m Could not connect to TraCI server at localhost:34447 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 3 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m Could not connect to TraCI server at localhost:54035 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 3 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m Could not connect to TraCI server at localhost:36289 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 3 seconds\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m Could not connect to TraCI server at localhost:34795 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 3 seconds\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m Could not connect to TraCI server at localhost:57509 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 3 seconds\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_26f87_00000:\n",
      "  agent_timesteps_total: 1815000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-08_12-01-18\n",
      "  done: false\n",
      "  episode_len_mean: 3000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2776.093748767079\n",
      "  episode_reward_mean: -2004.7456995159664\n",
      "  episode_reward_min: -7708.287380663002\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 605\n",
      "  experiment_id: edcbe001b14143f4bf76a6a2914a7bd0\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.36532360315322876\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.777878999710083\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.028095412999391556\n",
      "          model: {}\n",
      "          policy_loss: 0.015297963283956051\n",
      "          total_loss: 3340.347412109375\n",
      "          vf_explained_var: 6.622738357719982e-09\n",
      "          vf_loss: 3340.322265625\n",
      "    num_agent_steps_sampled: 1815000\n",
      "    num_agent_steps_trained: 1815000\n",
      "    num_steps_sampled: 1815000\n",
      "    num_steps_trained: 1815000\n",
      "  iterations_since_restore: 121\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.52093023255814\n",
      "    ram_util_percent: 30.66046511627907\n",
      "  pid: 307\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06564199213110884\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 3.4711568793093135\n",
      "    mean_inference_ms: 0.4830037627203477\n",
      "    mean_raw_obs_processing_ms: 3.794395335612403\n",
      "  time_since_restore: 3283.3817999362946\n",
      "  time_this_iter_s: 29.901944875717163\n",
      "  time_total_s: 3283.3817999362946\n",
      "  timers:\n",
      "    learn_throughput: 6319.713\n",
      "    learn_time_ms: 2373.526\n",
      "    load_throughput: 9890982.266\n",
      "    load_time_ms: 1.517\n",
      "    sample_throughput: 606.135\n",
      "    sample_time_ms: 24746.945\n",
      "    update_time_ms: 2.154\n",
      "  timestamp: 1665248478\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1815000\n",
      "  training_iteration: 121\n",
      "  trial_id: 26f87_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Memory usage on this node: 9.6/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.23 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_3_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_0_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_5_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_2_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_4_25daa8673b2c24aecf80a7fc79df4e93, 0.0/6.0 CPU_group_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_1_25daa8673b2c24aecf80a7fc79df4e93)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_onelane_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_26f87_00000 | RUNNING  | 141.225.10.229:307 |    121 |          3283.38 | 1815000 | -2004.75 |              2776.09 |             -7708.29 |               3000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m ring length: 233\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m v_max: 3.856343201216534\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m ring length: 249\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m v_max: 4.617190445131122\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m ring length: 249\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m v_max: 4.617190445131122\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m ring length: 242\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m v_max: 4.2844067680020546\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m ring length: 239\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m v_max: 4.141741239205832\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 1 seconds\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_26f87_00000:\n",
      "  agent_timesteps_total: 1830000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-08_12-01-45\n",
      "  done: false\n",
      "  episode_len_mean: 3000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1981.8981791924514\n",
      "  episode_reward_mean: -2089.512062248086\n",
      "  episode_reward_min: -8261.374423863153\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 610\n",
      "  experiment_id: edcbe001b14143f4bf76a6a2914a7bd0\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.36532360315322876\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 2.2633605003356934\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016834568232297897\n",
      "          model: {}\n",
      "          policy_loss: 0.007674333173781633\n",
      "          total_loss: 3581.0615234375\n",
      "          vf_explained_var: 6.622738357719982e-09\n",
      "          vf_loss: 3581.047607421875\n",
      "    num_agent_steps_sampled: 1830000\n",
      "    num_agent_steps_trained: 1830000\n",
      "    num_steps_sampled: 1830000\n",
      "    num_steps_trained: 1830000\n",
      "  iterations_since_restore: 122\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 23.636842105263153\n",
      "    ram_util_percent: 30.744736842105254\n",
      "  pid: 307\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06564155893611029\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 3.4708353144701714\n",
      "    mean_inference_ms: 0.48303993873330264\n",
      "    mean_raw_obs_processing_ms: 3.7945106945888303\n",
      "  time_since_restore: 3310.0030312538147\n",
      "  time_this_iter_s: 26.62123131752014\n",
      "  time_total_s: 3310.0030312538147\n",
      "  timers:\n",
      "    learn_throughput: 6207.941\n",
      "    learn_time_ms: 2416.26\n",
      "    load_throughput: 9023501.571\n",
      "    load_time_ms: 1.662\n",
      "    sample_throughput: 616.032\n",
      "    sample_time_ms: 24349.37\n",
      "    update_time_ms: 2.065\n",
      "  timestamp: 1665248505\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1830000\n",
      "  training_iteration: 122\n",
      "  trial_id: 26f87_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 9.6/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.23 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_1_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_4_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_3_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_0_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_5_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_2_25daa8673b2c24aecf80a7fc79df4e93, 0.0/6.0 CPU_group_25daa8673b2c24aecf80a7fc79df4e93)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_onelane_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_26f87_00000 | RUNNING  | 141.225.10.229:307 |    122 |             3310 | 1830000 | -2089.51 |               1981.9 |             -8261.37 |               3000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=307)\u001b[0m 2022-10-08 12:01:45,608\tWARNING ppo.py:242 -- The magnitude of your environment rewards are more than 209.0x the scale of `vf_clip_param`. This means that it will take more than 209.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m ring length: 259\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m v_max: 5.092292348291853\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m ring length: 233\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m v_max: 3.856343201216534\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m ring length: 260\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m v_max: 5.139779427502188\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m ring length: 250\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m v_max: 4.664717904914125\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m ring length: 234\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m v_max: 3.903915223349057\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 1 seconds\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_26f87_00000:\n",
      "  agent_timesteps_total: 1845000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-08_12-02-10\n",
      "  done: false\n",
      "  episode_len_mean: 3000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1981.8981791924514\n",
      "  episode_reward_mean: -2197.1478527683644\n",
      "  episode_reward_min: -8261.374423863153\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 615\n",
      "  experiment_id: edcbe001b14143f4bf76a6a2914a7bd0\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.36532360315322876\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.2267611026763916\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.029813475906848907\n",
      "          model: {}\n",
      "          policy_loss: 0.007937752641737461\n",
      "          total_loss: 3503.96826171875\n",
      "          vf_explained_var: -5.603855601776786e-09\n",
      "          vf_loss: 3503.94970703125\n",
      "    num_agent_steps_sampled: 1845000\n",
      "    num_agent_steps_trained: 1845000\n",
      "    num_steps_sampled: 1845000\n",
      "    num_steps_trained: 1845000\n",
      "  iterations_since_restore: 123\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 24.18333333333333\n",
      "    ram_util_percent: 30.74722222222222\n",
      "  pid: 307\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06564115667044804\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 3.4705024723191196\n",
      "    mean_inference_ms: 0.4830761330344263\n",
      "    mean_raw_obs_processing_ms: 3.794626890844933\n",
      "  time_since_restore: 3335.100461244583\n",
      "  time_this_iter_s: 25.097429990768433\n",
      "  time_total_s: 3335.100461244583\n",
      "  timers:\n",
      "    learn_throughput: 6311.684\n",
      "    learn_time_ms: 2376.545\n",
      "    load_throughput: 9007739.996\n",
      "    load_time_ms: 1.665\n",
      "    sample_throughput: 618.719\n",
      "    sample_time_ms: 24243.633\n",
      "    update_time_ms: 1.946\n",
      "  timestamp: 1665248530\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1845000\n",
      "  training_iteration: 123\n",
      "  trial_id: 26f87_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Memory usage on this node: 9.6/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.23 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_0_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_4_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_1_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_2_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_3_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_5_25daa8673b2c24aecf80a7fc79df4e93, 0.0/6.0 CPU_group_25daa8673b2c24aecf80a7fc79df4e93)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_onelane_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_26f87_00000 | RUNNING  | 141.225.10.229:307 |    123 |           3335.1 | 1845000 | -2197.15 |               1981.9 |             -8261.37 |               3000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=307)\u001b[0m 2022-10-08 12:02:10,794\tWARNING ppo.py:242 -- The magnitude of your environment rewards are more than 220.0x the scale of `vf_clip_param`. This means that it will take more than 220.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m ring length: 250\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m v_max: 4.664717904914125\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m ring length: 221\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m v_max: 3.285334164169417\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m ring length: 242\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m v_max: 4.2844067680020546\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m ring length: 220\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m v_max: 3.2377399006821497\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m ring length: 230\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m v_max: 3.7136148111012934\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 1 seconds\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_26f87_00000:\n",
      "  agent_timesteps_total: 1860000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-08_12-02-37\n",
      "  done: false\n",
      "  episode_len_mean: 3000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1981.8981791924514\n",
      "  episode_reward_mean: -2235.1628482809315\n",
      "  episode_reward_min: -8261.374423863153\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 620\n",
      "  experiment_id: edcbe001b14143f4bf76a6a2914a7bd0\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.36532360315322876\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 2.012096881866455\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.02009890228509903\n",
      "          model: {}\n",
      "          policy_loss: 0.010469278320670128\n",
      "          total_loss: 3748.61572265625\n",
      "          vf_explained_var: -6.622738357719982e-09\n",
      "          vf_loss: 3748.597412109375\n",
      "    num_agent_steps_sampled: 1860000\n",
      "    num_agent_steps_trained: 1860000\n",
      "    num_steps_sampled: 1860000\n",
      "    num_steps_trained: 1860000\n",
      "  iterations_since_restore: 124\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 23.908108108108106\n",
      "    ram_util_percent: 30.748648648648643\n",
      "  pid: 307\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06564050226810232\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 3.4701732397084863\n",
      "    mean_inference_ms: 0.4831102831289644\n",
      "    mean_raw_obs_processing_ms: 3.7948916368600503\n",
      "  time_since_restore: 3361.3261687755585\n",
      "  time_this_iter_s: 26.225707530975342\n",
      "  time_total_s: 3361.3261687755585\n",
      "  timers:\n",
      "    learn_throughput: 6274.573\n",
      "    learn_time_ms: 2390.601\n",
      "    load_throughput: 9076877.353\n",
      "    load_time_ms: 1.653\n",
      "    sample_throughput: 614.316\n",
      "    sample_time_ms: 24417.404\n",
      "    update_time_ms: 1.835\n",
      "  timestamp: 1665248557\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1860000\n",
      "  training_iteration: 124\n",
      "  trial_id: 26f87_00000\n",
      "  \n",
      "\u001b[2m\u001b[36m(pid=307)\u001b[0m 2022-10-08 12:02:37,097\tWARNING ppo.py:242 -- The magnitude of your environment rewards are more than 224.0x the scale of `vf_clip_param`. This means that it will take more than 224.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "== Status ==\n",
      "Memory usage on this node: 9.6/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.23 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_0_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_3_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_2_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_4_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 accelerator_type:G, 0.0/6.0 CPU_group_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_5_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_1_25daa8673b2c24aecf80a7fc79df4e93)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_onelane_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_26f87_00000 | RUNNING  | 141.225.10.229:307 |    124 |          3361.33 | 1860000 | -2235.16 |               1981.9 |             -8261.37 |               3000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m ring length: 242\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m v_max: 4.2844067680020546\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m ring length: 223\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m v_max: 3.3805185755587788\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m ring length: 238\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m v_max: 4.094180836186086\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m ring length: 266\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m v_max: 5.42459972166245\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m ring length: 246\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m v_max: 4.474587708995648\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 1 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m Could not connect to TraCI server at localhost:34447 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m Could not connect to TraCI server at localhost:57509 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m Could not connect to TraCI server at localhost:54035 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m Could not connect to TraCI server at localhost:36289 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m Could not connect to TraCI server at localhost:36289 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 3 seconds\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m Could not connect to TraCI server at localhost:34447 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 3 seconds\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m Could not connect to TraCI server at localhost:57509 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 3 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m Could not connect to TraCI server at localhost:54035 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 3 seconds\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_26f87_00000:\n",
      "  agent_timesteps_total: 1875000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-08_12-03-08\n",
      "  done: false\n",
      "  episode_len_mean: 3000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1981.8981791924514\n",
      "  episode_reward_mean: -2302.531601752667\n",
      "  episode_reward_min: -8261.374423863153\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 625\n",
      "  experiment_id: edcbe001b14143f4bf76a6a2914a7bd0\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.36532360315322876\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.7011469602584839\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.03084147535264492\n",
      "          model: {}\n",
      "          policy_loss: 0.008433015085756779\n",
      "          total_loss: 3510.522216796875\n",
      "          vf_explained_var: -1.0188828669654981e-09\n",
      "          vf_loss: 3510.502685546875\n",
      "    num_agent_steps_sampled: 1875000\n",
      "    num_agent_steps_trained: 1875000\n",
      "    num_steps_sampled: 1875000\n",
      "    num_steps_trained: 1875000\n",
      "  iterations_since_restore: 125\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.57111111111111\n",
      "    ram_util_percent: 30.69111111111111\n",
      "  pid: 307\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06563982066402753\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 3.4697944943525947\n",
      "    mean_inference_ms: 0.4831439723384529\n",
      "    mean_raw_obs_processing_ms: 3.7960780240677496\n",
      "  time_since_restore: 3392.183000802994\n",
      "  time_this_iter_s: 30.856832027435303\n",
      "  time_total_s: 3392.183000802994\n",
      "  timers:\n",
      "    learn_throughput: 6279.014\n",
      "    learn_time_ms: 2388.91\n",
      "    load_throughput: 9188094.751\n",
      "    load_time_ms: 1.633\n",
      "    sample_throughput: 599.747\n",
      "    sample_time_ms: 25010.543\n",
      "    update_time_ms: 1.995\n",
      "  timestamp: 1665248588\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1875000\n",
      "  training_iteration: 125\n",
      "  trial_id: 26f87_00000\n",
      "  \n",
      "\u001b[2m\u001b[36m(pid=307)\u001b[0m 2022-10-08 12:03:08,041\tWARNING ppo.py:242 -- The magnitude of your environment rewards are more than 230.0x the scale of `vf_clip_param`. This means that it will take more than 230.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "== Status ==\n",
      "Memory usage on this node: 9.6/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.23 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_1_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_3_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_5_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_4_25daa8673b2c24aecf80a7fc79df4e93, 0.0/6.0 CPU_group_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_0_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_2_25daa8673b2c24aecf80a7fc79df4e93)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_onelane_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_26f87_00000 | RUNNING  | 141.225.10.229:307 |    125 |          3392.18 | 1875000 | -2302.53 |               1981.9 |             -8261.37 |               3000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m ring length: 245\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m v_max: 4.427046998576354\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m ring length: 248\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m v_max: 4.569659527528101\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m ring length: 252\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m v_max: 4.759762049824181\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m ring length: 236\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m v_max: 3.999052674951912\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m ring length: 243\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m v_max: 4.331956438196479\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 1 seconds\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_26f87_00000:\n",
      "  agent_timesteps_total: 1890000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-08_12-03-33\n",
      "  done: false\n",
      "  episode_len_mean: 3000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1981.8981791924514\n",
      "  episode_reward_mean: -2381.8880892645375\n",
      "  episode_reward_min: -8261.374423863153\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 630\n",
      "  experiment_id: edcbe001b14143f4bf76a6a2914a7bd0\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.36532360315322876\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: -0.3410995900630951\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.019912701100111008\n",
      "          model: {}\n",
      "          policy_loss: 0.005852023139595985\n",
      "          total_loss: 3364.000244140625\n",
      "          vf_explained_var: -1.630212587144797e-08\n",
      "          vf_loss: 3363.987548828125\n",
      "    num_agent_steps_sampled: 1890000\n",
      "    num_agent_steps_trained: 1890000\n",
      "    num_steps_sampled: 1890000\n",
      "    num_steps_trained: 1890000\n",
      "  iterations_since_restore: 126\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 23.905555555555555\n",
      "    ram_util_percent: 30.75555555555555\n",
      "  pid: 307\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06563889855509941\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 3.469362997748787\n",
      "    mean_inference_ms: 0.48317518598502057\n",
      "    mean_raw_obs_processing_ms: 3.7973711887316988\n",
      "  time_since_restore: 3417.7201743125916\n",
      "  time_this_iter_s: 25.53717350959778\n",
      "  time_total_s: 3417.7201743125916\n",
      "  timers:\n",
      "    learn_throughput: 6409.326\n",
      "    learn_time_ms: 2340.34\n",
      "    load_throughput: 9493384.838\n",
      "    load_time_ms: 1.58\n",
      "    sample_throughput: 601.59\n",
      "    sample_time_ms: 24933.906\n",
      "    update_time_ms: 1.849\n",
      "  timestamp: 1665248613\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1890000\n",
      "  training_iteration: 126\n",
      "  trial_id: 26f87_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Memory usage on this node: 9.6/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.23 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_5_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_0_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_1_25daa8673b2c24aecf80a7fc79df4e93, 0.0/6.0 CPU_group_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_3_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_2_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_4_25daa8673b2c24aecf80a7fc79df4e93)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_onelane_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_26f87_00000 | RUNNING  | 141.225.10.229:307 |    126 |          3417.72 | 1890000 | -2381.89 |               1981.9 |             -8261.37 |               3000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=307)\u001b[0m 2022-10-08 12:03:33,667\tWARNING ppo.py:242 -- The magnitude of your environment rewards are more than 238.0x the scale of `vf_clip_param`. This means that it will take more than 238.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m ring length: 233\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m v_max: 3.856343201216534\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m ring length: 265\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m v_max: 5.37714246265477\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m ring length: 264\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m v_max: 5.329679917416892\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m ring length: 224\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m v_max: 3.4281086136538996\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m ring length: 229\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m v_max: 3.666034803266416\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_26f87_00000:\n",
      "  agent_timesteps_total: 1905000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-08_12-03-58\n",
      "  done: false\n",
      "  episode_len_mean: 3000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1981.8981791924514\n",
      "  episode_reward_mean: -2338.1988729448913\n",
      "  episode_reward_min: -8261.374423863153\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 635\n",
      "  experiment_id: edcbe001b14143f4bf76a6a2914a7bd0\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.36532360315322876\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: -1.3177014589309692\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.036770183593034744\n",
      "          model: {}\n",
      "          policy_loss: 0.011453652754426003\n",
      "          total_loss: 3153.36328125\n",
      "          vf_explained_var: -1.1207711203553572e-08\n",
      "          vf_loss: 3153.338134765625\n",
      "    num_agent_steps_sampled: 1905000\n",
      "    num_agent_steps_trained: 1905000\n",
      "    num_steps_sampled: 1905000\n",
      "    num_steps_trained: 1905000\n",
      "  iterations_since_restore: 127\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 24.563888888888886\n",
      "    ram_util_percent: 30.774999999999995\n",
      "  pid: 307\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06563743734803217\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 3.4689344921510576\n",
      "    mean_inference_ms: 0.48320261213195903\n",
      "    mean_raw_obs_processing_ms: 3.7977836821815676\n",
      "  time_since_restore: 3442.443201303482\n",
      "  time_this_iter_s: 24.723026990890503\n",
      "  time_total_s: 3442.443201303482\n",
      "  timers:\n",
      "    learn_throughput: 6308.614\n",
      "    learn_time_ms: 2377.701\n",
      "    load_throughput: 9540025.475\n",
      "    load_time_ms: 1.572\n",
      "    sample_throughput: 615.651\n",
      "    sample_time_ms: 24364.449\n",
      "    update_time_ms: 1.825\n",
      "  timestamp: 1665248638\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1905000\n",
      "  training_iteration: 127\n",
      "  trial_id: 26f87_00000\n",
      "  \n",
      "\u001b[2m\u001b[36m(pid=307)\u001b[0m 2022-10-08 12:03:58,476\tWARNING ppo.py:242 -- The magnitude of your environment rewards are more than 234.0x the scale of `vf_clip_param`. This means that it will take more than 234.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "== Status ==\n",
      "Memory usage on this node: 9.6/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.23 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_0_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_5_25daa8673b2c24aecf80a7fc79df4e93, 0.0/6.0 CPU_group_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_4_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_1_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_3_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_2_25daa8673b2c24aecf80a7fc79df4e93)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_onelane_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_26f87_00000 | RUNNING  | 141.225.10.229:307 |    127 |          3442.44 | 1905000 |  -2338.2 |               1981.9 |             -8261.37 |               3000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m ring length: 224\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m v_max: 3.4281086136538996\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m ring length: 257\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m v_max: 4.997304682316517\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m ring length: 263\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m v_max: 5.282212215151455\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m ring length: 262\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m v_max: 5.234739483008334\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m ring length: 257\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m v_max: 4.997304682316517\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 1 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m Could not connect to TraCI server at localhost:34447 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m Could not connect to TraCI server at localhost:57509 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m Could not connect to TraCI server at localhost:54035 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m Could not connect to TraCI server at localhost:36289 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m Could not connect to TraCI server at localhost:34795 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m Could not connect to TraCI server at localhost:34447 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 3 seconds\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m Could not connect to TraCI server at localhost:57509 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 3 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m Could not connect to TraCI server at localhost:54035 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 3 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m Could not connect to TraCI server at localhost:36289 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 3 seconds\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m Could not connect to TraCI server at localhost:34795 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 3 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 1 seconds\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_26f87_00000:\n",
      "  agent_timesteps_total: 1920000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-08_12-04-30\n",
      "  done: false\n",
      "  episode_len_mean: 3000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1981.8981791924514\n",
      "  episode_reward_mean: -2298.4349646708088\n",
      "  episode_reward_min: -8261.374423863153\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 640\n",
      "  experiment_id: edcbe001b14143f4bf76a6a2914a7bd0\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.36532360315322876\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: -0.047416675835847855\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.023879675194621086\n",
      "          model: {}\n",
      "          policy_loss: 0.006433519534766674\n",
      "          total_loss: 3599.10986328125\n",
      "          vf_explained_var: -1.5792684493476372e-08\n",
      "          vf_loss: 3599.0947265625\n",
      "    num_agent_steps_sampled: 1920000\n",
      "    num_agent_steps_trained: 1920000\n",
      "    num_steps_sampled: 1920000\n",
      "    num_steps_trained: 1920000\n",
      "  iterations_since_restore: 128\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.784444444444443\n",
      "    ram_util_percent: 30.713333333333328\n",
      "  pid: 307\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06563593328082284\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 3.4685220057676327\n",
      "    mean_inference_ms: 0.4832288971501685\n",
      "    mean_raw_obs_processing_ms: 3.7990321441302144\n",
      "  time_since_restore: 3474.0501618385315\n",
      "  time_this_iter_s: 31.60696053504944\n",
      "  time_total_s: 3474.0501618385315\n",
      "  timers:\n",
      "    learn_throughput: 6392.896\n",
      "    learn_time_ms: 2346.355\n",
      "    load_throughput: 9653469.995\n",
      "    load_time_ms: 1.554\n",
      "    sample_throughput: 604.111\n",
      "    sample_time_ms: 24829.863\n",
      "    update_time_ms: 1.77\n",
      "  timestamp: 1665248670\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1920000\n",
      "  training_iteration: 128\n",
      "  trial_id: 26f87_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 9.6/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.23 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_5_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_1_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_4_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_0_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_3_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_2_25daa8673b2c24aecf80a7fc79df4e93, 0.0/6.0 CPU_group_25daa8673b2c24aecf80a7fc79df4e93)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_onelane_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_26f87_00000 | RUNNING  | 141.225.10.229:307 |    128 |          3474.05 | 1920000 | -2298.43 |               1981.9 |             -8261.37 |               3000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=307)\u001b[0m 2022-10-08 12:04:30,154\tWARNING ppo.py:242 -- The magnitude of your environment rewards are more than 230.0x the scale of `vf_clip_param`. This means that it will take more than 230.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m ring length: 246\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m v_max: 4.474587708995648\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m ring length: 241\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m v_max: 4.236854288051661\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m ring length: 250\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m v_max: 4.664717904914125\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m ring length: 270\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m v_max: 5.6143732387852054\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m ring length: 258\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m v_max: 5.044800727535787\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 1 seconds\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_26f87_00000:\n",
      "  agent_timesteps_total: 1935000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-08_12-04-56\n",
      "  done: false\n",
      "  episode_len_mean: 3000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1981.8981791924514\n",
      "  episode_reward_mean: -2276.5904799029904\n",
      "  episode_reward_min: -8261.374423863153\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 645\n",
      "  experiment_id: edcbe001b14143f4bf76a6a2914a7bd0\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.36532360315322876\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: -0.8134556412696838\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.027958540245890617\n",
      "          model: {}\n",
      "          policy_loss: 0.011756679974496365\n",
      "          total_loss: 3337.460205078125\n",
      "          vf_explained_var: -4.58497284583359e-09\n",
      "          vf_loss: 3337.438232421875\n",
      "    num_agent_steps_sampled: 1935000\n",
      "    num_agent_steps_trained: 1935000\n",
      "    num_steps_sampled: 1935000\n",
      "    num_steps_trained: 1935000\n",
      "  iterations_since_restore: 129\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 24.313513513513513\n",
      "    ram_util_percent: 30.7972972972973\n",
      "  pid: 307\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06563463953568467\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 3.4681437463532063\n",
      "    mean_inference_ms: 0.48325679397379395\n",
      "    mean_raw_obs_processing_ms: 3.7994749779945276\n",
      "  time_since_restore: 3500.0744671821594\n",
      "  time_this_iter_s: 26.02430534362793\n",
      "  time_total_s: 3500.0744671821594\n",
      "  timers:\n",
      "    learn_throughput: 6373.093\n",
      "    learn_time_ms: 2353.645\n",
      "    load_throughput: 9582308.056\n",
      "    load_time_ms: 1.565\n",
      "    sample_throughput: 603.629\n",
      "    sample_time_ms: 24849.691\n",
      "    update_time_ms: 1.689\n",
      "  timestamp: 1665248696\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1935000\n",
      "  training_iteration: 129\n",
      "  trial_id: 26f87_00000\n",
      "  \n",
      "\u001b[2m\u001b[36m(pid=307)\u001b[0m 2022-10-08 12:04:56,243\tWARNING ppo.py:242 -- The magnitude of your environment rewards are more than 228.0x the scale of `vf_clip_param`. This means that it will take more than 228.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Memory usage on this node: 9.7/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.23 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_5_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_2_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_1_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_3_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_0_25daa8673b2c24aecf80a7fc79df4e93, 0.0/6.0 CPU_group_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_4_25daa8673b2c24aecf80a7fc79df4e93)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_onelane_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_26f87_00000 | RUNNING  | 141.225.10.229:307 |    129 |          3500.07 | 1935000 | -2276.59 |               1981.9 |             -8261.37 |               3000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m ring length: 248\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m v_max: 4.569659527528101\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m ring length: 221\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m v_max: 3.285334164169417\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m ring length: 238\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m v_max: 4.094180836186086\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m ring length: 239\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m v_max: 4.141741239205832\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m ring length: 250\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m v_max: 4.664717904914125\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 1 seconds\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_26f87_00000:\n",
      "  agent_timesteps_total: 1950000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-08_12-05-22\n",
      "  done: false\n",
      "  episode_len_mean: 3000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1981.8981791924514\n",
      "  episode_reward_mean: -2157.2239502099883\n",
      "  episode_reward_min: -8261.374423863153\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 650\n",
      "  experiment_id: edcbe001b14143f4bf76a6a2914a7bd0\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.36532360315322876\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: -2.2059757709503174\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.03979285806417465\n",
      "          model: {}\n",
      "          policy_loss: 0.014092911034822464\n",
      "          total_loss: 2783.66455078125\n",
      "          vf_explained_var: 8.660504313695583e-09\n",
      "          vf_loss: 2783.63623046875\n",
      "    num_agent_steps_sampled: 1950000\n",
      "    num_agent_steps_trained: 1950000\n",
      "    num_steps_sampled: 1950000\n",
      "    num_steps_trained: 1950000\n",
      "  iterations_since_restore: 130\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 24.191891891891885\n",
      "    ram_util_percent: 30.93783783783784\n",
      "  pid: 307\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06563382931383903\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 3.46784232545492\n",
      "    mean_inference_ms: 0.48328751426742067\n",
      "    mean_raw_obs_processing_ms: 3.799155223090541\n",
      "  time_since_restore: 3525.765612602234\n",
      "  time_this_iter_s: 25.691145420074463\n",
      "  time_total_s: 3525.765612602234\n",
      "  timers:\n",
      "    learn_throughput: 6470.062\n",
      "    learn_time_ms: 2318.37\n",
      "    load_throughput: 11231734.357\n",
      "    load_time_ms: 1.336\n",
      "    sample_throughput: 602.411\n",
      "    sample_time_ms: 24899.948\n",
      "    update_time_ms: 1.769\n",
      "  timestamp: 1665248722\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1950000\n",
      "  training_iteration: 130\n",
      "  trial_id: 26f87_00000\n",
      "  \n",
      "\u001b[2m\u001b[36m(pid=307)\u001b[0m 2022-10-08 12:05:22,043\tWARNING ppo.py:242 -- The magnitude of your environment rewards are more than 216.0x the scale of `vf_clip_param`. This means that it will take more than 216.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "== Status ==\n",
      "Memory usage on this node: 9.6/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.23 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_3_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_4_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_0_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_5_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_2_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_1_25daa8673b2c24aecf80a7fc79df4e93, 0.0/6.0 CPU_group_25daa8673b2c24aecf80a7fc79df4e93)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_onelane_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_26f87_00000 | RUNNING  | 141.225.10.229:307 |    130 |          3525.77 | 1950000 | -2157.22 |               1981.9 |             -8261.37 |               3000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m ring length: 221\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m v_max: 3.285334164169417\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m ring length: 257\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m v_max: 4.997304682316517\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m ring length: 248\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m v_max: 4.569659527528101\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m ring length: 266\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m v_max: 5.42459972166245\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m ring length: 246\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m v_max: 4.474587708995648\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 1 seconds\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_26f87_00000:\n",
      "  agent_timesteps_total: 1965000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-08_12-05-47\n",
      "  done: false\n",
      "  episode_len_mean: 3000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1981.8981791924514\n",
      "  episode_reward_mean: -2224.3023166423613\n",
      "  episode_reward_min: -8261.374423863153\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 655\n",
      "  experiment_id: edcbe001b14143f4bf76a6a2914a7bd0\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.36532360315322876\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.9423407912254333\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.022422729060053825\n",
      "          model: {}\n",
      "          policy_loss: 0.006087212357670069\n",
      "          total_loss: 3391.429443359375\n",
      "          vf_explained_var: -5.094414223805188e-09\n",
      "          vf_loss: 3391.415283203125\n",
      "    num_agent_steps_sampled: 1965000\n",
      "    num_agent_steps_trained: 1965000\n",
      "    num_steps_sampled: 1965000\n",
      "    num_steps_trained: 1965000\n",
      "  iterations_since_restore: 131\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 24.50810810810811\n",
      "    ram_util_percent: 30.827027027027036\n",
      "  pid: 307\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06563345473072309\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 3.4675321784574997\n",
      "    mean_inference_ms: 0.48331894947136556\n",
      "    mean_raw_obs_processing_ms: 3.7987429487298856\n",
      "  time_since_restore: 3551.4109122753143\n",
      "  time_this_iter_s: 25.645299673080444\n",
      "  time_total_s: 3551.4109122753143\n",
      "  timers:\n",
      "    learn_throughput: 6315.494\n",
      "    learn_time_ms: 2375.111\n",
      "    load_throughput: 11268346.677\n",
      "    load_time_ms: 1.331\n",
      "    sample_throughput: 614.318\n",
      "    sample_time_ms: 24417.341\n",
      "    update_time_ms: 1.773\n",
      "  timestamp: 1665248747\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1965000\n",
      "  training_iteration: 131\n",
      "  trial_id: 26f87_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Memory usage on this node: 9.6/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.23 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_0_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_5_25daa8673b2c24aecf80a7fc79df4e93, 0.0/6.0 CPU_group_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_4_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_3_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_2_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_1_25daa8673b2c24aecf80a7fc79df4e93)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_onelane_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_26f87_00000 | RUNNING  | 141.225.10.229:307 |    131 |          3551.41 | 1965000 |  -2224.3 |               1981.9 |             -8261.37 |               3000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=307)\u001b[0m 2022-10-08 12:05:47,779\tWARNING ppo.py:242 -- The magnitude of your environment rewards are more than 222.0x the scale of `vf_clip_param`. This means that it will take more than 222.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m ring length: 257\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m v_max: 4.997304682316517\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m ring length: 250\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m v_max: 4.664717904914125\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m ring length: 252\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m v_max: 4.759762049824181\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m ring length: 232\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m v_max: 3.8087690783250063\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m ring length: 262\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m v_max: 5.234739483008334\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 1 seconds\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_26f87_00000:\n",
      "  agent_timesteps_total: 1980000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-08_12-06-12\n",
      "  done: false\n",
      "  episode_len_mean: 3000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1593.5773241870947\n",
      "  episode_reward_mean: -2324.5161924932227\n",
      "  episode_reward_min: -8261.374423863153\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 660\n",
      "  experiment_id: edcbe001b14143f4bf76a6a2914a7bd0\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.36532360315322876\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 2.8843846321105957\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.020232394337654114\n",
      "          model: {}\n",
      "          policy_loss: 0.010090135037899017\n",
      "          total_loss: 3719.917236328125\n",
      "          vf_explained_var: 3.5660898678457897e-09\n",
      "          vf_loss: 3719.899658203125\n",
      "    num_agent_steps_sampled: 1980000\n",
      "    num_agent_steps_trained: 1980000\n",
      "    num_steps_sampled: 1980000\n",
      "    num_steps_trained: 1980000\n",
      "  iterations_since_restore: 132\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 24.482857142857146\n",
      "    ram_util_percent: 30.857142857142858\n",
      "  pid: 307\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06563296326065945\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 3.4672175525379587\n",
      "    mean_inference_ms: 0.4833511110304559\n",
      "    mean_raw_obs_processing_ms: 3.797565639356922\n",
      "  time_since_restore: 3576.3832845687866\n",
      "  time_this_iter_s: 24.97237229347229\n",
      "  time_total_s: 3576.3832845687866\n",
      "  timers:\n",
      "    learn_throughput: 6353.22\n",
      "    learn_time_ms: 2361.008\n",
      "    load_throughput: 12612677.92\n",
      "    load_time_ms: 1.189\n",
      "    sample_throughput: 618.127\n",
      "    sample_time_ms: 24266.846\n",
      "    update_time_ms: 1.706\n",
      "  timestamp: 1665248772\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1980000\n",
      "  training_iteration: 132\n",
      "  trial_id: 26f87_00000\n",
      "  \n",
      "\u001b[2m\u001b[36m(pid=307)\u001b[0m 2022-10-08 12:06:12,850\tWARNING ppo.py:242 -- The magnitude of your environment rewards are more than 232.0x the scale of `vf_clip_param`. This means that it will take more than 232.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "== Status ==\n",
      "Memory usage on this node: 9.6/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.23 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_4_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_2_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_0_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_3_25daa8673b2c24aecf80a7fc79df4e93, 0.0/6.0 CPU_group_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_1_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_5_25daa8673b2c24aecf80a7fc79df4e93)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_onelane_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_26f87_00000 | RUNNING  | 141.225.10.229:307 |    132 |          3576.38 | 1980000 | -2324.52 |              1593.58 |             -8261.37 |               3000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m ring length: 250\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m v_max: 4.664717904914125\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m ring length: 260\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m v_max: 5.139779427502188\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m ring length: 254\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m v_max: 4.854791141191876\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m ring length: 241\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m v_max: 4.236854288051661\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m ring length: 230\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m v_max: 3.7136148111012934\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 1 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 1 seconds\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_26f87_00000:\n",
      "  agent_timesteps_total: 1995000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-08_12-06-38\n",
      "  done: false\n",
      "  episode_len_mean: 3000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2558.7908574319563\n",
      "  episode_reward_mean: -2165.8613460293914\n",
      "  episode_reward_min: -8261.374423863153\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 665\n",
      "  experiment_id: edcbe001b14143f4bf76a6a2914a7bd0\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.36532360315322876\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: -0.861614465713501\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.04482780769467354\n",
      "          model: {}\n",
      "          policy_loss: 0.014618431217968464\n",
      "          total_loss: 3251.460693359375\n",
      "          vf_explained_var: 5.094414334827491e-10\n",
      "          vf_loss: 3251.429443359375\n",
      "    num_agent_steps_sampled: 1995000\n",
      "    num_agent_steps_trained: 1995000\n",
      "    num_steps_sampled: 1995000\n",
      "    num_steps_trained: 1995000\n",
      "  iterations_since_restore: 133\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 24.810810810810807\n",
      "    ram_util_percent: 30.856756756756763\n",
      "  pid: 307\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06563251512988305\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 3.466979023258208\n",
      "    mean_inference_ms: 0.48338363707463494\n",
      "    mean_raw_obs_processing_ms: 3.7963171594456133\n",
      "  time_since_restore: 3602.144923686981\n",
      "  time_this_iter_s: 25.76163911819458\n",
      "  time_total_s: 3602.144923686981\n",
      "  timers:\n",
      "    learn_throughput: 6375.466\n",
      "    learn_time_ms: 2352.769\n",
      "    load_throughput: 12538025.867\n",
      "    load_time_ms: 1.196\n",
      "    sample_throughput: 616.233\n",
      "    sample_time_ms: 24341.435\n",
      "    update_time_ms: 1.711\n",
      "  timestamp: 1665248798\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1995000\n",
      "  training_iteration: 133\n",
      "  trial_id: 26f87_00000\n",
      "  \n",
      "\u001b[2m\u001b[36m(pid=307)\u001b[0m 2022-10-08 12:06:38,684\tWARNING ppo.py:242 -- The magnitude of your environment rewards are more than 217.0x the scale of `vf_clip_param`. This means that it will take more than 217.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "== Status ==\n",
      "Memory usage on this node: 9.6/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.23 GiB heap, 0.0/14.65 GiB objects (0.0/6.0 CPU_group_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_5_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_1_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_2_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_3_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_4_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_0_25daa8673b2c24aecf80a7fc79df4e93)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_onelane_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_26f87_00000 | RUNNING  | 141.225.10.229:307 |    133 |          3602.14 | 1995000 | -2165.86 |              2558.79 |             -8261.37 |               3000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m ring length: 221\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m v_max: 3.285334164169417\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m ring length: 266\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m v_max: 5.42459972166245\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m ring length: 264\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m v_max: 5.329679917416892\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m ring length: 259\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m v_max: 5.092292348291853\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m ring length: 232\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m v_max: 3.8087690783250063\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 1 seconds\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_26f87_00000:\n",
      "  agent_timesteps_total: 2010000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-08_12-07-03\n",
      "  done: false\n",
      "  episode_len_mean: 3000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2558.7908574319563\n",
      "  episode_reward_mean: -2092.950768093964\n",
      "  episode_reward_min: -8261.374423863153\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 670\n",
      "  experiment_id: edcbe001b14143f4bf76a6a2914a7bd0\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5479854345321655\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.325933039188385\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.018613601103425026\n",
      "          model: {}\n",
      "          policy_loss: 0.006000249180942774\n",
      "          total_loss: 3488.930908203125\n",
      "          vf_explained_var: -5.094414334827491e-10\n",
      "          vf_loss: 3488.9150390625\n",
      "    num_agent_steps_sampled: 2010000\n",
      "    num_agent_steps_trained: 2010000\n",
      "    num_steps_sampled: 2010000\n",
      "    num_steps_trained: 2010000\n",
      "  iterations_since_restore: 134\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 24.17222222222222\n",
      "    ram_util_percent: 30.866666666666674\n",
      "  pid: 307\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0656319507487163\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 3.466755239275908\n",
      "    mean_inference_ms: 0.48341454138816364\n",
      "    mean_raw_obs_processing_ms: 3.7952561458648626\n",
      "  time_since_restore: 3627.1344974040985\n",
      "  time_this_iter_s: 24.98957371711731\n",
      "  time_total_s: 3627.1344974040985\n",
      "  timers:\n",
      "    learn_throughput: 6482.779\n",
      "    learn_time_ms: 2313.823\n",
      "    load_throughput: 12463017.769\n",
      "    load_time_ms: 1.204\n",
      "    sample_throughput: 618.366\n",
      "    sample_time_ms: 24257.483\n",
      "    update_time_ms: 1.538\n",
      "  timestamp: 1665248823\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2010000\n",
      "  training_iteration: 134\n",
      "  trial_id: 26f87_00000\n",
      "  \n",
      "\u001b[2m\u001b[36m(pid=307)\u001b[0m 2022-10-08 12:07:03,772\tWARNING ppo.py:242 -- The magnitude of your environment rewards are more than 209.0x the scale of `vf_clip_param`. This means that it will take more than 209.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "== Status ==\n",
      "Memory usage on this node: 9.6/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.23 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_2_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_4_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_3_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_5_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_0_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_1_25daa8673b2c24aecf80a7fc79df4e93, 0.0/6.0 CPU_group_25daa8673b2c24aecf80a7fc79df4e93)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_onelane_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_26f87_00000 | RUNNING  | 141.225.10.229:307 |    134 |          3627.13 | 2010000 | -2092.95 |              2558.79 |             -8261.37 |               3000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m ring length: 235\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m v_max: 3.9514850725282584\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m ring length: 246\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m v_max: 4.474587708995648\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m ring length: 265\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m v_max: 5.37714246265477\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m ring length: 220\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m v_max: 3.2377399006821497\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m ring length: 263\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m v_max: 5.282212215151455\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 1 seconds\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_26f87_00000:\n",
      "  agent_timesteps_total: 2025000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-08_12-07-29\n",
      "  done: false\n",
      "  episode_len_mean: 3000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2558.7908574319563\n",
      "  episode_reward_mean: -2041.7399992705905\n",
      "  episode_reward_min: -8261.374423863153\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 675\n",
      "  experiment_id: edcbe001b14143f4bf76a6a2914a7bd0\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5479854345321655\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: -1.8008862733840942\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01735592447221279\n",
      "          model: {}\n",
      "          policy_loss: 0.006200156174600124\n",
      "          total_loss: 2792.32373046875\n",
      "          vf_explained_var: -1.171715258152517e-08\n",
      "          vf_loss: 2792.3076171875\n",
      "    num_agent_steps_sampled: 2025000\n",
      "    num_agent_steps_trained: 2025000\n",
      "    num_steps_sampled: 2025000\n",
      "    num_steps_trained: 2025000\n",
      "  iterations_since_restore: 135\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 23.472972972972972\n",
      "    ram_util_percent: 30.854054054054064\n",
      "  pid: 307\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06563155415975296\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 3.466547006848931\n",
      "    mean_inference_ms: 0.4834443781347315\n",
      "    mean_raw_obs_processing_ms: 3.7943425721209976\n",
      "  time_since_restore: 3653.0455615520477\n",
      "  time_this_iter_s: 25.91106414794922\n",
      "  time_total_s: 3653.0455615520477\n",
      "  timers:\n",
      "    learn_throughput: 6531.22\n",
      "    learn_time_ms: 2296.661\n",
      "    load_throughput: 12104773.449\n",
      "    load_time_ms: 1.239\n",
      "    sample_throughput: 630.781\n",
      "    sample_time_ms: 23780.061\n",
      "    update_time_ms: 1.499\n",
      "  timestamp: 1665248849\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2025000\n",
      "  training_iteration: 135\n",
      "  trial_id: 26f87_00000\n",
      "  \n",
      "\u001b[2m\u001b[36m(pid=307)\u001b[0m 2022-10-08 12:07:29,767\tWARNING ppo.py:242 -- The magnitude of your environment rewards are more than 204.0x the scale of `vf_clip_param`. This means that it will take more than 204.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "== Status ==\n",
      "Memory usage on this node: 9.6/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.23 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_1_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_2_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_4_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_3_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_0_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_5_25daa8673b2c24aecf80a7fc79df4e93, 0.0/6.0 CPU_group_25daa8673b2c24aecf80a7fc79df4e93)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_onelane_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_26f87_00000 | RUNNING  | 141.225.10.229:307 |    135 |          3653.05 | 2025000 | -2041.74 |              2558.79 |             -8261.37 |               3000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m ring length: 262\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m v_max: 5.234739483008334\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m ring length: 223\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m v_max: 3.3805185755587788\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m ring length: 235\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m v_max: 3.9514850725282584\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m ring length: 234\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m v_max: 3.903915223349057\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m ring length: 250\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m v_max: 4.664717904914125\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 1 seconds\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_26f87_00000:\n",
      "  agent_timesteps_total: 2040000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-08_12-07-55\n",
      "  done: false\n",
      "  episode_len_mean: 3000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2558.7908574319563\n",
      "  episode_reward_mean: -1996.2261869749677\n",
      "  episode_reward_min: -8261.374423863153\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 680\n",
      "  experiment_id: edcbe001b14143f4bf76a6a2914a7bd0\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5479854345321655\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: -0.5798534154891968\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.07008674740791321\n",
      "          model: {}\n",
      "          policy_loss: 0.032076362520456314\n",
      "          total_loss: 3224.959716796875\n",
      "          vf_explained_var: -3.5660898678457897e-09\n",
      "          vf_loss: 3224.888916015625\n",
      "    num_agent_steps_sampled: 2040000\n",
      "    num_agent_steps_trained: 2040000\n",
      "    num_steps_sampled: 2040000\n",
      "    num_steps_trained: 2040000\n",
      "  iterations_since_restore: 136\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 23.91351351351351\n",
      "    ram_util_percent: 30.851351351351358\n",
      "  pid: 307\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06563160732072157\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 3.4663760510833628\n",
      "    mean_inference_ms: 0.4834760572809551\n",
      "    mean_raw_obs_processing_ms: 3.793392144766825\n",
      "  time_since_restore: 3679.1147122383118\n",
      "  time_this_iter_s: 26.069150686264038\n",
      "  time_total_s: 3679.1147122383118\n",
      "  timers:\n",
      "    learn_throughput: 6572.644\n",
      "    learn_time_ms: 2282.187\n",
      "    load_throughput: 10579921.3\n",
      "    load_time_ms: 1.418\n",
      "    sample_throughput: 628.988\n",
      "    sample_time_ms: 23847.814\n",
      "    update_time_ms: 1.451\n",
      "  timestamp: 1665248875\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2040000\n",
      "  training_iteration: 136\n",
      "  trial_id: 26f87_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Memory usage on this node: 9.6/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.23 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_4_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_2_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_1_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_3_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_5_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_0_25daa8673b2c24aecf80a7fc79df4e93, 0.0/6.0 CPU_group_25daa8673b2c24aecf80a7fc79df4e93)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_onelane_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_26f87_00000 | RUNNING  | 141.225.10.229:307 |    136 |          3679.11 | 2040000 | -1996.23 |              2558.79 |             -8261.37 |               3000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m ring length: 256\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m v_max: 4.949804327743507\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m ring length: 227\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m v_max: 3.570869368805752\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m ring length: 231\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m v_max: 3.761192925277308\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m ring length: 266\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m v_max: 5.42459972166245\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m ring length: 244\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m v_max: 4.379503211387676\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 1 seconds\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_26f87_00000:\n",
      "  agent_timesteps_total: 2055000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-08_12-08-21\n",
      "  done: false\n",
      "  episode_len_mean: 3000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2558.7908574319563\n",
      "  episode_reward_mean: -1913.7641584578555\n",
      "  episode_reward_min: -8261.374423863153\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 685\n",
      "  experiment_id: edcbe001b14143f4bf76a6a2914a7bd0\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.8219781517982483\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: -0.5591027736663818\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.02272135019302368\n",
      "          model: {}\n",
      "          policy_loss: 0.009189416654407978\n",
      "          total_loss: 3107.54638671875\n",
      "          vf_explained_var: 4.58497284583359e-09\n",
      "          vf_loss: 3107.51806640625\n",
      "    num_agent_steps_sampled: 2055000\n",
      "    num_agent_steps_trained: 2055000\n",
      "    num_steps_sampled: 2055000\n",
      "    num_steps_trained: 2055000\n",
      "  iterations_since_restore: 137\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 24.28918918918919\n",
      "    ram_util_percent: 30.85945945945947\n",
      "  pid: 307\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06563190714498118\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 3.4662201586017387\n",
      "    mean_inference_ms: 0.4835092118043901\n",
      "    mean_raw_obs_processing_ms: 3.7917505967537726\n",
      "  time_since_restore: 3704.4089245796204\n",
      "  time_this_iter_s: 25.294212341308594\n",
      "  time_total_s: 3704.4089245796204\n",
      "  timers:\n",
      "    learn_throughput: 6626.584\n",
      "    learn_time_ms: 2263.61\n",
      "    load_throughput: 10596136.421\n",
      "    load_time_ms: 1.416\n",
      "    sample_throughput: 627.007\n",
      "    sample_time_ms: 23923.159\n",
      "    update_time_ms: 1.447\n",
      "  timestamp: 1665248901\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2055000\n",
      "  training_iteration: 137\n",
      "  trial_id: 26f87_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 9.6/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.23 GiB heap, 0.0/14.65 GiB objects (0.0/6.0 CPU_group_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_2_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_5_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_0_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_4_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_1_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_3_25daa8673b2c24aecf80a7fc79df4e93)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_onelane_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_26f87_00000 | RUNNING  | 141.225.10.229:307 |    137 |          3704.41 | 2055000 | -1913.76 |              2558.79 |             -8261.37 |               3000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m ring length: 255\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m v_max: 4.902299776966978\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m ring length: 243\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m v_max: 4.331956438196479\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m ring length: 244\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m v_max: 4.379503211387676\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m ring length: 269\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m v_max: 5.566938458220921\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m ring length: 243\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m v_max: 4.331956438196479\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 1 seconds\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_26f87_00000:\n",
      "  agent_timesteps_total: 2070000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-08_12-08-46\n",
      "  done: false\n",
      "  episode_len_mean: 3000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2558.7908574319563\n",
      "  episode_reward_mean: -1919.9415641749358\n",
      "  episode_reward_min: -8261.374423863153\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 690\n",
      "  experiment_id: edcbe001b14143f4bf76a6a2914a7bd0\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.8219781517982483\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.9356129169464111\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011943849734961987\n",
      "          model: {}\n",
      "          policy_loss: 0.006612853612750769\n",
      "          total_loss: 3401.631103515625\n",
      "          vf_explained_var: -8.660504313695583e-09\n",
      "          vf_loss: 3401.61474609375\n",
      "    num_agent_steps_sampled: 2070000\n",
      "    num_agent_steps_trained: 2070000\n",
      "    num_steps_sampled: 2070000\n",
      "    num_steps_trained: 2070000\n",
      "  iterations_since_restore: 138\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 24.625\n",
      "    ram_util_percent: 30.866666666666674\n",
      "  pid: 307\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06563177932317997\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 3.4660429928367353\n",
      "    mean_inference_ms: 0.48354039999063714\n",
      "    mean_raw_obs_processing_ms: 3.7899933120291944\n",
      "  time_since_restore: 3729.6622366905212\n",
      "  time_this_iter_s: 25.25331211090088\n",
      "  time_total_s: 3729.6622366905212\n",
      "  timers:\n",
      "    learn_throughput: 6748.611\n",
      "    learn_time_ms: 2222.68\n",
      "    load_throughput: 10306090.489\n",
      "    load_time_ms: 1.455\n",
      "    sample_throughput: 642.994\n",
      "    sample_time_ms: 23328.375\n",
      "    update_time_ms: 1.426\n",
      "  timestamp: 1665248926\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2070000\n",
      "  training_iteration: 138\n",
      "  trial_id: 26f87_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Memory usage on this node: 9.6/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.23 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_0_25daa8673b2c24aecf80a7fc79df4e93, 0.0/6.0 CPU_group_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_2_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_1_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_5_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_3_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_4_25daa8673b2c24aecf80a7fc79df4e93)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_onelane_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_26f87_00000 | RUNNING  | 141.225.10.229:307 |    138 |          3729.66 | 2070000 | -1919.94 |              2558.79 |             -8261.37 |               3000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m ring length: 259\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m v_max: 5.092292348291853\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m ring length: 241\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m v_max: 4.236854288051661\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m ring length: 246\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m v_max: 4.474587708995648\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m ring length: 267\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m v_max: 5.472051563171826\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m ring length: 251\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m v_max: 4.71224180704279\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 1 seconds\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_26f87_00000:\n",
      "  agent_timesteps_total: 2085000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-08_12-09-12\n",
      "  done: false\n",
      "  episode_len_mean: 3000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2558.7908574319563\n",
      "  episode_reward_mean: -1797.4761057185285\n",
      "  episode_reward_min: -8261.374423863153\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 695\n",
      "  experiment_id: edcbe001b14143f4bf76a6a2914a7bd0\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.8219781517982483\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: -0.6952086687088013\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.022748198360204697\n",
      "          model: {}\n",
      "          policy_loss: 0.008871698752045631\n",
      "          total_loss: 3174.779052734375\n",
      "          vf_explained_var: 5.603855601776786e-09\n",
      "          vf_loss: 3174.751708984375\n",
      "    num_agent_steps_sampled: 2085000\n",
      "    num_agent_steps_trained: 2085000\n",
      "    num_steps_sampled: 2085000\n",
      "    num_steps_trained: 2085000\n",
      "  iterations_since_restore: 139\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 24.57222222222222\n",
      "    ram_util_percent: 30.89166666666667\n",
      "  pid: 307\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06563178396942791\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 3.4659000582891992\n",
      "    mean_inference_ms: 0.48357095436429554\n",
      "    mean_raw_obs_processing_ms: 3.7883392660556057\n",
      "  time_since_restore: 3755.3040313720703\n",
      "  time_this_iter_s: 25.641794681549072\n",
      "  time_total_s: 3755.3040313720703\n",
      "  timers:\n",
      "    learn_throughput: 6835.11\n",
      "    learn_time_ms: 2194.551\n",
      "    load_throughput: 10332324.974\n",
      "    load_time_ms: 1.452\n",
      "    sample_throughput: 643.271\n",
      "    sample_time_ms: 23318.311\n",
      "    update_time_ms: 1.406\n",
      "  timestamp: 1665248952\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2085000\n",
      "  training_iteration: 139\n",
      "  trial_id: 26f87_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 9.6/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.23 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_1_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_4_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_3_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_2_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_5_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_0_25daa8673b2c24aecf80a7fc79df4e93, 0.0/6.0 CPU_group_25daa8673b2c24aecf80a7fc79df4e93)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_onelane_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_26f87_00000 | RUNNING  | 141.225.10.229:307 |    139 |           3755.3 | 2085000 | -1797.48 |              2558.79 |             -8261.37 |               3000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m ring length: 240\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m v_max: 4.189299083856172\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m ring length: 259\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m v_max: 5.092292348291853\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m ring length: 241\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m v_max: 4.236854288051661\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m ring length: 235\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m v_max: 3.9514850725282584\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m ring length: 254\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m v_max: 4.854791141191876\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 1 seconds\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_26f87_00000:\n",
      "  agent_timesteps_total: 2100000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-08_12-09-38\n",
      "  done: false\n",
      "  episode_len_mean: 3000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2558.7908574319563\n",
      "  episode_reward_mean: -1764.234294185556\n",
      "  episode_reward_min: -8261.374423863153\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 700\n",
      "  experiment_id: edcbe001b14143f4bf76a6a2914a7bd0\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.8219781517982483\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.5420166850090027\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014317546039819717\n",
      "          model: {}\n",
      "          policy_loss: 0.007382854353636503\n",
      "          total_loss: 3444.591064453125\n",
      "          vf_explained_var: -8.151062935723985e-09\n",
      "          vf_loss: 3444.57177734375\n",
      "    num_agent_steps_sampled: 2100000\n",
      "    num_agent_steps_trained: 2100000\n",
      "    num_steps_sampled: 2100000\n",
      "    num_steps_trained: 2100000\n",
      "  iterations_since_restore: 140\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 23.673684210526318\n",
      "    ram_util_percent: 30.85526315789475\n",
      "  pid: 307\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06563162837099362\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 3.465783005415567\n",
      "    mean_inference_ms: 0.4835996836298928\n",
      "    mean_raw_obs_processing_ms: 3.7868928681226457\n",
      "  time_since_restore: 3781.834286212921\n",
      "  time_this_iter_s: 26.53025484085083\n",
      "  time_total_s: 3781.834286212921\n",
      "  timers:\n",
      "    learn_throughput: 6792.058\n",
      "    learn_time_ms: 2208.462\n",
      "    load_throughput: 10235668.5\n",
      "    load_time_ms: 1.465\n",
      "    sample_throughput: 641.359\n",
      "    sample_time_ms: 23387.835\n",
      "    update_time_ms: 1.34\n",
      "  timestamp: 1665248978\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2100000\n",
      "  training_iteration: 140\n",
      "  trial_id: 26f87_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Memory usage on this node: 9.6/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.23 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_1_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_2_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_0_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_5_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_3_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_4_25daa8673b2c24aecf80a7fc79df4e93, 0.0/6.0 CPU_group_25daa8673b2c24aecf80a7fc79df4e93)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_onelane_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_26f87_00000 | RUNNING  | 141.225.10.229:307 |    140 |          3781.83 | 2100000 | -1764.23 |              2558.79 |             -8261.37 |               3000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m ring length: 227\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m v_max: 3.570869368805752\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m ring length: 265\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m v_max: 5.37714246265477\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m ring length: 263\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m v_max: 5.282212215151455\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m ring length: 253\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m v_max: 4.807278529691987\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m ring length: 262\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m v_max: 5.234739483008334\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 1 seconds\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_26f87_00000:\n",
      "  agent_timesteps_total: 2115000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-08_12-10-04\n",
      "  done: false\n",
      "  episode_len_mean: 3000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2558.7908574319563\n",
      "  episode_reward_mean: -1686.3865801510783\n",
      "  episode_reward_min: -8261.374423863153\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 705\n",
      "  experiment_id: edcbe001b14143f4bf76a6a2914a7bd0\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.8219781517982483\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: -0.47885826230049133\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008604928851127625\n",
      "          model: {}\n",
      "          policy_loss: 0.004090027417987585\n",
      "          total_loss: 3122.058837890625\n",
      "          vf_explained_var: -1.0698269825581974e-08\n",
      "          vf_loss: 3122.047607421875\n",
      "    num_agent_steps_sampled: 2115000\n",
      "    num_agent_steps_trained: 2115000\n",
      "    num_steps_sampled: 2115000\n",
      "    num_steps_trained: 2115000\n",
      "  iterations_since_restore: 141\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 24.37837837837838\n",
      "    ram_util_percent: 31.010810810810806\n",
      "  pid: 307\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06563158592688495\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 3.465696971772244\n",
      "    mean_inference_ms: 0.4836279780846786\n",
      "    mean_raw_obs_processing_ms: 3.7847383648682165\n",
      "  time_since_restore: 3807.3529708385468\n",
      "  time_this_iter_s: 25.51868462562561\n",
      "  time_total_s: 3807.3529708385468\n",
      "  timers:\n",
      "    learn_throughput: 6773.275\n",
      "    learn_time_ms: 2214.586\n",
      "    load_throughput: 9983427.22\n",
      "    load_time_ms: 1.502\n",
      "    sample_throughput: 641.877\n",
      "    sample_time_ms: 23368.957\n",
      "    update_time_ms: 1.458\n",
      "  timestamp: 1665249004\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2115000\n",
      "  training_iteration: 141\n",
      "  trial_id: 26f87_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 9.7/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.23 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_3_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_4_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_5_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_1_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_0_25daa8673b2c24aecf80a7fc79df4e93, 0.0/6.0 CPU_group_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_2_25daa8673b2c24aecf80a7fc79df4e93)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_onelane_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_26f87_00000 | RUNNING  | 141.225.10.229:307 |    141 |          3807.35 | 2115000 | -1686.39 |              2558.79 |             -8261.37 |               3000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m ring length: 239\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m v_max: 4.141741239205832\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m ring length: 261\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m v_max: 5.187261846097525\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m ring length: 240\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m v_max: 4.189299083856172\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m ring length: 253\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m v_max: 4.807278529691987\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m ring length: 259\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m v_max: 5.092292348291853\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 1 seconds\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_26f87_00000:\n",
      "  agent_timesteps_total: 2130000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-08_12-10-31\n",
      "  done: false\n",
      "  episode_len_mean: 3000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2558.7908574319563\n",
      "  episode_reward_mean: -1663.7035325490715\n",
      "  episode_reward_min: -7913.932615843404\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 710\n",
      "  experiment_id: edcbe001b14143f4bf76a6a2914a7bd0\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.41098907589912415\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.6356151103973389\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.04006316512823105\n",
      "          model: {}\n",
      "          policy_loss: 0.02279338799417019\n",
      "          total_loss: 3547.844482421875\n",
      "          vf_explained_var: -1.3754918093411561e-08\n",
      "          vf_loss: 3547.8046875\n",
      "    num_agent_steps_sampled: 2130000\n",
      "    num_agent_steps_trained: 2130000\n",
      "    num_steps_sampled: 2130000\n",
      "    num_steps_trained: 2130000\n",
      "  iterations_since_restore: 142\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 23.505263157894735\n",
      "    ram_util_percent: 30.989473684210537\n",
      "  pid: 307\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06563142660153583\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 3.465605818166307\n",
      "    mean_inference_ms: 0.48365656036422505\n",
      "    mean_raw_obs_processing_ms: 3.7825574800321937\n",
      "  time_since_restore: 3833.905244588852\n",
      "  time_this_iter_s: 26.552273750305176\n",
      "  time_total_s: 3833.905244588852\n",
      "  timers:\n",
      "    learn_throughput: 6747.916\n",
      "    learn_time_ms: 2222.909\n",
      "    load_throughput: 9886164.145\n",
      "    load_time_ms: 1.517\n",
      "    sample_throughput: 637.794\n",
      "    sample_time_ms: 23518.579\n",
      "    update_time_ms: 1.472\n",
      "  timestamp: 1665249031\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2130000\n",
      "  training_iteration: 142\n",
      "  trial_id: 26f87_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Memory usage on this node: 9.6/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.23 GiB heap, 0.0/14.65 GiB objects (0.0/6.0 CPU_group_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_2_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_1_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_5_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_4_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_0_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_3_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 accelerator_type:G)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_onelane_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_26f87_00000 | RUNNING  | 141.225.10.229:307 |    142 |          3833.91 | 2130000 |  -1663.7 |              2558.79 |             -7913.93 |               3000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m ring length: 249\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m v_max: 4.617190445131122\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m ring length: 222\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m v_max: 3.3329270738617227\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m ring length: 262\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m v_max: 5.234739483008334\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m ring length: 248\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m v_max: 4.569659527528101\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m ring length: 229\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m v_max: 3.666034803266416\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 1 seconds\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_26f87_00000:\n",
      "  agent_timesteps_total: 2145000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-08_12-10-56\n",
      "  done: false\n",
      "  episode_len_mean: 3000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2558.7908574319563\n",
      "  episode_reward_mean: -1590.7573211423164\n",
      "  episode_reward_min: -7913.932615843404\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 715\n",
      "  experiment_id: edcbe001b14143f4bf76a6a2914a7bd0\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6164836287498474\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: -0.6202075481414795\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013806946575641632\n",
      "          model: {}\n",
      "          policy_loss: 0.004147643223404884\n",
      "          total_loss: 3250.4423828125\n",
      "          vf_explained_var: -5.094414223805188e-09\n",
      "          vf_loss: 3250.429931640625\n",
      "    num_agent_steps_sampled: 2145000\n",
      "    num_agent_steps_trained: 2145000\n",
      "    num_steps_sampled: 2145000\n",
      "    num_steps_trained: 2145000\n",
      "  iterations_since_restore: 143\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 24.84722222222222\n",
      "    ram_util_percent: 30.96388888888889\n",
      "  pid: 307\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06563164140336454\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 3.4655498739127695\n",
      "    mean_inference_ms: 0.4836876567276785\n",
      "    mean_raw_obs_processing_ms: 3.780339417915318\n",
      "  time_since_restore: 3858.9912855625153\n",
      "  time_this_iter_s: 25.08604097366333\n",
      "  time_total_s: 3858.9912855625153\n",
      "  timers:\n",
      "    learn_throughput: 6728.841\n",
      "    learn_time_ms: 2229.21\n",
      "    load_throughput: 9981526.55\n",
      "    load_time_ms: 1.503\n",
      "    sample_throughput: 639.802\n",
      "    sample_time_ms: 23444.747\n",
      "    update_time_ms: 1.467\n",
      "  timestamp: 1665249056\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2145000\n",
      "  training_iteration: 143\n",
      "  trial_id: 26f87_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 9.7/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.23 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_3_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_2_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_0_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_5_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_4_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_1_25daa8673b2c24aecf80a7fc79df4e93, 0.0/6.0 CPU_group_25daa8673b2c24aecf80a7fc79df4e93)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_onelane_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_26f87_00000 | RUNNING  | 141.225.10.229:307 |    143 |          3858.99 | 2145000 | -1590.76 |              2558.79 |             -7913.93 |               3000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m ring length: 257\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m v_max: 4.997304682316517\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m ring length: 220\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m v_max: 3.2377399006821497\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m ring length: 257\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m v_max: 4.997304682316517\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m ring length: 270\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m v_max: 5.6143732387852054\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m ring length: 245\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m v_max: 4.427046998576354\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 1 seconds\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_26f87_00000:\n",
      "  agent_timesteps_total: 2160000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-08_12-11-22\n",
      "  done: false\n",
      "  episode_len_mean: 3000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2558.7908574319563\n",
      "  episode_reward_mean: -1532.9702001293167\n",
      "  episode_reward_min: -7913.932615843404\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 720\n",
      "  experiment_id: edcbe001b14143f4bf76a6a2914a7bd0\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6164836287498474\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.08717840164899826\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016705622896552086\n",
      "          model: {}\n",
      "          policy_loss: 0.008063146844506264\n",
      "          total_loss: 3215.640869140625\n",
      "          vf_explained_var: 1.528324244937096e-09\n",
      "          vf_loss: 3215.6220703125\n",
      "    num_agent_steps_sampled: 2160000\n",
      "    num_agent_steps_trained: 2160000\n",
      "    num_steps_sampled: 2160000\n",
      "    num_steps_trained: 2160000\n",
      "  iterations_since_restore: 144\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 23.94324324324324\n",
      "    ram_util_percent: 30.93783783783784\n",
      "  pid: 307\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06563159885865762\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 3.465488364629444\n",
      "    mean_inference_ms: 0.4837178760179245\n",
      "    mean_raw_obs_processing_ms: 3.7781701063628943\n",
      "  time_since_restore: 3885.152812242508\n",
      "  time_this_iter_s: 26.161526679992676\n",
      "  time_total_s: 3885.152812242508\n",
      "  timers:\n",
      "    learn_throughput: 6571.6\n",
      "    learn_time_ms: 2282.549\n",
      "    load_throughput: 9062364.6\n",
      "    load_time_ms: 1.655\n",
      "    sample_throughput: 638.072\n",
      "    sample_time_ms: 23508.321\n",
      "    update_time_ms: 1.474\n",
      "  timestamp: 1665249082\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2160000\n",
      "  training_iteration: 144\n",
      "  trial_id: 26f87_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Memory usage on this node: 9.7/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.23 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_2_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_4_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_3_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_1_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_5_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_0_25daa8673b2c24aecf80a7fc79df4e93, 0.0/6.0 CPU_group_25daa8673b2c24aecf80a7fc79df4e93)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_onelane_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_26f87_00000 | RUNNING  | 141.225.10.229:307 |    144 |          3885.15 | 2160000 | -1532.97 |              2558.79 |             -7913.93 |               3000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m ring length: 226\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m v_max: 3.5232840694769565\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m ring length: 249\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m v_max: 4.617190445131122\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m ring length: 243\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m v_max: 4.331956438196479\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m ring length: 241\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m v_max: 4.236854288051661\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m ring length: 237\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m v_max: 4.04661795519353\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 1 seconds\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_26f87_00000:\n",
      "  agent_timesteps_total: 2175000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-08_12-11-48\n",
      "  done: false\n",
      "  episode_len_mean: 3000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2558.7908574319563\n",
      "  episode_reward_mean: -1487.5969035306753\n",
      "  episode_reward_min: -7913.932615843404\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 725\n",
      "  experiment_id: edcbe001b14143f4bf76a6a2914a7bd0\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6164836287498474\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.9088347554206848\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015675466507673264\n",
      "          model: {}\n",
      "          policy_loss: 0.008759738877415657\n",
      "          total_loss: 3387.67529296875\n",
      "          vf_explained_var: -1.528324244937096e-09\n",
      "          vf_loss: 3387.6572265625\n",
      "    num_agent_steps_sampled: 2175000\n",
      "    num_agent_steps_trained: 2175000\n",
      "    num_steps_sampled: 2175000\n",
      "    num_steps_trained: 2175000\n",
      "  iterations_since_restore: 145\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 24.23513513513514\n",
      "    ram_util_percent: 30.95945945945946\n",
      "  pid: 307\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06563180206640759\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 3.4654731000688286\n",
      "    mean_inference_ms: 0.4837502385520148\n",
      "    mean_raw_obs_processing_ms: 3.7752655317829427\n",
      "  time_since_restore: 3910.843968153\n",
      "  time_this_iter_s: 25.691155910491943\n",
      "  time_total_s: 3910.843968153\n",
      "  timers:\n",
      "    learn_throughput: 6474.077\n",
      "    learn_time_ms: 2316.933\n",
      "    load_throughput: 9271365.626\n",
      "    load_time_ms: 1.618\n",
      "    sample_throughput: 639.6\n",
      "    sample_time_ms: 23452.155\n",
      "    update_time_ms: 1.317\n",
      "  timestamp: 1665249108\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2175000\n",
      "  training_iteration: 145\n",
      "  trial_id: 26f87_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 9.7/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.23 GiB heap, 0.0/14.65 GiB objects (0.0/6.0 CPU_group_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_2_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_3_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_1_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_0_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_5_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_4_25daa8673b2c24aecf80a7fc79df4e93)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_onelane_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_26f87_00000 | RUNNING  | 141.225.10.229:307 |    145 |          3910.84 | 2175000 |  -1487.6 |              2558.79 |             -7913.93 |               3000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m ring length: 266\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m v_max: 5.42459972166245\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m ring length: 270\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m v_max: 5.6143732387852054\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m ring length: 235\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m v_max: 3.9514850725282584\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m ring length: 241\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m v_max: 4.236854288051661\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m ring length: 259\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m v_max: 5.092292348291853\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 1 seconds\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_26f87_00000:\n",
      "  agent_timesteps_total: 2190000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-08_12-12-13\n",
      "  done: false\n",
      "  episode_len_mean: 3000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2558.7908574319563\n",
      "  episode_reward_mean: -1570.1820832268847\n",
      "  episode_reward_min: -7913.932615843404\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 730\n",
      "  experiment_id: edcbe001b14143f4bf76a6a2914a7bd0\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6164836287498474\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.4574085474014282\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015185565687716007\n",
      "          model: {}\n",
      "          policy_loss: 0.007776038721203804\n",
      "          total_loss: 3431.861572265625\n",
      "          vf_explained_var: 1.528324244937096e-09\n",
      "          vf_loss: 3431.843994140625\n",
      "    num_agent_steps_sampled: 2190000\n",
      "    num_agent_steps_trained: 2190000\n",
      "    num_steps_sampled: 2190000\n",
      "    num_steps_trained: 2190000\n",
      "  iterations_since_restore: 146\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 24.035135135135135\n",
      "    ram_util_percent: 30.96216216216216\n",
      "  pid: 307\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06563237053974959\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 3.465465692495611\n",
      "    mean_inference_ms: 0.48378336555626517\n",
      "    mean_raw_obs_processing_ms: 3.772376825997985\n",
      "  time_since_restore: 3936.2679097652435\n",
      "  time_this_iter_s: 25.423941612243652\n",
      "  time_total_s: 3936.2679097652435\n",
      "  timers:\n",
      "    learn_throughput: 6356.489\n",
      "    learn_time_ms: 2359.793\n",
      "    load_throughput: 10337757.768\n",
      "    load_time_ms: 1.451\n",
      "    sample_throughput: 642.542\n",
      "    sample_time_ms: 23344.77\n",
      "    update_time_ms: 1.35\n",
      "  timestamp: 1665249133\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2190000\n",
      "  training_iteration: 146\n",
      "  trial_id: 26f87_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Memory usage on this node: 9.7/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.23 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_0_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_4_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_5_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_3_25daa8673b2c24aecf80a7fc79df4e93, 0.0/6.0 CPU_group_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_1_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_2_25daa8673b2c24aecf80a7fc79df4e93)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_onelane_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_26f87_00000 | RUNNING  | 141.225.10.229:307 |    146 |          3936.27 | 2190000 | -1570.18 |              2558.79 |             -7913.93 |               3000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m ring length: 270\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m v_max: 5.6143732387852054\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m ring length: 223\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m v_max: 3.3805185755587788\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m ring length: 223\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m v_max: 3.3805185755587788\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m ring length: 257\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m v_max: 4.997304682316517\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m ring length: 240\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m v_max: 4.189299083856172\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 1 seconds\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_26f87_00000:\n",
      "  agent_timesteps_total: 2205000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-08_12-12-39\n",
      "  done: false\n",
      "  episode_len_mean: 3000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2558.7908574319563\n",
      "  episode_reward_mean: -1600.3342705959353\n",
      "  episode_reward_min: -7913.932615843404\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 735\n",
      "  experiment_id: edcbe001b14143f4bf76a6a2914a7bd0\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6164836287498474\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: -0.17516162991523743\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.03203602880239487\n",
      "          model: {}\n",
      "          policy_loss: 0.01404493860900402\n",
      "          total_loss: 3328.692138671875\n",
      "          vf_explained_var: 3.5660898678457897e-09\n",
      "          vf_loss: 3328.65869140625\n",
      "    num_agent_steps_sampled: 2205000\n",
      "    num_agent_steps_trained: 2205000\n",
      "    num_steps_sampled: 2205000\n",
      "    num_steps_trained: 2205000\n",
      "  iterations_since_restore: 147\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 24.50277777777778\n",
      "    ram_util_percent: 30.97222222222222\n",
      "  pid: 307\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06563309216043625\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 3.4654710242756965\n",
      "    mean_inference_ms: 0.4838168219540267\n",
      "    mean_raw_obs_processing_ms: 3.769628599098311\n",
      "  time_since_restore: 3961.543600797653\n",
      "  time_this_iter_s: 25.275691032409668\n",
      "  time_total_s: 3961.543600797653\n",
      "  timers:\n",
      "    learn_throughput: 6407.337\n",
      "    learn_time_ms: 2341.066\n",
      "    load_throughput: 10382968.611\n",
      "    load_time_ms: 1.445\n",
      "    sample_throughput: 642.076\n",
      "    sample_time_ms: 23361.738\n",
      "    update_time_ms: 1.369\n",
      "  timestamp: 1665249159\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2205000\n",
      "  training_iteration: 147\n",
      "  trial_id: 26f87_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 9.7/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.23 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_4_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_2_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_5_25daa8673b2c24aecf80a7fc79df4e93, 0.0/6.0 CPU_group_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_0_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_3_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_1_25daa8673b2c24aecf80a7fc79df4e93)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_onelane_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_26f87_00000 | RUNNING  | 141.225.10.229:307 |    147 |          3961.54 | 2205000 | -1600.33 |              2558.79 |             -7913.93 |               3000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m ring length: 252\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m v_max: 4.759762049824181\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m ring length: 251\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m v_max: 4.71224180704279\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m ring length: 244\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m v_max: 4.379503211387676\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m ring length: 220\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m v_max: 3.2377399006821497\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m ring length: 238\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m v_max: 4.094180836186086\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 1 seconds\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_26f87_00000:\n",
      "  agent_timesteps_total: 2220000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-08_12-13-04\n",
      "  done: false\n",
      "  episode_len_mean: 3000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2558.7908574319563\n",
      "  episode_reward_mean: -1651.2138542166194\n",
      "  episode_reward_min: -7913.932615843404\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 740\n",
      "  experiment_id: edcbe001b14143f4bf76a6a2914a7bd0\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6164836287498474\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.3996723890304565\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01591774821281433\n",
      "          model: {}\n",
      "          policy_loss: 0.007999471388757229\n",
      "          total_loss: 3527.135498046875\n",
      "          vf_explained_var: 1.528324244937096e-09\n",
      "          vf_loss: 3527.118408203125\n",
      "    num_agent_steps_sampled: 2220000\n",
      "    num_agent_steps_trained: 2220000\n",
      "    num_steps_sampled: 2220000\n",
      "    num_steps_trained: 2220000\n",
      "  iterations_since_restore: 148\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 24.48857142857143\n",
      "    ram_util_percent: 30.962857142857143\n",
      "  pid: 307\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06563333454394774\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 3.4654478977925254\n",
      "    mean_inference_ms: 0.4838461584853792\n",
      "    mean_raw_obs_processing_ms: 3.7660612165485627\n",
      "  time_since_restore: 3986.3679428100586\n",
      "  time_this_iter_s: 24.824342012405396\n",
      "  time_total_s: 3986.3679428100586\n",
      "  timers:\n",
      "    learn_throughput: 6358.962\n",
      "    learn_time_ms: 2358.876\n",
      "    load_throughput: 10536687.322\n",
      "    load_time_ms: 1.424\n",
      "    sample_throughput: 643.744\n",
      "    sample_time_ms: 23301.197\n",
      "    update_time_ms: 1.395\n",
      "  timestamp: 1665249184\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2220000\n",
      "  training_iteration: 148\n",
      "  trial_id: 26f87_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Memory usage on this node: 9.7/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.23 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_3_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_0_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_5_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_2_25daa8673b2c24aecf80a7fc79df4e93, 0.0/6.0 CPU_group_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_4_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_1_25daa8673b2c24aecf80a7fc79df4e93)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_onelane_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_26f87_00000 | RUNNING  | 141.225.10.229:307 |    148 |          3986.37 | 2220000 | -1651.21 |              2558.79 |             -7913.93 |               3000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m ring length: 262\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m v_max: 5.234739483008334\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m ring length: 259\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m v_max: 5.092292348291853\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m ring length: 235\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m v_max: 3.9514850725282584\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m ring length: 239\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m v_max: 4.141741239205832\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m ring length: 236\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m v_max: 3.999052674951912\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m Could not connect to TraCI server at localhost:34795 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m Could not connect to TraCI server at localhost:34447 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m Could not connect to TraCI server at localhost:34795 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 3 seconds\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m Could not connect to TraCI server at localhost:34447 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 3 seconds\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_26f87_00000:\n",
      "  agent_timesteps_total: 2235000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-08_12-13-35\n",
      "  done: false\n",
      "  episode_len_mean: 3000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2558.7908574319563\n",
      "  episode_reward_mean: -1796.3791237713888\n",
      "  episode_reward_min: -7913.932615843404\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 745\n",
      "  experiment_id: edcbe001b14143f4bf76a6a2914a7bd0\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6164836287498474\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 2.609640121459961\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01615944504737854\n",
      "          model: {}\n",
      "          policy_loss: 0.007716448977589607\n",
      "          total_loss: 3508.667236328125\n",
      "          vf_explained_var: -4.58497284583359e-09\n",
      "          vf_loss: 3508.649658203125\n",
      "    num_agent_steps_sampled: 2235000\n",
      "    num_agent_steps_trained: 2235000\n",
      "    num_steps_sampled: 2235000\n",
      "    num_steps_trained: 2235000\n",
      "  iterations_since_restore: 149\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.865909090909092\n",
      "    ram_util_percent: 30.940909090909088\n",
      "  pid: 307\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06563348966753972\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 3.465395532776595\n",
      "    mean_inference_ms: 0.48387480379708636\n",
      "    mean_raw_obs_processing_ms: 3.763076904591836\n",
      "  time_since_restore: 4017.153605699539\n",
      "  time_this_iter_s: 30.78566288948059\n",
      "  time_total_s: 4017.153605699539\n",
      "  timers:\n",
      "    learn_throughput: 6373.142\n",
      "    learn_time_ms: 2353.627\n",
      "    load_throughput: 10404432.023\n",
      "    load_time_ms: 1.442\n",
      "    sample_throughput: 629.699\n",
      "    sample_time_ms: 23820.893\n",
      "    update_time_ms: 1.377\n",
      "  timestamp: 1665249215\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2235000\n",
      "  training_iteration: 149\n",
      "  trial_id: 26f87_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 9.7/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.23 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_4_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_5_25daa8673b2c24aecf80a7fc79df4e93, 0.0/6.0 CPU_group_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_3_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_0_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_2_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_1_25daa8673b2c24aecf80a7fc79df4e93)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_onelane_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_26f87_00000 | RUNNING  | 141.225.10.229:307 |    149 |          4017.15 | 2235000 | -1796.38 |              2558.79 |             -7913.93 |               3000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m ring length: 256\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m v_max: 4.949804327743507\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m ring length: 251\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m v_max: 4.71224180704279\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m ring length: 237\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m v_max: 4.04661795519353\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m ring length: 262\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m v_max: 5.234739483008334\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m ring length: 241\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m v_max: 4.236854288051661\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 1 seconds\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_26f87_00000:\n",
      "  agent_timesteps_total: 2250000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-08_12-14-00\n",
      "  done: false\n",
      "  episode_len_mean: 3000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2558.7908574319563\n",
      "  episode_reward_mean: -1831.6126700438015\n",
      "  episode_reward_min: -7913.932615843404\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 750\n",
      "  experiment_id: edcbe001b14143f4bf76a6a2914a7bd0\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6164836287498474\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: -1.4097710847854614\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01809283345937729\n",
      "          model: {}\n",
      "          policy_loss: 0.0048157162964344025\n",
      "          total_loss: 3037.162353515625\n",
      "          vf_explained_var: -6.113296979748384e-09\n",
      "          vf_loss: 3037.14599609375\n",
      "    num_agent_steps_sampled: 2250000\n",
      "    num_agent_steps_trained: 2250000\n",
      "    num_steps_sampled: 2250000\n",
      "    num_steps_trained: 2250000\n",
      "  iterations_since_restore: 150\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 24.330555555555556\n",
      "    ram_util_percent: 30.99166666666666\n",
      "  pid: 307\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06563321509052242\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 3.465291354783728\n",
      "    mean_inference_ms: 0.48390004625056654\n",
      "    mean_raw_obs_processing_ms: 3.7600831842221782\n",
      "  time_since_restore: 4041.9907281398773\n",
      "  time_this_iter_s: 24.837122440338135\n",
      "  time_total_s: 4041.9907281398773\n",
      "  timers:\n",
      "    learn_throughput: 6486.36\n",
      "    learn_time_ms: 2312.545\n",
      "    load_throughput: 9366327.731\n",
      "    load_time_ms: 1.601\n",
      "    sample_throughput: 633.097\n",
      "    sample_time_ms: 23693.044\n",
      "    update_time_ms: 1.358\n",
      "  timestamp: 1665249240\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2250000\n",
      "  training_iteration: 150\n",
      "  trial_id: 26f87_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Memory usage on this node: 9.7/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.23 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_5_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_2_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_0_25daa8673b2c24aecf80a7fc79df4e93, 0.0/6.0 CPU_group_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_1_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_3_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_4_25daa8673b2c24aecf80a7fc79df4e93)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_onelane_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_26f87_00000 | RUNNING  | 141.225.10.229:307 |    150 |          4041.99 | 2250000 | -1831.61 |              2558.79 |             -7913.93 |               3000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m ring length: 226\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m v_max: 3.5232840694769565\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m ring length: 230\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m v_max: 3.7136148111012934\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m ring length: 232\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m v_max: 3.8087690783250063\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m ring length: 220\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m v_max: 3.2377399006821497\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m ring length: 223\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m v_max: 3.3805185755587788\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 1 seconds\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_26f87_00000:\n",
      "  agent_timesteps_total: 2265000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-08_12-14-25\n",
      "  done: false\n",
      "  episode_len_mean: 3000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2558.7908574319563\n",
      "  episode_reward_mean: -1743.9328759637078\n",
      "  episode_reward_min: -7913.932615843404\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 755\n",
      "  experiment_id: edcbe001b14143f4bf76a6a2914a7bd0\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6164836287498474\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: -1.1259795427322388\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.020439255982637405\n",
      "          model: {}\n",
      "          policy_loss: 0.006509965751320124\n",
      "          total_loss: 3081.5029296875\n",
      "          vf_explained_var: 9.16994569166718e-09\n",
      "          vf_loss: 3081.4833984375\n",
      "    num_agent_steps_sampled: 2265000\n",
      "    num_agent_steps_trained: 2265000\n",
      "    num_steps_sampled: 2265000\n",
      "    num_steps_trained: 2265000\n",
      "  iterations_since_restore: 151\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 24.00277777777778\n",
      "    ram_util_percent: 31.05555555555555\n",
      "  pid: 307\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06563269853227462\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 3.4652074246952607\n",
      "    mean_inference_ms: 0.48392306781500694\n",
      "    mean_raw_obs_processing_ms: 3.757121829524201\n",
      "  time_since_restore: 4067.4778587818146\n",
      "  time_this_iter_s: 25.487130641937256\n",
      "  time_total_s: 4067.4778587818146\n",
      "  timers:\n",
      "    learn_throughput: 6513.253\n",
      "    learn_time_ms: 2302.997\n",
      "    load_throughput: 8941807.845\n",
      "    load_time_ms: 1.678\n",
      "    sample_throughput: 632.915\n",
      "    sample_time_ms: 23699.865\n",
      "    update_time_ms: 1.198\n",
      "  timestamp: 1665249265\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2265000\n",
      "  training_iteration: 151\n",
      "  trial_id: 26f87_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 9.7/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.23 GiB heap, 0.0/14.65 GiB objects (0.0/6.0 CPU_group_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_3_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_2_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_0_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_1_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_5_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_4_25daa8673b2c24aecf80a7fc79df4e93)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_onelane_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_26f87_00000 | RUNNING  | 141.225.10.229:307 |    151 |          4067.48 | 2265000 | -1743.93 |              2558.79 |             -7913.93 |               3000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m ring length: 244\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m v_max: 4.379503211387676\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m ring length: 256\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m v_max: 4.949804327743507\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m ring length: 234\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m v_max: 3.903915223349057\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m ring length: 252\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m v_max: 4.759762049824181\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m ring length: 259\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m v_max: 5.092292348291853\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 1 seconds\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_26f87_00000:\n",
      "  agent_timesteps_total: 2280000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-08_12-14-50\n",
      "  done: false\n",
      "  episode_len_mean: 3000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2558.7908574319563\n",
      "  episode_reward_mean: -1756.819973401973\n",
      "  episode_reward_min: -7913.932615843404\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 760\n",
      "  experiment_id: edcbe001b14143f4bf76a6a2914a7bd0\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6164836287498474\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 2.7895357608795166\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.003263790626078844\n",
      "          model: {}\n",
      "          policy_loss: 0.0022211300674825907\n",
      "          total_loss: 3397.693115234375\n",
      "          vf_explained_var: -3.5660898678457897e-09\n",
      "          vf_loss: 3397.688720703125\n",
      "    num_agent_steps_sampled: 2280000\n",
      "    num_agent_steps_trained: 2280000\n",
      "    num_steps_sampled: 2280000\n",
      "    num_steps_trained: 2280000\n",
      "  iterations_since_restore: 152\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 24.58333333333333\n",
      "    ram_util_percent: 31.03055555555555\n",
      "  pid: 307\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06563267160880465\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 3.4650957216956977\n",
      "    mean_inference_ms: 0.483947718869285\n",
      "    mean_raw_obs_processing_ms: 3.75426723094583\n",
      "  time_since_restore: 4092.1414709091187\n",
      "  time_this_iter_s: 24.663612127304077\n",
      "  time_total_s: 4092.1414709091187\n",
      "  timers:\n",
      "    learn_throughput: 6603.114\n",
      "    learn_time_ms: 2271.656\n",
      "    load_throughput: 8180921.669\n",
      "    load_time_ms: 1.834\n",
      "    sample_throughput: 637.145\n",
      "    sample_time_ms: 23542.506\n",
      "    update_time_ms: 1.178\n",
      "  timestamp: 1665249290\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2280000\n",
      "  training_iteration: 152\n",
      "  trial_id: 26f87_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Memory usage on this node: 9.7/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.23 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_1_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_3_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_2_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_5_25daa8673b2c24aecf80a7fc79df4e93, 0.0/6.0 CPU_group_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_0_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_4_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 accelerator_type:G)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_onelane_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_26f87_00000 | RUNNING  | 141.225.10.229:307 |    152 |          4092.14 | 2280000 | -1756.82 |              2558.79 |             -7913.93 |               3000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m ring length: 235\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m v_max: 3.9514850725282584\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m ring length: 255\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m v_max: 4.902299776966978\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m ring length: 233\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m v_max: 3.856343201216534\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m ring length: 245\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m v_max: 4.427046998576354\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m ring length: 264\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m v_max: 5.329679917416892\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 1 seconds\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_26f87_00000:\n",
      "  agent_timesteps_total: 2295000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-08_12-15-15\n",
      "  done: false\n",
      "  episode_len_mean: 3000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1938.9329557600133\n",
      "  episode_reward_mean: -1822.407689347823\n",
      "  episode_reward_min: -7913.932615843404\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 765\n",
      "  experiment_id: edcbe001b14143f4bf76a6a2914a7bd0\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3082418143749237\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.850398600101471\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.04133685678243637\n",
      "          model: {}\n",
      "          policy_loss: 0.018528549000620842\n",
      "          total_loss: 3432.212890625\n",
      "          vf_explained_var: -7.641621557752387e-09\n",
      "          vf_loss: 3432.181884765625\n",
      "    num_agent_steps_sampled: 2295000\n",
      "    num_agent_steps_trained: 2295000\n",
      "    num_steps_sampled: 2295000\n",
      "    num_steps_trained: 2295000\n",
      "  iterations_since_restore: 153\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 25.00857142857143\n",
      "    ram_util_percent: 31.168571428571433\n",
      "  pid: 307\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06563253696484744\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 3.464945891854125\n",
      "    mean_inference_ms: 0.4839714169087241\n",
      "    mean_raw_obs_processing_ms: 3.751352447357958\n",
      "  time_since_restore: 4116.977411270142\n",
      "  time_this_iter_s: 24.83594036102295\n",
      "  time_total_s: 4116.977411270142\n",
      "  timers:\n",
      "    learn_throughput: 6625.981\n",
      "    learn_time_ms: 2263.816\n",
      "    load_throughput: 7346827.816\n",
      "    load_time_ms: 2.042\n",
      "    sample_throughput: 637.617\n",
      "    sample_time_ms: 23525.089\n",
      "    update_time_ms: 1.199\n",
      "  timestamp: 1665249315\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2295000\n",
      "  training_iteration: 153\n",
      "  trial_id: 26f87_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 9.7/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.23 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_2_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_0_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_1_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_3_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_4_25daa8673b2c24aecf80a7fc79df4e93, 0.0/6.0 CPU_group_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_5_25daa8673b2c24aecf80a7fc79df4e93)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_onelane_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_26f87_00000 | RUNNING  | 141.225.10.229:307 |    153 |          4116.98 | 2295000 | -1822.41 |              1938.93 |             -7913.93 |               3000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m ring length: 235\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m v_max: 3.9514850725282584\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m ring length: 227\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m v_max: 3.570869368805752\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m ring length: 265\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m v_max: 5.37714246265477\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m ring length: 261\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m v_max: 5.187261846097525\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m ring length: 241\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m v_max: 4.236854288051661\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 1 seconds\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_26f87_00000:\n",
      "  agent_timesteps_total: 2310000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-08_12-15-41\n",
      "  done: false\n",
      "  episode_len_mean: 3000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1938.9329557600133\n",
      "  episode_reward_mean: -1859.9138676134964\n",
      "  episode_reward_min: -7913.932615843404\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 770\n",
      "  experiment_id: edcbe001b14143f4bf76a6a2914a7bd0\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.46236270666122437\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.492295265197754\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.030039526522159576\n",
      "          model: {}\n",
      "          policy_loss: 0.014369728043675423\n",
      "          total_loss: 3596.3544921875\n",
      "          vf_explained_var: -2.547207111902594e-09\n",
      "          vf_loss: 3596.325927734375\n",
      "    num_agent_steps_sampled: 2310000\n",
      "    num_agent_steps_trained: 2310000\n",
      "    num_steps_sampled: 2310000\n",
      "    num_steps_trained: 2310000\n",
      "  iterations_since_restore: 154\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 24.62702702702703\n",
      "    ram_util_percent: 31.078378378378364\n",
      "  pid: 307\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06563295296306103\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 3.464814618227444\n",
      "    mean_inference_ms: 0.4839988748528718\n",
      "    mean_raw_obs_processing_ms: 3.748464956974904\n",
      "  time_since_restore: 4142.792519330978\n",
      "  time_this_iter_s: 25.815108060836792\n",
      "  time_total_s: 4142.792519330978\n",
      "  timers:\n",
      "    learn_throughput: 6593.576\n",
      "    learn_time_ms: 2274.941\n",
      "    load_throughput: 7268653.819\n",
      "    load_time_ms: 2.064\n",
      "    sample_throughput: 638.868\n",
      "    sample_time_ms: 23479.019\n",
      "    update_time_ms: 1.207\n",
      "  timestamp: 1665249341\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2310000\n",
      "  training_iteration: 154\n",
      "  trial_id: 26f87_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Memory usage on this node: 9.7/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.23 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_5_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_2_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_0_25daa8673b2c24aecf80a7fc79df4e93, 0.0/6.0 CPU_group_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_4_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_1_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_3_25daa8673b2c24aecf80a7fc79df4e93)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_onelane_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_26f87_00000 | RUNNING  | 141.225.10.229:307 |    154 |          4142.79 | 2310000 | -1859.91 |              1938.93 |             -7913.93 |               3000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m ring length: 231\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m v_max: 3.761192925277308\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m ring length: 250\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m v_max: 4.664717904914125\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m ring length: 223\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m v_max: 3.3805185755587788\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m ring length: 252\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m v_max: 4.759762049824181\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m ring length: 233\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m v_max: 3.856343201216534\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 1 seconds\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_26f87_00000:\n",
      "  agent_timesteps_total: 2325000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-08_12-16-06\n",
      "  done: false\n",
      "  episode_len_mean: 3000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1938.9329557600133\n",
      "  episode_reward_mean: -2013.0283047228947\n",
      "  episode_reward_min: -7913.932615843404\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 775\n",
      "  experiment_id: edcbe001b14143f4bf76a6a2914a7bd0\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.46236270666122437\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 2.331838607788086\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.036527082324028015\n",
      "          model: {}\n",
      "          policy_loss: 0.022703664377331734\n",
      "          total_loss: 3521.374755859375\n",
      "          vf_explained_var: -3.056648489874192e-09\n",
      "          vf_loss: 3521.3349609375\n",
      "    num_agent_steps_sampled: 2325000\n",
      "    num_agent_steps_trained: 2325000\n",
      "    num_steps_sampled: 2325000\n",
      "    num_steps_trained: 2325000\n",
      "  iterations_since_restore: 155\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 23.889189189189196\n",
      "    ram_util_percent: 31.059459459459447\n",
      "  pid: 307\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06563349522330236\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 3.4646611055370475\n",
      "    mean_inference_ms: 0.4840267639154653\n",
      "    mean_raw_obs_processing_ms: 3.745556898132959\n",
      "  time_since_restore: 4168.070684194565\n",
      "  time_this_iter_s: 25.278164863586426\n",
      "  time_total_s: 4168.070684194565\n",
      "  timers:\n",
      "    learn_throughput: 6637.365\n",
      "    learn_time_ms: 2259.933\n",
      "    load_throughput: 6665454.661\n",
      "    load_time_ms: 2.25\n",
      "    sample_throughput: 639.593\n",
      "    sample_time_ms: 23452.42\n",
      "    update_time_ms: 1.243\n",
      "  timestamp: 1665249366\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2325000\n",
      "  training_iteration: 155\n",
      "  trial_id: 26f87_00000\n",
      "  \n",
      "\u001b[2m\u001b[36m(pid=307)\u001b[0m 2022-10-08 12:16:06,700\tWARNING ppo.py:242 -- The magnitude of your environment rewards are more than 201.0x the scale of `vf_clip_param`. This means that it will take more than 201.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "== Status ==\n",
      "Memory usage on this node: 9.7/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.23 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_3_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_0_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 accelerator_type:G, 0.0/6.0 CPU_group_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_4_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_5_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_1_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_2_25daa8673b2c24aecf80a7fc79df4e93)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_onelane_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_26f87_00000 | RUNNING  | 141.225.10.229:307 |    155 |          4168.07 | 2325000 | -2013.03 |              1938.93 |             -7913.93 |               3000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m ring length: 245\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m v_max: 4.427046998576354\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m ring length: 242\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m v_max: 4.2844067680020546\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m ring length: 255\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m v_max: 4.902299776966978\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m ring length: 268\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m v_max: 5.5194978538368025\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m ring length: 229\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m v_max: 3.666034803266416\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m Could not connect to TraCI server at localhost:34795 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 2 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m Could not connect to TraCI server at localhost:34447 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m Could not connect to TraCI server at localhost:54035 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m Could not connect to TraCI server at localhost:36289 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m Could not connect to TraCI server at localhost:57509 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m Could not connect to TraCI server at localhost:34795 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 3 seconds\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m Could not connect to TraCI server at localhost:34447 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 3 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m Could not connect to TraCI server at localhost:54035 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 3 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m Could not connect to TraCI server at localhost:36289 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 3 seconds\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m Could not connect to TraCI server at localhost:57509 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 3 seconds\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_26f87_00000:\n",
      "  agent_timesteps_total: 2340000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-08_12-16-37\n",
      "  done: false\n",
      "  episode_len_mean: 3000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1938.9329557600133\n",
      "  episode_reward_mean: -2101.9631489178278\n",
      "  episode_reward_min: -7913.932615843404\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 780\n",
      "  experiment_id: edcbe001b14143f4bf76a6a2914a7bd0\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.46236270666122437\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.926924228668213\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016672994941473007\n",
      "          model: {}\n",
      "          policy_loss: 0.008804179728031158\n",
      "          total_loss: 3565.983154296875\n",
      "          vf_explained_var: -6.113296979748384e-09\n",
      "          vf_loss: 3565.966552734375\n",
      "    num_agent_steps_sampled: 2340000\n",
      "    num_agent_steps_trained: 2340000\n",
      "    num_steps_sampled: 2340000\n",
      "    num_steps_trained: 2340000\n",
      "  iterations_since_restore: 156\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 22.42093023255814\n",
      "    ram_util_percent: 31.01627906976744\n",
      "  pid: 307\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06563385767221804\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 3.464481630668041\n",
      "    mean_inference_ms: 0.48405213513783935\n",
      "    mean_raw_obs_processing_ms: 3.7431653339942352\n",
      "  time_since_restore: 4198.542078256607\n",
      "  time_this_iter_s: 30.471394062042236\n",
      "  time_total_s: 4198.542078256607\n",
      "  timers:\n",
      "    learn_throughput: 6621.486\n",
      "    learn_time_ms: 2265.353\n",
      "    load_throughput: 6778490.546\n",
      "    load_time_ms: 2.213\n",
      "    sample_throughput: 626.257\n",
      "    sample_time_ms: 23951.835\n",
      "    update_time_ms: 1.21\n",
      "  timestamp: 1665249397\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2340000\n",
      "  training_iteration: 156\n",
      "  trial_id: 26f87_00000\n",
      "  \n",
      "\u001b[2m\u001b[36m(pid=307)\u001b[0m 2022-10-08 12:16:37,296\tWARNING ppo.py:242 -- The magnitude of your environment rewards are more than 210.0x the scale of `vf_clip_param`. This means that it will take more than 210.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "== Status ==\n",
      "Memory usage on this node: 9.7/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.23 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_5_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_2_25daa8673b2c24aecf80a7fc79df4e93, 0.0/6.0 CPU_group_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_4_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_3_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_0_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_1_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 accelerator_type:G)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_onelane_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_26f87_00000 | RUNNING  | 141.225.10.229:307 |    156 |          4198.54 | 2340000 | -2101.96 |              1938.93 |             -7913.93 |               3000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m ring length: 264\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m v_max: 5.329679917416892\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m ring length: 239\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m v_max: 4.141741239205832\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m ring length: 248\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m v_max: 4.569659527528101\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m ring length: 261\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m v_max: 5.187261846097525\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m ring length: 256\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m v_max: 4.949804327743507\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 1 seconds\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_26f87_00000:\n",
      "  agent_timesteps_total: 2355000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-08_12-17-03\n",
      "  done: false\n",
      "  episode_len_mean: 3000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1938.9329557600133\n",
      "  episode_reward_mean: -2038.8142670484626\n",
      "  episode_reward_min: -7913.932615843404\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 785\n",
      "  experiment_id: edcbe001b14143f4bf76a6a2914a7bd0\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.46236270666122437\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: -2.2800981998443604\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.040900249034166336\n",
      "          model: {}\n",
      "          policy_loss: 0.015501949936151505\n",
      "          total_loss: 2848.3125\n",
      "          vf_explained_var: -1.3245476715439963e-08\n",
      "          vf_loss: 2848.278076171875\n",
      "    num_agent_steps_sampled: 2355000\n",
      "    num_agent_steps_trained: 2355000\n",
      "    num_steps_sampled: 2355000\n",
      "    num_steps_trained: 2355000\n",
      "  iterations_since_restore: 157\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 23.349999999999998\n",
      "    ram_util_percent: 31.052631578947356\n",
      "  pid: 307\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0656334058587038\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 3.46429428759035\n",
      "    mean_inference_ms: 0.4840725544267649\n",
      "    mean_raw_obs_processing_ms: 3.7409210703358866\n",
      "  time_since_restore: 4224.624491691589\n",
      "  time_this_iter_s: 26.0824134349823\n",
      "  time_total_s: 4224.624491691589\n",
      "  timers:\n",
      "    learn_throughput: 6602.625\n",
      "    learn_time_ms: 2271.824\n",
      "    load_throughput: 6266640.105\n",
      "    load_time_ms: 2.394\n",
      "    sample_throughput: 624.321\n",
      "    sample_time_ms: 24026.109\n",
      "    update_time_ms: 1.154\n",
      "  timestamp: 1665249423\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2355000\n",
      "  training_iteration: 157\n",
      "  trial_id: 26f87_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Memory usage on this node: 9.7/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.23 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_1_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_4_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_2_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_3_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_0_25daa8673b2c24aecf80a7fc79df4e93, 0.0/6.0 CPU_group_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_5_25daa8673b2c24aecf80a7fc79df4e93)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_onelane_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_26f87_00000 | RUNNING  | 141.225.10.229:307 |    157 |          4224.62 | 2355000 | -2038.81 |              1938.93 |             -7913.93 |               3000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=307)\u001b[0m 2022-10-08 12:17:03,461\tWARNING ppo.py:242 -- The magnitude of your environment rewards are more than 204.0x the scale of `vf_clip_param`. This means that it will take more than 204.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m ring length: 222\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m v_max: 3.3329270738617227\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m ring length: 249\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m v_max: 4.617190445131122\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m ring length: 236\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m v_max: 3.999052674951912\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m ring length: 269\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m v_max: 5.566938458220921\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m ring length: 243\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m v_max: 4.331956438196479\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 1 seconds\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_26f87_00000:\n",
      "  agent_timesteps_total: 2370000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-08_12-17-29\n",
      "  done: false\n",
      "  episode_len_mean: 3000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1780.9896235052067\n",
      "  episode_reward_mean: -1986.913536419589\n",
      "  episode_reward_min: -7891.230137621037\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 790\n",
      "  experiment_id: edcbe001b14143f4bf76a6a2914a7bd0\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6935440301895142\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: -0.4154224991798401\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.031047126278281212\n",
      "          model: {}\n",
      "          policy_loss: 0.013025345280766487\n",
      "          total_loss: 3372.687255859375\n",
      "          vf_explained_var: -1.2736035337468365e-08\n",
      "          vf_loss: 3372.653564453125\n",
      "    num_agent_steps_sampled: 2370000\n",
      "    num_agent_steps_trained: 2370000\n",
      "    num_steps_sampled: 2370000\n",
      "    num_steps_trained: 2370000\n",
      "  iterations_since_restore: 158\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 23.505405405405405\n",
      "    ram_util_percent: 31.059459459459447\n",
      "  pid: 307\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06563288122915724\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 3.464118777190828\n",
      "    mean_inference_ms: 0.48409201240437383\n",
      "    mean_raw_obs_processing_ms: 3.738775796916838\n",
      "  time_since_restore: 4250.581258535385\n",
      "  time_this_iter_s: 25.956766843795776\n",
      "  time_total_s: 4250.581258535385\n",
      "  timers:\n",
      "    learn_throughput: 6542.76\n",
      "    learn_time_ms: 2292.611\n",
      "    load_throughput: 6333191.734\n",
      "    load_time_ms: 2.368\n",
      "    sample_throughput: 621.94\n",
      "    sample_time_ms: 24118.065\n",
      "    update_time_ms: 1.276\n",
      "  timestamp: 1665249449\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2370000\n",
      "  training_iteration: 158\n",
      "  trial_id: 26f87_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 9.7/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.23 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_0_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_5_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_1_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_3_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_2_25daa8673b2c24aecf80a7fc79df4e93, 0.0/6.0 CPU_group_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_4_25daa8673b2c24aecf80a7fc79df4e93)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_onelane_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_26f87_00000 | RUNNING  | 141.225.10.229:307 |    158 |          4250.58 | 2370000 | -1986.91 |              1780.99 |             -7891.23 |               3000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m ring length: 246\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m v_max: 4.474587708995648\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m ring length: 254\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m v_max: 4.854791141191876\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m ring length: 268\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m v_max: 5.5194978538368025\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m ring length: 228\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m v_max: 3.618452967700356\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m ring length: 266\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m v_max: 5.42459972166245\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 1 seconds\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_26f87_00000:\n",
      "  agent_timesteps_total: 2385000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-08_12-17-56\n",
      "  done: false\n",
      "  episode_len_mean: 3000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1780.9896235052067\n",
      "  episode_reward_mean: -1997.59133868329\n",
      "  episode_reward_min: -7891.230137621037\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 795\n",
      "  experiment_id: edcbe001b14143f4bf76a6a2914a7bd0\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6935440301895142\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: -0.352914422750473\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.02687148004770279\n",
      "          model: {}\n",
      "          policy_loss: 0.013181189075112343\n",
      "          total_loss: 3172.70703125\n",
      "          vf_explained_var: -7.1321797356915795e-09\n",
      "          vf_loss: 3172.675537109375\n",
      "    num_agent_steps_sampled: 2385000\n",
      "    num_agent_steps_trained: 2385000\n",
      "    num_steps_sampled: 2385000\n",
      "    num_steps_trained: 2385000\n",
      "  iterations_since_restore: 159\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 23.918421052631576\n",
      "    ram_util_percent: 31.065789473684198\n",
      "  pid: 307\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06563246691009557\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 3.4639257285942056\n",
      "    mean_inference_ms: 0.4841112749880051\n",
      "    mean_raw_obs_processing_ms: 3.7367350805622306\n",
      "  time_since_restore: 4277.066271781921\n",
      "  time_this_iter_s: 26.485013246536255\n",
      "  time_total_s: 4277.066271781921\n",
      "  timers:\n",
      "    learn_throughput: 6533.996\n",
      "    learn_time_ms: 2295.685\n",
      "    load_throughput: 6343216.648\n",
      "    load_time_ms: 2.365\n",
      "    sample_throughput: 633.309\n",
      "    sample_time_ms: 23685.113\n",
      "    update_time_ms: 1.332\n",
      "  timestamp: 1665249476\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2385000\n",
      "  training_iteration: 159\n",
      "  trial_id: 26f87_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Memory usage on this node: 9.7/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.23 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_5_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_3_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_0_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_1_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_2_25daa8673b2c24aecf80a7fc79df4e93, 0.0/6.0 CPU_group_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_4_25daa8673b2c24aecf80a7fc79df4e93)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_onelane_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_26f87_00000 | RUNNING  | 141.225.10.229:307 |    159 |          4277.07 | 2385000 | -1997.59 |              1780.99 |             -7891.23 |               3000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m ring length: 239\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m v_max: 4.141741239205832\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m ring length: 270\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m v_max: 5.6143732387852054\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m ring length: 229\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m v_max: 3.666034803266416\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m ring length: 226\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m v_max: 3.5232840694769565\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m ring length: 220\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m v_max: 3.2377399006821497\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 1 seconds\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_26f87_00000:\n",
      "  agent_timesteps_total: 2400000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-08_12-18-21\n",
      "  done: false\n",
      "  episode_len_mean: 3000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1780.9896235052067\n",
      "  episode_reward_mean: -2028.7145149096116\n",
      "  episode_reward_min: -7891.230137621037\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 800\n",
      "  experiment_id: edcbe001b14143f4bf76a6a2914a7bd0\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6935440301895142\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.46306574344635\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.026905464008450508\n",
      "          model: {}\n",
      "          policy_loss: 0.01543891616165638\n",
      "          total_loss: 3662.625244140625\n",
      "          vf_explained_var: 2.0377657339309962e-09\n",
      "          vf_loss: 3662.591796875\n",
      "    num_agent_steps_sampled: 2400000\n",
      "    num_agent_steps_trained: 2400000\n",
      "    num_steps_sampled: 2400000\n",
      "    num_steps_trained: 2400000\n",
      "  iterations_since_restore: 160\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 24.350000000000005\n",
      "    ram_util_percent: 31.055555555555543\n",
      "  pid: 307\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06563188798806563\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 3.4637187043012485\n",
      "    mean_inference_ms: 0.4841299090693105\n",
      "    mean_raw_obs_processing_ms: 3.7345838721055253\n",
      "  time_since_restore: 4302.135665178299\n",
      "  time_this_iter_s: 25.069393396377563\n",
      "  time_total_s: 4302.135665178299\n",
      "  timers:\n",
      "    learn_throughput: 6482.878\n",
      "    learn_time_ms: 2313.787\n",
      "    load_throughput: 6835492.878\n",
      "    load_time_ms: 2.194\n",
      "    sample_throughput: 633.169\n",
      "    sample_time_ms: 23690.343\n",
      "    update_time_ms: 1.368\n",
      "  timestamp: 1665249501\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2400000\n",
      "  training_iteration: 160\n",
      "  trial_id: 26f87_00000\n",
      "  \n",
      "\u001b[2m\u001b[36m(pid=307)\u001b[0m 2022-10-08 12:18:21,243\tWARNING ppo.py:242 -- The magnitude of your environment rewards are more than 203.0x the scale of `vf_clip_param`. This means that it will take more than 203.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "== Status ==\n",
      "Memory usage on this node: 9.7/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.23 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_2_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_3_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_1_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_5_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_4_25daa8673b2c24aecf80a7fc79df4e93, 0.0/6.0 CPU_group_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_0_25daa8673b2c24aecf80a7fc79df4e93)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_onelane_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_26f87_00000 | RUNNING  | 141.225.10.229:307 |    160 |          4302.14 | 2400000 | -2028.71 |              1780.99 |             -7891.23 |               3000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m ring length: 226\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m v_max: 3.5232840694769565\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m ring length: 243\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m v_max: 4.331956438196479\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m ring length: 221\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m v_max: 3.285334164169417\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m ring length: 244\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m v_max: 4.379503211387676\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m ring length: 266\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m v_max: 5.42459972166245\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m Could not connect to TraCI server at localhost:34795 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m Could not connect to TraCI server at localhost:57509 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 2 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m Could not connect to TraCI server at localhost:34447 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m Could not connect to TraCI server at localhost:36289 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m Could not connect to TraCI server at localhost:54035 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m Could not connect to TraCI server at localhost:34795 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 3 seconds\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m Could not connect to TraCI server at localhost:57509 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 3 seconds\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m Could not connect to TraCI server at localhost:34447 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 3 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m Could not connect to TraCI server at localhost:54035 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 3 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m Could not connect to TraCI server at localhost:36289 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 3 seconds\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_26f87_00000:\n",
      "  agent_timesteps_total: 2415000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-08_12-18-51\n",
      "  done: false\n",
      "  episode_len_mean: 3000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1780.9896235052067\n",
      "  episode_reward_mean: -2115.6057710106693\n",
      "  episode_reward_min: -7891.230137621037\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 805\n",
      "  experiment_id: edcbe001b14143f4bf76a6a2914a7bd0\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6935440301895142\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.8968710899353027\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012361845001578331\n",
      "          model: {}\n",
      "          policy_loss: 0.0070338197983801365\n",
      "          total_loss: 3464.635498046875\n",
      "          vf_explained_var: -5.603855601776786e-09\n",
      "          vf_loss: 3464.619873046875\n",
      "    num_agent_steps_sampled: 2415000\n",
      "    num_agent_steps_trained: 2415000\n",
      "    num_steps_sampled: 2415000\n",
      "    num_steps_trained: 2415000\n",
      "  iterations_since_restore: 161\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.658139534883723\n",
      "    ram_util_percent: 31.018604651162786\n",
      "  pid: 307\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0656313866621089\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 3.46348715887812\n",
      "    mean_inference_ms: 0.48414922139402555\n",
      "    mean_raw_obs_processing_ms: 3.7330425753809062\n",
      "  time_since_restore: 4332.173728227615\n",
      "  time_this_iter_s: 30.038063049316406\n",
      "  time_total_s: 4332.173728227615\n",
      "  timers:\n",
      "    learn_throughput: 6555.367\n",
      "    learn_time_ms: 2288.201\n",
      "    load_throughput: 7056603.52\n",
      "    load_time_ms: 2.126\n",
      "    sample_throughput: 620.584\n",
      "    sample_time_ms: 24170.797\n",
      "    update_time_ms: 1.354\n",
      "  timestamp: 1665249531\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2415000\n",
      "  training_iteration: 161\n",
      "  trial_id: 26f87_00000\n",
      "  \n",
      "\u001b[2m\u001b[36m(pid=307)\u001b[0m 2022-10-08 12:18:51,397\tWARNING ppo.py:242 -- The magnitude of your environment rewards are more than 212.0x the scale of `vf_clip_param`. This means that it will take more than 212.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "== Status ==\n",
      "Memory usage on this node: 9.7/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.23 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_4_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_2_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_3_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_5_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_0_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_1_25daa8673b2c24aecf80a7fc79df4e93, 0.0/6.0 CPU_group_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 accelerator_type:G)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_onelane_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_26f87_00000 | RUNNING  | 141.225.10.229:307 |    161 |          4332.17 | 2415000 | -2115.61 |              1780.99 |             -7891.23 |               3000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m ring length: 227\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m v_max: 3.570869368805752\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m ring length: 265\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m v_max: 5.37714246265477\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m ring length: 261\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m v_max: 5.187261846097525\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m ring length: 236\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m v_max: 3.999052674951912\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m ring length: 232\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m v_max: 3.8087690783250063\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 1 seconds\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_26f87_00000:\n",
      "  agent_timesteps_total: 2430000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-08_12-19-17\n",
      "  done: false\n",
      "  episode_len_mean: 3000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1780.9896235052067\n",
      "  episode_reward_mean: -2091.9396244094214\n",
      "  episode_reward_min: -7891.230137621037\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 810\n",
      "  experiment_id: edcbe001b14143f4bf76a6a2914a7bd0\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6935440301895142\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.6554456949234009\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013792913407087326\n",
      "          model: {}\n",
      "          policy_loss: 0.0076486472971737385\n",
      "          total_loss: 3589.325927734375\n",
      "          vf_explained_var: -9.16994569166718e-09\n",
      "          vf_loss: 3589.30908203125\n",
      "    num_agent_steps_sampled: 2430000\n",
      "    num_agent_steps_trained: 2430000\n",
      "    num_steps_sampled: 2430000\n",
      "    num_steps_trained: 2430000\n",
      "  iterations_since_restore: 162\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 23.572972972972973\n",
      "    ram_util_percent: 31.059459459459447\n",
      "  pid: 307\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06563094497864623\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 3.4632445226024955\n",
      "    mean_inference_ms: 0.48416899791618007\n",
      "    mean_raw_obs_processing_ms: 3.7314933716120966\n",
      "  time_since_restore: 4358.116352558136\n",
      "  time_this_iter_s: 25.94262433052063\n",
      "  time_total_s: 4358.116352558136\n",
      "  timers:\n",
      "    learn_throughput: 6548.386\n",
      "    learn_time_ms: 2290.641\n",
      "    load_throughput: 7707378.505\n",
      "    load_time_ms: 1.946\n",
      "    sample_throughput: 617.383\n",
      "    sample_time_ms: 24296.12\n",
      "    update_time_ms: 1.36\n",
      "  timestamp: 1665249557\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2430000\n",
      "  training_iteration: 162\n",
      "  trial_id: 26f87_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=307)\u001b[0m 2022-10-08 12:19:17,416\tWARNING ppo.py:242 -- The magnitude of your environment rewards are more than 209.0x the scale of `vf_clip_param`. This means that it will take more than 209.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "== Status ==\n",
      "Memory usage on this node: 9.7/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.23 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_5_25daa8673b2c24aecf80a7fc79df4e93, 0.0/6.0 CPU_group_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_4_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_3_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_2_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_0_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_1_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 accelerator_type:G)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_onelane_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_26f87_00000 | RUNNING  | 141.225.10.229:307 |    162 |          4358.12 | 2430000 | -2091.94 |              1780.99 |             -7891.23 |               3000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m ring length: 246\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m v_max: 4.474587708995648\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m ring length: 241\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m v_max: 4.236854288051661\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m ring length: 239\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m v_max: 4.141741239205832\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m ring length: 224\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m v_max: 3.4281086136538996\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m ring length: 230\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m v_max: 3.7136148111012934\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 1 seconds\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_26f87_00000:\n",
      "  agent_timesteps_total: 2445000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-08_12-19-42\n",
      "  done: false\n",
      "  episode_len_mean: 3000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1780.9896235052067\n",
      "  episode_reward_mean: -2174.582129956924\n",
      "  episode_reward_min: -7891.230137621037\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 815\n",
      "  experiment_id: edcbe001b14143f4bf76a6a2914a7bd0\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6935440301895142\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.5155187845230103\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.02502870000898838\n",
      "          model: {}\n",
      "          policy_loss: 0.015298508107662201\n",
      "          total_loss: 3734.38330078125\n",
      "          vf_explained_var: -7.641621557752387e-09\n",
      "          vf_loss: 3734.35107421875\n",
      "    num_agent_steps_sampled: 2445000\n",
      "    num_agent_steps_trained: 2445000\n",
      "    num_steps_sampled: 2445000\n",
      "    num_steps_trained: 2445000\n",
      "  iterations_since_restore: 163\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 24.794444444444444\n",
      "    ram_util_percent: 31.15555555555556\n",
      "  pid: 307\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06563051314902144\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 3.4629862211179785\n",
      "    mean_inference_ms: 0.48418780951092427\n",
      "    mean_raw_obs_processing_ms: 3.73003217667704\n",
      "  time_since_restore: 4383.339113473892\n",
      "  time_this_iter_s: 25.222760915756226\n",
      "  time_total_s: 4383.339113473892\n",
      "  timers:\n",
      "    learn_throughput: 6487.624\n",
      "    learn_time_ms: 2312.094\n",
      "    load_throughput: 8486943.384\n",
      "    load_time_ms: 1.767\n",
      "    sample_throughput: 616.939\n",
      "    sample_time_ms: 24313.58\n",
      "    update_time_ms: 1.342\n",
      "  timestamp: 1665249582\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2445000\n",
      "  training_iteration: 163\n",
      "  trial_id: 26f87_00000\n",
      "  \n",
      "\u001b[2m\u001b[36m(pid=307)\u001b[0m 2022-10-08 12:19:42,735\tWARNING ppo.py:242 -- The magnitude of your environment rewards are more than 217.0x the scale of `vf_clip_param`. This means that it will take more than 217.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "== Status ==\n",
      "Memory usage on this node: 9.7/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.23 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_0_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_2_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 accelerator_type:G, 0.0/6.0 CPU_group_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_5_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_3_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_4_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_1_25daa8673b2c24aecf80a7fc79df4e93)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_onelane_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_26f87_00000 | RUNNING  | 141.225.10.229:307 |    163 |          4383.34 | 2445000 | -2174.58 |              1780.99 |             -7891.23 |               3000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m ring length: 251\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m v_max: 4.71224180704279\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m ring length: 249\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m v_max: 4.617190445131122\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m ring length: 235\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m v_max: 3.9514850725282584\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m ring length: 246\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m v_max: 4.474587708995648\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m ring length: 229\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m v_max: 3.666034803266416\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 1 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_WaveAttenuationPOEnv-v0_26f87_00000:\n",
      "  agent_timesteps_total: 2460000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-08_12-20-07\n",
      "  done: false\n",
      "  episode_len_mean: 3000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1780.9896235052067\n",
      "  episode_reward_mean: -2202.6569691254244\n",
      "  episode_reward_min: -7891.230137621037\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 820\n",
      "  experiment_id: edcbe001b14143f4bf76a6a2914a7bd0\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6935440301895142\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.8456780314445496\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.035543739795684814\n",
      "          model: {}\n",
      "          policy_loss: 0.015179049223661423\n",
      "          total_loss: 3525.44970703125\n",
      "          vf_explained_var: -6.622738357719982e-09\n",
      "          vf_loss: 3525.409912109375\n",
      "    num_agent_steps_sampled: 2460000\n",
      "    num_agent_steps_trained: 2460000\n",
      "    num_steps_sampled: 2460000\n",
      "    num_steps_trained: 2460000\n",
      "  iterations_since_restore: 164\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 24.194444444444446\n",
      "    ram_util_percent: 31.23333333333333\n",
      "  pid: 307\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06563052573774879\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 3.4627370824959387\n",
      "    mean_inference_ms: 0.4842088013198109\n",
      "    mean_raw_obs_processing_ms: 3.728516893726086\n",
      "  time_since_restore: 4408.410985469818\n",
      "  time_this_iter_s: 25.071871995925903\n",
      "  time_total_s: 4408.410985469818\n",
      "  timers:\n",
      "    learn_throughput: 6670.979\n",
      "    learn_time_ms: 2248.545\n",
      "    load_throughput: 9436570.623\n",
      "    load_time_ms: 1.59\n",
      "    sample_throughput: 617.215\n",
      "    sample_time_ms: 24302.708\n",
      "    update_time_ms: 1.587\n",
      "  timestamp: 1665249607\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2460000\n",
      "  training_iteration: 164\n",
      "  trial_id: 26f87_00000\n",
      "  \n",
      "\u001b[2m\u001b[36m(pid=307)\u001b[0m 2022-10-08 12:20:07,927\tWARNING ppo.py:242 -- The magnitude of your environment rewards are more than 220.0x the scale of `vf_clip_param`. This means that it will take more than 220.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "== Status ==\n",
      "Memory usage on this node: 9.8/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.23 GiB heap, 0.0/14.65 GiB objects (0.0/6.0 CPU_group_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_5_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_3_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_1_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_0_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_4_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_2_25daa8673b2c24aecf80a7fc79df4e93)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_onelane_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_26f87_00000 | RUNNING  | 141.225.10.229:307 |    164 |          4408.41 | 2460000 | -2202.66 |              1780.99 |             -7891.23 |               3000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m ring length: 236\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m v_max: 3.999052674951912\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m ring length: 255\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m v_max: 4.902299776966978\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m ring length: 238\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m v_max: 4.094180836186086\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m ring length: 264\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m v_max: 5.329679917416892\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m ring length: 231\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m v_max: 3.761192925277308\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_26f87_00000:\n",
      "  agent_timesteps_total: 2475000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-08_12-20-31\n",
      "  done: false\n",
      "  episode_len_mean: 3000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1780.9896235052067\n",
      "  episode_reward_mean: -2168.199729869648\n",
      "  episode_reward_min: -7891.230137621037\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 825\n",
      "  experiment_id: edcbe001b14143f4bf76a6a2914a7bd0\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6935440301895142\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: -0.37549522519111633\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01430447306483984\n",
      "          model: {}\n",
      "          policy_loss: 0.0028491513803601265\n",
      "          total_loss: 3386.904296875\n",
      "          vf_explained_var: 3.056648489874192e-09\n",
      "          vf_loss: 3386.8916015625\n",
      "    num_agent_steps_sampled: 2475000\n",
      "    num_agent_steps_trained: 2475000\n",
      "    num_steps_sampled: 2475000\n",
      "    num_steps_trained: 2475000\n",
      "  iterations_since_restore: 165\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 25.31764705882353\n",
      "    ram_util_percent: 31.22352941176471\n",
      "  pid: 307\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0656301261196399\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 3.4624636170913847\n",
      "    mean_inference_ms: 0.48422677526067454\n",
      "    mean_raw_obs_processing_ms: 3.726955089159794\n",
      "  time_since_restore: 4432.063810825348\n",
      "  time_this_iter_s: 23.652825355529785\n",
      "  time_total_s: 4432.063810825348\n",
      "  timers:\n",
      "    learn_throughput: 6718.293\n",
      "    learn_time_ms: 2232.71\n",
      "    load_throughput: 9580411.147\n",
      "    load_time_ms: 1.566\n",
      "    sample_throughput: 620.967\n",
      "    sample_time_ms: 24155.863\n",
      "    update_time_ms: 1.744\n",
      "  timestamp: 1665249631\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2475000\n",
      "  training_iteration: 165\n",
      "  trial_id: 26f87_00000\n",
      "  \n",
      "\u001b[2m\u001b[36m(pid=307)\u001b[0m 2022-10-08 12:20:31,679\tWARNING ppo.py:242 -- The magnitude of your environment rewards are more than 217.0x the scale of `vf_clip_param`. This means that it will take more than 217.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "== Status ==\n",
      "Memory usage on this node: 9.7/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.23 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_3_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 accelerator_type:G, 0.0/6.0 CPU_group_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_0_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_1_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_2_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_4_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_5_25daa8673b2c24aecf80a7fc79df4e93)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_onelane_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_26f87_00000 | RUNNING  | 141.225.10.229:307 |    165 |          4432.06 | 2475000 |  -2168.2 |              1780.99 |             -7891.23 |               3000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m ring length: 240\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m v_max: 4.189299083856172\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m ring length: 230\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m v_max: 3.7136148111012934\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m ring length: 220\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m v_max: 3.2377399006821497\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m ring length: 239\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m v_max: 4.141741239205832\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m ring length: 226\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m v_max: 3.5232840694769565\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m Could not connect to TraCI server at localhost:34795 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m Could not connect to TraCI server at localhost:57509 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m Could not connect to TraCI server at localhost:54035 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m Could not connect to TraCI server at localhost:36289 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m Could not connect to TraCI server at localhost:34447 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m Could not connect to TraCI server at localhost:34795 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 3 seconds\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m Could not connect to TraCI server at localhost:57509 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 3 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m Could not connect to TraCI server at localhost:54035 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 3 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m Could not connect to TraCI server at localhost:36289 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 3 seconds\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m Could not connect to TraCI server at localhost:34447 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 3 seconds\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_26f87_00000:\n",
      "  agent_timesteps_total: 2490000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-08_12-21-02\n",
      "  done: false\n",
      "  episode_len_mean: 3000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1780.9896235052067\n",
      "  episode_reward_mean: -2049.225400587657\n",
      "  episode_reward_min: -7891.230137621037\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 830\n",
      "  experiment_id: edcbe001b14143f4bf76a6a2914a7bd0\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6935440301895142\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: -1.4733119010925293\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.02589697577059269\n",
      "          model: {}\n",
      "          policy_loss: 0.009912772104144096\n",
      "          total_loss: 2959.21875\n",
      "          vf_explained_var: -6.622738357719982e-09\n",
      "          vf_loss: 2959.19091796875\n",
      "    num_agent_steps_sampled: 2490000\n",
      "    num_agent_steps_trained: 2490000\n",
      "    num_steps_sampled: 2490000\n",
      "    num_steps_trained: 2490000\n",
      "  iterations_since_restore: 166\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.71627906976744\n",
      "    ram_util_percent: 31.118604651162794\n",
      "  pid: 307\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06563001438709842\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 3.462221818320441\n",
      "    mean_inference_ms: 0.4842467624942045\n",
      "    mean_raw_obs_processing_ms: 3.7259326812449314\n",
      "  time_since_restore: 4462.392566204071\n",
      "  time_this_iter_s: 30.328755378723145\n",
      "  time_total_s: 4462.392566204071\n",
      "  timers:\n",
      "    learn_throughput: 6777.94\n",
      "    learn_time_ms: 2213.062\n",
      "    load_throughput: 8594885.246\n",
      "    load_time_ms: 1.745\n",
      "    sample_throughput: 620.835\n",
      "    sample_time_ms: 24161.016\n",
      "    update_time_ms: 1.764\n",
      "  timestamp: 1665249662\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2490000\n",
      "  training_iteration: 166\n",
      "  trial_id: 26f87_00000\n",
      "  \n",
      "\u001b[2m\u001b[36m(pid=307)\u001b[0m 2022-10-08 12:21:02,146\tWARNING ppo.py:242 -- The magnitude of your environment rewards are more than 205.0x the scale of `vf_clip_param`. This means that it will take more than 205.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "== Status ==\n",
      "Memory usage on this node: 9.7/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.23 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_0_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_3_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_1_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_5_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 accelerator_type:G, 0.0/6.0 CPU_group_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_2_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_4_25daa8673b2c24aecf80a7fc79df4e93)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_onelane_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_26f87_00000 | RUNNING  | 141.225.10.229:307 |    166 |          4462.39 | 2490000 | -2049.23 |              1780.99 |             -7891.23 |               3000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m ring length: 257\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m v_max: 4.997304682316517\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m ring length: 258\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m v_max: 5.044800727535787\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m ring length: 260\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m v_max: 5.139779427502188\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m ring length: 250\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m v_max: 4.664717904914125\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m ring length: 251\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m v_max: 4.71224180704279\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 1 seconds\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_26f87_00000:\n",
      "  agent_timesteps_total: 2505000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-08_12-21-27\n",
      "  done: false\n",
      "  episode_len_mean: 3000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1780.9896235052067\n",
      "  episode_reward_mean: -2031.3723852316784\n",
      "  episode_reward_min: -7891.230137621037\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 835\n",
      "  experiment_id: edcbe001b14143f4bf76a6a2914a7bd0\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6935440301895142\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: -0.9904554486274719\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0186394564807415\n",
      "          model: {}\n",
      "          policy_loss: 0.006716258358210325\n",
      "          total_loss: 2817.170654296875\n",
      "          vf_explained_var: -1.0698269825581974e-08\n",
      "          vf_loss: 2817.15087890625\n",
      "    num_agent_steps_sampled: 2505000\n",
      "    num_agent_steps_trained: 2505000\n",
      "    num_steps_sampled: 2505000\n",
      "    num_steps_trained: 2505000\n",
      "  iterations_since_restore: 167\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 24.225\n",
      "    ram_util_percent: 31.16388888888889\n",
      "  pid: 307\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06562976889469019\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 3.461951843318887\n",
      "    mean_inference_ms: 0.4842660112258108\n",
      "    mean_raw_obs_processing_ms: 3.7249549027984723\n",
      "  time_since_restore: 4487.688298940659\n",
      "  time_this_iter_s: 25.295732736587524\n",
      "  time_total_s: 4487.688298940659\n",
      "  timers:\n",
      "    learn_throughput: 6727.397\n",
      "    learn_time_ms: 2229.688\n",
      "    load_throughput: 9562355.23\n",
      "    load_time_ms: 1.569\n",
      "    sample_throughput: 623.292\n",
      "    sample_time_ms: 24065.758\n",
      "    update_time_ms: 1.784\n",
      "  timestamp: 1665249687\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2505000\n",
      "  training_iteration: 167\n",
      "  trial_id: 26f87_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=307)\u001b[0m 2022-10-08 12:21:27,538\tWARNING ppo.py:242 -- The magnitude of your environment rewards are more than 203.0x the scale of `vf_clip_param`. This means that it will take more than 203.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "== Status ==\n",
      "Memory usage on this node: 9.7/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.23 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_2_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_4_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_0_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_3_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_1_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_5_25daa8673b2c24aecf80a7fc79df4e93, 0.0/6.0 CPU_group_25daa8673b2c24aecf80a7fc79df4e93)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_onelane_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_26f87_00000 | RUNNING  | 141.225.10.229:307 |    167 |          4487.69 | 2505000 | -2031.37 |              1780.99 |             -7891.23 |               3000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m ring length: 249\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m v_max: 4.617190445131122\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m ring length: 264\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m v_max: 5.329679917416892\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m ring length: 251\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m v_max: 4.71224180704279\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m ring length: 225\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m v_max: 3.4756971311168834\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m ring length: 267\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m v_max: 5.472051563171826\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 1 seconds\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_26f87_00000:\n",
      "  agent_timesteps_total: 2520000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-08_12-21-53\n",
      "  done: false\n",
      "  episode_len_mean: 3000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1780.9896235052067\n",
      "  episode_reward_mean: -1973.0595458539653\n",
      "  episode_reward_min: -7891.230137621037\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 840\n",
      "  experiment_id: edcbe001b14143f4bf76a6a2914a7bd0\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6935440301895142\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: -0.004749405197799206\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.021867722272872925\n",
      "          model: {}\n",
      "          policy_loss: 0.006595562677830458\n",
      "          total_loss: 3573.529541015625\n",
      "          vf_explained_var: 1.528324244937096e-09\n",
      "          vf_loss: 3573.5078125\n",
      "    num_agent_steps_sampled: 2520000\n",
      "    num_agent_steps_trained: 2520000\n",
      "    num_steps_sampled: 2520000\n",
      "    num_steps_trained: 2520000\n",
      "  iterations_since_restore: 168\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 24.464864864864865\n",
      "    ram_util_percent: 31.159459459459462\n",
      "  pid: 307\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06562965712058622\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 3.4617030833708338\n",
      "    mean_inference_ms: 0.48428646617669147\n",
      "    mean_raw_obs_processing_ms: 3.724052859672515\n",
      "  time_since_restore: 4513.242141723633\n",
      "  time_this_iter_s: 25.553842782974243\n",
      "  time_total_s: 4513.242141723633\n",
      "  timers:\n",
      "    learn_throughput: 6715.537\n",
      "    learn_time_ms: 2233.626\n",
      "    load_throughput: 8618787.073\n",
      "    load_time_ms: 1.74\n",
      "    sample_throughput: 624.435\n",
      "    sample_time_ms: 24021.726\n",
      "    update_time_ms: 1.633\n",
      "  timestamp: 1665249713\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2520000\n",
      "  training_iteration: 168\n",
      "  trial_id: 26f87_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 9.7/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.23 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_4_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_5_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_2_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_0_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_1_25daa8673b2c24aecf80a7fc79df4e93, 0.0/6.0 CPU_group_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_3_25daa8673b2c24aecf80a7fc79df4e93)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_onelane_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_26f87_00000 | RUNNING  | 141.225.10.229:307 |    168 |          4513.24 | 2520000 | -1973.06 |              1780.99 |             -7891.23 |               3000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m ring length: 227\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m v_max: 3.570869368805752\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m ring length: 262\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m v_max: 5.234739483008334\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m ring length: 253\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m v_max: 4.807278529691987\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m ring length: 224\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m v_max: 3.4281086136538996\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m ring length: 241\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m v_max: 4.236854288051661\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 1 seconds\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_26f87_00000:\n",
      "  agent_timesteps_total: 2535000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-08_12-22-19\n",
      "  done: false\n",
      "  episode_len_mean: 3000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1780.9896235052067\n",
      "  episode_reward_mean: -1818.3740460394608\n",
      "  episode_reward_min: -7891.230137621037\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 845\n",
      "  experiment_id: edcbe001b14143f4bf76a6a2914a7bd0\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6935440301895142\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: -1.2744505405426025\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.029903963208198547\n",
      "          model: {}\n",
      "          policy_loss: 0.012738454155623913\n",
      "          total_loss: 3040.5390625\n",
      "          vf_explained_var: -8.151062935723985e-09\n",
      "          vf_loss: 3040.50537109375\n",
      "    num_agent_steps_sampled: 2535000\n",
      "    num_agent_steps_trained: 2535000\n",
      "    num_steps_sampled: 2535000\n",
      "    num_steps_trained: 2535000\n",
      "  iterations_since_restore: 169\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 23.331578947368417\n",
      "    ram_util_percent: 31.155263157894744\n",
      "  pid: 307\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06562962659017868\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 3.4614900535902255\n",
      "    mean_inference_ms: 0.4843067795290945\n",
      "    mean_raw_obs_processing_ms: 3.7227337817429818\n",
      "  time_since_restore: 4539.892965555191\n",
      "  time_this_iter_s: 26.650823831558228\n",
      "  time_total_s: 4539.892965555191\n",
      "  timers:\n",
      "    learn_throughput: 6719.264\n",
      "    learn_time_ms: 2232.387\n",
      "    load_throughput: 8562250.439\n",
      "    load_time_ms: 1.752\n",
      "    sample_throughput: 623.97\n",
      "    sample_time_ms: 24039.625\n",
      "    update_time_ms: 1.584\n",
      "  timestamp: 1665249739\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2535000\n",
      "  training_iteration: 169\n",
      "  trial_id: 26f87_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Memory usage on this node: 9.7/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.23 GiB heap, 0.0/14.65 GiB objects (0.0/6.0 CPU_group_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_4_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_3_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_2_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_1_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_0_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_5_25daa8673b2c24aecf80a7fc79df4e93)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_onelane_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_26f87_00000 | RUNNING  | 141.225.10.229:307 |    169 |          4539.89 | 2535000 | -1818.37 |              1780.99 |             -7891.23 |               3000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m ring length: 249\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m v_max: 4.617190445131122\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m ring length: 233\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m v_max: 3.856343201216534\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m ring length: 265\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m v_max: 5.37714246265477\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m ring length: 250\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m v_max: 4.664717904914125\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m ring length: 227\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m v_max: 3.570869368805752\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 1 seconds\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_26f87_00000:\n",
      "  agent_timesteps_total: 2550000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-08_12-22-46\n",
      "  done: false\n",
      "  episode_len_mean: 3000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1780.9896235052067\n",
      "  episode_reward_mean: -1941.1610534517752\n",
      "  episode_reward_min: -7891.230137621037\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 850\n",
      "  experiment_id: edcbe001b14143f4bf76a6a2914a7bd0\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6935440301895142\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.998818039894104\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017860902473330498\n",
      "          model: {}\n",
      "          policy_loss: 0.010739183984696865\n",
      "          total_loss: 3476.550537109375\n",
      "          vf_explained_var: -1.2226593959496768e-08\n",
      "          vf_loss: 3476.52734375\n",
      "    num_agent_steps_sampled: 2550000\n",
      "    num_agent_steps_trained: 2550000\n",
      "    num_steps_sampled: 2550000\n",
      "    num_steps_trained: 2550000\n",
      "  iterations_since_restore: 170\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 23.560526315789474\n",
      "    ram_util_percent: 31.160526315789483\n",
      "  pid: 307\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06562940770909913\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 3.461257962382792\n",
      "    mean_inference_ms: 0.48432580533651226\n",
      "    mean_raw_obs_processing_ms: 3.721562943465948\n",
      "  time_since_restore: 4566.668307542801\n",
      "  time_this_iter_s: 26.775341987609863\n",
      "  time_total_s: 4566.668307542801\n",
      "  timers:\n",
      "    learn_throughput: 6582.1\n",
      "    learn_time_ms: 2278.908\n",
      "    load_throughput: 8534954.011\n",
      "    load_time_ms: 1.757\n",
      "    sample_throughput: 620.766\n",
      "    sample_time_ms: 24163.712\n",
      "    update_time_ms: 1.58\n",
      "  timestamp: 1665249766\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2550000\n",
      "  training_iteration: 170\n",
      "  trial_id: 26f87_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 9.7/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.23 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_5_25daa8673b2c24aecf80a7fc79df4e93, 0.0/6.0 CPU_group_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_2_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_0_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_4_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_3_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_1_25daa8673b2c24aecf80a7fc79df4e93)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_onelane_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_26f87_00000 | RUNNING  | 141.225.10.229:307 |    170 |          4566.67 | 2550000 | -1941.16 |              1780.99 |             -7891.23 |               3000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m ring length: 241\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m v_max: 4.236854288051661\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m ring length: 237\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m v_max: 4.04661795519353\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m ring length: 230\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m v_max: 3.7136148111012934\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m ring length: 244\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m v_max: 4.379503211387676\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m ring length: 249\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m v_max: 4.617190445131122\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 1 seconds\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_26f87_00000:\n",
      "  agent_timesteps_total: 2565000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-08_12-23-13\n",
      "  done: false\n",
      "  episode_len_mean: 3000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1455.326845455007\n",
      "  episode_reward_mean: -1956.065408770315\n",
      "  episode_reward_min: -7891.230137621037\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 855\n",
      "  experiment_id: edcbe001b14143f4bf76a6a2914a7bd0\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6935440301895142\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: -1.0062326192855835\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.020253418013453484\n",
      "          model: {}\n",
      "          policy_loss: 0.009122570045292377\n",
      "          total_loss: 3175.099853515625\n",
      "          vf_explained_var: -3.056648489874192e-09\n",
      "          vf_loss: 3175.076416015625\n",
      "    num_agent_steps_sampled: 2565000\n",
      "    num_agent_steps_trained: 2565000\n",
      "    num_steps_sampled: 2565000\n",
      "    num_steps_trained: 2565000\n",
      "  iterations_since_restore: 171\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 23.76842105263158\n",
      "    ram_util_percent: 31.221052631578942\n",
      "  pid: 307\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06562922007131027\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 3.4610231308222534\n",
      "    mean_inference_ms: 0.4843447802099518\n",
      "    mean_raw_obs_processing_ms: 3.720472687689126\n",
      "  time_since_restore: 4592.926911354065\n",
      "  time_this_iter_s: 26.258603811264038\n",
      "  time_total_s: 4592.926911354065\n",
      "  timers:\n",
      "    learn_throughput: 6465.56\n",
      "    learn_time_ms: 2319.985\n",
      "    load_throughput: 8731463.465\n",
      "    load_time_ms: 1.718\n",
      "    sample_throughput: 631.718\n",
      "    sample_time_ms: 23744.784\n",
      "    update_time_ms: 1.566\n",
      "  timestamp: 1665249793\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2565000\n",
      "  training_iteration: 171\n",
      "  trial_id: 26f87_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Memory usage on this node: 9.7/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.23 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_1_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_4_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_2_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_5_25daa8673b2c24aecf80a7fc79df4e93, 0.0/6.0 CPU_group_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_0_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_3_25daa8673b2c24aecf80a7fc79df4e93)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_onelane_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_26f87_00000 | RUNNING  | 141.225.10.229:307 |    171 |          4592.93 | 2565000 | -1956.07 |              1455.33 |             -7891.23 |               3000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m ring length: 264\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m v_max: 5.329679917416892\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m ring length: 268\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m v_max: 5.5194978538368025\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m ring length: 226\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m v_max: 3.5232840694769565\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m ring length: 223\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m v_max: 3.3805185755587788\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m ring length: 252\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m v_max: 4.759762049824181\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 1 seconds\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_26f87_00000:\n",
      "  agent_timesteps_total: 2580000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-08_12-23-38\n",
      "  done: false\n",
      "  episode_len_mean: 3000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1455.326845455007\n",
      "  episode_reward_mean: -1873.9573684901595\n",
      "  episode_reward_min: -7891.230137621037\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 860\n",
      "  experiment_id: edcbe001b14143f4bf76a6a2914a7bd0\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6935440301895142\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.9701129794120789\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008334948681294918\n",
      "          model: {}\n",
      "          policy_loss: 0.00492850923910737\n",
      "          total_loss: 3429.330810546875\n",
      "          vf_explained_var: -8.660504313695583e-09\n",
      "          vf_loss: 3429.320556640625\n",
      "    num_agent_steps_sampled: 2580000\n",
      "    num_agent_steps_trained: 2580000\n",
      "    num_steps_sampled: 2580000\n",
      "    num_steps_trained: 2580000\n",
      "  iterations_since_restore: 172\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 24.641666666666666\n",
      "    ram_util_percent: 31.258333333333333\n",
      "  pid: 307\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06562858285338082\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 3.460810505253797\n",
      "    mean_inference_ms: 0.4843613584582673\n",
      "    mean_raw_obs_processing_ms: 3.7193301475640688\n",
      "  time_since_restore: 4617.679140806198\n",
      "  time_this_iter_s: 24.75222945213318\n",
      "  time_total_s: 4617.679140806198\n",
      "  timers:\n",
      "    learn_throughput: 6407.251\n",
      "    learn_time_ms: 2341.098\n",
      "    load_throughput: 8698867.611\n",
      "    load_time_ms: 1.724\n",
      "    sample_throughput: 635.471\n",
      "    sample_time_ms: 23604.544\n",
      "    update_time_ms: 1.585\n",
      "  timestamp: 1665249818\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2580000\n",
      "  training_iteration: 172\n",
      "  trial_id: 26f87_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 9.7/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.23 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_1_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_3_25daa8673b2c24aecf80a7fc79df4e93, 0.0/6.0 CPU_group_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_5_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_2_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_4_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_0_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 accelerator_type:G)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_onelane_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_26f87_00000 | RUNNING  | 141.225.10.229:307 |    172 |          4617.68 | 2580000 | -1873.96 |              1455.33 |             -7891.23 |               3000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m ring length: 252\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m v_max: 4.759762049824181\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m ring length: 263\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m v_max: 5.282212215151455\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m ring length: 246\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m v_max: 4.474587708995648\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m ring length: 268\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m v_max: 5.5194978538368025\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m ring length: 223\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m v_max: 3.3805185755587788\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 1 seconds\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_26f87_00000:\n",
      "  agent_timesteps_total: 2595000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-08_12-24-03\n",
      "  done: false\n",
      "  episode_len_mean: 3000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1455.326845455007\n",
      "  episode_reward_mean: -1916.0630460051962\n",
      "  episode_reward_min: -7600.051626574294\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 865\n",
      "  experiment_id: edcbe001b14143f4bf76a6a2914a7bd0\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3467720150947571\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.8206602334976196\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.03836072236299515\n",
      "          model: {}\n",
      "          policy_loss: 0.01968165673315525\n",
      "          total_loss: 3559.212158203125\n",
      "          vf_explained_var: -3.5660898678457897e-09\n",
      "          vf_loss: 3559.178955078125\n",
      "    num_agent_steps_sampled: 2595000\n",
      "    num_agent_steps_trained: 2595000\n",
      "    num_steps_sampled: 2595000\n",
      "    num_steps_trained: 2595000\n",
      "  iterations_since_restore: 173\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 23.775000000000002\n",
      "    ram_util_percent: 31.238888888888887\n",
      "  pid: 307\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06562765362868075\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 3.460581415103789\n",
      "    mean_inference_ms: 0.48437637799831124\n",
      "    mean_raw_obs_processing_ms: 3.718357213730095\n",
      "  time_since_restore: 4643.442512273788\n",
      "  time_this_iter_s: 25.763371467590332\n",
      "  time_total_s: 4643.442512273788\n",
      "  timers:\n",
      "    learn_throughput: 6531.85\n",
      "    learn_time_ms: 2296.44\n",
      "    load_throughput: 8863202.975\n",
      "    load_time_ms: 1.692\n",
      "    sample_throughput: 632.831\n",
      "    sample_time_ms: 23703.023\n",
      "    update_time_ms: 1.76\n",
      "  timestamp: 1665249843\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2595000\n",
      "  training_iteration: 173\n",
      "  trial_id: 26f87_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Memory usage on this node: 9.8/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.23 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_0_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_5_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_4_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_3_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_1_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_2_25daa8673b2c24aecf80a7fc79df4e93, 0.0/6.0 CPU_group_25daa8673b2c24aecf80a7fc79df4e93)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_onelane_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_26f87_00000 | RUNNING  | 141.225.10.229:307 |    173 |          4643.44 | 2595000 | -1916.06 |              1455.33 |             -7600.05 |               3000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m ring length: 235\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m v_max: 3.9514850725282584\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m ring length: 258\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m v_max: 5.044800727535787\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m ring length: 221\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m v_max: 3.285334164169417\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m ring length: 238\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m v_max: 4.094180836186086\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m ring length: 230\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m v_max: 3.7136148111012934\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 1 seconds\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_26f87_00000:\n",
      "  agent_timesteps_total: 2610000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-08_12-24-29\n",
      "  done: false\n",
      "  episode_len_mean: 3000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1455.326845455007\n",
      "  episode_reward_mean: -1960.277396197579\n",
      "  episode_reward_min: -7600.051626574294\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 870\n",
      "  experiment_id: edcbe001b14143f4bf76a6a2914a7bd0\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3467720150947571\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 2.454362392425537\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.026213113218545914\n",
      "          model: {}\n",
      "          policy_loss: 0.01745900698006153\n",
      "          total_loss: 3796.18212890625\n",
      "          vf_explained_var: -4.58497284583359e-09\n",
      "          vf_loss: 3796.1552734375\n",
      "    num_agent_steps_sampled: 2610000\n",
      "    num_agent_steps_trained: 2610000\n",
      "    num_steps_sampled: 2610000\n",
      "    num_steps_trained: 2610000\n",
      "  iterations_since_restore: 174\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 24.2054054054054\n",
      "    ram_util_percent: 31.251351351351342\n",
      "  pid: 307\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06562654568805544\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 3.460349276730891\n",
      "    mean_inference_ms: 0.4843896529298561\n",
      "    mean_raw_obs_processing_ms: 3.7174199292887504\n",
      "  time_since_restore: 4668.897571563721\n",
      "  time_this_iter_s: 25.45505928993225\n",
      "  time_total_s: 4668.897571563721\n",
      "  timers:\n",
      "    learn_throughput: 6444.115\n",
      "    learn_time_ms: 2327.705\n",
      "    load_throughput: 8881219.65\n",
      "    load_time_ms: 1.689\n",
      "    sample_throughput: 632.634\n",
      "    sample_time_ms: 23710.375\n",
      "    update_time_ms: 1.515\n",
      "  timestamp: 1665249869\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2610000\n",
      "  training_iteration: 174\n",
      "  trial_id: 26f87_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 9.8/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.23 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_0_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_1_25daa8673b2c24aecf80a7fc79df4e93, 0.0/6.0 CPU_group_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_2_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_5_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_3_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_4_25daa8673b2c24aecf80a7fc79df4e93)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_onelane_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_26f87_00000 | RUNNING  | 141.225.10.229:307 |    174 |           4668.9 | 2610000 | -1960.28 |              1455.33 |             -7600.05 |               3000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m ring length: 246\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m v_max: 4.474587708995648\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m ring length: 245\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m v_max: 4.427046998576354\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m ring length: 258\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m v_max: 5.044800727535787\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m ring length: 247\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m v_max: 4.522125250095652\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m ring length: 255\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m v_max: 4.902299776966978\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 1 seconds\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_26f87_00000:\n",
      "  agent_timesteps_total: 2625000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-08_12-24-55\n",
      "  done: false\n",
      "  episode_len_mean: 3000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1455.326845455007\n",
      "  episode_reward_mean: -1991.1727418899382\n",
      "  episode_reward_min: -8149.869935808996\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 875\n",
      "  experiment_id: edcbe001b14143f4bf76a6a2914a7bd0\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3467720150947571\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 2.948429584503174\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0076310038566589355\n",
      "          model: {}\n",
      "          policy_loss: 0.0008453790796920657\n",
      "          total_loss: 3477.438232421875\n",
      "          vf_explained_var: -1.1207711203553572e-08\n",
      "          vf_loss: 3477.43505859375\n",
      "    num_agent_steps_sampled: 2625000\n",
      "    num_agent_steps_trained: 2625000\n",
      "    num_steps_sampled: 2625000\n",
      "    num_steps_trained: 2625000\n",
      "  iterations_since_restore: 175\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 24.370270270270268\n",
      "    ram_util_percent: 31.27567567567569\n",
      "  pid: 307\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06562554756806635\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 3.4601085718559896\n",
      "    mean_inference_ms: 0.4844032913483818\n",
      "    mean_raw_obs_processing_ms: 3.716503160102348\n",
      "  time_since_restore: 4694.550641536713\n",
      "  time_this_iter_s: 25.653069972991943\n",
      "  time_total_s: 4694.550641536713\n",
      "  timers:\n",
      "    learn_throughput: 6254.962\n",
      "    learn_time_ms: 2398.096\n",
      "    load_throughput: 8785599.978\n",
      "    load_time_ms: 1.707\n",
      "    sample_throughput: 629.188\n",
      "    sample_time_ms: 23840.25\n",
      "    update_time_ms: 1.328\n",
      "  timestamp: 1665249895\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2625000\n",
      "  training_iteration: 175\n",
      "  trial_id: 26f87_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Memory usage on this node: 9.8/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.23 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_2_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_1_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_5_25daa8673b2c24aecf80a7fc79df4e93, 0.0/6.0 CPU_group_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_4_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_0_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_3_25daa8673b2c24aecf80a7fc79df4e93)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_onelane_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_26f87_00000 | RUNNING  | 141.225.10.229:307 |    175 |          4694.55 | 2625000 | -1991.17 |              1455.33 |             -8149.87 |               3000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m ring length: 266\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m v_max: 5.42459972166245\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m ring length: 264\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m v_max: 5.329679917416892\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m ring length: 263\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m v_max: 5.282212215151455\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m ring length: 227\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m v_max: 3.570869368805752\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m ring length: 256\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m v_max: 4.949804327743507\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 1 seconds\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_26f87_00000:\n",
      "  agent_timesteps_total: 2640000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-08_12-25-22\n",
      "  done: false\n",
      "  episode_len_mean: 3000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1455.326845455007\n",
      "  episode_reward_mean: -1877.0421493861168\n",
      "  episode_reward_min: -8149.869935808996\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 880\n",
      "  experiment_id: edcbe001b14143f4bf76a6a2914a7bd0\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.17338600754737854\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: -1.1476936340332031\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.06141894310712814\n",
      "          model: {}\n",
      "          policy_loss: 0.020695975050330162\n",
      "          total_loss: 3222.7265625\n",
      "          vf_explained_var: 5.603855601776786e-09\n",
      "          vf_loss: 3222.6953125\n",
      "    num_agent_steps_sampled: 2640000\n",
      "    num_agent_steps_trained: 2640000\n",
      "    num_steps_sampled: 2640000\n",
      "    num_steps_trained: 2640000\n",
      "  iterations_since_restore: 176\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 23.374358974358973\n",
      "    ram_util_percent: 31.37435897435899\n",
      "  pid: 307\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06562423970089154\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 3.4598856342696367\n",
      "    mean_inference_ms: 0.48441600233593346\n",
      "    mean_raw_obs_processing_ms: 3.715097606625422\n",
      "  time_since_restore: 4721.642894983292\n",
      "  time_this_iter_s: 27.09225344657898\n",
      "  time_total_s: 4721.642894983292\n",
      "  timers:\n",
      "    learn_throughput: 6137.528\n",
      "    learn_time_ms: 2443.981\n",
      "    load_throughput: 9812461.594\n",
      "    load_time_ms: 1.529\n",
      "    sample_throughput: 639.089\n",
      "    sample_time_ms: 23470.917\n",
      "    update_time_ms: 1.336\n",
      "  timestamp: 1665249922\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2640000\n",
      "  training_iteration: 176\n",
      "  trial_id: 26f87_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 9.8/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.23 GiB heap, 0.0/14.65 GiB objects (0.0/6.0 CPU_group_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_1_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_4_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_0_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_2_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_5_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_3_25daa8673b2c24aecf80a7fc79df4e93)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_onelane_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_26f87_00000 | RUNNING  | 141.225.10.229:307 |    176 |          4721.64 | 2640000 | -1877.04 |              1455.33 |             -8149.87 |               3000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m ring length: 245\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m v_max: 4.427046998576354\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m ring length: 266\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m v_max: 5.42459972166245\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m ring length: 245\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m v_max: 4.427046998576354\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m ring length: 252\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m v_max: 4.759762049824181\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m ring length: 258\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m v_max: 5.044800727535787\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 1 seconds\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_26f87_00000:\n",
      "  agent_timesteps_total: 2655000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-08_12-25-47\n",
      "  done: false\n",
      "  episode_len_mean: 3000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2388.124275547652\n",
      "  episode_reward_mean: -1979.07750389564\n",
      "  episode_reward_min: -8149.869935808996\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 885\n",
      "  experiment_id: edcbe001b14143f4bf76a6a2914a7bd0\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.260079026222229\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.9580589532852173\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.05377630889415741\n",
      "          model: {}\n",
      "          policy_loss: 0.023845788091421127\n",
      "          total_loss: 3503.888916015625\n",
      "          vf_explained_var: -3.5660898678457897e-09\n",
      "          vf_loss: 3503.8505859375\n",
      "    num_agent_steps_sampled: 2655000\n",
      "    num_agent_steps_trained: 2655000\n",
      "    num_steps_sampled: 2655000\n",
      "    num_steps_trained: 2655000\n",
      "  iterations_since_restore: 177\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 25.111428571428572\n",
      "    ram_util_percent: 31.279999999999998\n",
      "  pid: 307\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06562345414922426\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 3.4596816444288017\n",
      "    mean_inference_ms: 0.484432362260023\n",
      "    mean_raw_obs_processing_ms: 3.713542814167372\n",
      "  time_since_restore: 4746.663213968277\n",
      "  time_this_iter_s: 25.02031898498535\n",
      "  time_total_s: 4746.663213968277\n",
      "  timers:\n",
      "    learn_throughput: 6265.302\n",
      "    learn_time_ms: 2394.138\n",
      "    load_throughput: 9855037.594\n",
      "    load_time_ms: 1.522\n",
      "    sample_throughput: 638.481\n",
      "    sample_time_ms: 23493.243\n",
      "    update_time_ms: 1.328\n",
      "  timestamp: 1665249947\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2655000\n",
      "  training_iteration: 177\n",
      "  trial_id: 26f87_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Memory usage on this node: 9.8/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.23 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_3_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_5_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_2_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_4_25daa8673b2c24aecf80a7fc79df4e93, 0.0/6.0 CPU_group_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_1_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_0_25daa8673b2c24aecf80a7fc79df4e93)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_onelane_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_26f87_00000 | RUNNING  | 141.225.10.229:307 |    177 |          4746.66 | 2655000 | -1979.08 |              2388.12 |             -8149.87 |               3000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m ring length: 264\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m v_max: 5.329679917416892\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m ring length: 231\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m v_max: 3.761192925277308\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m ring length: 242\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m v_max: 4.2844067680020546\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m ring length: 247\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m v_max: 4.522125250095652\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m ring length: 229\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m v_max: 3.666034803266416\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m Could not connect to TraCI server at localhost:34795 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m Could not connect to TraCI server at localhost:57509 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m Could not connect to TraCI server at localhost:34795 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 3 seconds\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m Could not connect to TraCI server at localhost:57509 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 3 seconds\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_26f87_00000:\n",
      "  agent_timesteps_total: 2670000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-08_12-26-18\n",
      "  done: false\n",
      "  episode_len_mean: 3000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2388.124275547652\n",
      "  episode_reward_mean: -2039.9007584025703\n",
      "  episode_reward_min: -8149.869935808996\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 890\n",
      "  experiment_id: edcbe001b14143f4bf76a6a2914a7bd0\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3901185393333435\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.3540726900100708\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.029823007062077522\n",
      "          model: {}\n",
      "          policy_loss: 0.01501560304313898\n",
      "          total_loss: 3692.36669921875\n",
      "          vf_explained_var: -9.679387069638778e-09\n",
      "          vf_loss: 3692.340087890625\n",
      "    num_agent_steps_sampled: 2670000\n",
      "    num_agent_steps_trained: 2670000\n",
      "    num_steps_sampled: 2670000\n",
      "    num_steps_trained: 2670000\n",
      "  iterations_since_restore: 178\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.47555555555555\n",
      "    ram_util_percent: 31.217777777777773\n",
      "  pid: 307\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0656225830654028\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 3.4594684489790275\n",
      "    mean_inference_ms: 0.4844480001358658\n",
      "    mean_raw_obs_processing_ms: 3.7124987816087\n",
      "  time_since_restore: 4777.808292627335\n",
      "  time_this_iter_s: 31.145078659057617\n",
      "  time_total_s: 4777.808292627335\n",
      "  timers:\n",
      "    learn_throughput: 6270.728\n",
      "    learn_time_ms: 2392.067\n",
      "    load_throughput: 11006553.419\n",
      "    load_time_ms: 1.363\n",
      "    sample_throughput: 623.582\n",
      "    sample_time_ms: 24054.563\n",
      "    update_time_ms: 1.336\n",
      "  timestamp: 1665249978\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2670000\n",
      "  training_iteration: 178\n",
      "  trial_id: 26f87_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 9.8/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.23 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_0_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_4_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_5_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_3_25daa8673b2c24aecf80a7fc79df4e93, 0.0/6.0 CPU_group_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_2_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_1_25daa8673b2c24aecf80a7fc79df4e93)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_onelane_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_26f87_00000 | RUNNING  | 141.225.10.229:307 |    178 |          4777.81 | 2670000 |  -2039.9 |              2388.12 |             -8149.87 |               3000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=307)\u001b[0m 2022-10-08 12:26:18,916\tWARNING ppo.py:242 -- The magnitude of your environment rewards are more than 204.0x the scale of `vf_clip_param`. This means that it will take more than 204.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m ring length: 220\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m v_max: 3.2377399006821497\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m ring length: 259\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m v_max: 5.092292348291853\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m ring length: 262\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m v_max: 5.234739483008334\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m ring length: 239\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m v_max: 4.141741239205832\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m ring length: 244\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m v_max: 4.379503211387676\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 1 seconds\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_26f87_00000:\n",
      "  agent_timesteps_total: 2685000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-08_12-26-44\n",
      "  done: false\n",
      "  episode_len_mean: 3000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2870.2741277451596\n",
      "  episode_reward_mean: -2091.147911642529\n",
      "  episode_reward_min: -8149.869935808996\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 895\n",
      "  experiment_id: edcbe001b14143f4bf76a6a2914a7bd0\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3901185393333435\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.9394162893295288\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.05590121075510979\n",
      "          model: {}\n",
      "          policy_loss: 0.02748888172209263\n",
      "          total_loss: 3752.6845703125\n",
      "          vf_explained_var: 3.5660898678457897e-09\n",
      "          vf_loss: 3752.63525390625\n",
      "    num_agent_steps_sampled: 2685000\n",
      "    num_agent_steps_trained: 2685000\n",
      "    num_steps_sampled: 2685000\n",
      "    num_steps_trained: 2685000\n",
      "  iterations_since_restore: 179\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 24.39189189189189\n",
      "    ram_util_percent: 31.267567567567557\n",
      "  pid: 307\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06562153625171507\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 3.4592669272550065\n",
      "    mean_inference_ms: 0.48446230471191115\n",
      "    mean_raw_obs_processing_ms: 3.7114222201397022\n",
      "  time_since_restore: 4803.660845518112\n",
      "  time_this_iter_s: 25.852552890777588\n",
      "  time_total_s: 4803.660845518112\n",
      "  timers:\n",
      "    learn_throughput: 6269.849\n",
      "    learn_time_ms: 2392.402\n",
      "    load_throughput: 10006769.309\n",
      "    load_time_ms: 1.499\n",
      "    sample_throughput: 625.68\n",
      "    sample_time_ms: 23973.904\n",
      "    update_time_ms: 1.337\n",
      "  timestamp: 1665250004\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2685000\n",
      "  training_iteration: 179\n",
      "  trial_id: 26f87_00000\n",
      "  \n",
      "\u001b[2m\u001b[36m(pid=307)\u001b[0m 2022-10-08 12:26:44,848\tWARNING ppo.py:242 -- The magnitude of your environment rewards are more than 209.0x the scale of `vf_clip_param`. This means that it will take more than 209.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "== Status ==\n",
      "Memory usage on this node: 9.8/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.23 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_0_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_1_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_4_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_5_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_3_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_2_25daa8673b2c24aecf80a7fc79df4e93, 0.0/6.0 CPU_group_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 accelerator_type:G)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_onelane_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_26f87_00000 | RUNNING  | 141.225.10.229:307 |    179 |          4803.66 | 2685000 | -2091.15 |              2870.27 |             -8149.87 |               3000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m ring length: 243\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m v_max: 4.331956438196479\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m ring length: 233\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m v_max: 3.856343201216534\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m ring length: 247\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m v_max: 4.522125250095652\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m ring length: 245\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m v_max: 4.427046998576354\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m ring length: 246\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m v_max: 4.474587708995648\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 1 seconds\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_26f87_00000:\n",
      "  agent_timesteps_total: 2700000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-08_12-27-09\n",
      "  done: false\n",
      "  episode_len_mean: 3000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2870.2741277451596\n",
      "  episode_reward_mean: -2017.2200310942458\n",
      "  episode_reward_min: -8149.869935808996\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 900\n",
      "  experiment_id: edcbe001b14143f4bf76a6a2914a7bd0\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5851777791976929\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: -1.0063426494598389\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.029561763629317284\n",
      "          model: {}\n",
      "          policy_loss: 0.010842215269804\n",
      "          total_loss: 3282.847412109375\n",
      "          vf_explained_var: 3.056648489874192e-09\n",
      "          vf_loss: 3282.8193359375\n",
      "    num_agent_steps_sampled: 2700000\n",
      "    num_agent_steps_trained: 2700000\n",
      "    num_steps_sampled: 2700000\n",
      "    num_steps_trained: 2700000\n",
      "  iterations_since_restore: 180\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 24.691666666666666\n",
      "    ram_util_percent: 31.313888888888883\n",
      "  pid: 307\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0656204434860576\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 3.4590697048828747\n",
      "    mean_inference_ms: 0.4844755667512322\n",
      "    mean_raw_obs_processing_ms: 3.710386937974164\n",
      "  time_since_restore: 4828.442032337189\n",
      "  time_this_iter_s: 24.781186819076538\n",
      "  time_total_s: 4828.442032337189\n",
      "  timers:\n",
      "    learn_throughput: 6432.779\n",
      "    learn_time_ms: 2331.807\n",
      "    load_throughput: 10132800.773\n",
      "    load_time_ms: 1.48\n",
      "    sample_throughput: 629.329\n",
      "    sample_time_ms: 23834.924\n",
      "    update_time_ms: 1.461\n",
      "  timestamp: 1665250029\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2700000\n",
      "  training_iteration: 180\n",
      "  trial_id: 26f87_00000\n",
      "  \n",
      "\u001b[2m\u001b[36m(pid=307)\u001b[0m 2022-10-08 12:27:09,733\tWARNING ppo.py:242 -- The magnitude of your environment rewards are more than 202.0x the scale of `vf_clip_param`. This means that it will take more than 202.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "== Status ==\n",
      "Memory usage on this node: 9.8/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.23 GiB heap, 0.0/14.65 GiB objects (0.0/6.0 CPU_group_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_1_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_5_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_4_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_2_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_3_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_0_25daa8673b2c24aecf80a7fc79df4e93)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_onelane_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_26f87_00000 | RUNNING  | 141.225.10.229:307 |    180 |          4828.44 | 2700000 | -2017.22 |              2870.27 |             -8149.87 |               3000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m ring length: 266\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m v_max: 5.42459972166245\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m ring length: 237\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m v_max: 4.04661795519353\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m ring length: 224\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m v_max: 3.4281086136538996\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m ring length: 236\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m v_max: 3.999052674951912\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m ring length: 230\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m v_max: 3.7136148111012934\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m Could not connect to TraCI server at localhost:34447 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m Could not connect to TraCI server at localhost:34795 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m Could not connect to TraCI server at localhost:36289 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m Could not connect to TraCI server at localhost:57509 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m Could not connect to TraCI server at localhost:54035 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m Could not connect to TraCI server at localhost:34795 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 3 seconds\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m Could not connect to TraCI server at localhost:34447 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 3 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m Could not connect to TraCI server at localhost:36289 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 3 seconds\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m Could not connect to TraCI server at localhost:57509 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 3 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m Could not connect to TraCI server at localhost:54035 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 3 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 1 seconds\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_26f87_00000:\n",
      "  agent_timesteps_total: 2715000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-08_12-27-41\n",
      "  done: false\n",
      "  episode_len_mean: 3000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2870.2741277451596\n",
      "  episode_reward_mean: -1956.0391232053635\n",
      "  episode_reward_min: -8149.869935808996\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 905\n",
      "  experiment_id: edcbe001b14143f4bf76a6a2914a7bd0\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5851777791976929\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.28332072496414185\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013734111562371254\n",
      "          model: {}\n",
      "          policy_loss: 0.004752952139824629\n",
      "          total_loss: 3388.447021484375\n",
      "          vf_explained_var: -1.1207711203553572e-08\n",
      "          vf_loss: 3388.434326171875\n",
      "    num_agent_steps_sampled: 2715000\n",
      "    num_agent_steps_trained: 2715000\n",
      "    num_steps_sampled: 2715000\n",
      "    num_steps_trained: 2715000\n",
      "  iterations_since_restore: 181\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.87727272727273\n",
      "    ram_util_percent: 31.27954545454546\n",
      "  pid: 307\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06561888005146504\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 3.4588749651381936\n",
      "    mean_inference_ms: 0.4844857717790469\n",
      "    mean_raw_obs_processing_ms: 3.709372065544261\n",
      "  time_since_restore: 4859.604816675186\n",
      "  time_this_iter_s: 31.162784337997437\n",
      "  time_total_s: 4859.604816675186\n",
      "  timers:\n",
      "    learn_throughput: 6527.933\n",
      "    learn_time_ms: 2297.818\n",
      "    load_throughput: 9860907.182\n",
      "    load_time_ms: 1.521\n",
      "    sample_throughput: 615.792\n",
      "    sample_time_ms: 24358.881\n",
      "    update_time_ms: 1.654\n",
      "  timestamp: 1665250061\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2715000\n",
      "  training_iteration: 181\n",
      "  trial_id: 26f87_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 9.8/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.23 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_5_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_1_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_4_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_3_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_2_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 accelerator_type:G, 0.0/6.0 CPU_group_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_0_25daa8673b2c24aecf80a7fc79df4e93)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_onelane_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_26f87_00000 | RUNNING  | 141.225.10.229:307 |    181 |           4859.6 | 2715000 | -1956.04 |              2870.27 |             -8149.87 |               3000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m ring length: 258\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m v_max: 5.044800727535787\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m ring length: 266\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m v_max: 5.42459972166245\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m ring length: 244\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m v_max: 4.379503211387676\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m ring length: 242\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m v_max: 4.2844067680020546\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m ring length: 258\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m v_max: 5.044800727535787\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 1 seconds\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_26f87_00000:\n",
      "  agent_timesteps_total: 2730000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-08_12-28-06\n",
      "  done: false\n",
      "  episode_len_mean: 3000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2870.2741277451596\n",
      "  episode_reward_mean: -1866.3750061705061\n",
      "  episode_reward_min: -8149.869935808996\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 910\n",
      "  experiment_id: edcbe001b14143f4bf76a6a2914a7bd0\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5851777791976929\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: -1.8675642013549805\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.023239444941282272\n",
      "          model: {}\n",
      "          policy_loss: 0.0070831202901899815\n",
      "          total_loss: 2658.92919921875\n",
      "          vf_explained_var: -6.113296979748384e-09\n",
      "          vf_loss: 2658.908447265625\n",
      "    num_agent_steps_sampled: 2730000\n",
      "    num_agent_steps_trained: 2730000\n",
      "    num_steps_sampled: 2730000\n",
      "    num_steps_trained: 2730000\n",
      "  iterations_since_restore: 182\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 24.36388888888889\n",
      "    ram_util_percent: 31.35833333333334\n",
      "  pid: 307\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06561718792011188\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 3.4586808624587944\n",
      "    mean_inference_ms: 0.48449373162144477\n",
      "    mean_raw_obs_processing_ms: 3.708351471733587\n",
      "  time_since_restore: 4884.779177188873\n",
      "  time_this_iter_s: 25.174360513687134\n",
      "  time_total_s: 4884.779177188873\n",
      "  timers:\n",
      "    learn_throughput: 6529.527\n",
      "    learn_time_ms: 2297.257\n",
      "    load_throughput: 8840039.342\n",
      "    load_time_ms: 1.697\n",
      "    sample_throughput: 614.721\n",
      "    sample_time_ms: 24401.314\n",
      "    update_time_ms: 1.784\n",
      "  timestamp: 1665250086\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2730000\n",
      "  training_iteration: 182\n",
      "  trial_id: 26f87_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Memory usage on this node: 9.8/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.23 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_0_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_5_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_4_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_3_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_2_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_1_25daa8673b2c24aecf80a7fc79df4e93, 0.0/6.0 CPU_group_25daa8673b2c24aecf80a7fc79df4e93)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_onelane_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_26f87_00000 | RUNNING  | 141.225.10.229:307 |    182 |          4884.78 | 2730000 | -1866.38 |              2870.27 |             -8149.87 |               3000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m ring length: 250\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m v_max: 4.664717904914125\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m ring length: 245\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m v_max: 4.427046998576354\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m ring length: 221\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m v_max: 3.285334164169417\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m ring length: 246\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m v_max: 4.474587708995648\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m ring length: 222\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m v_max: 3.3329270738617227\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 1 seconds\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_26f87_00000:\n",
      "  agent_timesteps_total: 2745000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-08_12-28-32\n",
      "  done: false\n",
      "  episode_len_mean: 3000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2870.2741277451596\n",
      "  episode_reward_mean: -1762.6015275008226\n",
      "  episode_reward_min: -8149.869935808996\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 915\n",
      "  experiment_id: edcbe001b14143f4bf76a6a2914a7bd0\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5851777791976929\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: -1.2679554224014282\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.026875220239162445\n",
      "          model: {}\n",
      "          policy_loss: 0.009888596832752228\n",
      "          total_loss: 3157.632568359375\n",
      "          vf_explained_var: 8.151062935723985e-09\n",
      "          vf_loss: 3157.60693359375\n",
      "    num_agent_steps_sampled: 2745000\n",
      "    num_agent_steps_trained: 2745000\n",
      "    num_steps_sampled: 2745000\n",
      "    num_steps_trained: 2745000\n",
      "  iterations_since_restore: 183\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 23.223684210526315\n",
      "    ram_util_percent: 31.347368421052646\n",
      "  pid: 307\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06561528215258777\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 3.4585071220205426\n",
      "    mean_inference_ms: 0.4845006058604194\n",
      "    mean_raw_obs_processing_ms: 3.7074453845453736\n",
      "  time_since_restore: 4911.325086593628\n",
      "  time_this_iter_s: 26.54590940475464\n",
      "  time_total_s: 4911.325086593628\n",
      "  timers:\n",
      "    learn_throughput: 6424.685\n",
      "    learn_time_ms: 2334.745\n",
      "    load_throughput: 8641160.312\n",
      "    load_time_ms: 1.736\n",
      "    sample_throughput: 613.691\n",
      "    sample_time_ms: 24442.286\n",
      "    update_time_ms: 1.756\n",
      "  timestamp: 1665250112\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2745000\n",
      "  training_iteration: 183\n",
      "  trial_id: 26f87_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 9.8/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.23 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_5_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_2_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_3_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_4_25daa8673b2c24aecf80a7fc79df4e93, 0.0/6.0 CPU_group_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_0_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_1_25daa8673b2c24aecf80a7fc79df4e93)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_onelane_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_26f87_00000 | RUNNING  | 141.225.10.229:307 |    183 |          4911.33 | 2745000 |  -1762.6 |              2870.27 |             -8149.87 |               3000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m ring length: 237\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m v_max: 4.04661795519353\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m ring length: 234\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m v_max: 3.903915223349057\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m ring length: 247\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m v_max: 4.522125250095652\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m ring length: 259\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m v_max: 5.092292348291853\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m ring length: 268\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m v_max: 5.5194978538368025\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m Could not connect to TraCI server at localhost:57509 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m Could not connect to TraCI server at localhost:34795 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m Could not connect to TraCI server at localhost:54035 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 2 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m Could not connect to TraCI server at localhost:34447 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m Could not connect to TraCI server at localhost:57509 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 3 seconds\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m Could not connect to TraCI server at localhost:34795 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 3 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m Could not connect to TraCI server at localhost:54035 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 3 seconds\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m Could not connect to TraCI server at localhost:34447 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 3 seconds\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_26f87_00000:\n",
      "  agent_timesteps_total: 2760000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-08_12-29-03\n",
      "  done: false\n",
      "  episode_len_mean: 3000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2870.2741277451596\n",
      "  episode_reward_mean: -1753.397442231074\n",
      "  episode_reward_min: -8149.869935808996\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 920\n",
      "  experiment_id: edcbe001b14143f4bf76a6a2914a7bd0\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5851777791976929\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.6414239406585693\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012648344971239567\n",
      "          model: {}\n",
      "          policy_loss: 0.00411427766084671\n",
      "          total_loss: 3097.00244140625\n",
      "          vf_explained_var: -7.641621557752387e-09\n",
      "          vf_loss: 3096.99072265625\n",
      "    num_agent_steps_sampled: 2760000\n",
      "    num_agent_steps_trained: 2760000\n",
      "    num_steps_sampled: 2760000\n",
      "    num_steps_trained: 2760000\n",
      "  iterations_since_restore: 184\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.669767441860465\n",
      "    ram_util_percent: 31.318604651162797\n",
      "  pid: 307\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0656133837524372\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 3.4583370476608737\n",
      "    mean_inference_ms: 0.4845075288850981\n",
      "    mean_raw_obs_processing_ms: 3.707010912513533\n",
      "  time_since_restore: 4941.392063856125\n",
      "  time_this_iter_s: 30.06697726249695\n",
      "  time_total_s: 4941.392063856125\n",
      "  timers:\n",
      "    learn_throughput: 6518.955\n",
      "    learn_time_ms: 2300.982\n",
      "    load_throughput: 8637008.36\n",
      "    load_time_ms: 1.737\n",
      "    sample_throughput: 601.508\n",
      "    sample_time_ms: 24937.336\n",
      "    update_time_ms: 1.737\n",
      "  timestamp: 1665250143\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2760000\n",
      "  training_iteration: 184\n",
      "  trial_id: 26f87_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 9.8/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.23 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_1_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_5_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_2_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_0_25daa8673b2c24aecf80a7fc79df4e93, 0.0/6.0 CPU_group_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_4_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_3_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 accelerator_type:G)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_onelane_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_26f87_00000 | RUNNING  | 141.225.10.229:307 |    184 |          4941.39 | 2760000 |  -1753.4 |              2870.27 |             -8149.87 |               3000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m ring length: 261\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m v_max: 5.187261846097525\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m ring length: 246\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m v_max: 4.474587708995648\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m ring length: 221\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m v_max: 3.285334164169417\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m ring length: 241\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m v_max: 4.236854288051661\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m ring length: 230\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m v_max: 3.7136148111012934\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 1 seconds\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_26f87_00000:\n",
      "  agent_timesteps_total: 2775000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-08_12-29-29\n",
      "  done: false\n",
      "  episode_len_mean: 3000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2870.2741277451596\n",
      "  episode_reward_mean: -1804.871538870011\n",
      "  episode_reward_min: -8149.869935808996\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 925\n",
      "  experiment_id: edcbe001b14143f4bf76a6a2914a7bd0\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5851777791976929\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.453515648841858\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0160432867705822\n",
      "          model: {}\n",
      "          policy_loss: 0.008884639479219913\n",
      "          total_loss: 3458.85986328125\n",
      "          vf_explained_var: 4.58497284583359e-09\n",
      "          vf_loss: 3458.841796875\n",
      "    num_agent_steps_sampled: 2775000\n",
      "    num_agent_steps_trained: 2775000\n",
      "    num_steps_sampled: 2775000\n",
      "    num_steps_trained: 2775000\n",
      "  iterations_since_restore: 185\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 23.45263157894737\n",
      "    ram_util_percent: 31.363157894736858\n",
      "  pid: 307\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06561167798278286\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 3.4581813401527546\n",
      "    mean_inference_ms: 0.48451517679816164\n",
      "    mean_raw_obs_processing_ms: 3.706726790126085\n",
      "  time_since_restore: 4967.278487205505\n",
      "  time_this_iter_s: 25.886423349380493\n",
      "  time_total_s: 4967.278487205505\n",
      "  timers:\n",
      "    learn_throughput: 6587.834\n",
      "    learn_time_ms: 2276.924\n",
      "    load_throughput: 9430488.353\n",
      "    load_time_ms: 1.591\n",
      "    sample_throughput: 600.356\n",
      "    sample_time_ms: 24985.194\n",
      "    update_time_ms: 1.725\n",
      "  timestamp: 1665250169\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2775000\n",
      "  training_iteration: 185\n",
      "  trial_id: 26f87_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 9.8/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.23 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_0_25daa8673b2c24aecf80a7fc79df4e93, 0.0/6.0 CPU_group_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_4_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_3_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_1_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_5_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_2_25daa8673b2c24aecf80a7fc79df4e93)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_onelane_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_26f87_00000 | RUNNING  | 141.225.10.229:307 |    185 |          4967.28 | 2775000 | -1804.87 |              2870.27 |             -8149.87 |               3000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m ring length: 223\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m v_max: 3.3805185755587788\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m ring length: 259\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m v_max: 5.092292348291853\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m ring length: 246\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m v_max: 4.474587708995648\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m ring length: 270\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m v_max: 5.6143732387852054\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m ring length: 244\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m v_max: 4.379503211387676\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 1 seconds\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_26f87_00000:\n",
      "  agent_timesteps_total: 2790000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-08_12-29-54\n",
      "  done: false\n",
      "  episode_len_mean: 3000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2870.2741277451596\n",
      "  episode_reward_mean: -1888.9386518220153\n",
      "  episode_reward_min: -8340.588520434227\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 930\n",
      "  experiment_id: edcbe001b14143f4bf76a6a2914a7bd0\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5851777791976929\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.8970360159873962\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.025064490735530853\n",
      "          model: {}\n",
      "          policy_loss: 0.014358100481331348\n",
      "          total_loss: 3420.452392578125\n",
      "          vf_explained_var: 3.056648489874192e-09\n",
      "          vf_loss: 3420.424072265625\n",
      "    num_agent_steps_sampled: 2790000\n",
      "    num_agent_steps_trained: 2790000\n",
      "    num_steps_sampled: 2790000\n",
      "    num_steps_trained: 2790000\n",
      "  iterations_since_restore: 186\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 24.752777777777776\n",
      "    ram_util_percent: 31.383333333333326\n",
      "  pid: 307\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06560982918465064\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 3.4580099913035327\n",
      "    mean_inference_ms: 0.48452185720791824\n",
      "    mean_raw_obs_processing_ms: 3.705944259688442\n",
      "  time_since_restore: 4992.692663192749\n",
      "  time_this_iter_s: 25.414175987243652\n",
      "  time_total_s: 4992.692663192749\n",
      "  timers:\n",
      "    learn_throughput: 6636.134\n",
      "    learn_time_ms: 2260.352\n",
      "    load_throughput: 9205841.21\n",
      "    load_time_ms: 1.629\n",
      "    sample_throughput: 604.012\n",
      "    sample_time_ms: 24833.94\n",
      "    update_time_ms: 1.712\n",
      "  timestamp: 1665250194\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2790000\n",
      "  training_iteration: 186\n",
      "  trial_id: 26f87_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 9.8/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.23 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_0_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_5_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_4_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_2_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_1_25daa8673b2c24aecf80a7fc79df4e93, 0.0/6.0 CPU_group_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_3_25daa8673b2c24aecf80a7fc79df4e93)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_onelane_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_26f87_00000 | RUNNING  | 141.225.10.229:307 |    186 |          4992.69 | 2790000 | -1888.94 |              2870.27 |             -8340.59 |               3000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m ring length: 220\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m v_max: 3.2377399006821497\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m ring length: 252\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m v_max: 4.759762049824181\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m ring length: 244\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m v_max: 4.379503211387676\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m ring length: 239\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m v_max: 4.141741239205832\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m ring length: 256\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m v_max: 4.949804327743507\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 1 seconds\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_26f87_00000:\n",
      "  agent_timesteps_total: 2805000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-08_12-30-20\n",
      "  done: false\n",
      "  episode_len_mean: 3000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2870.2741277451596\n",
      "  episode_reward_mean: -1933.332131679838\n",
      "  episode_reward_min: -8340.588520434227\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 935\n",
      "  experiment_id: edcbe001b14143f4bf76a6a2914a7bd0\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5851777791976929\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.6179674863815308\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0330522321164608\n",
      "          model: {}\n",
      "          policy_loss: 0.020627852529287338\n",
      "          total_loss: 3476.1240234375\n",
      "          vf_explained_var: -5.094414223805188e-09\n",
      "          vf_loss: 3476.083984375\n",
      "    num_agent_steps_sampled: 2805000\n",
      "    num_agent_steps_trained: 2805000\n",
      "    num_steps_sampled: 2805000\n",
      "    num_steps_trained: 2805000\n",
      "  iterations_since_restore: 187\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 24.488888888888887\n",
      "    ram_util_percent: 31.46388888888889\n",
      "  pid: 307\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06560817491310306\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 3.457869611684481\n",
      "    mean_inference_ms: 0.4845295888280938\n",
      "    mean_raw_obs_processing_ms: 3.7051503329714093\n",
      "  time_since_restore: 5018.08442902565\n",
      "  time_this_iter_s: 25.391765832901\n",
      "  time_total_s: 5018.08442902565\n",
      "  timers:\n",
      "    learn_throughput: 6403.555\n",
      "    learn_time_ms: 2342.449\n",
      "    load_throughput: 8407777.733\n",
      "    load_time_ms: 1.784\n",
      "    sample_throughput: 605.119\n",
      "    sample_time_ms: 24788.498\n",
      "    update_time_ms: 1.864\n",
      "  timestamp: 1665250220\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2805000\n",
      "  training_iteration: 187\n",
      "  trial_id: 26f87_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 9.8/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.23 GiB heap, 0.0/14.65 GiB objects (0.0/6.0 CPU_group_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_5_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_2_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_0_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_3_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_1_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_4_25daa8673b2c24aecf80a7fc79df4e93)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_onelane_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_26f87_00000 | RUNNING  | 141.225.10.229:307 |    187 |          5018.08 | 2805000 | -1933.33 |              2870.27 |             -8340.59 |               3000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m ring length: 270\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m v_max: 5.6143732387852054\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m ring length: 231\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m v_max: 3.761192925277308\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m ring length: 229\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m v_max: 3.666034803266416\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m ring length: 265\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m v_max: 5.37714246265477\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m ring length: 235\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m v_max: 3.9514850725282584\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 1 seconds\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_26f87_00000:\n",
      "  agent_timesteps_total: 2820000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-08_12-30-46\n",
      "  done: false\n",
      "  episode_len_mean: 3000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2870.2741277451596\n",
      "  episode_reward_mean: -1911.1819218681983\n",
      "  episode_reward_min: -8340.588520434227\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 940\n",
      "  experiment_id: edcbe001b14143f4bf76a6a2914a7bd0\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5851777791976929\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: -0.7320817708969116\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.02287897653877735\n",
      "          model: {}\n",
      "          policy_loss: 0.006671463139355183\n",
      "          total_loss: 3171.4296875\n",
      "          vf_explained_var: 1.528324244937096e-09\n",
      "          vf_loss: 3171.409423828125\n",
      "    num_agent_steps_sampled: 2820000\n",
      "    num_agent_steps_trained: 2820000\n",
      "    num_steps_sampled: 2820000\n",
      "    num_steps_trained: 2820000\n",
      "  iterations_since_restore: 188\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 23.71578947368421\n",
      "    ram_util_percent: 31.36842105263159\n",
      "  pid: 307\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06560654758484544\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 3.4577190014240067\n",
      "    mean_inference_ms: 0.48453764328970483\n",
      "    mean_raw_obs_processing_ms: 3.704412000615615\n",
      "  time_since_restore: 5044.3103795051575\n",
      "  time_this_iter_s: 26.225950479507446\n",
      "  time_total_s: 5044.3103795051575\n",
      "  timers:\n",
      "    learn_throughput: 6436.829\n",
      "    learn_time_ms: 2330.34\n",
      "    load_throughput: 8446721.444\n",
      "    load_time_ms: 1.776\n",
      "    sample_throughput: 617.056\n",
      "    sample_time_ms: 24308.957\n",
      "    update_time_ms: 1.862\n",
      "  timestamp: 1665250246\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2820000\n",
      "  training_iteration: 188\n",
      "  trial_id: 26f87_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 9.8/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.23 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_1_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_5_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_3_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_0_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_4_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_2_25daa8673b2c24aecf80a7fc79df4e93, 0.0/6.0 CPU_group_25daa8673b2c24aecf80a7fc79df4e93)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_onelane_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_26f87_00000 | RUNNING  | 141.225.10.229:307 |    188 |          5044.31 | 2820000 | -1911.18 |              2870.27 |             -8340.59 |               3000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m ring length: 246\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m v_max: 4.474587708995648\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m ring length: 241\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m v_max: 4.236854288051661\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m ring length: 244\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m v_max: 4.379503211387676\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m ring length: 261\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m v_max: 5.187261846097525\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m ring length: 231\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m v_max: 3.761192925277308\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m Could not connect to TraCI server at localhost:54035 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m Could not connect to TraCI server at localhost:34795 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m Could not connect to TraCI server at localhost:34447 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m Could not connect to TraCI server at localhost:57509 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m Could not connect to TraCI server at localhost:36289 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m Could not connect to TraCI server at localhost:34795 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 3 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m Could not connect to TraCI server at localhost:54035 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 3 seconds\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m Could not connect to TraCI server at localhost:34447 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 3 seconds\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m Could not connect to TraCI server at localhost:57509 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 3 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m Could not connect to TraCI server at localhost:36289 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 3 seconds\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m Could not connect to TraCI server at localhost:34795 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m Could not connect to TraCI server at localhost:34447 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m Could not connect to TraCI server at localhost:57509 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m Could not connect to TraCI server at localhost:36289 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 2 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m Could not connect to TraCI server at localhost:34795 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 3 seconds\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m Could not connect to TraCI server at localhost:34447 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 3 seconds\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m Could not connect to TraCI server at localhost:57509 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 3 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m Could not connect to TraCI server at localhost:36289 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 3 seconds\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_26f87_00000:\n",
      "  agent_timesteps_total: 2835000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-08_12-31-22\n",
      "  done: false\n",
      "  episode_len_mean: 3000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2870.2741277451596\n",
      "  episode_reward_mean: -1964.5598509008175\n",
      "  episode_reward_min: -8340.588520434227\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 945\n",
      "  experiment_id: edcbe001b14143f4bf76a6a2914a7bd0\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5851777791976929\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.24217724800109863\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.04927981272339821\n",
      "          model: {}\n",
      "          policy_loss: 0.024936648085713387\n",
      "          total_loss: 3396.61083984375\n",
      "          vf_explained_var: -1.171715258152517e-08\n",
      "          vf_loss: 3396.556884765625\n",
      "    num_agent_steps_sampled: 2835000\n",
      "    num_agent_steps_trained: 2835000\n",
      "    num_steps_sampled: 2835000\n",
      "    num_steps_trained: 2835000\n",
      "  iterations_since_restore: 189\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 18.680769230769233\n",
      "    ram_util_percent: 31.290384615384617\n",
      "  pid: 307\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06560484546413256\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 3.457545311816655\n",
      "    mean_inference_ms: 0.48454538154122906\n",
      "    mean_raw_obs_processing_ms: 3.7045542912396887\n",
      "  time_since_restore: 5080.567144155502\n",
      "  time_this_iter_s: 36.25676465034485\n",
      "  time_total_s: 5080.567144155502\n",
      "  timers:\n",
      "    learn_throughput: 6452.189\n",
      "    learn_time_ms: 2324.792\n",
      "    load_throughput: 8482137.705\n",
      "    load_time_ms: 1.768\n",
      "    sample_throughput: 591.602\n",
      "    sample_time_ms: 25354.866\n",
      "    update_time_ms: 1.867\n",
      "  timestamp: 1665250282\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2835000\n",
      "  training_iteration: 189\n",
      "  trial_id: 26f87_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 9.8/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.23 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_3_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_5_25daa8673b2c24aecf80a7fc79df4e93, 0.0/6.0 CPU_group_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_0_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_2_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_1_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_4_25daa8673b2c24aecf80a7fc79df4e93)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_onelane_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_26f87_00000 | RUNNING  | 141.225.10.229:307 |    189 |          5080.57 | 2835000 | -1964.56 |              2870.27 |             -8340.59 |               3000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m ring length: 257\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m v_max: 4.997304682316517\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m ring length: 233\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m v_max: 3.856343201216534\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m ring length: 269\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m v_max: 5.566938458220921\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m ring length: 246\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m v_max: 4.474587708995648\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m ring length: 220\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m v_max: 3.2377399006821497\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 1 seconds\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_26f87_00000:\n",
      "  agent_timesteps_total: 2850000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-08_12-31-48\n",
      "  done: false\n",
      "  episode_len_mean: 3000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2870.2741277451596\n",
      "  episode_reward_mean: -1885.8162586051578\n",
      "  episode_reward_min: -8340.588520434227\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 950\n",
      "  experiment_id: edcbe001b14143f4bf76a6a2914a7bd0\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.8777666687965393\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: -0.17134054005146027\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.020164895802736282\n",
      "          model: {}\n",
      "          policy_loss: 0.009802374057471752\n",
      "          total_loss: 3268.418212890625\n",
      "          vf_explained_var: -2.088709827319235e-08\n",
      "          vf_loss: 3268.39111328125\n",
      "    num_agent_steps_sampled: 2850000\n",
      "    num_agent_steps_trained: 2850000\n",
      "    num_steps_sampled: 2850000\n",
      "    num_steps_trained: 2850000\n",
      "  iterations_since_restore: 190\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 24.119444444444444\n",
      "    ram_util_percent: 31.35833333333334\n",
      "  pid: 307\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06560332869166134\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 3.4573901186432288\n",
      "    mean_inference_ms: 0.4845542262145585\n",
      "    mean_raw_obs_processing_ms: 3.704685519137792\n",
      "  time_since_restore: 5106.110896348953\n",
      "  time_this_iter_s: 25.543752193450928\n",
      "  time_total_s: 5106.110896348953\n",
      "  timers:\n",
      "    learn_throughput: 6413.573\n",
      "    learn_time_ms: 2338.79\n",
      "    load_throughput: 8404408.288\n",
      "    load_time_ms: 1.785\n",
      "    sample_throughput: 590.145\n",
      "    sample_time_ms: 25417.466\n",
      "    update_time_ms: 1.716\n",
      "  timestamp: 1665250308\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2850000\n",
      "  training_iteration: 190\n",
      "  trial_id: 26f87_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 9.8/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.23 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_3_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_5_25daa8673b2c24aecf80a7fc79df4e93, 0.0/6.0 CPU_group_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_4_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_1_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_2_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_0_25daa8673b2c24aecf80a7fc79df4e93)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_onelane_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_26f87_00000 | RUNNING  | 141.225.10.229:307 |    190 |          5106.11 | 2850000 | -1885.82 |              2870.27 |             -8340.59 |               3000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m ring length: 221\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m v_max: 3.285334164169417\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m ring length: 226\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m v_max: 3.5232840694769565\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m ring length: 241\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m v_max: 4.236854288051661\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m ring length: 264\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m v_max: 5.329679917416892\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m ring length: 234\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m v_max: 3.903915223349057\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 1 seconds\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_26f87_00000:\n",
      "  agent_timesteps_total: 2865000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-08_12-32-14\n",
      "  done: false\n",
      "  episode_len_mean: 3000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2870.2741277451596\n",
      "  episode_reward_mean: -1856.843771435448\n",
      "  episode_reward_min: -8340.588520434227\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 955\n",
      "  experiment_id: edcbe001b14143f4bf76a6a2914a7bd0\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.8777666687965393\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: -1.5114880800247192\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01875985413789749\n",
      "          model: {}\n",
      "          policy_loss: 0.00759407551959157\n",
      "          total_loss: 2876.848388671875\n",
      "          vf_explained_var: -1.0188828669654981e-09\n",
      "          vf_loss: 2876.82421875\n",
      "    num_agent_steps_sampled: 2865000\n",
      "    num_agent_steps_trained: 2865000\n",
      "    num_steps_sampled: 2865000\n",
      "    num_steps_trained: 2865000\n",
      "  iterations_since_restore: 191\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 23.386842105263156\n",
      "    ram_util_percent: 31.35789473684212\n",
      "  pid: 307\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06560168211828907\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 3.457230232656556\n",
      "    mean_inference_ms: 0.48456160052710073\n",
      "    mean_raw_obs_processing_ms: 3.704846543923236\n",
      "  time_since_restore: 5132.070897817612\n",
      "  time_this_iter_s: 25.960001468658447\n",
      "  time_total_s: 5132.070897817612\n",
      "  timers:\n",
      "    learn_throughput: 6344.593\n",
      "    learn_time_ms: 2364.218\n",
      "    load_throughput: 8369525.482\n",
      "    load_time_ms: 1.792\n",
      "    sample_throughput: 603.085\n",
      "    sample_time_ms: 24872.112\n",
      "    update_time_ms: 1.527\n",
      "  timestamp: 1665250334\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2865000\n",
      "  training_iteration: 191\n",
      "  trial_id: 26f87_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 9.8/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.23 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_3_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_2_25daa8673b2c24aecf80a7fc79df4e93, 0.0/6.0 CPU_group_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_1_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_5_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_4_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_0_25daa8673b2c24aecf80a7fc79df4e93)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_onelane_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_26f87_00000 | RUNNING  | 141.225.10.229:307 |    191 |          5132.07 | 2865000 | -1856.84 |              2870.27 |             -8340.59 |               3000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m ring length: 239\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m v_max: 4.141741239205832\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m ring length: 253\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m v_max: 4.807278529691987\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m ring length: 225\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m v_max: 3.4756971311168834\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m ring length: 234\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m v_max: 3.903915223349057\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m ring length: 240\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m v_max: 4.189299083856172\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 1 seconds\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_26f87_00000:\n",
      "  agent_timesteps_total: 2880000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-08_12-32-39\n",
      "  done: false\n",
      "  episode_len_mean: 3000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2870.2741277451596\n",
      "  episode_reward_mean: -1948.9398107969628\n",
      "  episode_reward_min: -8340.588520434227\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 960\n",
      "  experiment_id: edcbe001b14143f4bf76a6a2914a7bd0\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.8777666687965393\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 3.7393736839294434\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0066861435770988464\n",
      "          model: {}\n",
      "          policy_loss: 0.0023233278188854456\n",
      "          total_loss: 3524.853515625\n",
      "          vf_explained_var: -1.0188828447610376e-08\n",
      "          vf_loss: 3524.845703125\n",
      "    num_agent_steps_sampled: 2880000\n",
      "    num_agent_steps_trained: 2880000\n",
      "    num_steps_sampled: 2880000\n",
      "    num_steps_trained: 2880000\n",
      "  iterations_since_restore: 192\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 24.797222222222228\n",
      "    ram_util_percent: 31.38888888888889\n",
      "  pid: 307\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06560039932617447\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 3.457080505162213\n",
      "    mean_inference_ms: 0.48457037939883824\n",
      "    mean_raw_obs_processing_ms: 3.705116674161677\n",
      "  time_since_restore: 5157.19713640213\n",
      "  time_this_iter_s: 25.126238584518433\n",
      "  time_total_s: 5157.19713640213\n",
      "  timers:\n",
      "    learn_throughput: 6449.005\n",
      "    learn_time_ms: 2325.94\n",
      "    load_throughput: 9263584.428\n",
      "    load_time_ms: 1.619\n",
      "    sample_throughput: 602.265\n",
      "    sample_time_ms: 24905.996\n",
      "    update_time_ms: 1.387\n",
      "  timestamp: 1665250359\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2880000\n",
      "  training_iteration: 192\n",
      "  trial_id: 26f87_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 9.8/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.23 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_1_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_0_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_2_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_4_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_5_25daa8673b2c24aecf80a7fc79df4e93, 0.0/6.0 CPU_group_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_3_25daa8673b2c24aecf80a7fc79df4e93)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_onelane_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_26f87_00000 | RUNNING  | 141.225.10.229:307 |    192 |           5157.2 | 2880000 | -1948.94 |              2870.27 |             -8340.59 |               3000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m ring length: 234\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m v_max: 3.903915223349057\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m ring length: 238\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m v_max: 4.094180836186086\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m ring length: 261\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m v_max: 5.187261846097525\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m ring length: 220\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m v_max: 3.2377399006821497\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m ring length: 242\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m v_max: 4.2844067680020546\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m Could not connect to TraCI server at localhost:34447 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m Could not connect to TraCI server at localhost:57509 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m Could not connect to TraCI server at localhost:54035 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m Could not connect to TraCI server at localhost:36289 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m Could not connect to TraCI server at localhost:54035 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 3 seconds\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m Could not connect to TraCI server at localhost:34447 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 3 seconds\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m Could not connect to TraCI server at localhost:57509 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 3 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m Could not connect to TraCI server at localhost:36289 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 3 seconds\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_26f87_00000:\n",
      "  agent_timesteps_total: 2895000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-08_12-33-10\n",
      "  done: false\n",
      "  episode_len_mean: 3000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2870.2741277451596\n",
      "  episode_reward_mean: -1814.8413881462545\n",
      "  episode_reward_min: -8340.588520434227\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 965\n",
      "  experiment_id: edcbe001b14143f4bf76a6a2914a7bd0\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.43888333439826965\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: -1.8730745315551758\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016286185011267662\n",
      "          model: {}\n",
      "          policy_loss: 0.004930523689836264\n",
      "          total_loss: 2486.76025390625\n",
      "          vf_explained_var: -8.660504313695583e-09\n",
      "          vf_loss: 2486.748291015625\n",
      "    num_agent_steps_sampled: 2895000\n",
      "    num_agent_steps_trained: 2895000\n",
      "    num_steps_sampled: 2895000\n",
      "    num_steps_trained: 2895000\n",
      "  iterations_since_restore: 193\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.900000000000002\n",
      "    ram_util_percent: 31.404545454545453\n",
      "  pid: 307\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0655992605134187\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 3.4569423855466717\n",
      "    mean_inference_ms: 0.4845792602220189\n",
      "    mean_raw_obs_processing_ms: 3.705826222412885\n",
      "  time_since_restore: 5187.949251413345\n",
      "  time_this_iter_s: 30.75211501121521\n",
      "  time_total_s: 5187.949251413345\n",
      "  timers:\n",
      "    learn_throughput: 6552.694\n",
      "    learn_time_ms: 2289.135\n",
      "    load_throughput: 9403847.361\n",
      "    load_time_ms: 1.595\n",
      "    sample_throughput: 591.401\n",
      "    sample_time_ms: 25363.509\n",
      "    update_time_ms: 1.232\n",
      "  timestamp: 1665250390\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2895000\n",
      "  training_iteration: 193\n",
      "  trial_id: 26f87_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 9.8/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.23 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_1_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_3_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_0_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_4_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_5_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_2_25daa8673b2c24aecf80a7fc79df4e93, 0.0/6.0 CPU_group_25daa8673b2c24aecf80a7fc79df4e93)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_onelane_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_26f87_00000 | RUNNING  | 141.225.10.229:307 |    193 |          5187.95 | 2895000 | -1814.84 |              2870.27 |             -8340.59 |               3000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m ring length: 226\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m v_max: 3.5232840694769565\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m ring length: 247\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m v_max: 4.522125250095652\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m ring length: 224\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m v_max: 3.4281086136538996\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m ring length: 246\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m v_max: 4.474587708995648\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m ring length: 266\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m v_max: 5.42459972166245\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 1 seconds\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_26f87_00000:\n",
      "  agent_timesteps_total: 2910000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-08_12-33-36\n",
      "  done: false\n",
      "  episode_len_mean: 3000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2870.2741277451596\n",
      "  episode_reward_mean: -1732.3143566442536\n",
      "  episode_reward_min: -8340.588520434227\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 970\n",
      "  experiment_id: edcbe001b14143f4bf76a6a2914a7bd0\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.43888333439826965\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.2837193012237549\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017918908968567848\n",
      "          model: {}\n",
      "          policy_loss: 0.0030560109298676252\n",
      "          total_loss: 3075.006591796875\n",
      "          vf_explained_var: 1.528324244937096e-09\n",
      "          vf_loss: 3074.99560546875\n",
      "    num_agent_steps_sampled: 2910000\n",
      "    num_agent_steps_trained: 2910000\n",
      "    num_steps_sampled: 2910000\n",
      "    num_steps_trained: 2910000\n",
      "  iterations_since_restore: 194\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 23.975\n",
      "    ram_util_percent: 31.44166666666667\n",
      "  pid: 307\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06559782637199346\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 3.4567840835515913\n",
      "    mean_inference_ms: 0.4845865801797431\n",
      "    mean_raw_obs_processing_ms: 3.7065411352833575\n",
      "  time_since_restore: 5213.186140298843\n",
      "  time_this_iter_s: 25.236888885498047\n",
      "  time_total_s: 5213.186140298843\n",
      "  timers:\n",
      "    learn_throughput: 6589.017\n",
      "    learn_time_ms: 2276.516\n",
      "    load_throughput: 8608644.965\n",
      "    load_time_ms: 1.742\n",
      "    sample_throughput: 602.585\n",
      "    sample_time_ms: 24892.751\n",
      "    update_time_ms: 1.29\n",
      "  timestamp: 1665250416\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2910000\n",
      "  training_iteration: 194\n",
      "  trial_id: 26f87_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Memory usage on this node: 9.8/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.23 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_4_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_3_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_2_25daa8673b2c24aecf80a7fc79df4e93, 0.0/6.0 CPU_group_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_5_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_0_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_1_25daa8673b2c24aecf80a7fc79df4e93)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_onelane_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_26f87_00000 | RUNNING  | 141.225.10.229:307 |    194 |          5213.19 | 2910000 | -1732.31 |              2870.27 |             -8340.59 |               3000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m ring length: 230\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m v_max: 3.7136148111012934\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m ring length: 244\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m v_max: 4.379503211387676\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m ring length: 234\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m v_max: 3.903915223349057\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m ring length: 223\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m v_max: 3.3805185755587788\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m ring length: 264\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m v_max: 5.329679917416892\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 1 seconds\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_26f87_00000:\n",
      "  agent_timesteps_total: 2925000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-08_12-34-00\n",
      "  done: false\n",
      "  episode_len_mean: 3000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2870.2741277451596\n",
      "  episode_reward_mean: -1621.6353388072464\n",
      "  episode_reward_min: -8340.588520434227\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 975\n",
      "  experiment_id: edcbe001b14143f4bf76a6a2914a7bd0\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.43888333439826965\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.03558128699660301\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.03680453822016716\n",
      "          model: {}\n",
      "          policy_loss: 0.019581902772188187\n",
      "          total_loss: 3275.832275390625\n",
      "          vf_explained_var: -1.2226593959496768e-08\n",
      "          vf_loss: 3275.796630859375\n",
      "    num_agent_steps_sampled: 2925000\n",
      "    num_agent_steps_trained: 2925000\n",
      "    num_steps_sampled: 2925000\n",
      "    num_steps_trained: 2925000\n",
      "  iterations_since_restore: 195\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 24.317142857142855\n",
      "    ram_util_percent: 31.45142857142857\n",
      "  pid: 307\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06559614338062099\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 3.4566342819747815\n",
      "    mean_inference_ms: 0.48459237436296104\n",
      "    mean_raw_obs_processing_ms: 3.7072603241075424\n",
      "  time_since_restore: 5237.970171689987\n",
      "  time_this_iter_s: 24.7840313911438\n",
      "  time_total_s: 5237.970171689987\n",
      "  timers:\n",
      "    learn_throughput: 6750.74\n",
      "    learn_time_ms: 2221.979\n",
      "    load_throughput: 8647930.613\n",
      "    load_time_ms: 1.735\n",
      "    sample_throughput: 603.943\n",
      "    sample_time_ms: 24836.787\n",
      "    update_time_ms: 1.296\n",
      "  timestamp: 1665250440\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2925000\n",
      "  training_iteration: 195\n",
      "  trial_id: 26f87_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 9.8/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.23 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_4_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_5_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_2_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_1_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_0_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_3_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 accelerator_type:G, 0.0/6.0 CPU_group_25daa8673b2c24aecf80a7fc79df4e93)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_onelane_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_26f87_00000 | RUNNING  | 141.225.10.229:307 |    195 |          5237.97 | 2925000 | -1621.64 |              2870.27 |             -8340.59 |               3000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m ring length: 267\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m v_max: 5.472051563171826\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m ring length: 269\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m v_max: 5.566938458220921\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m ring length: 230\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m v_max: 3.7136148111012934\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m ring length: 253\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m v_max: 4.807278529691987\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m ring length: 252\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m v_max: 4.759762049824181\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 1 seconds\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_26f87_00000:\n",
      "  agent_timesteps_total: 2940000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-08_12-34-27\n",
      "  done: false\n",
      "  episode_len_mean: 3000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2870.2741277451596\n",
      "  episode_reward_mean: -1700.4077542516886\n",
      "  episode_reward_min: -8340.588520434227\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 980\n",
      "  experiment_id: edcbe001b14143f4bf76a6a2914a7bd0\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.43888333439826965\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.0274029970169067\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.03388646990060806\n",
      "          model: {}\n",
      "          policy_loss: 0.017497099936008453\n",
      "          total_loss: 3246.73193359375\n",
      "          vf_explained_var: 9.16994569166718e-09\n",
      "          vf_loss: 3246.699951171875\n",
      "    num_agent_steps_sampled: 2940000\n",
      "    num_agent_steps_trained: 2940000\n",
      "    num_steps_sampled: 2940000\n",
      "    num_steps_trained: 2940000\n",
      "  iterations_since_restore: 196\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 23.1421052631579\n",
      "    ram_util_percent: 31.44736842105263\n",
      "  pid: 307\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06559432302835248\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 3.456462434086763\n",
      "    mean_inference_ms: 0.48459672445110835\n",
      "    mean_raw_obs_processing_ms: 3.7079712081774647\n",
      "  time_since_restore: 5264.428423404694\n",
      "  time_this_iter_s: 26.45825171470642\n",
      "  time_total_s: 5264.428423404694\n",
      "  timers:\n",
      "    learn_throughput: 6626.013\n",
      "    learn_time_ms: 2263.805\n",
      "    load_throughput: 8698266.28\n",
      "    load_time_ms: 1.724\n",
      "    sample_throughput: 602.424\n",
      "    sample_time_ms: 24899.409\n",
      "    update_time_ms: 1.283\n",
      "  timestamp: 1665250467\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2940000\n",
      "  training_iteration: 196\n",
      "  trial_id: 26f87_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Memory usage on this node: 9.8/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.23 GiB heap, 0.0/14.65 GiB objects (0.0/6.0 CPU_group_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_1_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_3_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_2_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_0_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_5_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_4_25daa8673b2c24aecf80a7fc79df4e93)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_onelane_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_26f87_00000 | RUNNING  | 141.225.10.229:307 |    196 |          5264.43 | 2940000 | -1700.41 |              2870.27 |             -8340.59 |               3000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m ring length: 251\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m v_max: 4.71224180704279\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m ring length: 228\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m v_max: 3.618452967700356\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m ring length: 264\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m v_max: 5.329679917416892\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m ring length: 253\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m v_max: 4.807278529691987\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m ring length: 234\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m v_max: 3.903915223349057\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 1 seconds\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_26f87_00000:\n",
      "  agent_timesteps_total: 2955000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-08_12-34-54\n",
      "  done: false\n",
      "  episode_len_mean: 3000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2870.2741277451596\n",
      "  episode_reward_mean: -1701.733416155227\n",
      "  episode_reward_min: -8340.588520434227\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 985\n",
      "  experiment_id: edcbe001b14143f4bf76a6a2914a7bd0\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.43888333439826965\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.101831316947937\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.026008564978837967\n",
      "          model: {}\n",
      "          policy_loss: 0.012180950492620468\n",
      "          total_loss: 3416.670166015625\n",
      "          vf_explained_var: -5.603855601776786e-09\n",
      "          vf_loss: 3416.646728515625\n",
      "    num_agent_steps_sampled: 2955000\n",
      "    num_agent_steps_trained: 2955000\n",
      "    num_steps_sampled: 2955000\n",
      "    num_steps_trained: 2955000\n",
      "  iterations_since_restore: 197\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 23.505128205128205\n",
      "    ram_util_percent: 31.45897435897435\n",
      "  pid: 307\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06559265360729748\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 3.4562961545618047\n",
      "    mean_inference_ms: 0.4846015888635771\n",
      "    mean_raw_obs_processing_ms: 3.708876388414558\n",
      "  time_since_restore: 5291.765627622604\n",
      "  time_this_iter_s: 27.337204217910767\n",
      "  time_total_s: 5291.765627622604\n",
      "  timers:\n",
      "    learn_throughput: 6762.436\n",
      "    learn_time_ms: 2218.136\n",
      "    load_throughput: 9133672.077\n",
      "    load_time_ms: 1.642\n",
      "    sample_throughput: 596.66\n",
      "    sample_time_ms: 25139.944\n",
      "    update_time_ms: 1.133\n",
      "  timestamp: 1665250494\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2955000\n",
      "  training_iteration: 197\n",
      "  trial_id: 26f87_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 9.9/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.23 GiB heap, 0.0/14.65 GiB objects (0.0/6.0 CPU_group_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_3_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_0_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_1_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_2_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_4_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_5_25daa8673b2c24aecf80a7fc79df4e93)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_onelane_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_26f87_00000 | RUNNING  | 141.225.10.229:307 |    197 |          5291.77 | 2955000 | -1701.73 |              2870.27 |             -8340.59 |               3000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m ring length: 263\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m v_max: 5.282212215151455\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m ring length: 236\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m v_max: 3.999052674951912\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m ring length: 248\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m v_max: 4.569659527528101\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m ring length: 266\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m v_max: 5.42459972166245\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m ring length: 257\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m v_max: 4.997304682316517\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m Could not connect to TraCI server at localhost:36289 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m Could not connect to TraCI server at localhost:34795 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m Could not connect to TraCI server at localhost:34447 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 2 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m Could not connect to TraCI server at localhost:57509 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m Could not connect to TraCI server at localhost:54035 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m Could not connect to TraCI server at localhost:36289 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 3 seconds\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m Could not connect to TraCI server at localhost:34795 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 3 seconds\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m Could not connect to TraCI server at localhost:34447 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 3 seconds\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m Could not connect to TraCI server at localhost:57509 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 3 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m Could not connect to TraCI server at localhost:54035 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 3 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 1 seconds\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_26f87_00000:\n",
      "  agent_timesteps_total: 2970000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-08_12-35-26\n",
      "  done: false\n",
      "  episode_len_mean: 3000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2870.2741277451596\n",
      "  episode_reward_mean: -1656.8912028058328\n",
      "  episode_reward_min: -8340.588520434227\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 990\n",
      "  experiment_id: edcbe001b14143f4bf76a6a2914a7bd0\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.43888333439826965\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.08872649818658829\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.019915636628866196\n",
      "          model: {}\n",
      "          policy_loss: 0.01001968327909708\n",
      "          total_loss: 3140.817626953125\n",
      "          vf_explained_var: -1.2226593959496768e-08\n",
      "          vf_loss: 3140.798583984375\n",
      "    num_agent_steps_sampled: 2970000\n",
      "    num_agent_steps_trained: 2970000\n",
      "    num_steps_sampled: 2970000\n",
      "    num_steps_trained: 2970000\n",
      "  iterations_since_restore: 198\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.586956521739133\n",
      "    ram_util_percent: 31.484782608695657\n",
      "  pid: 307\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06559106447970821\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 3.456130395070959\n",
      "    mean_inference_ms: 0.4846073145917336\n",
      "    mean_raw_obs_processing_ms: 3.7097476435982513\n",
      "  time_since_restore: 5323.297960281372\n",
      "  time_this_iter_s: 31.5323326587677\n",
      "  time_total_s: 5323.297960281372\n",
      "  timers:\n",
      "    learn_throughput: 6785.466\n",
      "    learn_time_ms: 2210.607\n",
      "    load_throughput: 8974076.768\n",
      "    load_time_ms: 1.671\n",
      "    sample_throughput: 584.161\n",
      "    sample_time_ms: 25677.837\n",
      "    update_time_ms: 1.148\n",
      "  timestamp: 1665250526\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2970000\n",
      "  training_iteration: 198\n",
      "  trial_id: 26f87_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 9.8/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.23 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_2_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_0_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_4_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_3_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_1_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_5_25daa8673b2c24aecf80a7fc79df4e93, 0.0/6.0 CPU_group_25daa8673b2c24aecf80a7fc79df4e93)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_onelane_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_26f87_00000 | RUNNING  | 141.225.10.229:307 |    198 |           5323.3 | 2970000 | -1656.89 |              2870.27 |             -8340.59 |               3000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m ring length: 265\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m v_max: 5.37714246265477\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m ring length: 237\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m v_max: 4.04661795519353\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m ring length: 267\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m v_max: 5.472051563171826\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m ring length: 247\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m v_max: 4.522125250095652\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m ring length: 263\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m v_max: 5.282212215151455\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 1 seconds\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_26f87_00000:\n",
      "  agent_timesteps_total: 2985000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-08_12-35-53\n",
      "  done: false\n",
      "  episode_len_mean: 3000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2165.524982355543\n",
      "  episode_reward_mean: -1733.3122655874276\n",
      "  episode_reward_min: -8340.588520434227\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 995\n",
      "  experiment_id: edcbe001b14143f4bf76a6a2914a7bd0\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.43888333439826965\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 3.9048187732696533\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.02826649695634842\n",
      "          model: {}\n",
      "          policy_loss: 0.019324542954564095\n",
      "          total_loss: 3916.050537109375\n",
      "          vf_explained_var: -5.603855601776786e-09\n",
      "          vf_loss: 3916.018798828125\n",
      "    num_agent_steps_sampled: 2985000\n",
      "    num_agent_steps_trained: 2985000\n",
      "    num_steps_sampled: 2985000\n",
      "    num_steps_trained: 2985000\n",
      "  iterations_since_restore: 199\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 23.726315789473684\n",
      "    ram_util_percent: 31.489473684210523\n",
      "  pid: 307\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06558969050938518\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 3.4559582585053126\n",
      "    mean_inference_ms: 0.484615050244187\n",
      "    mean_raw_obs_processing_ms: 3.710704583811006\n",
      "  time_since_restore: 5349.910170078278\n",
      "  time_this_iter_s: 26.612209796905518\n",
      "  time_total_s: 5349.910170078278\n",
      "  timers:\n",
      "    learn_throughput: 6757.318\n",
      "    learn_time_ms: 2219.816\n",
      "    load_throughput: 8799854.535\n",
      "    load_time_ms: 1.705\n",
      "    sample_throughput: 607.188\n",
      "    sample_time_ms: 24704.047\n",
      "    update_time_ms: 1.221\n",
      "  timestamp: 1665250553\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2985000\n",
      "  training_iteration: 199\n",
      "  trial_id: 26f87_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 9.8/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.23 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_4_25daa8673b2c24aecf80a7fc79df4e93, 0.0/6.0 CPU_group_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_5_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_0_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_3_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_1_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_2_25daa8673b2c24aecf80a7fc79df4e93)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_onelane_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_26f87_00000 | RUNNING  | 141.225.10.229:307 |    199 |          5349.91 | 2985000 | -1733.31 |              2165.52 |             -8340.59 |               3000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m ring length: 226\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m v_max: 3.5232840694769565\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m ring length: 238\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m v_max: 4.094180836186086\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m ring length: 238\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m v_max: 4.094180836186086\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m ring length: 225\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m v_max: 3.4756971311168834\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m ring length: 243\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m v_max: 4.331956438196479\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m Could not connect to TraCI server at localhost:34795 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m Could not connect to TraCI server at localhost:34447 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m Could not connect to TraCI server at localhost:57509 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m Could not connect to TraCI server at localhost:54035 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m Could not connect to TraCI server at localhost:36289 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m Could not connect to TraCI server at localhost:34795 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 3 seconds\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m Could not connect to TraCI server at localhost:34447 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 3 seconds\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m Could not connect to TraCI server at localhost:57509 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 3 seconds\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m Could not connect to TraCI server at localhost:54035 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=303)\u001b[0m  Retrying in 3 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m Could not connect to TraCI server at localhost:36289 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 3 seconds\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=305)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=308)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m  Retrying in 1 seconds\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_26f87_00000:\n",
      "  agent_timesteps_total: 3000000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-08_12-36-25\n",
      "  done: true\n",
      "  episode_len_mean: 3000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2165.524982355543\n",
      "  episode_reward_mean: -1735.6408919515138\n",
      "  episode_reward_min: -8340.588520434227\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 1000\n",
      "  experiment_id: edcbe001b14143f4bf76a6a2914a7bd0\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.43888333439826965\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: -0.3036324381828308\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.025970499962568283\n",
      "          model: {}\n",
      "          policy_loss: 0.012625528499484062\n",
      "          total_loss: 3255.02099609375\n",
      "          vf_explained_var: -8.660504313695583e-09\n",
      "          vf_loss: 3254.997314453125\n",
      "    num_agent_steps_sampled: 3000000\n",
      "    num_agent_steps_trained: 3000000\n",
      "    num_steps_sampled: 3000000\n",
      "    num_steps_trained: 3000000\n",
      "  iterations_since_restore: 200\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.546666666666663\n",
      "    ram_util_percent: 31.415555555555557\n",
      "  pid: 307\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0655884929100469\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 3.4558032417542233\n",
      "    mean_inference_ms: 0.4846243018580486\n",
      "    mean_raw_obs_processing_ms: 3.7121980799596206\n",
      "  time_since_restore: 5381.739073514938\n",
      "  time_this_iter_s: 31.828903436660767\n",
      "  time_total_s: 5381.739073514938\n",
      "  timers:\n",
      "    learn_throughput: 6724.942\n",
      "    learn_time_ms: 2230.503\n",
      "    load_throughput: 8842524.245\n",
      "    load_time_ms: 1.696\n",
      "    sample_throughput: 592.368\n",
      "    sample_time_ms: 25322.112\n",
      "    update_time_ms: 1.228\n",
      "  timestamp: 1665250585\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 3000000\n",
      "  training_iteration: 200\n",
      "  trial_id: 26f87_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 9.8/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.23 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_2_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_3_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_1_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_0_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_5_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_4_25daa8673b2c24aecf80a7fc79df4e93, 0.0/6.0 CPU_group_25daa8673b2c24aecf80a7fc79df4e93)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_onelane_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_26f87_00000 | RUNNING  | 141.225.10.229:307 |    200 |          5381.74 | 3000000 | -1735.64 |              2165.52 |             -8340.59 |               3000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Memory usage on this node: 9.8/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 0/6 CPUs, 0/6 GPUs, 0.0/7.23 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_2_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_0_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_5_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_1_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_3_25daa8673b2c24aecf80a7fc79df4e93, 0.0/6.0 CPU_group_25daa8673b2c24aecf80a7fc79df4e93, 0.0/1.0 CPU_group_4_25daa8673b2c24aecf80a7fc79df4e93)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_onelane_ring\n",
      "Number of trials: 1/1 (1 TERMINATED)\n",
      "+-----------------------------------------+------------+-------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status     | loc   |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+------------+-------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_26f87_00000 | TERMINATED |       |    200 |          5381.74 | 3000000 | -1735.64 |              2165.52 |             -8340.59 |               3000 |\n",
      "+-----------------------------------------+------------+-------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m 2022-10-08 12:36:26,117\tERROR worker.py:421 -- SystemExit was raised from the worker\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m   File \"python/ray/_raylet.pyx\", line 523, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m   File \"python/ray/_raylet.pyx\", line 530, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m   File \"python/ray/_raylet.pyx\", line 534, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m   File \"python/ray/_raylet.pyx\", line 484, in ray._raylet.execute_task.function_executor\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m   File \"/home/michael-lab/anaconda3/envs/flow/lib/python3.7/site-packages/ray/_private/function_manager.py\", line 563, in actor_method_executor\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m     return method(__ray_actor, *args, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m   File \"/home/michael-lab/anaconda3/envs/flow/lib/python3.7/site-packages/ray/actor.py\", line 1027, in __ray_terminate__\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m     ray.actor.exit_actor()\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m   File \"/home/michael-lab/anaconda3/envs/flow/lib/python3.7/site-packages/ray/actor.py\", line 1103, in exit_actor\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m     raise exit\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m SystemExit: 0\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m During handling of the above exception, another exception occurred:\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m   File \"python/ray/_raylet.pyx\", line 632, in ray._raylet.task_execution_handler\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m   File \"python/ray/_raylet.pyx\", line 486, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m   File \"python/ray/includes/libcoreworker.pxi\", line 33, in ray._raylet.ProfileEvent.__exit__\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m   File \"/home/michael-lab/anaconda3/envs/flow/lib/python3.7/traceback.py\", line 167, in format_exc\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m     return \"\".join(format_exception(*sys.exc_info(), limit=limit, chain=chain))\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m   File \"/home/michael-lab/anaconda3/envs/flow/lib/python3.7/traceback.py\", line 121, in format_exception\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m     type(value), value, tb, limit=limit).format(chain=chain))\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m   File \"/home/michael-lab/anaconda3/envs/flow/lib/python3.7/traceback.py\", line 508, in __init__\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m     capture_locals=capture_locals)\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m   File \"/home/michael-lab/anaconda3/envs/flow/lib/python3.7/traceback.py\", line 359, in extract\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m     linecache.checkcache(filename)\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m   File \"/home/michael-lab/anaconda3/envs/flow/lib/python3.7/linecache.py\", line 74, in checkcache\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m     stat = os.stat(fullname)\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m   File \"/home/michael-lab/anaconda3/envs/flow/lib/python3.7/site-packages/ray/worker.py\", line 418, in sigterm_handler\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m     sys.exit(1)\n",
      "\u001b[2m\u001b[36m(pid=301)\u001b[0m SystemExit: 1\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m 2022-10-08 12:36:26,118\tERROR worker.py:421 -- SystemExit was raised from the worker\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m   File \"python/ray/_raylet.pyx\", line 530, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m   File \"python/ray/_raylet.pyx\", line 534, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m   File \"python/ray/_raylet.pyx\", line 484, in ray._raylet.execute_task.function_executor\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m   File \"/home/michael-lab/anaconda3/envs/flow/lib/python3.7/site-packages/ray/_private/function_manager.py\", line 563, in actor_method_executor\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m     return method(__ray_actor, *args, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m   File \"/home/michael-lab/anaconda3/envs/flow/lib/python3.7/site-packages/ray/actor.py\", line 1027, in __ray_terminate__\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m     ray.actor.exit_actor()\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m   File \"/home/michael-lab/anaconda3/envs/flow/lib/python3.7/site-packages/ray/actor.py\", line 1103, in exit_actor\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m     raise exit\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m SystemExit: 0\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m During handling of the above exception, another exception occurred:\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m   File \"python/ray/_raylet.pyx\", line 632, in ray._raylet.task_execution_handler\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m   File \"python/ray/_raylet.pyx\", line 486, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m   File \"python/ray/_raylet.pyx\", line 523, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m   File \"python/ray/includes/libcoreworker.pxi\", line 42, in ray._raylet.ProfileEvent.__exit__\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m   File \"/home/michael-lab/anaconda3/envs/flow/lib/python3.7/json/__init__.py\", line 183, in dumps\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m     def dumps(obj, *, skipkeys=False, ensure_ascii=True, check_circular=True,\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m   File \"/home/michael-lab/anaconda3/envs/flow/lib/python3.7/site-packages/ray/worker.py\", line 418, in sigterm_handler\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m     sys.exit(1)\n",
      "\u001b[2m\u001b[36m(pid=304)\u001b[0m SystemExit: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-10-08 12:36:26,228\tINFO tune.py:550 -- Total run time: 5404.09 seconds (5403.26 seconds for the tuning loop).\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!python examples/train.py singleagent_ring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca1e948b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
