{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1375af72",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Bad key text.latex.preview in file /home/michael/anaconda3/envs/flow/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 123 ('text.latex.preview : False')\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "https://github.com/matplotlib/matplotlib/blob/v3.5.1/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n",
      "\n",
      "Bad key mathtext.fallback_to_cm in file /home/michael/anaconda3/envs/flow/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 155 ('mathtext.fallback_to_cm : True  # When True, use symbols from the Computer Modern')\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "https://github.com/matplotlib/matplotlib/blob/v3.5.1/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n",
      "\n",
      "Bad key savefig.jpeg_quality in file /home/michael/anaconda3/envs/flow/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 418 ('savefig.jpeg_quality: 95       # when a jpeg is saved, the default quality parameter.')\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "https://github.com/matplotlib/matplotlib/blob/v3.5.1/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n",
      "\n",
      "Bad key keymap.all_axes in file /home/michael/anaconda3/envs/flow/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 466 ('keymap.all_axes : a                 # enable all axes')\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "https://github.com/matplotlib/matplotlib/blob/v3.5.1/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n",
      "\n",
      "Bad key animation.avconv_path in file /home/michael/anaconda3/envs/flow/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 477 ('animation.avconv_path: avconv     # Path to avconv binary. Without full path')\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "https://github.com/matplotlib/matplotlib/blob/v3.5.1/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n",
      "\n",
      "Bad key animation.avconv_args in file /home/michael/anaconda3/envs/flow/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 479 ('animation.avconv_args:            # Additional arguments to pass to avconv')\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "https://github.com/matplotlib/matplotlib/blob/v3.5.1/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n",
      "{'num_workers': 5, 'num_envs_per_worker': 1, 'create_env_on_driver': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'gamma': 0.999, 'lr': 5e-05, 'train_batch_size': 20000, 'model': {'_use_default_native_models': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': [[16, [8, 8], 4], [32, [4, 4], 2], [256, [11, 11], 1]], 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'num_framestacks': 'auto', 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1, 'framestack': True}, 'optimizer': {}, 'horizon': 4000, 'soft_horizon': False, 'no_done_at_end': False, 'env': None, 'observation_space': None, 'action_space': None, 'env_config': {}, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'torch', 'eager_tracing': False, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_num_episodes': 10, 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'collect_metrics_timeout': 180, 'metrics_smoothing_episodes': 100, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'min_iter_time_s': 0, 'timesteps_per_iteration': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {}, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, 'simple_optimizer': -1, 'monitor': -1, 'use_critic': True, 'use_gae': True, 'lambda': 0.97, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 10, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.02, 'vf_share_layers': -1}\n",
      "2022-08-01 09:19:43,031\tINFO services.py:1247 -- View the Ray dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265\u001b[39m\u001b[22m\n",
      "== Status ==\n",
      "Memory usage on this node: 10.9/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 0/6 CPUs, 0/6 GPUs, 0.0/4.91 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Result logdir: /home/michael/ray_results/stabilizing_the_ring\n",
      "Number of trials: 1/1 (1 PENDING)\n",
      "+-----------------------------------------+----------+-------+\n",
      "| Trial name                              | status   | loc   |\n",
      "|-----------------------------------------+----------+-------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_fd1ac_00000 | PENDING  |       |\n",
      "+-----------------------------------------+----------+-------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=25804)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25804)\u001b[0m Bad key text.latex.preview in file /home/michael/anaconda3/envs/flow/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 123 ('text.latex.preview : False')\n",
      "\u001b[2m\u001b[36m(pid=25804)\u001b[0m You probably need to get an updated matplotlibrc file from\n",
      "\u001b[2m\u001b[36m(pid=25804)\u001b[0m https://github.com/matplotlib/matplotlib/blob/v3.5.1/matplotlibrc.template\n",
      "\u001b[2m\u001b[36m(pid=25804)\u001b[0m or from the matplotlib source distribution\n",
      "\u001b[2m\u001b[36m(pid=25804)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25804)\u001b[0m Bad key mathtext.fallback_to_cm in file /home/michael/anaconda3/envs/flow/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 155 ('mathtext.fallback_to_cm : True  # When True, use symbols from the Computer Modern')\n",
      "\u001b[2m\u001b[36m(pid=25804)\u001b[0m You probably need to get an updated matplotlibrc file from\n",
      "\u001b[2m\u001b[36m(pid=25804)\u001b[0m https://github.com/matplotlib/matplotlib/blob/v3.5.1/matplotlibrc.template\n",
      "\u001b[2m\u001b[36m(pid=25804)\u001b[0m or from the matplotlib source distribution\n",
      "\u001b[2m\u001b[36m(pid=25804)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25804)\u001b[0m Bad key savefig.jpeg_quality in file /home/michael/anaconda3/envs/flow/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 418 ('savefig.jpeg_quality: 95       # when a jpeg is saved, the default quality parameter.')\n",
      "\u001b[2m\u001b[36m(pid=25804)\u001b[0m You probably need to get an updated matplotlibrc file from\n",
      "\u001b[2m\u001b[36m(pid=25804)\u001b[0m https://github.com/matplotlib/matplotlib/blob/v3.5.1/matplotlibrc.template\n",
      "\u001b[2m\u001b[36m(pid=25804)\u001b[0m or from the matplotlib source distribution\n",
      "\u001b[2m\u001b[36m(pid=25804)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25804)\u001b[0m Bad key keymap.all_axes in file /home/michael/anaconda3/envs/flow/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 466 ('keymap.all_axes : a                 # enable all axes')\n",
      "\u001b[2m\u001b[36m(pid=25804)\u001b[0m You probably need to get an updated matplotlibrc file from\n",
      "\u001b[2m\u001b[36m(pid=25804)\u001b[0m https://github.com/matplotlib/matplotlib/blob/v3.5.1/matplotlibrc.template\n",
      "\u001b[2m\u001b[36m(pid=25804)\u001b[0m or from the matplotlib source distribution\n",
      "\u001b[2m\u001b[36m(pid=25804)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25804)\u001b[0m Bad key animation.avconv_path in file /home/michael/anaconda3/envs/flow/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 477 ('animation.avconv_path: avconv     # Path to avconv binary. Without full path')\n",
      "\u001b[2m\u001b[36m(pid=25804)\u001b[0m You probably need to get an updated matplotlibrc file from\n",
      "\u001b[2m\u001b[36m(pid=25804)\u001b[0m https://github.com/matplotlib/matplotlib/blob/v3.5.1/matplotlibrc.template\n",
      "\u001b[2m\u001b[36m(pid=25804)\u001b[0m or from the matplotlib source distribution\n",
      "\u001b[2m\u001b[36m(pid=25804)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25804)\u001b[0m Bad key animation.avconv_args in file /home/michael/anaconda3/envs/flow/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 479 ('animation.avconv_args:            # Additional arguments to pass to avconv')\n",
      "\u001b[2m\u001b[36m(pid=25804)\u001b[0m You probably need to get an updated matplotlibrc file from\n",
      "\u001b[2m\u001b[36m(pid=25804)\u001b[0m https://github.com/matplotlib/matplotlib/blob/v3.5.1/matplotlibrc.template\n",
      "\u001b[2m\u001b[36m(pid=25804)\u001b[0m or from the matplotlib source distribution\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=25804)\u001b[0m 2022-08-01 09:19:45,587\tINFO trainer.py:720 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m Bad key text.latex.preview in file /home/michael/anaconda3/envs/flow/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 123 ('text.latex.preview : False')\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m You probably need to get an updated matplotlibrc file from\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m https://github.com/matplotlib/matplotlib/blob/v3.5.1/matplotlibrc.template\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m or from the matplotlib source distribution\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m Bad key mathtext.fallback_to_cm in file /home/michael/anaconda3/envs/flow/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 155 ('mathtext.fallback_to_cm : True  # When True, use symbols from the Computer Modern')\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m You probably need to get an updated matplotlibrc file from\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m https://github.com/matplotlib/matplotlib/blob/v3.5.1/matplotlibrc.template\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m or from the matplotlib source distribution\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m Bad key savefig.jpeg_quality in file /home/michael/anaconda3/envs/flow/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 418 ('savefig.jpeg_quality: 95       # when a jpeg is saved, the default quality parameter.')\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m You probably need to get an updated matplotlibrc file from\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m https://github.com/matplotlib/matplotlib/blob/v3.5.1/matplotlibrc.template\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m or from the matplotlib source distribution\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m Bad key keymap.all_axes in file /home/michael/anaconda3/envs/flow/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 466 ('keymap.all_axes : a                 # enable all axes')\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m You probably need to get an updated matplotlibrc file from\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m https://github.com/matplotlib/matplotlib/blob/v3.5.1/matplotlibrc.template\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m or from the matplotlib source distribution\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m Bad key animation.avconv_path in file /home/michael/anaconda3/envs/flow/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 477 ('animation.avconv_path: avconv     # Path to avconv binary. Without full path')\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m You probably need to get an updated matplotlibrc file from\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m https://github.com/matplotlib/matplotlib/blob/v3.5.1/matplotlibrc.template\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m or from the matplotlib source distribution\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m Bad key animation.avconv_args in file /home/michael/anaconda3/envs/flow/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 479 ('animation.avconv_args:            # Additional arguments to pass to avconv')\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m You probably need to get an updated matplotlibrc file from\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m https://github.com/matplotlib/matplotlib/blob/v3.5.1/matplotlibrc.template\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m or from the matplotlib source distribution\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m Bad key text.latex.preview in file /home/michael/anaconda3/envs/flow/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 123 ('text.latex.preview : False')\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m You probably need to get an updated matplotlibrc file from\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m https://github.com/matplotlib/matplotlib/blob/v3.5.1/matplotlibrc.template\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m or from the matplotlib source distribution\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m Bad key mathtext.fallback_to_cm in file /home/michael/anaconda3/envs/flow/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 155 ('mathtext.fallback_to_cm : True  # When True, use symbols from the Computer Modern')\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m You probably need to get an updated matplotlibrc file from\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m https://github.com/matplotlib/matplotlib/blob/v3.5.1/matplotlibrc.template\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m or from the matplotlib source distribution\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m Bad key savefig.jpeg_quality in file /home/michael/anaconda3/envs/flow/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 418 ('savefig.jpeg_quality: 95       # when a jpeg is saved, the default quality parameter.')\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m You probably need to get an updated matplotlibrc file from\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m https://github.com/matplotlib/matplotlib/blob/v3.5.1/matplotlibrc.template\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m or from the matplotlib source distribution\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m Bad key keymap.all_axes in file /home/michael/anaconda3/envs/flow/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 466 ('keymap.all_axes : a                 # enable all axes')\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m You probably need to get an updated matplotlibrc file from\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m https://github.com/matplotlib/matplotlib/blob/v3.5.1/matplotlibrc.template\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m or from the matplotlib source distribution\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m Bad key animation.avconv_path in file /home/michael/anaconda3/envs/flow/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 477 ('animation.avconv_path: avconv     # Path to avconv binary. Without full path')\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m You probably need to get an updated matplotlibrc file from\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m https://github.com/matplotlib/matplotlib/blob/v3.5.1/matplotlibrc.template\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m or from the matplotlib source distribution\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m Bad key animation.avconv_args in file /home/michael/anaconda3/envs/flow/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 479 ('animation.avconv_args:            # Additional arguments to pass to avconv')\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m You probably need to get an updated matplotlibrc file from\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m https://github.com/matplotlib/matplotlib/blob/v3.5.1/matplotlibrc.template\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m or from the matplotlib source distribution\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m Bad key text.latex.preview in file /home/michael/anaconda3/envs/flow/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 123 ('text.latex.preview : False')\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m You probably need to get an updated matplotlibrc file from\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m https://github.com/matplotlib/matplotlib/blob/v3.5.1/matplotlibrc.template\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m or from the matplotlib source distribution\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m Bad key mathtext.fallback_to_cm in file /home/michael/anaconda3/envs/flow/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 155 ('mathtext.fallback_to_cm : True  # When True, use symbols from the Computer Modern')\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m You probably need to get an updated matplotlibrc file from\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m https://github.com/matplotlib/matplotlib/blob/v3.5.1/matplotlibrc.template\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m or from the matplotlib source distribution\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m Bad key savefig.jpeg_quality in file /home/michael/anaconda3/envs/flow/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 418 ('savefig.jpeg_quality: 95       # when a jpeg is saved, the default quality parameter.')\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m You probably need to get an updated matplotlibrc file from\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m https://github.com/matplotlib/matplotlib/blob/v3.5.1/matplotlibrc.template\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m or from the matplotlib source distribution\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m Bad key keymap.all_axes in file /home/michael/anaconda3/envs/flow/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 466 ('keymap.all_axes : a                 # enable all axes')\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m You probably need to get an updated matplotlibrc file from\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m https://github.com/matplotlib/matplotlib/blob/v3.5.1/matplotlibrc.template\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m or from the matplotlib source distribution\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m Bad key animation.avconv_path in file /home/michael/anaconda3/envs/flow/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 477 ('animation.avconv_path: avconv     # Path to avconv binary. Without full path')\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m You probably need to get an updated matplotlibrc file from\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m https://github.com/matplotlib/matplotlib/blob/v3.5.1/matplotlibrc.template\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m or from the matplotlib source distribution\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m Bad key animation.avconv_args in file /home/michael/anaconda3/envs/flow/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 479 ('animation.avconv_args:            # Additional arguments to pass to avconv')\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m You probably need to get an updated matplotlibrc file from\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m https://github.com/matplotlib/matplotlib/blob/v3.5.1/matplotlibrc.template\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m or from the matplotlib source distribution\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m Bad key text.latex.preview in file /home/michael/anaconda3/envs/flow/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 123 ('text.latex.preview : False')\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m You probably need to get an updated matplotlibrc file from\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m https://github.com/matplotlib/matplotlib/blob/v3.5.1/matplotlibrc.template\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m or from the matplotlib source distribution\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m Bad key mathtext.fallback_to_cm in file /home/michael/anaconda3/envs/flow/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 155 ('mathtext.fallback_to_cm : True  # When True, use symbols from the Computer Modern')\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m You probably need to get an updated matplotlibrc file from\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m https://github.com/matplotlib/matplotlib/blob/v3.5.1/matplotlibrc.template\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m or from the matplotlib source distribution\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m Bad key savefig.jpeg_quality in file /home/michael/anaconda3/envs/flow/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 418 ('savefig.jpeg_quality: 95       # when a jpeg is saved, the default quality parameter.')\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m You probably need to get an updated matplotlibrc file from\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m https://github.com/matplotlib/matplotlib/blob/v3.5.1/matplotlibrc.template\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m or from the matplotlib source distribution\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m Bad key keymap.all_axes in file /home/michael/anaconda3/envs/flow/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 466 ('keymap.all_axes : a                 # enable all axes')\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m You probably need to get an updated matplotlibrc file from\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m https://github.com/matplotlib/matplotlib/blob/v3.5.1/matplotlibrc.template\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m or from the matplotlib source distribution\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m Bad key animation.avconv_path in file /home/michael/anaconda3/envs/flow/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 477 ('animation.avconv_path: avconv     # Path to avconv binary. Without full path')\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m You probably need to get an updated matplotlibrc file from\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m https://github.com/matplotlib/matplotlib/blob/v3.5.1/matplotlibrc.template\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m or from the matplotlib source distribution\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m Bad key animation.avconv_args in file /home/michael/anaconda3/envs/flow/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 479 ('animation.avconv_args:            # Additional arguments to pass to avconv')\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m You probably need to get an updated matplotlibrc file from\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m https://github.com/matplotlib/matplotlib/blob/v3.5.1/matplotlibrc.template\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m or from the matplotlib source distribution\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m Bad key text.latex.preview in file /home/michael/anaconda3/envs/flow/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 123 ('text.latex.preview : False')\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m You probably need to get an updated matplotlibrc file from\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m https://github.com/matplotlib/matplotlib/blob/v3.5.1/matplotlibrc.template\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m or from the matplotlib source distribution\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m Bad key mathtext.fallback_to_cm in file /home/michael/anaconda3/envs/flow/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 155 ('mathtext.fallback_to_cm : True  # When True, use symbols from the Computer Modern')\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m You probably need to get an updated matplotlibrc file from\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m https://github.com/matplotlib/matplotlib/blob/v3.5.1/matplotlibrc.template\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m or from the matplotlib source distribution\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m Bad key savefig.jpeg_quality in file /home/michael/anaconda3/envs/flow/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 418 ('savefig.jpeg_quality: 95       # when a jpeg is saved, the default quality parameter.')\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m You probably need to get an updated matplotlibrc file from\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m https://github.com/matplotlib/matplotlib/blob/v3.5.1/matplotlibrc.template\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m or from the matplotlib source distribution\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m Bad key keymap.all_axes in file /home/michael/anaconda3/envs/flow/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 466 ('keymap.all_axes : a                 # enable all axes')\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m You probably need to get an updated matplotlibrc file from\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m https://github.com/matplotlib/matplotlib/blob/v3.5.1/matplotlibrc.template\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m or from the matplotlib source distribution\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m Bad key animation.avconv_path in file /home/michael/anaconda3/envs/flow/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 477 ('animation.avconv_path: avconv     # Path to avconv binary. Without full path')\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m You probably need to get an updated matplotlibrc file from\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m https://github.com/matplotlib/matplotlib/blob/v3.5.1/matplotlibrc.template\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m or from the matplotlib source distribution\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m Bad key animation.avconv_args in file /home/michael/anaconda3/envs/flow/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 479 ('animation.avconv_args:            # Additional arguments to pass to avconv')\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m You probably need to get an updated matplotlibrc file from\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m https://github.com/matplotlib/matplotlib/blob/v3.5.1/matplotlibrc.template\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m or from the matplotlib source distribution\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=25804)\u001b[0m 2022-08-01 09:19:49,121\tWARNING util.py:55 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m ring length: 222\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m v_max: 3.3329270738617227\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m ring length: 246\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m v_max: 4.474587708995648\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m ring length: 242\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m v_max: 4.2844067680020546\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m ring length: 270\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m v_max: 5.6143732387852054\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m ring length: 250\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m v_max: 4.664717904914125\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m ring length: 264\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m v_max: 5.329679917416892\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m ring length: 254\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m v_max: 4.854791141191876\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m ring length: 223\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m v_max: 3.3805185755587788\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m ring length: 246\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m v_max: 4.474587708995648\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m ring length: 223\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m v_max: 3.3805185755587788\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_fd1ac_00000:\n",
      "  agent_timesteps_total: 20000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-01_09-26-14\n",
      "  done: false\n",
      "  episode_len_mean: 4000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -5899.678735099759\n",
      "  episode_reward_mean: -6265.01298199783\n",
      "  episode_reward_min: -6797.871298231539\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 5\n",
      "  experiment_id: 2b7c5715c95c4828907cbc45e49df6f0\n",
      "  hostname: michael-home\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.2\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.36008505532696\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.004220580015495378\n",
      "          policy_loss: -0.002983005452088442\n",
      "          total_loss: 1449.6198768567128\n",
      "          vf_explained_var: -8.16241918499827e-09\n",
      "          vf_loss: 1449.6220098216063\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 20000\n",
      "    num_steps_sampled: 20000\n",
      "    num_steps_trained: 20000\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 192.168.0.33\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 26.08472727272727\n",
      "    ram_util_percent: 42.97127272727272\n",
      "  pid: 25804\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09491276186843181\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 52.64792563884862\n",
      "    mean_inference_ms: 1.9175919673646042\n",
      "    mean_raw_obs_processing_ms: 21.61292520888714\n",
      "  time_since_restore: 384.85485005378723\n",
      "  time_this_iter_s: 384.85485005378723\n",
      "  time_total_s: 384.85485005378723\n",
      "  timers:\n",
      "    learn_throughput: 253.278\n",
      "    learn_time_ms: 78964.718\n",
      "    sample_throughput: 65.387\n",
      "    sample_time_ms: 305872.22\n",
      "    update_time_ms: 3.478\n",
      "  timestamp: 1659363974\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 20000\n",
      "  training_iteration: 1\n",
      "  trial_id: fd1ac_00000\n",
      "  \n",
      "\u001b[2m\u001b[36m(pid=25804)\u001b[0m 2022-08-01 09:26:14,012\tWARNING ppo.py:242 -- The magnitude of your environment rewards are more than 627.0x the scale of `vf_clip_param`. This means that it will take more than 627.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "== Status ==\n",
      "Memory usage on this node: 14.1/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/4.91 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Result logdir: /home/michael/ray_results/stabilizing_the_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |    ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_fd1ac_00000 | RUNNING  | 192.168.0.33:25804 |      1 |          384.855 | 20000 | -6265.01 |             -5899.68 |             -6797.87 |               4000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m ring length: 258\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m v_max: 5.044800727535787\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m ring length: 250\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m v_max: 4.664717904914125\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m ring length: 246\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m v_max: 4.474587708995648\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m ring length: 246\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m v_max: 4.474587708995648\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m ring length: 232\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m v_max: 3.8087690783250063\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_fd1ac_00000:\n",
      "  agent_timesteps_total: 40000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-01_09-31-12\n",
      "  done: false\n",
      "  episode_len_mean: 4000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -5899.678735099759\n",
      "  episode_reward_mean: -6369.986064662039\n",
      "  episode_reward_min: -6797.871298231539\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 10\n",
      "  experiment_id: 2b7c5715c95c4828907cbc45e49df6f0\n",
      "  hostname: michael-home\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.1\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.2614081488293447\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008752481673901996\n",
      "          policy_loss: -0.005090505789348464\n",
      "          total_loss: 1370.7625038875897\n",
      "          vf_explained_var: -6.2641825060438805e-09\n",
      "          vf_loss: 1370.7667202894854\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 40000\n",
      "    num_steps_sampled: 40000\n",
      "    num_steps_trained: 40000\n",
      "  iterations_since_restore: 2\n",
      "  node_ip: 192.168.0.33\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 25.427230046948356\n",
      "    ram_util_percent: 47.9943661971831\n",
      "  pid: 25804\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09492489724264794\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 47.28939453961777\n",
      "    mean_inference_ms: 1.9174689254339525\n",
      "    mean_raw_obs_processing_ms: 21.625814791595598\n",
      "  time_since_restore: 683.3851962089539\n",
      "  time_this_iter_s: 298.5303461551666\n",
      "  time_total_s: 683.3851962089539\n",
      "  timers:\n",
      "    learn_throughput: 254.321\n",
      "    learn_time_ms: 78640.81\n",
      "    sample_throughput: 76.035\n",
      "    sample_time_ms: 263035.766\n",
      "    update_time_ms: 3.198\n",
      "  timestamp: 1659364272\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 40000\n",
      "  training_iteration: 2\n",
      "  trial_id: fd1ac_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Memory usage on this node: 15.1/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/4.91 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_2_28b5456e04c30e30e910ccff5ed5840d, 0.0/6.0 CPU_group_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_1_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_4_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_0_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_5_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_3_28b5456e04c30e30e910ccff5ed5840d)\n",
      "Result logdir: /home/michael/ray_results/stabilizing_the_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |    ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_fd1ac_00000 | RUNNING  | 192.168.0.33:25804 |      2 |          683.385 | 40000 | -6369.99 |             -5899.68 |             -6797.87 |               4000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=25804)\u001b[0m 2022-08-01 09:31:12,607\tWARNING ppo.py:242 -- The magnitude of your environment rewards are more than 637.0x the scale of `vf_clip_param`. This means that it will take more than 637.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m ring length: 241\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m v_max: 4.236854288051661\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m ring length: 245\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m v_max: 4.427046998576354\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m ring length: 233\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m v_max: 3.856343201216534\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m ring length: 247\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m v_max: 4.522125250095652\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m ring length: 237\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m v_max: 4.04661795519353\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_fd1ac_00000:\n",
      "  agent_timesteps_total: 60000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-01_09-36-11\n",
      "  done: false\n",
      "  episode_len_mean: 4000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -5409.0140267952365\n",
      "  episode_reward_mean: -6227.080921447717\n",
      "  episode_reward_min: -6797.871298231539\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 15\n",
      "  experiment_id: 2b7c5715c95c4828907cbc45e49df6f0\n",
      "  hostname: michael-home\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.05\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.167897502631898\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008750506061159673\n",
      "          policy_loss: -0.004105436042170663\n",
      "          total_loss: 1111.083120824729\n",
      "          vf_explained_var: -1.025048046443544e-08\n",
      "          vf_loss: 1111.0867874437076\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 60000\n",
      "    num_steps_sampled: 60000\n",
      "    num_steps_trained: 60000\n",
      "  iterations_since_restore: 3\n",
      "  node_ip: 192.168.0.33\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 25.44225352112676\n",
      "    ram_util_percent: 48.43732394366197\n",
      "  pid: 25804\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09491955937409387\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 44.31462170157495\n",
      "    mean_inference_ms: 1.9133630078936625\n",
      "    mean_raw_obs_processing_ms: 21.627838029516123\n",
      "  time_since_restore: 981.8737459182739\n",
      "  time_this_iter_s: 298.48854970932007\n",
      "  time_total_s: 981.8737459182739\n",
      "  timers:\n",
      "    learn_throughput: 254.409\n",
      "    learn_time_ms: 78613.449\n",
      "    sample_throughput: 80.43\n",
      "    sample_time_ms: 248662.054\n",
      "    update_time_ms: 3.318\n",
      "  timestamp: 1659364571\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 60000\n",
      "  training_iteration: 3\n",
      "  trial_id: fd1ac_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 15.2/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/4.91 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_0_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_4_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_3_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_5_28b5456e04c30e30e910ccff5ed5840d, 0.0/6.0 CPU_group_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_1_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_2_28b5456e04c30e30e910ccff5ed5840d)\n",
      "Result logdir: /home/michael/ray_results/stabilizing_the_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |    ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_fd1ac_00000 | RUNNING  | 192.168.0.33:25804 |      3 |          981.874 | 60000 | -6227.08 |             -5409.01 |             -6797.87 |               4000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=25804)\u001b[0m 2022-08-01 09:36:11,136\tWARNING ppo.py:242 -- The magnitude of your environment rewards are more than 623.0x the scale of `vf_clip_param`. This means that it will take more than 623.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m ring length: 247\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m v_max: 4.522125250095652\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m ring length: 254\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m v_max: 4.854791141191876\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m ring length: 238\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m v_max: 4.094180836186086\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m ring length: 248\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m v_max: 4.569659527528101\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m ring length: 237\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m v_max: 4.04661795519353\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m  Retrying in 1 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_WaveAttenuationPOEnv-v0_fd1ac_00000:\n",
      "  agent_timesteps_total: 80000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-01_09-41-10\n",
      "  done: false\n",
      "  episode_len_mean: 4000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -5129.667192783215\n",
      "  episode_reward_mean: -6048.261078660233\n",
      "  episode_reward_min: -6797.871298231539\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 20\n",
      "  experiment_id: 2b7c5715c95c4828907cbc45e49df6f0\n",
      "  hostname: michael-home\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.025\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.080449835206293\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009789555590326533\n",
      "          policy_loss: -0.0036830672697656473\n",
      "          total_loss: 923.995864537719\n",
      "          vf_explained_var: -1.1389422738261601e-10\n",
      "          vf_loss: 923.9993041992187\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 80000\n",
      "    num_steps_sampled: 80000\n",
      "    num_steps_trained: 80000\n",
      "  iterations_since_restore: 4\n",
      "  node_ip: 192.168.0.33\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 25.321077283372365\n",
      "    ram_util_percent: 48.700702576112405\n",
      "  pid: 25804\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09485067515494613\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 42.3846487780097\n",
      "    mean_inference_ms: 1.9068385275997144\n",
      "    mean_raw_obs_processing_ms: 21.64492606667141\n",
      "  time_since_restore: 1281.4777908325195\n",
      "  time_this_iter_s: 299.6040449142456\n",
      "  time_total_s: 1281.4777908325195\n",
      "  timers:\n",
      "    learn_throughput: 254.463\n",
      "    learn_time_ms: 78596.923\n",
      "    sample_throughput: 82.728\n",
      "    sample_time_ms: 241756.866\n",
      "    update_time_ms: 3.375\n",
      "  timestamp: 1659364870\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 80000\n",
      "  training_iteration: 4\n",
      "  trial_id: fd1ac_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 15.2/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/4.91 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 accelerator_type:G, 0.0/6.0 CPU_group_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_2_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_4_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_3_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_0_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_1_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_5_28b5456e04c30e30e910ccff5ed5840d)\n",
      "Result logdir: /home/michael/ray_results/stabilizing_the_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |    ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_fd1ac_00000 | RUNNING  | 192.168.0.33:25804 |      4 |          1281.48 | 80000 | -6048.26 |             -5129.67 |             -6797.87 |               4000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=25804)\u001b[0m 2022-08-01 09:41:10,787\tWARNING ppo.py:242 -- The magnitude of your environment rewards are more than 605.0x the scale of `vf_clip_param`. This means that it will take more than 605.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m ring length: 258\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m v_max: 5.044800727535787\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m ring length: 253\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m v_max: 4.807278529691987\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m ring length: 269\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m v_max: 5.566938458220921\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m ring length: 231\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m v_max: 3.761192925277308\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m ring length: 269\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m v_max: 5.566938458220921\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m  Retrying in 1 seconds\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_fd1ac_00000:\n",
      "  agent_timesteps_total: 100000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-01_09-46-10\n",
      "  done: false\n",
      "  episode_len_mean: 4000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -4103.384005613945\n",
      "  episode_reward_mean: -5854.759709261654\n",
      "  episode_reward_min: -6797.871298231539\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 25\n",
      "  experiment_id: 2b7c5715c95c4828907cbc45e49df6f0\n",
      "  hostname: michael-home\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.0125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.9465736404725701\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.020337169045598364\n",
      "          policy_loss: -0.00859667226956908\n",
      "          total_loss: 749.5932209962492\n",
      "          vf_explained_var: -7.5929484921744e-11\n",
      "          vf_loss: 749.60156174192\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 100000\n",
      "    num_steps_sampled: 100000\n",
      "    num_steps_trained: 100000\n",
      "  iterations_since_restore: 5\n",
      "  node_ip: 192.168.0.33\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 25.382903981264636\n",
      "    ram_util_percent: 48.5480093676815\n",
      "  pid: 25804\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09481805073743989\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 41.01207145735482\n",
      "    mean_inference_ms: 1.9032571624266028\n",
      "    mean_raw_obs_processing_ms: 21.66145916120394\n",
      "  time_since_restore: 1580.711508512497\n",
      "  time_this_iter_s: 299.2337176799774\n",
      "  time_total_s: 1580.711508512497\n",
      "  timers:\n",
      "    learn_throughput: 254.674\n",
      "    learn_time_ms: 78531.854\n",
      "    sample_throughput: 84.177\n",
      "    sample_time_ms: 237595.019\n",
      "    update_time_ms: 3.304\n",
      "  timestamp: 1659365170\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 100000\n",
      "  training_iteration: 5\n",
      "  trial_id: fd1ac_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 15.2/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/4.91 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_0_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_4_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_2_28b5456e04c30e30e910ccff5ed5840d, 0.0/6.0 CPU_group_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_3_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_5_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_1_28b5456e04c30e30e910ccff5ed5840d)\n",
      "Result logdir: /home/michael/ray_results/stabilizing_the_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_fd1ac_00000 | RUNNING  | 192.168.0.33:25804 |      5 |          1580.71 | 100000 | -5854.76 |             -4103.38 |             -6797.87 |               4000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=25804)\u001b[0m 2022-08-01 09:46:10,063\tWARNING ppo.py:242 -- The magnitude of your environment rewards are more than 585.0x the scale of `vf_clip_param`. This means that it will take more than 585.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m ring length: 239\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m v_max: 4.141741239205832\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m ring length: 256\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m v_max: 4.949804327743507\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m ring length: 220\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m v_max: 3.2377399006821497\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m ring length: 230\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m v_max: 3.7136148111012934\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m ring length: 233\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m v_max: 3.856343201216534\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_fd1ac_00000:\n",
      "  agent_timesteps_total: 120000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-01_09-51-08\n",
      "  done: false\n",
      "  episode_len_mean: 4000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3496.794229083237\n",
      "  episode_reward_mean: -5617.208872426635\n",
      "  episode_reward_min: -6797.871298231539\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 30\n",
      "  experiment_id: 2b7c5715c95c4828907cbc45e49df6f0\n",
      "  hostname: michael-home\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.0125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.7880770670000915\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.02160288975151422\n",
      "          policy_loss: -0.009031665641339911\n",
      "          total_loss: 529.0911419133472\n",
      "          vf_explained_var: -6.07435879373952e-10\n",
      "          vf_loss: 529.0999046957417\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 120000\n",
      "    num_steps_sampled: 120000\n",
      "    num_steps_trained: 120000\n",
      "  iterations_since_restore: 6\n",
      "  node_ip: 192.168.0.33\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 25.448941176470587\n",
      "    ram_util_percent: 48.60235294117647\n",
      "  pid: 25804\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09479593601232675\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 39.97735290903815\n",
      "    mean_inference_ms: 1.9016465008620456\n",
      "    mean_raw_obs_processing_ms: 21.670336324531146\n",
      "  time_since_restore: 1878.6494917869568\n",
      "  time_this_iter_s: 297.93798327445984\n",
      "  time_total_s: 1878.6494917869568\n",
      "  timers:\n",
      "    learn_throughput: 255.073\n",
      "    learn_time_ms: 78408.777\n",
      "    sample_throughput: 85.221\n",
      "    sample_time_ms: 234684.187\n",
      "    update_time_ms: 3.273\n",
      "  timestamp: 1659365468\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 120000\n",
      "  training_iteration: 6\n",
      "  trial_id: fd1ac_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 15.2/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/4.91 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_3_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_2_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_5_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_1_28b5456e04c30e30e910ccff5ed5840d, 0.0/6.0 CPU_group_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_0_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_4_28b5456e04c30e30e910ccff5ed5840d)\n",
      "Result logdir: /home/michael/ray_results/stabilizing_the_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_fd1ac_00000 | RUNNING  | 192.168.0.33:25804 |      6 |          1878.65 | 120000 | -5617.21 |             -3496.79 |             -6797.87 |               4000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=25804)\u001b[0m 2022-08-01 09:51:08,040\tWARNING ppo.py:242 -- The magnitude of your environment rewards are more than 562.0x the scale of `vf_clip_param`. This means that it will take more than 562.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m ring length: 236\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m v_max: 3.999052674951912\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m ring length: 242\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m v_max: 4.2844067680020546\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m ring length: 260\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m v_max: 5.139779427502188\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m ring length: 257\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m v_max: 4.997304682316517\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m ring length: 222\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m v_max: 3.3329270738617227\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_fd1ac_00000:\n",
      "  agent_timesteps_total: 140000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-01_09-56-06\n",
      "  done: false\n",
      "  episode_len_mean: 4000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2809.679834073416\n",
      "  episode_reward_mean: -5283.046332230998\n",
      "  episode_reward_min: -6797.871298231539\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 35\n",
      "  experiment_id: 2b7c5715c95c4828907cbc45e49df6f0\n",
      "  hostname: michael-home\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.0125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.6615507493353193\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.02050415697457698\n",
      "          policy_loss: -0.00903019705894077\n",
      "          total_loss: 251.71749300622636\n",
      "          vf_explained_var: -1.8223076381218561e-09\n",
      "          vf_loss: 251.72626680021833\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 140000\n",
      "    num_steps_sampled: 140000\n",
      "    num_steps_trained: 140000\n",
      "  iterations_since_restore: 7\n",
      "  node_ip: 192.168.0.33\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 25.36541176470588\n",
      "    ram_util_percent: 48.64094117647059\n",
      "  pid: 25804\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09478820196090974\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 39.16493537518973\n",
      "    mean_inference_ms: 1.9010010367666776\n",
      "    mean_raw_obs_processing_ms: 21.674961507340505\n",
      "  time_since_restore: 2176.651228427887\n",
      "  time_this_iter_s: 298.0017366409302\n",
      "  time_total_s: 2176.651228427887\n",
      "  timers:\n",
      "    learn_throughput: 255.293\n",
      "    learn_time_ms: 78341.413\n",
      "    sample_throughput: 85.987\n",
      "    sample_time_ms: 232593.462\n",
      "    update_time_ms: 3.31\n",
      "  timestamp: 1659365766\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 140000\n",
      "  training_iteration: 7\n",
      "  trial_id: fd1ac_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 15.3/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/4.91 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_1_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_5_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_0_28b5456e04c30e30e910ccff5ed5840d, 0.0/6.0 CPU_group_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_3_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_4_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_2_28b5456e04c30e30e910ccff5ed5840d)\n",
      "Result logdir: /home/michael/ray_results/stabilizing_the_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_fd1ac_00000 | RUNNING  | 192.168.0.33:25804 |      7 |          2176.65 | 140000 | -5283.05 |             -2809.68 |             -6797.87 |               4000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=25804)\u001b[0m 2022-08-01 09:56:06,084\tWARNING ppo.py:242 -- The magnitude of your environment rewards are more than 528.0x the scale of `vf_clip_param`. This means that it will take more than 528.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m ring length: 232\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m v_max: 3.8087690783250063\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m ring length: 248\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m v_max: 4.569659527528101\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m ring length: 234\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m v_max: 3.903915223349057\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m ring length: 260\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m v_max: 5.139779427502188\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m ring length: 236\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m v_max: 3.999052674951912\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_fd1ac_00000:\n",
      "  agent_timesteps_total: 160000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-01_10-01-04\n",
      "  done: false\n",
      "  episode_len_mean: 4000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -1924.511289299591\n",
      "  episode_reward_mean: -4919.452399062139\n",
      "  episode_reward_min: -6797.871298231539\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 40\n",
      "  experiment_id: 2b7c5715c95c4828907cbc45e49df6f0\n",
      "  hostname: michael-home\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.0125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.5406989953889968\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.020191713104888008\n",
      "          policy_loss: -0.008630312184037961\n",
      "          total_loss: 120.1834910253051\n",
      "          vf_explained_var: 8.731890766000561e-10\n",
      "          vf_loss: 120.1918685864491\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 160000\n",
      "    num_steps_sampled: 160000\n",
      "    num_steps_trained: 160000\n",
      "  iterations_since_restore: 8\n",
      "  node_ip: 192.168.0.33\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 25.49460093896714\n",
      "    ram_util_percent: 48.74507042253522\n",
      "  pid: 25804\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09477768348641316\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 38.50786120423957\n",
      "    mean_inference_ms: 1.9005199256636505\n",
      "    mean_raw_obs_processing_ms: 21.677593985860995\n",
      "  time_since_restore: 2474.9549555778503\n",
      "  time_this_iter_s: 298.3037271499634\n",
      "  time_total_s: 2474.9549555778503\n",
      "  timers:\n",
      "    learn_throughput: 255.371\n",
      "    learn_time_ms: 78317.45\n",
      "    sample_throughput: 86.566\n",
      "    sample_time_ms: 231036.696\n",
      "    update_time_ms: 3.283\n",
      "  timestamp: 1659366064\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 160000\n",
      "  training_iteration: 8\n",
      "  trial_id: fd1ac_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 15.2/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/4.91 GiB heap, 0.0/14.65 GiB objects (0.0/6.0 CPU_group_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_5_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_1_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_4_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_0_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_3_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_2_28b5456e04c30e30e910ccff5ed5840d)\n",
      "Result logdir: /home/michael/ray_results/stabilizing_the_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_fd1ac_00000 | RUNNING  | 192.168.0.33:25804 |      8 |          2474.95 | 160000 | -4919.45 |             -1924.51 |             -6797.87 |               4000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=25804)\u001b[0m 2022-08-01 10:01:04,428\tWARNING ppo.py:242 -- The magnitude of your environment rewards are more than 492.0x the scale of `vf_clip_param`. This means that it will take more than 492.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m ring length: 244\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m v_max: 4.379503211387676\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m ring length: 246\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m v_max: 4.474587708995648\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m ring length: 261\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m v_max: 5.187261846097525\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m ring length: 240\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m v_max: 4.189299083856172\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m ring length: 259\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m v_max: 5.092292348291853\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_fd1ac_00000:\n",
      "  agent_timesteps_total: 180000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-01_10-06-03\n",
      "  done: false\n",
      "  episode_len_mean: 4000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -1624.7988806585038\n",
      "  episode_reward_mean: -4598.5205336957015\n",
      "  episode_reward_min: -6797.871298231539\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 45\n",
      "  experiment_id: 2b7c5715c95c4828907cbc45e49df6f0\n",
      "  hostname: michael-home\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.0125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.39154020316281896\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01555257196045795\n",
      "          policy_loss: -0.006620190652816966\n",
      "          total_loss: 100.5252402846221\n",
      "          vf_explained_var: -2.6575319722610402e-09\n",
      "          vf_loss: 100.5316663972891\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 180000\n",
      "    num_steps_sampled: 180000\n",
      "    num_steps_trained: 180000\n",
      "  iterations_since_restore: 9\n",
      "  node_ip: 192.168.0.33\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 25.47488262910798\n",
      "    ram_util_percent: 48.833802816901404\n",
      "  pid: 25804\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0947685104120664\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 37.96346084446126\n",
      "    mean_inference_ms: 1.900456084898459\n",
      "    mean_raw_obs_processing_ms: 21.678759997394287\n",
      "  time_since_restore: 2773.518554210663\n",
      "  time_this_iter_s: 298.5635986328125\n",
      "  time_total_s: 2773.518554210663\n",
      "  timers:\n",
      "    learn_throughput: 255.292\n",
      "    learn_time_ms: 78341.716\n",
      "    sample_throughput: 87.028\n",
      "    sample_time_ms: 229811.818\n",
      "    update_time_ms: 3.298\n",
      "  timestamp: 1659366363\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 180000\n",
      "  training_iteration: 9\n",
      "  trial_id: fd1ac_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 15.3/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/4.91 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_3_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_4_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_2_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_0_28b5456e04c30e30e910ccff5ed5840d, 0.0/6.0 CPU_group_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_1_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_5_28b5456e04c30e30e910ccff5ed5840d)\n",
      "Result logdir: /home/michael/ray_results/stabilizing_the_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_fd1ac_00000 | RUNNING  | 192.168.0.33:25804 |      9 |          2773.52 | 180000 | -4598.52 |              -1624.8 |             -6797.87 |               4000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=25804)\u001b[0m 2022-08-01 10:06:03,038\tWARNING ppo.py:242 -- The magnitude of your environment rewards are more than 460.0x the scale of `vf_clip_param`. This means that it will take more than 460.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m ring length: 249\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m v_max: 4.617190445131122\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m ring length: 240\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m v_max: 4.189299083856172\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m ring length: 224\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m v_max: 3.4281086136538996\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m ring length: 226\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m v_max: 3.5232840694769565\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m ring length: 241\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m v_max: 4.236854288051661\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_fd1ac_00000:\n",
      "  agent_timesteps_total: 200000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-01_10-11-01\n",
      "  done: false\n",
      "  episode_len_mean: 4000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -234.21876322870116\n",
      "  episode_reward_mean: -4234.378512022437\n",
      "  episode_reward_min: -6797.871298231539\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 50\n",
      "  experiment_id: 2b7c5715c95c4828907cbc45e49df6f0\n",
      "  hostname: michael-home\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.0125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.2450464041369736\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01773663548123829\n",
      "          policy_loss: -0.005653256562888433\n",
      "          total_loss: 81.51362473919133\n",
      "          vf_explained_var: 0.0\n",
      "          vf_loss: 81.51905649634683\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 200000\n",
      "    num_steps_sampled: 200000\n",
      "    num_steps_trained: 200000\n",
      "  iterations_since_restore: 10\n",
      "  node_ip: 192.168.0.33\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 25.495070422535214\n",
      "    ram_util_percent: 48.95211267605634\n",
      "  pid: 25804\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09475709558556324\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 37.50434925055295\n",
      "    mean_inference_ms: 1.900159774943949\n",
      "    mean_raw_obs_processing_ms: 21.67904032009396\n",
      "  time_since_restore: 3071.771462202072\n",
      "  time_this_iter_s: 298.2529079914093\n",
      "  time_total_s: 3071.771462202072\n",
      "  timers:\n",
      "    learn_throughput: 255.332\n",
      "    learn_time_ms: 78329.469\n",
      "    sample_throughput: 87.4\n",
      "    sample_time_ms: 228832.479\n",
      "    update_time_ms: 3.287\n",
      "  timestamp: 1659366661\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 200000\n",
      "  training_iteration: 10\n",
      "  trial_id: fd1ac_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 15.4/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/4.91 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_1_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_2_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_5_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_0_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_4_28b5456e04c30e30e910ccff5ed5840d, 0.0/6.0 CPU_group_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_3_28b5456e04c30e30e910ccff5ed5840d)\n",
      "Result logdir: /home/michael/ray_results/stabilizing_the_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_fd1ac_00000 | RUNNING  | 192.168.0.33:25804 |     10 |          3071.77 | 200000 | -4234.38 |             -234.219 |             -6797.87 |               4000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=25804)\u001b[0m 2022-08-01 10:11:01,334\tWARNING ppo.py:242 -- The magnitude of your environment rewards are more than 423.0x the scale of `vf_clip_param`. This means that it will take more than 423.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m ring length: 266\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m v_max: 5.42459972166245\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m ring length: 248\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m v_max: 4.569659527528101\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m ring length: 237\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m v_max: 4.04661795519353\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m ring length: 267\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m v_max: 5.472051563171826\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m ring length: 258\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m v_max: 5.044800727535787\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_fd1ac_00000:\n",
      "  agent_timesteps_total: 220000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-01_10-16-00\n",
      "  done: false\n",
      "  episode_len_mean: 4000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -234.21876322870116\n",
      "  episode_reward_mean: -3945.7226942545362\n",
      "  episode_reward_min: -6797.871298231539\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 55\n",
      "  experiment_id: 2b7c5715c95c4828907cbc45e49df6f0\n",
      "  hostname: michael-home\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.0125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.14783402840802623\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013326147971022282\n",
      "          policy_loss: -0.004631826521199979\n",
      "          total_loss: 73.97411631930406\n",
      "          vf_explained_var: -2.201955062730576e-09\n",
      "          vf_loss: 73.97858129853655\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 220000\n",
      "    num_steps_sampled: 220000\n",
      "    num_steps_trained: 220000\n",
      "  iterations_since_restore: 11\n",
      "  node_ip: 192.168.0.33\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 25.405399061032867\n",
      "    ram_util_percent: 49.31314553990611\n",
      "  pid: 25804\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09475033977332721\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 37.11098475678254\n",
      "    mean_inference_ms: 1.8999035061084932\n",
      "    mean_raw_obs_processing_ms: 21.678698621604727\n",
      "  time_since_restore: 3370.8243849277496\n",
      "  time_this_iter_s: 299.0529227256775\n",
      "  time_total_s: 3370.8243849277496\n",
      "  timers:\n",
      "    learn_throughput: 255.272\n",
      "    learn_time_ms: 78347.84\n",
      "    sample_throughput: 90.812\n",
      "    sample_time_ms: 220234.243\n",
      "    update_time_ms: 3.247\n",
      "  timestamp: 1659366960\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 220000\n",
      "  training_iteration: 11\n",
      "  trial_id: fd1ac_00000\n",
      "  \n",
      "\u001b[2m\u001b[36m(pid=25804)\u001b[0m 2022-08-01 10:16:00,427\tWARNING ppo.py:242 -- The magnitude of your environment rewards are more than 395.0x the scale of `vf_clip_param`. This means that it will take more than 395.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "== Status ==\n",
      "Memory usage on this node: 15.4/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/4.91 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_1_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_4_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_3_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_0_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_5_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_2_28b5456e04c30e30e910ccff5ed5840d, 0.0/6.0 CPU_group_28b5456e04c30e30e910ccff5ed5840d)\n",
      "Result logdir: /home/michael/ray_results/stabilizing_the_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_fd1ac_00000 | RUNNING  | 192.168.0.33:25804 |     11 |          3370.82 | 220000 | -3945.72 |             -234.219 |             -6797.87 |               4000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m ring length: 249\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m v_max: 4.617190445131122\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m ring length: 234\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m v_max: 3.903915223349057\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m ring length: 222\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m v_max: 3.3329270738617227\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m ring length: 270\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m v_max: 5.6143732387852054\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m ring length: 266\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m v_max: 5.42459972166245\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_fd1ac_00000:\n",
      "  agent_timesteps_total: 240000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-01_10-20-58\n",
      "  done: false\n",
      "  episode_len_mean: 4000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 477.3096064974113\n",
      "  episode_reward_mean: -3601.5073134087916\n",
      "  episode_reward_min: -6797.871298231539\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 60\n",
      "  experiment_id: 2b7c5715c95c4828907cbc45e49df6f0\n",
      "  hostname: michael-home\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.0125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -0.009767798481473498\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.018526723994028164\n",
      "          policy_loss: -0.005859741662587425\n",
      "          total_loss: 76.37477480772954\n",
      "          vf_explained_var: -3.2649678516349923e-09\n",
      "          vf_loss: 76.38040291002602\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 240000\n",
      "    num_steps_sampled: 240000\n",
      "    num_steps_trained: 240000\n",
      "  iterations_since_restore: 12\n",
      "  node_ip: 192.168.0.33\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 25.551643192488264\n",
      "    ram_util_percent: 49.37723004694837\n",
      "  pid: 25804\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09473571830973834\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 36.769881531452306\n",
      "    mean_inference_ms: 1.8994017186402161\n",
      "    mean_raw_obs_processing_ms: 21.67805655382347\n",
      "  time_since_restore: 3669.0154128074646\n",
      "  time_this_iter_s: 298.19102787971497\n",
      "  time_total_s: 3669.0154128074646\n",
      "  timers:\n",
      "    learn_throughput: 255.3\n",
      "    learn_time_ms: 78339.084\n",
      "    sample_throughput: 90.823\n",
      "    sample_time_ms: 220208.858\n",
      "    update_time_ms: 3.287\n",
      "  timestamp: 1659367258\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 240000\n",
      "  training_iteration: 12\n",
      "  trial_id: fd1ac_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 15.4/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/4.91 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_3_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_5_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_4_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_2_28b5456e04c30e30e910ccff5ed5840d, 0.0/6.0 CPU_group_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_0_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_1_28b5456e04c30e30e910ccff5ed5840d)\n",
      "Result logdir: /home/michael/ray_results/stabilizing_the_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_fd1ac_00000 | RUNNING  | 192.168.0.33:25804 |     12 |          3669.02 | 240000 | -3601.51 |               477.31 |             -6797.87 |               4000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=25804)\u001b[0m 2022-08-01 10:20:58,663\tWARNING ppo.py:242 -- The magnitude of your environment rewards are more than 360.0x the scale of `vf_clip_param`. This means that it will take more than 360.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m ring length: 224\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m v_max: 3.4281086136538996\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m ring length: 223\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m v_max: 3.3805185755587788\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m ring length: 265\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m v_max: 5.37714246265477\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m ring length: 223\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m v_max: 3.3805185755587788\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m ring length: 243\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m v_max: 4.331956438196479\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_fd1ac_00000:\n",
      "  agent_timesteps_total: 260000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-01_10-25-57\n",
      "  done: false\n",
      "  episode_len_mean: 4000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 799.9604022513242\n",
      "  episode_reward_mean: -3322.5215892731994\n",
      "  episode_reward_min: -6797.871298231539\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 65\n",
      "  experiment_id: 2b7c5715c95c4828907cbc45e49df6f0\n",
      "  hostname: michael-home\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.0125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -0.09302010232475913\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012021764461840898\n",
      "          policy_loss: -0.0038584744485071413\n",
      "          total_loss: 91.73697875684994\n",
      "          vf_explained_var: -2.65753197226104e-10\n",
      "          vf_loss: 91.74068693659108\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 260000\n",
      "    num_steps_sampled: 260000\n",
      "    num_steps_trained: 260000\n",
      "  iterations_since_restore: 13\n",
      "  node_ip: 192.168.0.33\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 25.413145539906104\n",
      "    ram_util_percent: 49.485915492957744\n",
      "  pid: 25804\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09471960340445891\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 36.47075435584687\n",
      "    mean_inference_ms: 1.8988568010168085\n",
      "    mean_raw_obs_processing_ms: 21.677141358189917\n",
      "  time_since_restore: 3967.459094762802\n",
      "  time_this_iter_s: 298.4436819553375\n",
      "  time_total_s: 3967.459094762802\n",
      "  timers:\n",
      "    learn_throughput: 255.297\n",
      "    learn_time_ms: 78340.091\n",
      "    sample_throughput: 90.825\n",
      "    sample_time_ms: 220203.391\n",
      "    update_time_ms: 3.229\n",
      "  timestamp: 1659367557\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 260000\n",
      "  training_iteration: 13\n",
      "  trial_id: fd1ac_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 15.4/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/4.91 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_5_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_2_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_1_28b5456e04c30e30e910ccff5ed5840d, 0.0/6.0 CPU_group_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_0_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_3_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_4_28b5456e04c30e30e910ccff5ed5840d)\n",
      "Result logdir: /home/michael/ray_results/stabilizing_the_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_fd1ac_00000 | RUNNING  | 192.168.0.33:25804 |     13 |          3967.46 | 260000 | -3322.52 |               799.96 |             -6797.87 |               4000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=25804)\u001b[0m 2022-08-01 10:25:57,158\tWARNING ppo.py:242 -- The magnitude of your environment rewards are more than 332.0x the scale of `vf_clip_param`. This means that it will take more than 332.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m ring length: 245\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m v_max: 4.427046998576354\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m ring length: 254\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m v_max: 4.854791141191876\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m ring length: 242\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m v_max: 4.2844067680020546\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m ring length: 233\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m v_max: 3.856343201216534\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m ring length: 227\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m v_max: 3.570869368805752\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_fd1ac_00000:\n",
      "  agent_timesteps_total: 280000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-01_10-30-56\n",
      "  done: false\n",
      "  episode_len_mean: 4000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1098.4673595269849\n",
      "  episode_reward_mean: -3094.2053602068154\n",
      "  episode_reward_min: -6797.871298231539\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 70\n",
      "  experiment_id: 2b7c5715c95c4828907cbc45e49df6f0\n",
      "  hostname: michael-home\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.0125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -0.15417934542248962\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009640594382227592\n",
      "          policy_loss: -0.0026235437843723187\n",
      "          total_loss: 70.91181187113379\n",
      "          vf_explained_var: -2.315849290113192e-09\n",
      "          vf_loss: 70.91431475353848\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 280000\n",
      "    num_steps_sampled: 280000\n",
      "    num_steps_trained: 280000\n",
      "  iterations_since_restore: 14\n",
      "  node_ip: 192.168.0.33\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 25.580985915492956\n",
      "    ram_util_percent: 49.5744131455399\n",
      "  pid: 25804\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09470795068452274\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 36.205785276579725\n",
      "    mean_inference_ms: 1.89847061973825\n",
      "    mean_raw_obs_processing_ms: 21.676084237320957\n",
      "  time_since_restore: 4266.583370685577\n",
      "  time_this_iter_s: 299.12427592277527\n",
      "  time_total_s: 4266.583370685577\n",
      "  timers:\n",
      "    learn_throughput: 255.06\n",
      "    learn_time_ms: 78412.966\n",
      "    sample_throughput: 90.875\n",
      "    sample_time_ms: 220082.584\n",
      "    update_time_ms: 3.173\n",
      "  timestamp: 1659367856\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 280000\n",
      "  training_iteration: 14\n",
      "  trial_id: fd1ac_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 15.5/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/4.91 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_0_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_3_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_1_28b5456e04c30e30e910ccff5ed5840d, 0.0/6.0 CPU_group_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_5_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_4_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_2_28b5456e04c30e30e910ccff5ed5840d)\n",
      "Result logdir: /home/michael/ray_results/stabilizing_the_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_fd1ac_00000 | RUNNING  | 192.168.0.33:25804 |     14 |          4266.58 | 280000 | -3094.21 |              1098.47 |             -6797.87 |               4000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=25804)\u001b[0m 2022-08-01 10:30:56,336\tWARNING ppo.py:242 -- The magnitude of your environment rewards are more than 309.0x the scale of `vf_clip_param`. This means that it will take more than 309.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m ring length: 269\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m v_max: 5.566938458220921\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m ring length: 249\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m v_max: 4.617190445131122\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m ring length: 266\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m v_max: 5.42459972166245\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m ring length: 258\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m v_max: 5.044800727535787\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m ring length: 222\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m v_max: 3.3329270738617227\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_fd1ac_00000:\n",
      "  agent_timesteps_total: 300000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-01_10-35-54\n",
      "  done: false\n",
      "  episode_len_mean: 4000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1098.4673595269849\n",
      "  episode_reward_mean: -2858.946718541859\n",
      "  episode_reward_min: -6797.871298231539\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 75\n",
      "  experiment_id: 2b7c5715c95c4828907cbc45e49df6f0\n",
      "  hostname: michael-home\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.00625\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -0.2657359125507865\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017442100024901697\n",
      "          policy_loss: -0.00436067376537901\n",
      "          total_loss: 51.829191139852924\n",
      "          vf_explained_var: -2.353814032574064e-09\n",
      "          vf_loss: 51.83344278153341\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 300000\n",
      "    num_steps_sampled: 300000\n",
      "    num_steps_trained: 300000\n",
      "  iterations_since_restore: 15\n",
      "  node_ip: 192.168.0.33\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 25.529812206572768\n",
      "    ram_util_percent: 49.685680751173706\n",
      "  pid: 25804\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0946939700474957\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 35.969482317748124\n",
      "    mean_inference_ms: 1.8979737649141437\n",
      "    mean_raw_obs_processing_ms: 21.675011428618845\n",
      "  time_since_restore: 4565.059062719345\n",
      "  time_this_iter_s: 298.4756920337677\n",
      "  time_total_s: 4565.059062719345\n",
      "  timers:\n",
      "    learn_throughput: 254.98\n",
      "    learn_time_ms: 78437.517\n",
      "    sample_throughput: 90.916\n",
      "    sample_time_ms: 219982.212\n",
      "    update_time_ms: 3.174\n",
      "  timestamp: 1659368154\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 300000\n",
      "  training_iteration: 15\n",
      "  trial_id: fd1ac_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Memory usage on this node: 15.5/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/4.91 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_4_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_1_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_3_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_0_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_5_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 accelerator_type:G, 0.0/6.0 CPU_group_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_2_28b5456e04c30e30e910ccff5ed5840d)\n",
      "Result logdir: /home/michael/ray_results/stabilizing_the_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_fd1ac_00000 | RUNNING  | 192.168.0.33:25804 |     15 |          4565.06 | 300000 | -2858.95 |              1098.47 |             -6797.87 |               4000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=25804)\u001b[0m 2022-08-01 10:35:54,859\tWARNING ppo.py:242 -- The magnitude of your environment rewards are more than 286.0x the scale of `vf_clip_param`. This means that it will take more than 286.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m ring length: 241\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m v_max: 4.236854288051661\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m ring length: 237\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m v_max: 4.04661795519353\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m ring length: 266\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m v_max: 5.42459972166245\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m ring length: 262\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m v_max: 5.234739483008334\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m ring length: 221\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m v_max: 3.285334164169417\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_fd1ac_00000:\n",
      "  agent_timesteps_total: 320000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-01_10-40-53\n",
      "  done: false\n",
      "  episode_len_mean: 4000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1450.0589518342097\n",
      "  episode_reward_mean: -2622.1675591503663\n",
      "  episode_reward_min: -6797.871298231539\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 80\n",
      "  experiment_id: 2b7c5715c95c4828907cbc45e49df6f0\n",
      "  hostname: michael-home\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.00625\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -0.32578353555339157\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.007963612668010838\n",
      "          policy_loss: -0.0017054821886596785\n",
      "          total_loss: 71.0477056114537\n",
      "          vf_explained_var: -4.1761216706959203e-10\n",
      "          vf_loss: 71.04936147799158\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 320000\n",
      "    num_steps_sampled: 320000\n",
      "    num_steps_trained: 320000\n",
      "  iterations_since_restore: 16\n",
      "  node_ip: 192.168.0.33\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 25.495550351288053\n",
      "    ram_util_percent: 49.74098360655738\n",
      "  pid: 25804\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09467916951286548\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 35.757233303919534\n",
      "    mean_inference_ms: 1.8974112464539932\n",
      "    mean_raw_obs_processing_ms: 21.67389470228291\n",
      "  time_since_restore: 4863.854377985001\n",
      "  time_this_iter_s: 298.7953152656555\n",
      "  time_total_s: 4863.854377985001\n",
      "  timers:\n",
      "    learn_throughput: 254.616\n",
      "    learn_time_ms: 78549.761\n",
      "    sample_throughput: 90.927\n",
      "    sample_time_ms: 219955.653\n",
      "    update_time_ms: 3.154\n",
      "  timestamp: 1659368453\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 320000\n",
      "  training_iteration: 16\n",
      "  trial_id: fd1ac_00000\n",
      "  \n",
      "\u001b[2m\u001b[36m(pid=25804)\u001b[0m 2022-08-01 10:40:53,698\tWARNING ppo.py:242 -- The magnitude of your environment rewards are more than 262.0x the scale of `vf_clip_param`. This means that it will take more than 262.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "== Status ==\n",
      "Memory usage on this node: 15.6/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/4.91 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_0_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_3_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_2_28b5456e04c30e30e910ccff5ed5840d, 0.0/6.0 CPU_group_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_5_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_4_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_1_28b5456e04c30e30e910ccff5ed5840d)\n",
      "Result logdir: /home/michael/ray_results/stabilizing_the_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_fd1ac_00000 | RUNNING  | 192.168.0.33:25804 |     16 |          4863.85 | 320000 | -2622.17 |              1450.06 |             -6797.87 |               4000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m ring length: 220\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m v_max: 3.2377399006821497\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m ring length: 248\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m v_max: 4.569659527528101\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m ring length: 258\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m v_max: 5.044800727535787\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m ring length: 243\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m v_max: 4.331956438196479\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m ring length: 232\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m v_max: 3.8087690783250063\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_fd1ac_00000:\n",
      "  agent_timesteps_total: 340000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-01_10-45-52\n",
      "  done: false\n",
      "  episode_len_mean: 4000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1453.9667976432893\n",
      "  episode_reward_mean: -2420.973191836566\n",
      "  episode_reward_min: -6797.871298231539\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 85\n",
      "  experiment_id: 2b7c5715c95c4828907cbc45e49df6f0\n",
      "  hostname: michael-home\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.003125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -0.4090255066088051\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008356456788980468\n",
      "          policy_loss: -0.0027464519029447606\n",
      "          total_loss: 64.23420596031626\n",
      "          vf_explained_var: -2.27788454765232e-09\n",
      "          vf_loss: 64.23692627682048\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 340000\n",
      "    num_steps_sampled: 340000\n",
      "    num_steps_trained: 340000\n",
      "  iterations_since_restore: 17\n",
      "  node_ip: 192.168.0.33\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 25.501643192488263\n",
      "    ram_util_percent: 49.92394366197183\n",
      "  pid: 25804\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0946657272270689\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 35.56540382208617\n",
      "    mean_inference_ms: 1.8967919976197158\n",
      "    mean_raw_obs_processing_ms: 21.672742961099736\n",
      "  time_since_restore: 5162.614058732986\n",
      "  time_this_iter_s: 298.75968074798584\n",
      "  time_total_s: 5162.614058732986\n",
      "  timers:\n",
      "    learn_throughput: 254.311\n",
      "    learn_time_ms: 78643.98\n",
      "    sample_throughput: 90.935\n",
      "    sample_time_ms: 219937.301\n",
      "    update_time_ms: 3.088\n",
      "  timestamp: 1659368752\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 340000\n",
      "  training_iteration: 17\n",
      "  trial_id: fd1ac_00000\n",
      "  \n",
      "\u001b[2m\u001b[36m(pid=25804)\u001b[0m 2022-08-01 10:45:52,512\tWARNING ppo.py:242 -- The magnitude of your environment rewards are more than 242.0x the scale of `vf_clip_param`. This means that it will take more than 242.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Memory usage on this node: 15.6/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/4.91 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_3_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_4_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 accelerator_type:G, 0.0/6.0 CPU_group_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_2_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_0_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_5_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_1_28b5456e04c30e30e910ccff5ed5840d)\n",
      "Result logdir: /home/michael/ray_results/stabilizing_the_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_fd1ac_00000 | RUNNING  | 192.168.0.33:25804 |     17 |          5162.61 | 340000 | -2420.97 |              1453.97 |             -6797.87 |               4000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m ring length: 251\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m v_max: 4.71224180704279\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m ring length: 244\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m v_max: 4.379503211387676\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m ring length: 243\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m v_max: 4.331956438196479\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m ring length: 243\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m v_max: 4.331956438196479\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m ring length: 259\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m v_max: 5.092292348291853\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_fd1ac_00000:\n",
      "  agent_timesteps_total: 360000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-01_10-50-50\n",
      "  done: false\n",
      "  episode_len_mean: 4000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1453.9667976432893\n",
      "  episode_reward_mean: -2265.736177775872\n",
      "  episode_reward_min: -6797.871298231539\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 90\n",
      "  experiment_id: 2b7c5715c95c4828907cbc45e49df6f0\n",
      "  hostname: michael-home\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.0015625\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -0.5111628517983066\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.00867507063908278\n",
      "          policy_loss: -0.0009429497325460718\n",
      "          total_loss: 56.5269949202325\n",
      "          vf_explained_var: 1.51858969843488e-10\n",
      "          vf_loss: 56.52792420235409\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 360000\n",
      "    num_steps_sampled: 360000\n",
      "    num_steps_trained: 360000\n",
      "  iterations_since_restore: 18\n",
      "  node_ip: 192.168.0.33\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 25.425176470588234\n",
      "    ram_util_percent: 50.09223529411764\n",
      "  pid: 25804\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09465571981618696\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 35.39090621652138\n",
      "    mean_inference_ms: 1.8962963364644854\n",
      "    mean_raw_obs_processing_ms: 21.671610518389297\n",
      "  time_since_restore: 5460.305358171463\n",
      "  time_this_iter_s: 297.69129943847656\n",
      "  time_total_s: 5460.305358171463\n",
      "  timers:\n",
      "    learn_throughput: 254.42\n",
      "    learn_time_ms: 78610.196\n",
      "    sample_throughput: 90.946\n",
      "    sample_time_ms: 219909.819\n",
      "    update_time_ms: 3.075\n",
      "  timestamp: 1659369050\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 360000\n",
      "  training_iteration: 18\n",
      "  trial_id: fd1ac_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 15.6/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/4.91 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_0_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_4_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_1_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_3_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_2_28b5456e04c30e30e910ccff5ed5840d, 0.0/6.0 CPU_group_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_5_28b5456e04c30e30e910ccff5ed5840d)\n",
      "Result logdir: /home/michael/ray_results/stabilizing_the_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_fd1ac_00000 | RUNNING  | 192.168.0.33:25804 |     18 |          5460.31 | 360000 | -2265.74 |              1453.97 |             -6797.87 |               4000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=25804)\u001b[0m 2022-08-01 10:50:50,252\tWARNING ppo.py:242 -- The magnitude of your environment rewards are more than 227.0x the scale of `vf_clip_param`. This means that it will take more than 227.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m ring length: 242\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m v_max: 4.2844067680020546\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m ring length: 225\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m v_max: 3.4756971311168834\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m ring length: 246\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m v_max: 4.474587708995648\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m ring length: 268\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m v_max: 5.5194978538368025\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m ring length: 260\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m v_max: 5.139779427502188\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_fd1ac_00000:\n",
      "  agent_timesteps_total: 380000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-01_10-55-48\n",
      "  done: false\n",
      "  episode_len_mean: 4000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1453.9667976432893\n",
      "  episode_reward_mean: -2098.966255860891\n",
      "  episode_reward_min: -6797.871298231539\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 95\n",
      "  experiment_id: 2b7c5715c95c4828907cbc45e49df6f0\n",
      "  hostname: michael-home\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.00078125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -0.5595992677151018\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009750541320788396\n",
      "          policy_loss: -0.0031607731349267967\n",
      "          total_loss: 50.273237008501766\n",
      "          vf_explained_var: -1.5185896984348801e-09\n",
      "          vf_loss: 50.2763900538159\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 380000\n",
      "    num_steps_sampled: 380000\n",
      "    num_steps_trained: 380000\n",
      "  iterations_since_restore: 19\n",
      "  node_ip: 192.168.0.33\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 25.493176470588235\n",
      "    ram_util_percent: 50.0510588235294\n",
      "  pid: 25804\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09464612324829698\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 35.23142594630972\n",
      "    mean_inference_ms: 1.8958779529371261\n",
      "    mean_raw_obs_processing_ms: 21.670509247035483\n",
      "  time_since_restore: 5758.547954082489\n",
      "  time_this_iter_s: 298.242595911026\n",
      "  time_total_s: 5758.547954082489\n",
      "  timers:\n",
      "    learn_throughput: 254.472\n",
      "    learn_time_ms: 78593.99\n",
      "    sample_throughput: 90.953\n",
      "    sample_time_ms: 219893.933\n",
      "    update_time_ms: 3.041\n",
      "  timestamp: 1659369348\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 380000\n",
      "  training_iteration: 19\n",
      "  trial_id: fd1ac_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Memory usage on this node: 15.6/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/4.91 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_5_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_4_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_0_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_2_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_3_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_1_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 accelerator_type:G, 0.0/6.0 CPU_group_28b5456e04c30e30e910ccff5ed5840d)\n",
      "Result logdir: /home/michael/ray_results/stabilizing_the_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_fd1ac_00000 | RUNNING  | 192.168.0.33:25804 |     19 |          5758.55 | 380000 | -2098.97 |              1453.97 |             -6797.87 |               4000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=25804)\u001b[0m 2022-08-01 10:55:48,545\tWARNING ppo.py:242 -- The magnitude of your environment rewards are more than 210.0x the scale of `vf_clip_param`. This means that it will take more than 210.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m ring length: 268\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m v_max: 5.5194978538368025\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m ring length: 242\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m v_max: 4.2844067680020546\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m ring length: 252\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m v_max: 4.759762049824181\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m ring length: 231\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m v_max: 3.761192925277308\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m ring length: 259\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m v_max: 5.092292348291853\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m Could not connect to TraCI server at localhost:35657 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m Could not connect to TraCI server at localhost:57823 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m Could not connect to TraCI server at localhost:41611 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m Could not connect to TraCI server at localhost:45453 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m Could not connect to TraCI server at localhost:45851 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m Could not connect to TraCI server at localhost:35657 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m  Retrying in 3 seconds\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m Could not connect to TraCI server at localhost:45851 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m  Retrying in 3 seconds\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m Could not connect to TraCI server at localhost:57823 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m  Retrying in 3 seconds\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m Could not connect to TraCI server at localhost:41611 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m  Retrying in 3 seconds\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m Could not connect to TraCI server at localhost:45453 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m  Retrying in 3 seconds\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_fd1ac_00000:\n",
      "  agent_timesteps_total: 400000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-01_11-00-53\n",
      "  done: false\n",
      "  episode_len_mean: 4000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2122.7753328870995\n",
      "  episode_reward_mean: -1925.7637004578612\n",
      "  episode_reward_min: -6797.871298231539\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 100\n",
      "  experiment_id: 2b7c5715c95c4828907cbc45e49df6f0\n",
      "  hostname: michael-home\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.000390625\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -0.6672581554977757\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008157776319106727\n",
      "          policy_loss: -0.0017498145321870496\n",
      "          total_loss: 50.213675700631114\n",
      "          vf_explained_var: -8.731890766000561e-10\n",
      "          vf_loss: 50.21542227556751\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 400000\n",
      "    num_steps_sampled: 400000\n",
      "    num_steps_trained: 400000\n",
      "  iterations_since_restore: 20\n",
      "  node_ip: 192.168.0.33\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 25.061379310344826\n",
      "    ram_util_percent: 50.1071264367816\n",
      "  pid: 25804\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09463632805657402\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 35.08506888361599\n",
      "    mean_inference_ms: 1.8954895644480323\n",
      "    mean_raw_obs_processing_ms: 21.673175445838844\n",
      "  time_since_restore: 6063.406213760376\n",
      "  time_this_iter_s: 304.85825967788696\n",
      "  time_total_s: 6063.406213760376\n",
      "  timers:\n",
      "    learn_throughput: 254.227\n",
      "    learn_time_ms: 78669.809\n",
      "    sample_throughput: 90.712\n",
      "    sample_time_ms: 220478.654\n",
      "    update_time_ms: 3.021\n",
      "  timestamp: 1659369653\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 400000\n",
      "  training_iteration: 20\n",
      "  trial_id: fd1ac_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 15.6/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/4.91 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_0_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_2_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_5_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_4_28b5456e04c30e30e910ccff5ed5840d, 0.0/6.0 CPU_group_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_1_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_3_28b5456e04c30e30e910ccff5ed5840d)\n",
      "Result logdir: /home/michael/ray_results/stabilizing_the_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_fd1ac_00000 | RUNNING  | 192.168.0.33:25804 |     20 |          6063.41 | 400000 | -1925.76 |              2122.78 |             -6797.87 |               4000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m ring length: 224\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m v_max: 3.4281086136538996\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m ring length: 248\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m v_max: 4.569659527528101\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m ring length: 270\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m v_max: 5.6143732387852054\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m ring length: 223\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m v_max: 3.3805185755587788\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m ring length: 255\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m v_max: 4.902299776966978\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_fd1ac_00000:\n",
      "  agent_timesteps_total: 420000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-01_11-05-52\n",
      "  done: false\n",
      "  episode_len_mean: 4000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2122.7753328870995\n",
      "  episode_reward_mean: -1540.9278412434617\n",
      "  episode_reward_min: -6628.118744617556\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 105\n",
      "  experiment_id: 2b7c5715c95c4828907cbc45e49df6f0\n",
      "  hostname: michael-home\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.0001953125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -0.7013890638472927\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008727353284892748\n",
      "          policy_loss: -0.002681067351038289\n",
      "          total_loss: 43.414275166942815\n",
      "          vf_explained_var: -1.8602723805827281e-09\n",
      "          vf_loss: 43.41695445722835\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 420000\n",
      "    num_steps_sampled: 420000\n",
      "    num_steps_trained: 420000\n",
      "  iterations_since_restore: 21\n",
      "  node_ip: 192.168.0.33\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 25.56510538641686\n",
      "    ram_util_percent: 50.323419203747086\n",
      "  pid: 25804\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09461197330105886\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 34.06534163640386\n",
      "    mean_inference_ms: 1.8940036143418928\n",
      "    mean_raw_obs_processing_ms: 21.67849182984816\n",
      "  time_since_restore: 6362.50497174263\n",
      "  time_this_iter_s: 299.098757982254\n",
      "  time_total_s: 6362.50497174263\n",
      "  timers:\n",
      "    learn_throughput: 254.221\n",
      "    learn_time_ms: 78671.599\n",
      "    sample_throughput: 90.711\n",
      "    sample_time_ms: 220481.452\n",
      "    update_time_ms: 3.004\n",
      "  timestamp: 1659369952\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 420000\n",
      "  training_iteration: 21\n",
      "  trial_id: fd1ac_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 15.7/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/4.91 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_2_28b5456e04c30e30e910ccff5ed5840d, 0.0/6.0 CPU_group_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_5_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_1_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_0_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_4_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_3_28b5456e04c30e30e910ccff5ed5840d)\n",
      "Result logdir: /home/michael/ray_results/stabilizing_the_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_fd1ac_00000 | RUNNING  | 192.168.0.33:25804 |     21 |           6362.5 | 420000 | -1540.93 |              2122.78 |             -6628.12 |               4000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m ring length: 236\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m v_max: 3.999052674951912\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m ring length: 222\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m v_max: 3.3329270738617227\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m ring length: 228\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m v_max: 3.618452967700356\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m ring length: 250\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m v_max: 4.664717904914125\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m ring length: 223\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m v_max: 3.3805185755587788\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_fd1ac_00000:\n",
      "  agent_timesteps_total: 440000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-01_11-10-51\n",
      "  done: false\n",
      "  episode_len_mean: 4000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2129.677760092617\n",
      "  episode_reward_mean: -1153.3076040582662\n",
      "  episode_reward_min: -6434.724690982717\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 110\n",
      "  experiment_id: 2b7c5715c95c4828907cbc45e49df6f0\n",
      "  hostname: michael-home\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 9.765625e-05\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -0.737254556728776\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.006091564029301695\n",
      "          policy_loss: 7.002281505543336e-05\n",
      "          total_loss: 50.34976105781118\n",
      "          vf_explained_var: -4.93541651991336e-10\n",
      "          vf_loss: 50.34969039722613\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 440000\n",
      "    num_steps_sampled: 440000\n",
      "    num_steps_trained: 440000\n",
      "  iterations_since_restore: 22\n",
      "  node_ip: 192.168.0.33\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 25.445774647887326\n",
      "    ram_util_percent: 50.34107981220658\n",
      "  pid: 25804\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09458655022409308\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 33.579118112444064\n",
      "    mean_inference_ms: 1.892531153420928\n",
      "    mean_raw_obs_processing_ms: 21.68229013543253\n",
      "  time_since_restore: 6661.232644557953\n",
      "  time_this_iter_s: 298.7276728153229\n",
      "  time_total_s: 6661.232644557953\n",
      "  timers:\n",
      "    learn_throughput: 254.014\n",
      "    learn_time_ms: 78735.694\n",
      "    sample_throughput: 90.715\n",
      "    sample_time_ms: 220471.035\n",
      "    update_time_ms: 3.034\n",
      "  timestamp: 1659370251\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 440000\n",
      "  training_iteration: 22\n",
      "  trial_id: fd1ac_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 15.7/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/4.91 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_4_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_5_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_2_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_1_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_0_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_3_28b5456e04c30e30e910ccff5ed5840d, 0.0/6.0 CPU_group_28b5456e04c30e30e910ccff5ed5840d)\n",
      "Result logdir: /home/michael/ray_results/stabilizing_the_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_fd1ac_00000 | RUNNING  | 192.168.0.33:25804 |     22 |          6661.23 | 440000 | -1153.31 |              2129.68 |             -6434.72 |               4000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m ring length: 257\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m v_max: 4.997304682316517\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m ring length: 260\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m v_max: 5.139779427502188\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m ring length: 243\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m v_max: 4.331956438196479\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m ring length: 255\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m v_max: 4.902299776966978\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m ring length: 228\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m v_max: 3.618452967700356\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_fd1ac_00000:\n",
      "  agent_timesteps_total: 460000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-01_11-15-50\n",
      "  done: false\n",
      "  episode_len_mean: 4000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2129.677760092617\n",
      "  episode_reward_mean: -806.695808382118\n",
      "  episode_reward_min: -5910.053584409723\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 115\n",
      "  experiment_id: 2b7c5715c95c4828907cbc45e49df6f0\n",
      "  hostname: michael-home\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 4.8828125e-05\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -0.8292042956230746\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009412168022126674\n",
      "          policy_loss: -0.002722112894366691\n",
      "          total_loss: 31.76456664626006\n",
      "          vf_explained_var: -9.111538190609281e-10\n",
      "          vf_loss: 31.7672882128673\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 460000\n",
      "    num_steps_sampled: 460000\n",
      "    num_steps_trained: 460000\n",
      "  iterations_since_restore: 23\n",
      "  node_ip: 192.168.0.33\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 25.507276995305165\n",
      "    ram_util_percent: 50.4406103286385\n",
      "  pid: 25804\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09456272811040453\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 33.26909267863643\n",
      "    mean_inference_ms: 1.8916397413128667\n",
      "    mean_raw_obs_processing_ms: 21.68620616290475\n",
      "  time_since_restore: 6959.812755346298\n",
      "  time_this_iter_s: 298.58011078834534\n",
      "  time_total_s: 6959.812755346298\n",
      "  timers:\n",
      "    learn_throughput: 253.98\n",
      "    learn_time_ms: 78746.305\n",
      "    sample_throughput: 90.714\n",
      "    sample_time_ms: 220473.903\n",
      "    update_time_ms: 3.138\n",
      "  timestamp: 1659370550\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 460000\n",
      "  training_iteration: 23\n",
      "  trial_id: fd1ac_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 15.8/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/4.91 GiB heap, 0.0/14.65 GiB objects (0.0/6.0 CPU_group_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_3_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_5_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_0_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_4_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_1_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_2_28b5456e04c30e30e910ccff5ed5840d)\n",
      "Result logdir: /home/michael/ray_results/stabilizing_the_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_fd1ac_00000 | RUNNING  | 192.168.0.33:25804 |     23 |          6959.81 | 460000 | -806.696 |              2129.68 |             -5910.05 |               4000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m ring length: 228\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m v_max: 3.618452967700356\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m ring length: 237\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m v_max: 4.04661795519353\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m ring length: 260\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m v_max: 5.139779427502188\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m ring length: 233\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m v_max: 3.856343201216534\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m ring length: 268\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m v_max: 5.5194978538368025\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_fd1ac_00000:\n",
      "  agent_timesteps_total: 480000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-01_11-20-48\n",
      "  done: false\n",
      "  episode_len_mean: 4000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2129.677760092617\n",
      "  episode_reward_mean: -468.18408117352527\n",
      "  episode_reward_min: -5749.261679593213\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 120\n",
      "  experiment_id: 2b7c5715c95c4828907cbc45e49df6f0\n",
      "  hostname: michael-home\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.44140625e-05\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -0.8283514655319748\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.005073357499049006\n",
      "          policy_loss: 0.00016540650450357585\n",
      "          total_loss: 52.799342535711396\n",
      "          vf_explained_var: -5.8465703389742885e-09\n",
      "          vf_loss: 52.799176912247\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 480000\n",
      "    num_steps_sampled: 480000\n",
      "    num_steps_trained: 480000\n",
      "  iterations_since_restore: 24\n",
      "  node_ip: 192.168.0.33\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 25.487793427230045\n",
      "    ram_util_percent: 50.66596244131455\n",
      "  pid: 25804\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09455061769392588\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 33.04572446404362\n",
      "    mean_inference_ms: 1.8915654319850441\n",
      "    mean_raw_obs_processing_ms: 21.686740166789686\n",
      "  time_since_restore: 7258.515087842941\n",
      "  time_this_iter_s: 298.70233249664307\n",
      "  time_total_s: 7258.515087842941\n",
      "  timers:\n",
      "    learn_throughput: 254.158\n",
      "    learn_time_ms: 78691.077\n",
      "    sample_throughput: 90.708\n",
      "    sample_time_ms: 220487.012\n",
      "    update_time_ms: 3.133\n",
      "  timestamp: 1659370848\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 480000\n",
      "  training_iteration: 24\n",
      "  trial_id: fd1ac_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 15.8/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/4.91 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_1_28b5456e04c30e30e910ccff5ed5840d, 0.0/6.0 CPU_group_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_4_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_3_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_2_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_5_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_0_28b5456e04c30e30e910ccff5ed5840d)\n",
      "Result logdir: /home/michael/ray_results/stabilizing_the_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_fd1ac_00000 | RUNNING  | 192.168.0.33:25804 |     24 |          7258.52 | 480000 | -468.184 |              2129.68 |             -5749.26 |               4000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m ring length: 260\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m v_max: 5.139779427502188\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m ring length: 260\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m v_max: 5.139779427502188\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m ring length: 263\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m v_max: 5.282212215151455\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m ring length: 239\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m v_max: 4.141741239205832\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m ring length: 227\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m v_max: 3.570869368805752\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_fd1ac_00000:\n",
      "  agent_timesteps_total: 500000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-01_11-25-46\n",
      "  done: false\n",
      "  episode_len_mean: 4000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2129.677760092617\n",
      "  episode_reward_mean: -163.57494637700248\n",
      "  episode_reward_min: -4884.660168920818\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 125\n",
      "  experiment_id: 2b7c5715c95c4828907cbc45e49df6f0\n",
      "  hostname: michael-home\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.220703125e-05\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -0.9187033381431725\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.007784838966819885\n",
      "          policy_loss: -0.0008837057876691317\n",
      "          total_loss: 54.9616486591898\n",
      "          vf_explained_var: -4.138156928235048e-09\n",
      "          vf_loss: 54.962532320447785\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 500000\n",
      "    num_steps_sampled: 500000\n",
      "    num_steps_trained: 500000\n",
      "  iterations_since_restore: 25\n",
      "  node_ip: 192.168.0.33\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 25.507058823529412\n",
      "    ram_util_percent: 50.65176470588235\n",
      "  pid: 25804\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09453678898164926\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 32.87421775226289\n",
      "    mean_inference_ms: 1.8914016543659005\n",
      "    mean_raw_obs_processing_ms: 21.6855316682012\n",
      "  time_since_restore: 7556.554523229599\n",
      "  time_this_iter_s: 298.0394353866577\n",
      "  time_total_s: 7556.554523229599\n",
      "  timers:\n",
      "    learn_throughput: 254.278\n",
      "    learn_time_ms: 78654.135\n",
      "    sample_throughput: 90.711\n",
      "    sample_time_ms: 220480.215\n",
      "    update_time_ms: 3.186\n",
      "  timestamp: 1659371146\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 500000\n",
      "  training_iteration: 25\n",
      "  trial_id: fd1ac_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 15.8/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/4.91 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_2_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_1_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_0_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_4_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_5_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_3_28b5456e04c30e30e910ccff5ed5840d, 0.0/6.0 CPU_group_28b5456e04c30e30e910ccff5ed5840d)\n",
      "Result logdir: /home/michael/ray_results/stabilizing_the_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_fd1ac_00000 | RUNNING  | 192.168.0.33:25804 |     25 |          7556.55 | 500000 | -163.575 |              2129.68 |             -4884.66 |               4000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m ring length: 257\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m v_max: 4.997304682316517\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m ring length: 268\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m v_max: 5.5194978538368025\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m ring length: 253\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m v_max: 4.807278529691987\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m ring length: 246\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m v_max: 4.474587708995648\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m ring length: 269\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m v_max: 5.566938458220921\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_fd1ac_00000:\n",
      "  agent_timesteps_total: 520000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-01_11-30-45\n",
      "  done: false\n",
      "  episode_len_mean: 4000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2451.0499314100603\n",
      "  episode_reward_mean: 137.8065714506933\n",
      "  episode_reward_min: -4068.532084155927\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 130\n",
      "  experiment_id: 2b7c5715c95c4828907cbc45e49df6f0\n",
      "  hostname: michael-home\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 6.103515625e-06\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -0.9518494044519534\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.007965707760764086\n",
      "          policy_loss: -0.0005180708672495404\n",
      "          total_loss: 56.00185838687192\n",
      "          vf_explained_var: -2.7334614571827842e-09\n",
      "          vf_loss: 56.00237635229803\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 520000\n",
      "    num_steps_sampled: 520000\n",
      "    num_steps_trained: 520000\n",
      "  iterations_since_restore: 26\n",
      "  node_ip: 192.168.0.33\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 25.571428571428573\n",
      "    ram_util_percent: 50.73700234192037\n",
      "  pid: 25804\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09452244260987953\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 32.73697652105948\n",
      "    mean_inference_ms: 1.890978778273617\n",
      "    mean_raw_obs_processing_ms: 21.68481530343307\n",
      "  time_since_restore: 7855.48216342926\n",
      "  time_this_iter_s: 298.92764019966125\n",
      "  time_total_s: 7855.48216342926\n",
      "  timers:\n",
      "    learn_throughput: 254.254\n",
      "    learn_time_ms: 78661.593\n",
      "    sample_throughput: 90.709\n",
      "    sample_time_ms: 220486.036\n",
      "    update_time_ms: 3.191\n",
      "  timestamp: 1659371445\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 520000\n",
      "  training_iteration: 26\n",
      "  trial_id: fd1ac_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 15.8/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/4.91 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_1_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_4_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_5_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 accelerator_type:G, 0.0/6.0 CPU_group_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_2_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_0_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_3_28b5456e04c30e30e910ccff5ed5840d)\n",
      "Result logdir: /home/michael/ray_results/stabilizing_the_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_fd1ac_00000 | RUNNING  | 192.168.0.33:25804 |     26 |          7855.48 | 520000 |  137.807 |              2451.05 |             -4068.53 |               4000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m ring length: 241\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m v_max: 4.236854288051661\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m ring length: 241\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m v_max: 4.236854288051661\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m ring length: 228\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m v_max: 3.618452967700356\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m ring length: 245\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m v_max: 4.427046998576354\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m ring length: 270\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m v_max: 5.6143732387852054\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_fd1ac_00000:\n",
      "  agent_timesteps_total: 540000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-01_11-35-43\n",
      "  done: false\n",
      "  episode_len_mean: 4000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2451.0499314100603\n",
      "  episode_reward_mean: 394.6950219953383\n",
      "  episode_reward_min: -2865.586883001366\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 135\n",
      "  experiment_id: 2b7c5715c95c4828907cbc45e49df6f0\n",
      "  hostname: michael-home\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 3.0517578125e-06\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -0.9964015563582159\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.006680322256459036\n",
      "          policy_loss: -8.390693314326037e-05\n",
      "          total_loss: 56.506346813129014\n",
      "          vf_explained_var: -8.352243341391841e-10\n",
      "          vf_loss: 56.50643064899809\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 540000\n",
      "    num_steps_sampled: 540000\n",
      "    num_steps_trained: 540000\n",
      "  iterations_since_restore: 27\n",
      "  node_ip: 192.168.0.33\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 25.66305882352941\n",
      "    ram_util_percent: 50.84541176470589\n",
      "  pid: 25804\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09450404632882449\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 32.623894982863796\n",
      "    mean_inference_ms: 1.890353973056859\n",
      "    mean_raw_obs_processing_ms: 21.684555787176155\n",
      "  time_since_restore: 8153.525189638138\n",
      "  time_this_iter_s: 298.04302620887756\n",
      "  time_total_s: 8153.525189638138\n",
      "  timers:\n",
      "    learn_throughput: 254.506\n",
      "    learn_time_ms: 78583.574\n",
      "    sample_throughput: 90.706\n",
      "    sample_time_ms: 220492.302\n",
      "    update_time_ms: 3.182\n",
      "  timestamp: 1659371743\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 540000\n",
      "  training_iteration: 27\n",
      "  trial_id: fd1ac_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 15.9/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/4.91 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_0_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_3_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_5_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_1_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_4_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_2_28b5456e04c30e30e910ccff5ed5840d, 0.0/6.0 CPU_group_28b5456e04c30e30e910ccff5ed5840d)\n",
      "Result logdir: /home/michael/ray_results/stabilizing_the_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_fd1ac_00000 | RUNNING  | 192.168.0.33:25804 |     27 |          8153.53 | 540000 |  394.695 |              2451.05 |             -2865.59 |               4000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m ring length: 269\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m v_max: 5.566938458220921\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m ring length: 256\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m v_max: 4.949804327743507\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m ring length: 222\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m v_max: 3.3329270738617227\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m ring length: 268\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m v_max: 5.5194978538368025\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m ring length: 254\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m v_max: 4.854791141191876\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m  Retrying in 1 seconds\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_fd1ac_00000:\n",
      "  agent_timesteps_total: 560000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-01_11-40-43\n",
      "  done: false\n",
      "  episode_len_mean: 4000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2451.0499314100603\n",
      "  episode_reward_mean: 593.4415296648415\n",
      "  episode_reward_min: -2865.586883001366\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 140\n",
      "  experiment_id: 2b7c5715c95c4828907cbc45e49df6f0\n",
      "  hostname: michael-home\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.52587890625e-06\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -0.9873123103266309\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0067642872184692955\n",
      "          policy_loss: -0.0002939699116833271\n",
      "          total_loss: 42.30227259860676\n",
      "          vf_explained_var: -4.1761216706959203e-10\n",
      "          vf_loss: 42.302566550188004\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 560000\n",
      "    num_steps_sampled: 560000\n",
      "    num_steps_trained: 560000\n",
      "  iterations_since_restore: 28\n",
      "  node_ip: 192.168.0.33\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 25.43084112149533\n",
      "    ram_util_percent: 50.98060747663551\n",
      "  pid: 25804\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09448735772480221\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 32.52846575692939\n",
      "    mean_inference_ms: 1.8897538625007833\n",
      "    mean_raw_obs_processing_ms: 21.684946516068752\n",
      "  time_since_restore: 8453.445495843887\n",
      "  time_this_iter_s: 299.9203062057495\n",
      "  time_total_s: 8453.445495843887\n",
      "  timers:\n",
      "    learn_throughput: 254.136\n",
      "    learn_time_ms: 78697.92\n",
      "    sample_throughput: 90.661\n",
      "    sample_time_ms: 220600.891\n",
      "    update_time_ms: 3.164\n",
      "  timestamp: 1659372043\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 560000\n",
      "  training_iteration: 28\n",
      "  trial_id: fd1ac_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 15.9/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/4.91 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_5_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_2_28b5456e04c30e30e910ccff5ed5840d, 0.0/6.0 CPU_group_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_4_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_0_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_1_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_3_28b5456e04c30e30e910ccff5ed5840d)\n",
      "Result logdir: /home/michael/ray_results/stabilizing_the_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_fd1ac_00000 | RUNNING  | 192.168.0.33:25804 |     28 |          8453.45 | 560000 |  593.442 |              2451.05 |             -2865.59 |               4000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m ring length: 260\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m v_max: 5.139779427502188\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m ring length: 253\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m v_max: 4.807278529691987\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m ring length: 257\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m v_max: 4.997304682316517\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m ring length: 231\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m v_max: 3.761192925277308\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m ring length: 267\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m v_max: 5.472051563171826\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_fd1ac_00000:\n",
      "  agent_timesteps_total: 580000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-01_11-45-42\n",
      "  done: false\n",
      "  episode_len_mean: 4000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2451.0499314100603\n",
      "  episode_reward_mean: 795.7783842735357\n",
      "  episode_reward_min: -1544.125459457408\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 145\n",
      "  experiment_id: 2b7c5715c95c4828907cbc45e49df6f0\n",
      "  hostname: michael-home\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 7.62939453125e-07\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -1.0004590836679859\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.007486443524285973\n",
      "          policy_loss: -0.0008755382272017419\n",
      "          total_loss: 52.55852921358339\n",
      "          vf_explained_var: -9.87083303982672e-10\n",
      "          vf_loss: 52.55940473398585\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 580000\n",
      "    num_steps_sampled: 580000\n",
      "    num_steps_trained: 580000\n",
      "  iterations_since_restore: 29\n",
      "  node_ip: 192.168.0.33\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 25.475409836065573\n",
      "    ram_util_percent: 51.11803278688524\n",
      "  pid: 25804\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09446971845776984\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 32.44675923101737\n",
      "    mean_inference_ms: 1.8889735826801042\n",
      "    mean_raw_obs_processing_ms: 21.68560077571874\n",
      "  time_since_restore: 8752.235382318497\n",
      "  time_this_iter_s: 298.7898864746094\n",
      "  time_total_s: 8752.235382318497\n",
      "  timers:\n",
      "    learn_throughput: 253.994\n",
      "    learn_time_ms: 78741.982\n",
      "    sample_throughput: 90.657\n",
      "    sample_time_ms: 220611.572\n",
      "    update_time_ms: 3.145\n",
      "  timestamp: 1659372342\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 580000\n",
      "  training_iteration: 29\n",
      "  trial_id: fd1ac_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 15.9/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/4.91 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_4_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_3_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_5_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 accelerator_type:G, 0.0/6.0 CPU_group_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_1_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_2_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_0_28b5456e04c30e30e910ccff5ed5840d)\n",
      "Result logdir: /home/michael/ray_results/stabilizing_the_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_fd1ac_00000 | RUNNING  | 192.168.0.33:25804 |     29 |          8752.24 | 580000 |  795.778 |              2451.05 |             -1544.13 |               4000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m ring length: 270\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m v_max: 5.6143732387852054\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m ring length: 261\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m v_max: 5.187261846097525\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m ring length: 247\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m v_max: 4.522125250095652\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m ring length: 251\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m v_max: 4.71224180704279\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m ring length: 250\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m v_max: 4.664717904914125\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_fd1ac_00000:\n",
      "  agent_timesteps_total: 600000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-01_11-50-41\n",
      "  done: false\n",
      "  episode_len_mean: 4000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2451.0499314100603\n",
      "  episode_reward_mean: 934.2157082923197\n",
      "  episode_reward_min: -1327.4010187883728\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 150\n",
      "  experiment_id: 2b7c5715c95c4828907cbc45e49df6f0\n",
      "  hostname: michael-home\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 3.814697265625e-07\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -1.065197352353175\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.007242542376910865\n",
      "          policy_loss: -0.0011188487080370734\n",
      "          total_loss: 60.45993851339741\n",
      "          vf_explained_var: 7.213301067565681e-10\n",
      "          vf_loss: 60.46105733008901\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 600000\n",
      "    num_steps_sampled: 600000\n",
      "    num_steps_trained: 600000\n",
      "  iterations_since_restore: 30\n",
      "  node_ip: 192.168.0.33\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 25.535529411764703\n",
      "    ram_util_percent: 51.126588235294115\n",
      "  pid: 25804\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09445340186379247\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 32.3756076383999\n",
      "    mean_inference_ms: 1.8883113041948838\n",
      "    mean_raw_obs_processing_ms: 21.68643820997936\n",
      "  time_since_restore: 9050.49762248993\n",
      "  time_this_iter_s: 298.2622401714325\n",
      "  time_total_s: 9050.49762248993\n",
      "  timers:\n",
      "    learn_throughput: 254.179\n",
      "    learn_time_ms: 78684.75\n",
      "    sample_throughput: 90.905\n",
      "    sample_time_ms: 220009.212\n",
      "    update_time_ms: 3.147\n",
      "  timestamp: 1659372641\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 600000\n",
      "  training_iteration: 30\n",
      "  trial_id: fd1ac_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 16.0/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/4.91 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_5_28b5456e04c30e30e910ccff5ed5840d, 0.0/6.0 CPU_group_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_3_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_2_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_1_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_4_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_0_28b5456e04c30e30e910ccff5ed5840d)\n",
      "Result logdir: /home/michael/ray_results/stabilizing_the_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_fd1ac_00000 | RUNNING  | 192.168.0.33:25804 |     30 |           9050.5 | 600000 |  934.216 |              2451.05 |              -1327.4 |               4000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m ring length: 268\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m v_max: 5.5194978538368025\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m ring length: 238\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m v_max: 4.094180836186086\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m ring length: 225\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m v_max: 3.4756971311168834\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m ring length: 262\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m v_max: 5.234739483008334\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m ring length: 266\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m v_max: 5.42459972166245\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_fd1ac_00000:\n",
      "  agent_timesteps_total: 620000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-01_11-55-39\n",
      "  done: false\n",
      "  episode_len_mean: 4000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2571.4192655182173\n",
      "  episode_reward_mean: 1094.6850188953958\n",
      "  episode_reward_min: -862.3358963670104\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 155\n",
      "  experiment_id: 2b7c5715c95c4828907cbc45e49df6f0\n",
      "  hostname: michael-home\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.9073486328125e-07\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -1.124076505679234\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009388159230132\n",
      "          policy_loss: -0.0011704887367286689\n",
      "          total_loss: 59.68527918773092\n",
      "          vf_explained_var: -1.214871758747904e-09\n",
      "          vf_loss: 59.68644965590945\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 620000\n",
      "    num_steps_sampled: 620000\n",
      "    num_steps_trained: 620000\n",
      "  iterations_since_restore: 31\n",
      "  node_ip: 192.168.0.33\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 25.526291079812204\n",
      "    ram_util_percent: 51.24812206572769\n",
      "  pid: 25804\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09443440324944831\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 32.31307459514707\n",
      "    mean_inference_ms: 1.8876373960487274\n",
      "    mean_raw_obs_processing_ms: 21.687462693235258\n",
      "  time_since_restore: 9348.99567580223\n",
      "  time_this_iter_s: 298.49805331230164\n",
      "  time_total_s: 9348.99567580223\n",
      "  timers:\n",
      "    learn_throughput: 254.362\n",
      "    learn_time_ms: 78628.224\n",
      "    sample_throughput: 90.907\n",
      "    sample_time_ms: 220005.645\n",
      "    update_time_ms: 3.158\n",
      "  timestamp: 1659372939\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 620000\n",
      "  training_iteration: 31\n",
      "  trial_id: fd1ac_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 16.0/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/4.91 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_2_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_5_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_3_28b5456e04c30e30e910ccff5ed5840d, 0.0/6.0 CPU_group_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_4_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_0_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_1_28b5456e04c30e30e910ccff5ed5840d)\n",
      "Result logdir: /home/michael/ray_results/stabilizing_the_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_fd1ac_00000 | RUNNING  | 192.168.0.33:25804 |     31 |             9349 | 620000 |  1094.69 |              2571.42 |             -862.336 |               4000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m ring length: 243\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m v_max: 4.331956438196479\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m ring length: 231\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m v_max: 3.761192925277308\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m ring length: 245\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m v_max: 4.427046998576354\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m ring length: 223\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m v_max: 3.3805185755587788\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m ring length: 239\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m v_max: 4.141741239205832\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_fd1ac_00000:\n",
      "  agent_timesteps_total: 640000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-01_12-00-38\n",
      "  done: false\n",
      "  episode_len_mean: 4000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2571.4192655182173\n",
      "  episode_reward_mean: 1174.5254295624236\n",
      "  episode_reward_min: -862.3358963670104\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 160\n",
      "  experiment_id: 2b7c5715c95c4828907cbc45e49df6f0\n",
      "  hostname: michael-home\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 9.5367431640625e-08\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -1.1569468695646639\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.006794994023545485\n",
      "          policy_loss: 0.0005646353198962797\n",
      "          total_loss: 69.26743521477766\n",
      "          vf_explained_var: -4.1761216706959203e-10\n",
      "          vf_loss: 69.2668705253844\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 640000\n",
      "    num_steps_sampled: 640000\n",
      "    num_steps_trained: 640000\n",
      "  iterations_since_restore: 32\n",
      "  node_ip: 192.168.0.33\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 25.492488262910797\n",
      "    ram_util_percent: 51.34953051643193\n",
      "  pid: 25804\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09442057182584732\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 32.25745310720572\n",
      "    mean_inference_ms: 1.887116805548328\n",
      "    mean_raw_obs_processing_ms: 21.688598992567268\n",
      "  time_since_restore: 9647.620408296585\n",
      "  time_this_iter_s: 298.62473249435425\n",
      "  time_total_s: 9647.620408296585\n",
      "  timers:\n",
      "    learn_throughput: 254.419\n",
      "    learn_time_ms: 78610.62\n",
      "    sample_throughput: 90.904\n",
      "    sample_time_ms: 220013.105\n",
      "    update_time_ms: 3.091\n",
      "  timestamp: 1659373238\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 640000\n",
      "  training_iteration: 32\n",
      "  trial_id: fd1ac_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 16.0/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/4.91 GiB heap, 0.0/14.65 GiB objects (0.0/6.0 CPU_group_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_3_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_5_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_2_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_4_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_1_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_0_28b5456e04c30e30e910ccff5ed5840d)\n",
      "Result logdir: /home/michael/ray_results/stabilizing_the_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_fd1ac_00000 | RUNNING  | 192.168.0.33:25804 |     32 |          9647.62 | 640000 |  1174.53 |              2571.42 |             -862.336 |               4000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m ring length: 267\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m v_max: 5.472051563171826\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m ring length: 229\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m v_max: 3.666034803266416\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m ring length: 221\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m v_max: 3.285334164169417\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m ring length: 263\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m v_max: 5.282212215151455\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m ring length: 250\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m v_max: 4.664717904914125\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_fd1ac_00000:\n",
      "  agent_timesteps_total: 660000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-01_12-05-36\n",
      "  done: false\n",
      "  episode_len_mean: 4000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2571.4192655182173\n",
      "  episode_reward_mean: 1242.3976913304439\n",
      "  episode_reward_min: -623.3634641663101\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 165\n",
      "  experiment_id: 2b7c5715c95c4828907cbc45e49df6f0\n",
      "  hostname: michael-home\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 4.76837158203125e-08\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -1.1986565837434902\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.006786530455442795\n",
      "          policy_loss: 0.0005830701894370044\n",
      "          total_loss: 56.15605408735336\n",
      "          vf_explained_var: -8.731890766000561e-10\n",
      "          vf_loss: 56.15547085901734\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 660000\n",
      "    num_steps_sampled: 660000\n",
      "    num_steps_trained: 660000\n",
      "  iterations_since_restore: 33\n",
      "  node_ip: 192.168.0.33\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 25.569483568075114\n",
      "    ram_util_percent: 51.52840375586854\n",
      "  pid: 25804\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09440956658565383\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 32.20763657023721\n",
      "    mean_inference_ms: 1.8866678939377846\n",
      "    mean_raw_obs_processing_ms: 21.6898724293987\n",
      "  time_since_restore: 9946.075985908508\n",
      "  time_this_iter_s: 298.4555776119232\n",
      "  time_total_s: 9946.075985908508\n",
      "  timers:\n",
      "    learn_throughput: 254.466\n",
      "    learn_time_ms: 78595.882\n",
      "    sample_throughput: 90.903\n",
      "    sample_time_ms: 220015.586\n",
      "    update_time_ms: 2.983\n",
      "  timestamp: 1659373536\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 660000\n",
      "  training_iteration: 33\n",
      "  trial_id: fd1ac_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 16.1/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/4.91 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_3_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_1_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_4_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_0_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_2_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_5_28b5456e04c30e30e910ccff5ed5840d, 0.0/6.0 CPU_group_28b5456e04c30e30e910ccff5ed5840d)\n",
      "Result logdir: /home/michael/ray_results/stabilizing_the_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_fd1ac_00000 | RUNNING  | 192.168.0.33:25804 |     33 |          9946.08 | 660000 |   1242.4 |              2571.42 |             -623.363 |               4000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m ring length: 266\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m v_max: 5.42459972166245\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m ring length: 247\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m v_max: 4.522125250095652\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m ring length: 220\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m v_max: 3.2377399006821497\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m ring length: 253\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m v_max: 4.807278529691987\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m ring length: 231\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m v_max: 3.761192925277308\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_fd1ac_00000:\n",
      "  agent_timesteps_total: 680000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-01_12-10-39\n",
      "  done: false\n",
      "  episode_len_mean: 4000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2571.4192655182173\n",
      "  episode_reward_mean: 1341.8237791457836\n",
      "  episode_reward_min: -53.774744726804435\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 170\n",
      "  experiment_id: 2b7c5715c95c4828907cbc45e49df6f0\n",
      "  hostname: michael-home\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.384185791015625e-08\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -1.2531943790472237\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008458394661334833\n",
      "          policy_loss: -0.00015572450705062432\n",
      "          total_loss: 71.99697378000636\n",
      "          vf_explained_var: -3.03717939686976e-10\n",
      "          vf_loss: 71.9971296431912\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 680000\n",
      "    num_steps_sampled: 680000\n",
      "    num_steps_trained: 680000\n",
      "  iterations_since_restore: 34\n",
      "  node_ip: 192.168.0.33\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 28.382366589327145\n",
      "    ram_util_percent: 53.507656612529\n",
      "  pid: 25804\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09440191592476253\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 32.16285174243826\n",
      "    mean_inference_ms: 1.8862763264529827\n",
      "    mean_raw_obs_processing_ms: 21.69144926845227\n",
      "  time_since_restore: 10248.267259120941\n",
      "  time_this_iter_s: 302.19127321243286\n",
      "  time_total_s: 10248.267259120941\n",
      "  timers:\n",
      "    learn_throughput: 253.649\n",
      "    learn_time_ms: 78849.243\n",
      "    sample_throughput: 90.863\n",
      "    sample_time_ms: 220111.086\n",
      "    update_time_ms: 2.973\n",
      "  timestamp: 1659373839\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 680000\n",
      "  training_iteration: 34\n",
      "  trial_id: fd1ac_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 17.2/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/4.91 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_0_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_2_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_1_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_5_28b5456e04c30e30e910ccff5ed5840d, 0.0/6.0 CPU_group_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_4_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_3_28b5456e04c30e30e910ccff5ed5840d)\n",
      "Result logdir: /home/michael/ray_results/stabilizing_the_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_fd1ac_00000 | RUNNING  | 192.168.0.33:25804 |     34 |          10248.3 | 680000 |  1341.82 |              2571.42 |             -53.7747 |               4000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m ring length: 245\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m v_max: 4.427046998576354\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m ring length: 255\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m v_max: 4.902299776966978\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m ring length: 239\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m v_max: 4.141741239205832\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m ring length: 234\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m v_max: 3.903915223349057\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m ring length: 247\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m v_max: 4.522125250095652\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_fd1ac_00000:\n",
      "  agent_timesteps_total: 700000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-01_12-15-40\n",
      "  done: false\n",
      "  episode_len_mean: 4000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2571.4192655182173\n",
      "  episode_reward_mean: 1401.596591503595\n",
      "  episode_reward_min: -53.774744726804435\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 175\n",
      "  experiment_id: 2b7c5715c95c4828907cbc45e49df6f0\n",
      "  hostname: michael-home\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.1920928955078126e-08\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -1.2294341811708585\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.007795366050251392\n",
      "          policy_loss: -0.0004267118183101059\n",
      "          total_loss: 80.46438026914171\n",
      "          vf_explained_var: -3.0371793968697602e-09\n",
      "          vf_loss: 80.46480680575037\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 700000\n",
      "    num_steps_sampled: 700000\n",
      "    num_steps_trained: 700000\n",
      "  iterations_since_restore: 35\n",
      "  node_ip: 192.168.0.33\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 26.78674418604651\n",
      "    ram_util_percent: 54.84232558139535\n",
      "  pid: 25804\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09439846912680178\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 32.122114480012186\n",
      "    mean_inference_ms: 1.8860572615742326\n",
      "    mean_raw_obs_processing_ms: 21.69303004133899\n",
      "  time_since_restore: 10549.540196418762\n",
      "  time_this_iter_s: 301.27293729782104\n",
      "  time_total_s: 10549.540196418762\n",
      "  timers:\n",
      "    learn_throughput: 252.624\n",
      "    learn_time_ms: 79169.048\n",
      "    sample_throughput: 90.862\n",
      "    sample_time_ms: 220114.701\n",
      "    update_time_ms: 2.905\n",
      "  timestamp: 1659374140\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 700000\n",
      "  training_iteration: 35\n",
      "  trial_id: fd1ac_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 17.6/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/4.91 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_4_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_2_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_0_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_5_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_3_28b5456e04c30e30e910ccff5ed5840d, 0.0/6.0 CPU_group_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_1_28b5456e04c30e30e910ccff5ed5840d)\n",
      "Result logdir: /home/michael/ray_results/stabilizing_the_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_fd1ac_00000 | RUNNING  | 192.168.0.33:25804 |     35 |          10549.5 | 700000 |   1401.6 |              2571.42 |             -53.7747 |               4000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m ring length: 250\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m v_max: 4.664717904914125\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m ring length: 251\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m v_max: 4.71224180704279\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m ring length: 245\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m v_max: 4.427046998576354\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m ring length: 222\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m v_max: 3.3329270738617227\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m ring length: 230\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m v_max: 3.7136148111012934\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m  Retrying in 1 seconds\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_fd1ac_00000:\n",
      "  agent_timesteps_total: 720000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-01_12-20-44\n",
      "  done: false\n",
      "  episode_len_mean: 4000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2571.4192655182173\n",
      "  episode_reward_mean: 1430.8103474969512\n",
      "  episode_reward_min: 10.941691724137794\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 180\n",
      "  experiment_id: 2b7c5715c95c4828907cbc45e49df6f0\n",
      "  hostname: michael-home\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 5.960464477539063e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -1.2323411230828352\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.006194823013509588\n",
      "          policy_loss: 0.00023341418754095863\n",
      "          total_loss: 78.2438605533284\n",
      "          vf_explained_var: -4.593733837765512e-09\n",
      "          vf_loss: 78.24362679256755\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 720000\n",
      "    num_steps_sampled: 720000\n",
      "    num_steps_trained: 720000\n",
      "  iterations_since_restore: 36\n",
      "  node_ip: 192.168.0.33\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 29.002771362586607\n",
      "    ram_util_percent: 56.68290993071593\n",
      "  pid: 25804\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09441142062760764\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 32.08485699256747\n",
      "    mean_inference_ms: 1.88629938101821\n",
      "    mean_raw_obs_processing_ms: 21.69477567104879\n",
      "  time_since_restore: 10853.451545476913\n",
      "  time_this_iter_s: 303.91134905815125\n",
      "  time_total_s: 10853.451545476913\n",
      "  timers:\n",
      "    learn_throughput: 251.468\n",
      "    learn_time_ms: 79533.03\n",
      "    sample_throughput: 90.806\n",
      "    sample_time_ms: 220249.051\n",
      "    update_time_ms: 2.889\n",
      "  timestamp: 1659374444\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 720000\n",
      "  training_iteration: 36\n",
      "  trial_id: fd1ac_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 17.7/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/4.91 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_4_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_0_28b5456e04c30e30e910ccff5ed5840d, 0.0/6.0 CPU_group_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_5_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_2_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_3_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_1_28b5456e04c30e30e910ccff5ed5840d)\n",
      "Result logdir: /home/michael/ray_results/stabilizing_the_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_fd1ac_00000 | RUNNING  | 192.168.0.33:25804 |     36 |          10853.5 | 720000 |  1430.81 |              2571.42 |              10.9417 |               4000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m ring length: 253\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m v_max: 4.807278529691987\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m ring length: 227\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m v_max: 3.570869368805752\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m ring length: 231\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m v_max: 3.761192925277308\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m ring length: 230\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m v_max: 3.7136148111012934\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m ring length: 251\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m v_max: 4.71224180704279\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_fd1ac_00000:\n",
      "  agent_timesteps_total: 740000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-01_12-25-49\n",
      "  done: false\n",
      "  episode_len_mean: 4000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2571.4192655182173\n",
      "  episode_reward_mean: 1441.1746603823779\n",
      "  episode_reward_min: 10.941691724137794\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 185\n",
      "  experiment_id: 2b7c5715c95c4828907cbc45e49df6f0\n",
      "  hostname: michael-home\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.9802322387695314e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -1.2556186665395264\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.007267833387197064\n",
      "          policy_loss: 9.5930431856756e-05\n",
      "          total_loss: 79.17366355604427\n",
      "          vf_explained_var: -2.6954967147219122e-09\n",
      "          vf_loss: 79.17356766621778\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 740000\n",
      "    num_steps_sampled: 740000\n",
      "    num_steps_trained: 740000\n",
      "  iterations_since_restore: 37\n",
      "  node_ip: 192.168.0.33\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 31.11490825688073\n",
      "    ram_util_percent: 57.33256880733945\n",
      "  pid: 25804\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0944412844190961\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 32.050164497554356\n",
      "    mean_inference_ms: 1.887119536809684\n",
      "    mean_raw_obs_processing_ms: 21.69663082604923\n",
      "  time_since_restore: 11158.61555480957\n",
      "  time_this_iter_s: 305.16400933265686\n",
      "  time_total_s: 11158.61555480957\n",
      "  timers:\n",
      "    learn_throughput: 249.288\n",
      "    learn_time_ms: 80228.422\n",
      "    sample_throughput: 90.799\n",
      "    sample_time_ms: 220265.76\n",
      "    update_time_ms: 2.932\n",
      "  timestamp: 1659374749\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 740000\n",
      "  training_iteration: 37\n",
      "  trial_id: fd1ac_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 18.0/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/4.91 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_4_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_0_28b5456e04c30e30e910ccff5ed5840d, 0.0/6.0 CPU_group_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_5_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_1_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_2_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_3_28b5456e04c30e30e910ccff5ed5840d)\n",
      "Result logdir: /home/michael/ray_results/stabilizing_the_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_fd1ac_00000 | RUNNING  | 192.168.0.33:25804 |     37 |          11158.6 | 740000 |  1441.17 |              2571.42 |              10.9417 |               4000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m ring length: 239\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m v_max: 4.141741239205832\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m ring length: 257\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m v_max: 4.997304682316517\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m ring length: 235\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m v_max: 3.9514850725282584\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m ring length: 258\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m v_max: 5.044800727535787\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m ring length: 252\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m v_max: 4.759762049824181\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_fd1ac_00000:\n",
      "  agent_timesteps_total: 760000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-01_12-30-55\n",
      "  done: false\n",
      "  episode_len_mean: 4000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2571.4192655182173\n",
      "  episode_reward_mean: 1509.6936977351031\n",
      "  episode_reward_min: 412.99706745729503\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 190\n",
      "  experiment_id: 2b7c5715c95c4828907cbc45e49df6f0\n",
      "  hostname: michael-home\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.4901161193847657e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -1.3451008031322698\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009518584058394656\n",
      "          policy_loss: -7.538959357270579e-05\n",
      "          total_loss: 81.48058628884091\n",
      "          vf_explained_var: -1.7843428956609841e-09\n",
      "          vf_loss: 81.48066154164114\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 760000\n",
      "    num_steps_sampled: 760000\n",
      "    num_steps_trained: 760000\n",
      "  iterations_since_restore: 38\n",
      "  node_ip: 192.168.0.33\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 31.582339449541287\n",
      "    ram_util_percent: 57.79105504587155\n",
      "  pid: 25804\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09448776650283987\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 32.01774616516074\n",
      "    mean_inference_ms: 1.8884310059738862\n",
      "    mean_raw_obs_processing_ms: 21.698563494908047\n",
      "  time_since_restore: 11464.111005067825\n",
      "  time_this_iter_s: 305.495450258255\n",
      "  time_total_s: 11464.111005067825\n",
      "  timers:\n",
      "    learn_throughput: 247.357\n",
      "    learn_time_ms: 80854.755\n",
      "    sample_throughput: 90.828\n",
      "    sample_time_ms: 220196.862\n",
      "    update_time_ms: 2.952\n",
      "  timestamp: 1659375055\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 760000\n",
      "  training_iteration: 38\n",
      "  trial_id: fd1ac_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 18.0/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/4.91 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_0_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_4_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_5_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_3_28b5456e04c30e30e910ccff5ed5840d, 0.0/6.0 CPU_group_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_2_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_1_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 accelerator_type:G)\n",
      "Result logdir: /home/michael/ray_results/stabilizing_the_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_fd1ac_00000 | RUNNING  | 192.168.0.33:25804 |     38 |          11464.1 | 760000 |  1509.69 |              2571.42 |              412.997 |               4000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m ring length: 268\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m v_max: 5.5194978538368025\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m ring length: 241\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m v_max: 4.236854288051661\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m ring length: 259\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m v_max: 5.092292348291853\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m ring length: 248\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m v_max: 4.569659527528101\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m ring length: 234\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m v_max: 3.903915223349057\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_fd1ac_00000:\n",
      "  agent_timesteps_total: 780000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-01_12-36-01\n",
      "  done: false\n",
      "  episode_len_mean: 4000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2646.839373047762\n",
      "  episode_reward_mean: 1570.0326542879623\n",
      "  episode_reward_min: 412.99706745729503\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 195\n",
      "  experiment_id: 2b7c5715c95c4828907cbc45e49df6f0\n",
      "  hostname: michael-home\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 7.450580596923829e-10\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -1.3825523323314206\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012008490494945885\n",
      "          policy_loss: -0.0006246052835148517\n",
      "          total_loss: 97.97864757707924\n",
      "          vf_explained_var: -2.9992146544088882e-09\n",
      "          vf_loss: 97.97927234430982\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 780000\n",
      "    num_steps_sampled: 780000\n",
      "    num_steps_trained: 780000\n",
      "  iterations_since_restore: 39\n",
      "  node_ip: 192.168.0.33\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 32.84220183486239\n",
      "    ram_util_percent: 60.31192660550459\n",
      "  pid: 25804\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0945473136419774\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.987504526941706\n",
      "    mean_inference_ms: 1.8900534807686278\n",
      "    mean_raw_obs_processing_ms: 21.700529942902403\n",
      "  time_since_restore: 11770.007805585861\n",
      "  time_this_iter_s: 305.8968005180359\n",
      "  time_total_s: 11770.007805585861\n",
      "  timers:\n",
      "    learn_throughput: 245.304\n",
      "    learn_time_ms: 81531.383\n",
      "    sample_throughput: 90.814\n",
      "    sample_time_ms: 220230.856\n",
      "    update_time_ms: 2.943\n",
      "  timestamp: 1659375361\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 780000\n",
      "  training_iteration: 39\n",
      "  trial_id: fd1ac_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 17.9/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/4.91 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_0_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_4_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_3_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_1_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_2_28b5456e04c30e30e910ccff5ed5840d, 0.0/6.0 CPU_group_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_5_28b5456e04c30e30e910ccff5ed5840d)\n",
      "Result logdir: /home/michael/ray_results/stabilizing_the_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_fd1ac_00000 | RUNNING  | 192.168.0.33:25804 |     39 |            11770 | 780000 |  1570.03 |              2646.84 |              412.997 |               4000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m ring length: 246\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m v_max: 4.474587708995648\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m ring length: 242\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m v_max: 4.2844067680020546\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m ring length: 264\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m v_max: 5.329679917416892\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m ring length: 270\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m v_max: 5.6143732387852054\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m ring length: 254\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m v_max: 4.854791141191876\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_fd1ac_00000:\n",
      "  agent_timesteps_total: 800000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-01_12-41-05\n",
      "  done: false\n",
      "  episode_len_mean: 4000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2646.839373047762\n",
      "  episode_reward_mean: 1600.5246720125622\n",
      "  episode_reward_min: 412.99706745729503\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 200\n",
      "  experiment_id: 2b7c5715c95c4828907cbc45e49df6f0\n",
      "  hostname: michael-home\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 7.450580596923829e-10\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -1.3568278149434716\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.006863749948611025\n",
      "          policy_loss: 0.0010194774709677526\n",
      "          total_loss: 112.40863954459026\n",
      "          vf_explained_var: -5.201169717139464e-09\n",
      "          vf_loss: 112.40761977760656\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 800000\n",
      "    num_steps_sampled: 800000\n",
      "    num_steps_trained: 800000\n",
      "  iterations_since_restore: 40\n",
      "  node_ip: 192.168.0.33\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 31.275402298850572\n",
      "    ram_util_percent: 58.048505747126434\n",
      "  pid: 25804\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09462433319129243\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.95891489634257\n",
      "    mean_inference_ms: 1.8921342427050194\n",
      "    mean_raw_obs_processing_ms: 21.69877718547177\n",
      "  time_since_restore: 12074.589230060577\n",
      "  time_this_iter_s: 304.5814244747162\n",
      "  time_total_s: 12074.589230060577\n",
      "  timers:\n",
      "    learn_throughput: 243.464\n",
      "    learn_time_ms: 82147.683\n",
      "    sample_throughput: 90.807\n",
      "    sample_time_ms: 220246.468\n",
      "    update_time_ms: 2.944\n",
      "  timestamp: 1659375665\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 800000\n",
      "  training_iteration: 40\n",
      "  trial_id: fd1ac_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 18.3/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/4.91 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_4_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_2_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_1_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_0_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_5_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_3_28b5456e04c30e30e910ccff5ed5840d, 0.0/6.0 CPU_group_28b5456e04c30e30e910ccff5ed5840d)\n",
      "Result logdir: /home/michael/ray_results/stabilizing_the_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_fd1ac_00000 | RUNNING  | 192.168.0.33:25804 |     40 |          12074.6 | 800000 |  1600.52 |              2646.84 |              412.997 |               4000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m ring length: 235\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m v_max: 3.9514850725282584\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m ring length: 253\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m v_max: 4.807278529691987\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m ring length: 257\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m v_max: 4.997304682316517\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m ring length: 229\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m v_max: 3.666034803266416\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m ring length: 242\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m v_max: 4.2844067680020546\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_fd1ac_00000:\n",
      "  agent_timesteps_total: 820000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-01_12-46-09\n",
      "  done: false\n",
      "  episode_len_mean: 4000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3007.723107023102\n",
      "  episode_reward_mean: 1643.6704712930507\n",
      "  episode_reward_min: 412.99706745729503\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 205\n",
      "  experiment_id: 2b7c5715c95c4828907cbc45e49df6f0\n",
      "  hostname: michael-home\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 3.7252902984619143e-10\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -1.3882885067326247\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.007359262457358508\n",
      "          policy_loss: 0.0006698526872120276\n",
      "          total_loss: 127.08698348634562\n",
      "          vf_explained_var: -2.353814032574064e-09\n",
      "          vf_loss: 127.08631372634012\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 820000\n",
      "    num_steps_sampled: 820000\n",
      "    num_steps_trained: 820000\n",
      "  iterations_since_restore: 41\n",
      "  node_ip: 192.168.0.33\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 30.1337182448037\n",
      "    ram_util_percent: 58.12933025404157\n",
      "  pid: 25804\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09471460271651366\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.93185208333682\n",
      "    mean_inference_ms: 1.8945912816338557\n",
      "    mean_raw_obs_processing_ms: 21.697213606408937\n",
      "  time_since_restore: 12378.154554128647\n",
      "  time_this_iter_s: 303.56532406806946\n",
      "  time_total_s: 12378.154554128647\n",
      "  timers:\n",
      "    learn_throughput: 242.066\n",
      "    learn_time_ms: 82621.998\n",
      "    sample_throughput: 90.794\n",
      "    sample_time_ms: 220278.795\n",
      "    update_time_ms: 2.928\n",
      "  timestamp: 1659375969\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 820000\n",
      "  training_iteration: 41\n",
      "  trial_id: fd1ac_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 18.0/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/4.91 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_0_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_1_28b5456e04c30e30e910ccff5ed5840d, 0.0/6.0 CPU_group_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_3_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_4_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_5_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_2_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 accelerator_type:G)\n",
      "Result logdir: /home/michael/ray_results/stabilizing_the_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_fd1ac_00000 | RUNNING  | 192.168.0.33:25804 |     41 |          12378.2 | 820000 |  1643.67 |              3007.72 |              412.997 |               4000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m ring length: 220\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m v_max: 3.2377399006821497\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m ring length: 237\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m v_max: 4.04661795519353\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m ring length: 233\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m v_max: 3.856343201216534\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m ring length: 244\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m v_max: 4.379503211387676\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m ring length: 234\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m v_max: 3.903915223349057\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_fd1ac_00000:\n",
      "  agent_timesteps_total: 840000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-01_12-51-11\n",
      "  done: false\n",
      "  episode_len_mean: 4000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3007.723107023102\n",
      "  episode_reward_mean: 1671.433717473224\n",
      "  episode_reward_min: 412.99706745729503\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 210\n",
      "  experiment_id: 2b7c5715c95c4828907cbc45e49df6f0\n",
      "  hostname: michael-home\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.8626451492309571e-10\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -1.3802636883061403\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008198950351443947\n",
      "          policy_loss: 0.00022021551945715384\n",
      "          total_loss: 127.56012406197323\n",
      "          vf_explained_var: -9.794903554904977e-09\n",
      "          vf_loss: 127.55990383609844\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 840000\n",
      "    num_steps_sampled: 840000\n",
      "    num_steps_trained: 840000\n",
      "  iterations_since_restore: 42\n",
      "  node_ip: 192.168.0.33\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 28.194444444444443\n",
      "    ram_util_percent: 57.99351851851853\n",
      "  pid: 25804\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0948094934649179\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.90643448172641\n",
      "    mean_inference_ms: 1.8971345430923239\n",
      "    mean_raw_obs_processing_ms: 21.695941530002525\n",
      "  time_since_restore: 12680.783296346664\n",
      "  time_this_iter_s: 302.6287422180176\n",
      "  time_total_s: 12680.783296346664\n",
      "  timers:\n",
      "    learn_throughput: 241.082\n",
      "    learn_time_ms: 82959.189\n",
      "    sample_throughput: 90.768\n",
      "    sample_time_ms: 220341.917\n",
      "    update_time_ms: 2.925\n",
      "  timestamp: 1659376271\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 840000\n",
      "  training_iteration: 42\n",
      "  trial_id: fd1ac_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 18.2/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/4.91 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_2_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_4_28b5456e04c30e30e910ccff5ed5840d, 0.0/6.0 CPU_group_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_3_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_5_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_0_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_1_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 accelerator_type:G)\n",
      "Result logdir: /home/michael/ray_results/stabilizing_the_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_fd1ac_00000 | RUNNING  | 192.168.0.33:25804 |     42 |          12680.8 | 840000 |  1671.43 |              3007.72 |              412.997 |               4000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m ring length: 258\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m v_max: 5.044800727535787\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m ring length: 234\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m v_max: 3.903915223349057\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m ring length: 252\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m v_max: 4.759762049824181\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m ring length: 242\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m v_max: 4.2844067680020546\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m ring length: 223\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m v_max: 3.3805185755587788\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_fd1ac_00000:\n",
      "  agent_timesteps_total: 860000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-01_12-56-15\n",
      "  done: false\n",
      "  episode_len_mean: 4000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3007.723107023102\n",
      "  episode_reward_mean: 1688.18267393598\n",
      "  episode_reward_min: 412.99706745729503\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 215\n",
      "  experiment_id: 2b7c5715c95c4828907cbc45e49df6f0\n",
      "  hostname: michael-home\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 9.313225746154786e-11\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -1.4265402370197757\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011123965757529865\n",
      "          policy_loss: -0.00011590340821321603\n",
      "          total_loss: 125.25674962572232\n",
      "          vf_explained_var: 8.352243341391841e-10\n",
      "          vf_loss: 125.25686587801405\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 860000\n",
      "    num_steps_sampled: 860000\n",
      "    num_steps_trained: 860000\n",
      "  iterations_since_restore: 43\n",
      "  node_ip: 192.168.0.33\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 28.17205542725173\n",
      "    ram_util_percent: 58.43856812933026\n",
      "  pid: 25804\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09491068658440961\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.882352569173506\n",
      "    mean_inference_ms: 1.8998505628681475\n",
      "    mean_raw_obs_processing_ms: 21.694838725941658\n",
      "  time_since_restore: 12984.50550198555\n",
      "  time_this_iter_s: 303.7222056388855\n",
      "  time_total_s: 12984.50550198555\n",
      "  timers:\n",
      "    learn_throughput: 239.587\n",
      "    learn_time_ms: 83477.083\n",
      "    sample_throughput: 90.764\n",
      "    sample_time_ms: 220350.657\n",
      "    update_time_ms: 2.967\n",
      "  timestamp: 1659376575\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 860000\n",
      "  training_iteration: 43\n",
      "  trial_id: fd1ac_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 18.4/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/4.91 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_1_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_3_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_5_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_4_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_2_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_0_28b5456e04c30e30e910ccff5ed5840d, 0.0/6.0 CPU_group_28b5456e04c30e30e910ccff5ed5840d)\n",
      "Result logdir: /home/michael/ray_results/stabilizing_the_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_fd1ac_00000 | RUNNING  | 192.168.0.33:25804 |     43 |          12984.5 | 860000 |  1688.18 |              3007.72 |              412.997 |               4000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m ring length: 251\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m v_max: 4.71224180704279\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m ring length: 234\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m v_max: 3.903915223349057\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m ring length: 229\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m v_max: 3.666034803266416\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m ring length: 253\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m v_max: 4.807278529691987\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m ring length: 248\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m v_max: 4.569659527528101\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_fd1ac_00000:\n",
      "  agent_timesteps_total: 880000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-01_13-01-19\n",
      "  done: false\n",
      "  episode_len_mean: 4000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3007.723107023102\n",
      "  episode_reward_mean: 1711.7438293827463\n",
      "  episode_reward_min: 412.99706745729503\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 220\n",
      "  experiment_id: 2b7c5715c95c4828907cbc45e49df6f0\n",
      "  hostname: michael-home\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 9.313225746154786e-11\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -1.4570772849830094\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008187414484021993\n",
      "          policy_loss: 0.0003948397314543747\n",
      "          total_loss: 147.6096812302899\n",
      "          vf_explained_var: -3.2270031091741203e-09\n",
      "          vf_loss: 147.6092862341814\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 880000\n",
      "    num_steps_sampled: 880000\n",
      "    num_steps_trained: 880000\n",
      "  iterations_since_restore: 44\n",
      "  node_ip: 192.168.0.33\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 29.458429561200923\n",
      "    ram_util_percent: 59.70900692840647\n",
      "  pid: 25804\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09501881687332141\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.859437698931284\n",
      "    mean_inference_ms: 1.9027763916202505\n",
      "    mean_raw_obs_processing_ms: 21.693941993114223\n",
      "  time_since_restore: 13287.890251398087\n",
      "  time_this_iter_s: 303.3847494125366\n",
      "  time_total_s: 13287.890251398087\n",
      "  timers:\n",
      "    learn_throughput: 239.103\n",
      "    learn_time_ms: 83645.973\n",
      "    sample_throughput: 90.785\n",
      "    sample_time_ms: 220300.194\n",
      "    update_time_ms: 3.07\n",
      "  timestamp: 1659376879\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 880000\n",
      "  training_iteration: 44\n",
      "  trial_id: fd1ac_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 18.7/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/4.91 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_0_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_1_28b5456e04c30e30e910ccff5ed5840d, 0.0/6.0 CPU_group_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_2_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_4_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_3_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_5_28b5456e04c30e30e910ccff5ed5840d)\n",
      "Result logdir: /home/michael/ray_results/stabilizing_the_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_fd1ac_00000 | RUNNING  | 192.168.0.33:25804 |     44 |          13287.9 | 880000 |  1711.74 |              3007.72 |              412.997 |               4000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m ring length: 244\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m v_max: 4.379503211387676\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m ring length: 250\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m v_max: 4.664717904914125\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m ring length: 245\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m v_max: 4.427046998576354\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m ring length: 223\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m v_max: 3.3805185755587788\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m ring length: 238\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m v_max: 4.094180836186086\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_fd1ac_00000:\n",
      "  agent_timesteps_total: 900000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-01_13-06-25\n",
      "  done: false\n",
      "  episode_len_mean: 4000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3007.723107023102\n",
      "  episode_reward_mean: 1733.3294190137149\n",
      "  episode_reward_min: 812.5148252959195\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 225\n",
      "  experiment_id: 2b7c5715c95c4828907cbc45e49df6f0\n",
      "  hostname: michael-home\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 4.656612873077393e-11\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -1.4505733391281905\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008776932599332226\n",
      "          policy_loss: -0.0003259813054138498\n",
      "          total_loss: 150.5094897883713\n",
      "          vf_explained_var: -1.5945191833566241e-09\n",
      "          vf_loss: 150.509815949847\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 900000\n",
      "    num_steps_sampled: 900000\n",
      "    num_steps_trained: 900000\n",
      "  iterations_since_restore: 45\n",
      "  node_ip: 192.168.0.33\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 32.22745995423341\n",
      "    ram_util_percent: 60.7858123569794\n",
      "  pid: 25804\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0951407371863128\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.837283830353787\n",
      "    mean_inference_ms: 1.9061505960150233\n",
      "    mean_raw_obs_processing_ms: 21.69322213665494\n",
      "  time_since_restore: 13594.161202907562\n",
      "  time_this_iter_s: 306.2709515094757\n",
      "  time_total_s: 13594.161202907562\n",
      "  timers:\n",
      "    learn_throughput: 237.758\n",
      "    learn_time_ms: 84119.081\n",
      "    sample_throughput: 90.774\n",
      "    sample_time_ms: 220326.802\n",
      "    update_time_ms: 3.138\n",
      "  timestamp: 1659377185\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 900000\n",
      "  training_iteration: 45\n",
      "  trial_id: fd1ac_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 19.1/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/4.91 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_5_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_4_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_2_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_1_28b5456e04c30e30e910ccff5ed5840d, 0.0/6.0 CPU_group_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_0_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_3_28b5456e04c30e30e910ccff5ed5840d)\n",
      "Result logdir: /home/michael/ray_results/stabilizing_the_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_fd1ac_00000 | RUNNING  | 192.168.0.33:25804 |     45 |          13594.2 | 900000 |  1733.33 |              3007.72 |              812.515 |               4000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m ring length: 251\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m v_max: 4.71224180704279\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m ring length: 258\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m v_max: 5.044800727535787\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m ring length: 228\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m v_max: 3.618452967700356\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m ring length: 255\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m v_max: 4.902299776966978\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m ring length: 265\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m v_max: 5.37714246265477\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_fd1ac_00000:\n",
      "  agent_timesteps_total: 920000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-01_13-11-29\n",
      "  done: false\n",
      "  episode_len_mean: 4000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3007.723107023102\n",
      "  episode_reward_mean: 1700.5689421842849\n",
      "  episode_reward_min: 192.13283790281903\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 230\n",
      "  experiment_id: 2b7c5715c95c4828907cbc45e49df6f0\n",
      "  hostname: michael-home\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.3283064365386964e-11\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -1.484934573188709\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.00989496037296346\n",
      "          policy_loss: 0.0003694776200932587\n",
      "          total_loss: 139.53093977399692\n",
      "          vf_explained_var: -1.8982371230436e-09\n",
      "          vf_loss: 139.53056988321293\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 920000\n",
      "    num_steps_sampled: 920000\n",
      "    num_steps_trained: 920000\n",
      "  iterations_since_restore: 46\n",
      "  node_ip: 192.168.0.33\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 32.36774193548387\n",
      "    ram_util_percent: 61.39539170506912\n",
      "  pid: 25804\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09527432039052833\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.815926569770735\n",
      "    mean_inference_ms: 1.909843098780059\n",
      "    mean_raw_obs_processing_ms: 21.692656508969957\n",
      "  time_since_restore: 13898.433292627335\n",
      "  time_this_iter_s: 304.27208971977234\n",
      "  time_total_s: 13898.433292627335\n",
      "  timers:\n",
      "    learn_throughput: 237.348\n",
      "    learn_time_ms: 84264.523\n",
      "    sample_throughput: 90.819\n",
      "    sample_time_ms: 220217.448\n",
      "    update_time_ms: 3.131\n",
      "  timestamp: 1659377489\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 920000\n",
      "  training_iteration: 46\n",
      "  trial_id: fd1ac_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 19.1/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/4.91 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_3_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_5_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_0_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_4_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_1_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_2_28b5456e04c30e30e910ccff5ed5840d, 0.0/6.0 CPU_group_28b5456e04c30e30e910ccff5ed5840d)\n",
      "Result logdir: /home/michael/ray_results/stabilizing_the_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_fd1ac_00000 | RUNNING  | 192.168.0.33:25804 |     46 |          13898.4 | 920000 |  1700.57 |              3007.72 |              192.133 |               4000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m ring length: 229\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m v_max: 3.666034803266416\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m ring length: 242\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m v_max: 4.2844067680020546\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m ring length: 222\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m v_max: 3.3329270738617227\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m ring length: 221\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m v_max: 3.285334164169417\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m ring length: 265\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m v_max: 5.37714246265477\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_fd1ac_00000:\n",
      "  agent_timesteps_total: 940000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-01_13-16-35\n",
      "  done: false\n",
      "  episode_len_mean: 4000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3007.723107023102\n",
      "  episode_reward_mean: 1644.5818812093903\n",
      "  episode_reward_min: 172.91604585868458\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 235\n",
      "  experiment_id: 2b7c5715c95c4828907cbc45e49df6f0\n",
      "  hostname: michael-home\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.1641532182693482e-11\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -1.5540228762444417\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013744511802022375\n",
      "          policy_loss: 0.00029657011136863454\n",
      "          total_loss: 128.36856385795934\n",
      "          vf_explained_var: -4.441874867922024e-09\n",
      "          vf_loss: 128.36826734239128\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 940000\n",
      "    num_steps_sampled: 940000\n",
      "    num_steps_trained: 940000\n",
      "  iterations_since_restore: 47\n",
      "  node_ip: 192.168.0.33\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 31.101379310344825\n",
      "    ram_util_percent: 62.4151724137931\n",
      "  pid: 25804\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09541628971205304\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.795405800829702\n",
      "    mean_inference_ms: 1.9137356726441916\n",
      "    mean_raw_obs_processing_ms: 21.692187292845734\n",
      "  time_since_restore: 14203.7828104496\n",
      "  time_this_iter_s: 305.3495178222656\n",
      "  time_total_s: 14203.7828104496\n",
      "  timers:\n",
      "    learn_throughput: 237.327\n",
      "    learn_time_ms: 84271.893\n",
      "    sample_throughput: 90.815\n",
      "    sample_time_ms: 220228.701\n",
      "    update_time_ms: 3.123\n",
      "  timestamp: 1659377795\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 940000\n",
      "  training_iteration: 47\n",
      "  trial_id: fd1ac_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 19.5/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/4.91 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_4_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 accelerator_type:G, 0.0/6.0 CPU_group_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_0_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_1_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_2_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_3_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_5_28b5456e04c30e30e910ccff5ed5840d)\n",
      "Result logdir: /home/michael/ray_results/stabilizing_the_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_fd1ac_00000 | RUNNING  | 192.168.0.33:25804 |     47 |          14203.8 | 940000 |  1644.58 |              3007.72 |              172.916 |               4000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m ring length: 248\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m v_max: 4.569659527528101\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m ring length: 257\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m v_max: 4.997304682316517\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m ring length: 263\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m v_max: 5.282212215151455\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m ring length: 252\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m v_max: 4.759762049824181\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m ring length: 234\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m v_max: 3.903915223349057\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_fd1ac_00000:\n",
      "  agent_timesteps_total: 960000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-01_13-21-38\n",
      "  done: false\n",
      "  episode_len_mean: 4000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3007.723107023102\n",
      "  episode_reward_mean: 1565.568442980944\n",
      "  episode_reward_min: -422.1655580086222\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 240\n",
      "  experiment_id: 2b7c5715c95c4828907cbc45e49df6f0\n",
      "  hostname: michael-home\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.1641532182693482e-11\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -1.581053359569258\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008406975034496096\n",
      "          policy_loss: 0.0009275388844595972\n",
      "          total_loss: 130.0189432867014\n",
      "          vf_explained_var: -1.3667307285913921e-09\n",
      "          vf_loss: 130.01801653272787\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 960000\n",
      "    num_steps_sampled: 960000\n",
      "    num_steps_trained: 960000\n",
      "  iterations_since_restore: 48\n",
      "  node_ip: 192.168.0.33\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 28.631870669745954\n",
      "    ram_util_percent: 62.73764434180138\n",
      "  pid: 25804\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09556556119701116\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.77568110635476\n",
      "    mean_inference_ms: 1.917760069416409\n",
      "    mean_raw_obs_processing_ms: 21.691391174767883\n",
      "  time_since_restore: 14507.165568590164\n",
      "  time_this_iter_s: 303.38275814056396\n",
      "  time_total_s: 14507.165568590164\n",
      "  timers:\n",
      "    learn_throughput: 237.889\n",
      "    learn_time_ms: 84072.785\n",
      "    sample_throughput: 90.82\n",
      "    sample_time_ms: 220216.535\n",
      "    update_time_ms: 3.132\n",
      "  timestamp: 1659378098\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 960000\n",
      "  training_iteration: 48\n",
      "  trial_id: fd1ac_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 19.6/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/4.91 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_2_28b5456e04c30e30e910ccff5ed5840d, 0.0/6.0 CPU_group_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_4_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_3_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_1_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_5_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_0_28b5456e04c30e30e910ccff5ed5840d)\n",
      "Result logdir: /home/michael/ray_results/stabilizing_the_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_fd1ac_00000 | RUNNING  | 192.168.0.33:25804 |     48 |          14507.2 | 960000 |  1565.57 |              3007.72 |             -422.166 |               4000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m ring length: 222\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m v_max: 3.3329270738617227\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m ring length: 244\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m v_max: 4.379503211387676\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m ring length: 229\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m v_max: 3.666034803266416\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m ring length: 246\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m v_max: 4.474587708995648\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m ring length: 228\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m v_max: 3.618452967700356\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_fd1ac_00000:\n",
      "  agent_timesteps_total: 980000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-01_13-26-42\n",
      "  done: false\n",
      "  episode_len_mean: 4000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3007.723107023102\n",
      "  episode_reward_mean: 1464.605000296265\n",
      "  episode_reward_min: -422.1655580086222\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 245\n",
      "  experiment_id: 2b7c5715c95c4828907cbc45e49df6f0\n",
      "  hostname: michael-home\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 5.820766091346741e-12\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -1.6445465034739986\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011302098145102076\n",
      "          policy_loss: 0.0009573541858951283\n",
      "          total_loss: 126.73318345744138\n",
      "          vf_explained_var: 1.13894227382616e-09\n",
      "          vf_loss: 126.73222640578155\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 980000\n",
      "    num_steps_sampled: 980000\n",
      "    num_steps_trained: 980000\n",
      "  iterations_since_restore: 49\n",
      "  node_ip: 192.168.0.33\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 30.36520737327189\n",
      "    ram_util_percent: 63.76359447004609\n",
      "  pid: 25804\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09572389696031167\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.75656262034715\n",
      "    mean_inference_ms: 1.9220274892248863\n",
      "    mean_raw_obs_processing_ms: 21.690708108529176\n",
      "  time_since_restore: 14810.966505765915\n",
      "  time_this_iter_s: 303.80093717575073\n",
      "  time_total_s: 14810.966505765915\n",
      "  timers:\n",
      "    learn_throughput: 238.423\n",
      "    learn_time_ms: 83884.42\n",
      "    sample_throughput: 90.828\n",
      "    sample_time_ms: 220195.309\n",
      "    update_time_ms: 3.153\n",
      "  timestamp: 1659378402\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 980000\n",
      "  training_iteration: 49\n",
      "  trial_id: fd1ac_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 19.7/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/4.91 GiB heap, 0.0/14.65 GiB objects (0.0/6.0 CPU_group_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_3_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_0_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_4_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_5_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_1_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_2_28b5456e04c30e30e910ccff5ed5840d)\n",
      "Result logdir: /home/michael/ray_results/stabilizing_the_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_fd1ac_00000 | RUNNING  | 192.168.0.33:25804 |     49 |            14811 | 980000 |  1464.61 |              3007.72 |             -422.166 |               4000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m ring length: 228\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m v_max: 3.618452967700356\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m ring length: 241\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m v_max: 4.236854288051661\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m ring length: 241\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m v_max: 4.236854288051661\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m ring length: 259\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m v_max: 5.092292348291853\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m ring length: 261\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m v_max: 5.187261846097525\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_fd1ac_00000:\n",
      "  agent_timesteps_total: 1000000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-01_13-31-46\n",
      "  done: false\n",
      "  episode_len_mean: 4000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3007.723107023102\n",
      "  episode_reward_mean: 1397.2085163697336\n",
      "  episode_reward_min: -422.1655580086222\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 250\n",
      "  experiment_id: 2b7c5715c95c4828907cbc45e49df6f0\n",
      "  hostname: michael-home\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 5.820766091346741e-12\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -1.718084756811713\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.007720793424093366\n",
      "          policy_loss: 0.0006676035323684477\n",
      "          total_loss: 136.6454676634187\n",
      "          vf_explained_var: -4.55576909530464e-09\n",
      "          vf_loss: 136.6447997488034\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1000000\n",
      "    num_steps_sampled: 1000000\n",
      "    num_steps_trained: 1000000\n",
      "  iterations_since_restore: 50\n",
      "  node_ip: 192.168.0.33\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 29.300692840646647\n",
      "    ram_util_percent: 62.75542725173211\n",
      "  pid: 25804\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09588894689360003\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.738095202601176\n",
      "    mean_inference_ms: 1.9264524336860576\n",
      "    mean_raw_obs_processing_ms: 21.690176146108733\n",
      "  time_since_restore: 15114.880197763443\n",
      "  time_this_iter_s: 303.9136919975281\n",
      "  time_total_s: 15114.880197763443\n",
      "  timers:\n",
      "    learn_throughput: 238.715\n",
      "    learn_time_ms: 83781.963\n",
      "    sample_throughput: 90.814\n",
      "    sample_time_ms: 220231.014\n",
      "    update_time_ms: 3.146\n",
      "  timestamp: 1659378706\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1000000\n",
      "  training_iteration: 50\n",
      "  trial_id: fd1ac_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 19.6/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/4.91 GiB heap, 0.0/14.65 GiB objects (0.0/6.0 CPU_group_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_1_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_4_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_0_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_2_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_5_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_3_28b5456e04c30e30e910ccff5ed5840d)\n",
      "Result logdir: /home/michael/ray_results/stabilizing_the_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_fd1ac_00000 | RUNNING  | 192.168.0.33:25804 |     50 |          15114.9 | 1000000 |  1397.21 |              3007.72 |             -422.166 |               4000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m ring length: 222\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m v_max: 3.3329270738617227\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m ring length: 223\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m v_max: 3.3805185755587788\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m ring length: 229\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m v_max: 3.666034803266416\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m ring length: 240\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m v_max: 4.189299083856172\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m ring length: 261\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m v_max: 5.187261846097525\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_fd1ac_00000:\n",
      "  agent_timesteps_total: 1020000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-01_13-36-49\n",
      "  done: false\n",
      "  episode_len_mean: 4000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3007.723107023102\n",
      "  episode_reward_mean: 1369.310461626791\n",
      "  episode_reward_min: -422.1655580086222\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 255\n",
      "  experiment_id: 2b7c5715c95c4828907cbc45e49df6f0\n",
      "  hostname: michael-home\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.9103830456733705e-12\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -1.68825225700998\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.007432608786528553\n",
      "          policy_loss: 0.0006481636828701398\n",
      "          total_loss: 139.9079043831795\n",
      "          vf_explained_var: -1.3287659861305201e-09\n",
      "          vf_loss: 139.9072556198023\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1020000\n",
      "    num_steps_sampled: 1020000\n",
      "    num_steps_trained: 1020000\n",
      "  iterations_since_restore: 51\n",
      "  node_ip: 192.168.0.33\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 28.23995381062355\n",
      "    ram_util_percent: 62.584757505773666\n",
      "  pid: 25804\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09605979486735769\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.720194081946037\n",
      "    mean_inference_ms: 1.9310298798293213\n",
      "    mean_raw_obs_processing_ms: 21.689742063354352\n",
      "  time_since_restore: 15417.713631391525\n",
      "  time_this_iter_s: 302.8334336280823\n",
      "  time_total_s: 15417.713631391525\n",
      "  timers:\n",
      "    learn_throughput: 238.887\n",
      "    learn_time_ms: 83721.559\n",
      "    sample_throughput: 90.819\n",
      "    sample_time_ms: 220218.315\n",
      "    update_time_ms: 3.13\n",
      "  timestamp: 1659379009\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1020000\n",
      "  training_iteration: 51\n",
      "  trial_id: fd1ac_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 19.5/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/4.91 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_2_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_1_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_3_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_4_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_0_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_5_28b5456e04c30e30e910ccff5ed5840d, 0.0/6.0 CPU_group_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 accelerator_type:G)\n",
      "Result logdir: /home/michael/ray_results/stabilizing_the_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_fd1ac_00000 | RUNNING  | 192.168.0.33:25804 |     51 |          15417.7 | 1020000 |  1369.31 |              3007.72 |             -422.166 |               4000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m ring length: 257\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m v_max: 4.997304682316517\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m ring length: 225\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m v_max: 3.4756971311168834\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m ring length: 233\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m v_max: 3.856343201216534\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m ring length: 259\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m v_max: 5.092292348291853\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m ring length: 269\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m v_max: 5.566938458220921\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_fd1ac_00000:\n",
      "  agent_timesteps_total: 1040000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-01_13-41-49\n",
      "  done: false\n",
      "  episode_len_mean: 4000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3007.723107023102\n",
      "  episode_reward_mean: 1304.976633541051\n",
      "  episode_reward_min: -422.1655580086222\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 260\n",
      "  experiment_id: 2b7c5715c95c4828907cbc45e49df6f0\n",
      "  hostname: michael-home\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.4551915228366853e-12\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -1.6513332512727967\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.006522125811248179\n",
      "          policy_loss: 0.0011624636712845914\n",
      "          total_loss: 117.06332625978312\n",
      "          vf_explained_var: -8.731890766000561e-10\n",
      "          vf_loss: 117.06216349389143\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1040000\n",
      "    num_steps_sampled: 1040000\n",
      "    num_steps_trained: 1040000\n",
      "  iterations_since_restore: 52\n",
      "  node_ip: 192.168.0.33\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 27.978271028037383\n",
      "    ram_util_percent: 61.87219626168225\n",
      "  pid: 25804\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09623630167585814\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.70283506661921\n",
      "    mean_inference_ms: 1.9357617365151598\n",
      "    mean_raw_obs_processing_ms: 21.689405145820032\n",
      "  time_since_restore: 15718.218550682068\n",
      "  time_this_iter_s: 300.5049192905426\n",
      "  time_total_s: 15718.218550682068\n",
      "  timers:\n",
      "    learn_throughput: 239.422\n",
      "    learn_time_ms: 83534.607\n",
      "    sample_throughput: 90.829\n",
      "    sample_time_ms: 220192.949\n",
      "    update_time_ms: 3.131\n",
      "  timestamp: 1659379309\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1040000\n",
      "  training_iteration: 52\n",
      "  trial_id: fd1ac_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 19.2/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/4.91 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_0_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_4_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_1_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_2_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_3_28b5456e04c30e30e910ccff5ed5840d, 0.0/6.0 CPU_group_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_5_28b5456e04c30e30e910ccff5ed5840d)\n",
      "Result logdir: /home/michael/ray_results/stabilizing_the_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_fd1ac_00000 | RUNNING  | 192.168.0.33:25804 |     52 |          15718.2 | 1040000 |  1304.98 |              3007.72 |             -422.166 |               4000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m ring length: 245\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m v_max: 4.427046998576354\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m ring length: 225\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m v_max: 3.4756971311168834\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m ring length: 232\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m v_max: 3.8087690783250063\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m ring length: 265\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m v_max: 5.37714246265477\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m ring length: 237\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m v_max: 4.04661795519353\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_fd1ac_00000:\n",
      "  agent_timesteps_total: 1060000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-01_13-46-51\n",
      "  done: false\n",
      "  episode_len_mean: 4000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3007.723107023102\n",
      "  episode_reward_mean: 1280.7932047868367\n",
      "  episode_reward_min: -422.1655580086222\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 265\n",
      "  experiment_id: 2b7c5715c95c4828907cbc45e49df6f0\n",
      "  hostname: michael-home\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 7.275957614183426e-13\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -1.7562208842320048\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011170294528854357\n",
      "          policy_loss: 0.00045964573509991167\n",
      "          total_loss: 117.79199433539324\n",
      "          vf_explained_var: 7.592948492174401e-10\n",
      "          vf_loss: 117.79153502579707\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1060000\n",
      "    num_steps_sampled: 1060000\n",
      "    num_steps_trained: 1060000\n",
      "  iterations_since_restore: 53\n",
      "  node_ip: 192.168.0.33\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 27.333410672853827\n",
      "    ram_util_percent: 61.69071925754061\n",
      "  pid: 25804\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09641133070952974\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.686114162645282\n",
      "    mean_inference_ms: 1.9404946030884174\n",
      "    mean_raw_obs_processing_ms: 21.6891573731833\n",
      "  time_since_restore: 16020.148415327072\n",
      "  time_this_iter_s: 301.9298646450043\n",
      "  time_total_s: 16020.148415327072\n",
      "  timers:\n",
      "    learn_throughput: 239.968\n",
      "    learn_time_ms: 83344.512\n",
      "    sample_throughput: 90.825\n",
      "    sample_time_ms: 220203.809\n",
      "    update_time_ms: 3.062\n",
      "  timestamp: 1659379611\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1060000\n",
      "  training_iteration: 53\n",
      "  trial_id: fd1ac_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 19.4/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/4.91 GiB heap, 0.0/14.65 GiB objects (0.0/6.0 CPU_group_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_5_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_2_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_0_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_1_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_4_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_3_28b5456e04c30e30e910ccff5ed5840d)\n",
      "Result logdir: /home/michael/ray_results/stabilizing_the_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_fd1ac_00000 | RUNNING  | 192.168.0.33:25804 |     53 |          16020.1 | 1060000 |  1280.79 |              3007.72 |             -422.166 |               4000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m ring length: 249\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m v_max: 4.617190445131122\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m ring length: 253\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m v_max: 4.807278529691987\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m ring length: 242\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m v_max: 4.2844067680020546\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m ring length: 269\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m v_max: 5.566938458220921\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m ring length: 231\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m v_max: 3.761192925277308\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_fd1ac_00000:\n",
      "  agent_timesteps_total: 1080000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-01_13-51-54\n",
      "  done: false\n",
      "  episode_len_mean: 4000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3007.723107023102\n",
      "  episode_reward_mean: 1262.2767826574284\n",
      "  episode_reward_min: -422.1655580086222\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 270\n",
      "  experiment_id: 2b7c5715c95c4828907cbc45e49df6f0\n",
      "  hostname: michael-home\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 7.275957614183426e-13\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -1.7779027099062683\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0074827400736131285\n",
      "          policy_loss: 0.0016642986832388267\n",
      "          total_loss: 147.1451950680678\n",
      "          vf_explained_var: -7.592948492174401e-10\n",
      "          vf_loss: 147.1435312186077\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1080000\n",
      "    num_steps_sampled: 1080000\n",
      "    num_steps_trained: 1080000\n",
      "  iterations_since_restore: 54\n",
      "  node_ip: 192.168.0.33\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 27.785416666666666\n",
      "    ram_util_percent: 62.49791666666666\n",
      "  pid: 25804\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09658422424438254\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.669930941485124\n",
      "    mean_inference_ms: 1.945156736942653\n",
      "    mean_raw_obs_processing_ms: 21.68878757788336\n",
      "  time_since_restore: 16322.70269036293\n",
      "  time_this_iter_s: 302.55427503585815\n",
      "  time_total_s: 16322.70269036293\n",
      "  timers:\n",
      "    learn_throughput: 240.123\n",
      "    learn_time_ms: 83290.589\n",
      "    sample_throughput: 90.837\n",
      "    sample_time_ms: 220175.526\n",
      "    update_time_ms: 2.991\n",
      "  timestamp: 1659379914\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1080000\n",
      "  training_iteration: 54\n",
      "  trial_id: fd1ac_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 19.5/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/4.91 GiB heap, 0.0/14.65 GiB objects (0.0/6.0 CPU_group_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_0_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_4_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_5_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_3_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_2_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_1_28b5456e04c30e30e910ccff5ed5840d)\n",
      "Result logdir: /home/michael/ray_results/stabilizing_the_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_fd1ac_00000 | RUNNING  | 192.168.0.33:25804 |     54 |          16322.7 | 1080000 |  1262.28 |              3007.72 |             -422.166 |               4000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m ring length: 228\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m v_max: 3.618452967700356\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m ring length: 261\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m v_max: 5.187261846097525\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m ring length: 261\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m v_max: 5.187261846097525\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m ring length: 266\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m v_max: 5.42459972166245\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m ring length: 236\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m v_max: 3.999052674951912\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_fd1ac_00000:\n",
      "  agent_timesteps_total: 1100000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-01_13-56-57\n",
      "  done: false\n",
      "  episode_len_mean: 4000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3007.723107023102\n",
      "  episode_reward_mean: 1231.5884817247534\n",
      "  episode_reward_min: -422.1655580086222\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 275\n",
      "  experiment_id: 2b7c5715c95c4828907cbc45e49df6f0\n",
      "  hostname: michael-home\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 3.637978807091713e-13\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -1.8276510648666673\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01136794070465565\n",
      "          policy_loss: 0.0006460281897182013\n",
      "          total_loss: 129.63295673200278\n",
      "          vf_explained_var: -2.7714261996436562e-09\n",
      "          vf_loss: 129.63231021370856\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1100000\n",
      "    num_steps_sampled: 1100000\n",
      "    num_steps_trained: 1100000\n",
      "  iterations_since_restore: 55\n",
      "  node_ip: 192.168.0.33\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 28.69143518518519\n",
      "    ram_util_percent: 62.967592592592595\n",
      "  pid: 25804\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09675906109359056\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.654236568211214\n",
      "    mean_inference_ms: 1.9498436888409265\n",
      "    mean_raw_obs_processing_ms: 21.688520819562395\n",
      "  time_since_restore: 16625.90215611458\n",
      "  time_this_iter_s: 303.19946575164795\n",
      "  time_total_s: 16625.90215611458\n",
      "  timers:\n",
      "    learn_throughput: 241.002\n",
      "    learn_time_ms: 82986.799\n",
      "    sample_throughput: 90.838\n",
      "    sample_time_ms: 220172.219\n",
      "    update_time_ms: 2.963\n",
      "  timestamp: 1659380217\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1100000\n",
      "  training_iteration: 55\n",
      "  trial_id: fd1ac_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 19.8/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/4.91 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_1_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_2_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_3_28b5456e04c30e30e910ccff5ed5840d, 0.0/6.0 CPU_group_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_0_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_5_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_4_28b5456e04c30e30e910ccff5ed5840d)\n",
      "Result logdir: /home/michael/ray_results/stabilizing_the_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_fd1ac_00000 | RUNNING  | 192.168.0.33:25804 |     55 |          16625.9 | 1100000 |  1231.59 |              3007.72 |             -422.166 |               4000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m ring length: 263\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m v_max: 5.282212215151455\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m ring length: 241\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m v_max: 4.236854288051661\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m ring length: 270\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m v_max: 5.6143732387852054\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m ring length: 230\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m v_max: 3.7136148111012934\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m ring length: 242\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m v_max: 4.2844067680020546\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_fd1ac_00000:\n",
      "  agent_timesteps_total: 1120000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-01_14-02-05\n",
      "  done: false\n",
      "  episode_len_mean: 4000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3007.723107023102\n",
      "  episode_reward_mean: 1160.331424631155\n",
      "  episode_reward_min: -422.1655580086222\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 280\n",
      "  experiment_id: 2b7c5715c95c4828907cbc45e49df6f0\n",
      "  hostname: michael-home\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 3.637978807091713e-13\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -1.9278120397002834\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012203708635776595\n",
      "          policy_loss: 0.0009619274992305951\n",
      "          total_loss: 125.1858599936127\n",
      "          vf_explained_var: -3.872403731008944e-09\n",
      "          vf_loss: 125.18489738816669\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1120000\n",
      "    num_steps_sampled: 1120000\n",
      "    num_steps_trained: 1120000\n",
      "  iterations_since_restore: 56\n",
      "  node_ip: 192.168.0.33\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 32.71685649202734\n",
      "    ram_util_percent: 60.7107061503417\n",
      "  pid: 25804\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09692926658713188\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.63877308986702\n",
      "    mean_inference_ms: 1.9544699876695295\n",
      "    mean_raw_obs_processing_ms: 21.688279074089174\n",
      "  time_since_restore: 16933.69678711891\n",
      "  time_this_iter_s: 307.7946310043335\n",
      "  time_total_s: 16933.69678711891\n",
      "  timers:\n",
      "    learn_throughput: 240.044\n",
      "    learn_time_ms: 83318.149\n",
      "    sample_throughput: 90.829\n",
      "    sample_time_ms: 220193.082\n",
      "    update_time_ms: 2.974\n",
      "  timestamp: 1659380525\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1120000\n",
      "  training_iteration: 56\n",
      "  trial_id: fd1ac_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 18.8/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/4.91 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_5_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_4_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_1_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_0_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_3_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_2_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 accelerator_type:G, 0.0/6.0 CPU_group_28b5456e04c30e30e910ccff5ed5840d)\n",
      "Result logdir: /home/michael/ray_results/stabilizing_the_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_fd1ac_00000 | RUNNING  | 192.168.0.33:25804 |     56 |          16933.7 | 1120000 |  1160.33 |              3007.72 |             -422.166 |               4000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m ring length: 269\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m v_max: 5.566938458220921\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m ring length: 265\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m v_max: 5.37714246265477\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m ring length: 247\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m v_max: 4.522125250095652\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m ring length: 232\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m v_max: 3.8087690783250063\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m ring length: 261\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m v_max: 5.187261846097525\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_fd1ac_00000:\n",
      "  agent_timesteps_total: 1140000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-01_14-07-10\n",
      "  done: false\n",
      "  episode_len_mean: 4000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3007.723107023102\n",
      "  episode_reward_mean: 1125.911236530151\n",
      "  episode_reward_min: -422.1655580086222\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 285\n",
      "  experiment_id: 2b7c5715c95c4828907cbc45e49df6f0\n",
      "  hostname: michael-home\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 3.637978807091713e-13\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -1.9591288846009856\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013588595606941766\n",
      "          policy_loss: 0.001927891350277124\n",
      "          total_loss: 107.79210497376266\n",
      "          vf_explained_var: -3.948333215930688e-09\n",
      "          vf_loss: 107.79017726479063\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1140000\n",
      "    num_steps_sampled: 1140000\n",
      "    num_steps_trained: 1140000\n",
      "  iterations_since_restore: 57\n",
      "  node_ip: 192.168.0.33\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 31.33394495412844\n",
      "    ram_util_percent: 60.43302752293578\n",
      "  pid: 25804\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09708849552183998\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.624097498918953\n",
      "    mean_inference_ms: 1.9587853707636576\n",
      "    mean_raw_obs_processing_ms: 21.688066982521235\n",
      "  time_since_restore: 17238.640553236008\n",
      "  time_this_iter_s: 304.94376611709595\n",
      "  time_total_s: 17238.640553236008\n",
      "  timers:\n",
      "    learn_throughput: 240.163\n",
      "    learn_time_ms: 83276.733\n",
      "    sample_throughput: 90.829\n",
      "    sample_time_ms: 220193.865\n",
      "    update_time_ms: 2.987\n",
      "  timestamp: 1659380830\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1140000\n",
      "  training_iteration: 57\n",
      "  trial_id: fd1ac_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 18.9/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/4.91 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_1_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_4_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_3_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_2_28b5456e04c30e30e910ccff5ed5840d, 0.0/6.0 CPU_group_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_5_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_0_28b5456e04c30e30e910ccff5ed5840d)\n",
      "Result logdir: /home/michael/ray_results/stabilizing_the_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_fd1ac_00000 | RUNNING  | 192.168.0.33:25804 |     57 |          17238.6 | 1140000 |  1125.91 |              3007.72 |             -422.166 |               4000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m ring length: 232\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m v_max: 3.8087690783250063\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m ring length: 236\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m v_max: 3.999052674951912\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m ring length: 264\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m v_max: 5.329679917416892\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m ring length: 247\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m v_max: 4.522125250095652\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m ring length: 224\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m v_max: 3.4281086136538996\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_fd1ac_00000:\n",
      "  agent_timesteps_total: 1160000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-01_14-12-15\n",
      "  done: false\n",
      "  episode_len_mean: 4000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3007.723107023102\n",
      "  episode_reward_mean: 1124.5860129545931\n",
      "  episode_reward_min: -422.1655580086222\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 290\n",
      "  experiment_id: 2b7c5715c95c4828907cbc45e49df6f0\n",
      "  hostname: michael-home\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 3.637978807091713e-13\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -2.044147059871892\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011949491968795889\n",
      "          policy_loss: 0.0011720930181657243\n",
      "          total_loss: 126.67331604562747\n",
      "          vf_explained_var: -4.441874867922024e-09\n",
      "          vf_loss: 126.67214422043722\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1160000\n",
      "    num_steps_sampled: 1160000\n",
      "    num_steps_trained: 1160000\n",
      "  iterations_since_restore: 58\n",
      "  node_ip: 192.168.0.33\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 31.043087557603688\n",
      "    ram_util_percent: 61.40483870967741\n",
      "  pid: 25804\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09723710254250666\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.610138023881856\n",
      "    mean_inference_ms: 1.9628304519643527\n",
      "    mean_raw_obs_processing_ms: 21.687892758174534\n",
      "  time_since_restore: 17543.366230487823\n",
      "  time_this_iter_s: 304.7256772518158\n",
      "  time_total_s: 17543.366230487823\n",
      "  timers:\n",
      "    learn_throughput: 239.84\n",
      "    learn_time_ms: 83388.871\n",
      "    sample_throughput: 90.82\n",
      "    sample_time_ms: 220216.056\n",
      "    update_time_ms: 2.991\n",
      "  timestamp: 1659381135\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1160000\n",
      "  training_iteration: 58\n",
      "  trial_id: fd1ac_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 19.4/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/4.91 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_4_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_1_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_0_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_2_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_5_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_3_28b5456e04c30e30e910ccff5ed5840d, 0.0/6.0 CPU_group_28b5456e04c30e30e910ccff5ed5840d)\n",
      "Result logdir: /home/michael/ray_results/stabilizing_the_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_fd1ac_00000 | RUNNING  | 192.168.0.33:25804 |     58 |          17543.4 | 1160000 |  1124.59 |              3007.72 |             -422.166 |               4000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m ring length: 243\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m v_max: 4.331956438196479\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m ring length: 253\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m v_max: 4.807278529691987\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m ring length: 270\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m v_max: 5.6143732387852054\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m ring length: 261\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m v_max: 5.187261846097525\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m ring length: 222\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m v_max: 3.3329270738617227\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_fd1ac_00000:\n",
      "  agent_timesteps_total: 1180000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-01_14-17-16\n",
      "  done: false\n",
      "  episode_len_mean: 4000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3007.723107023102\n",
      "  episode_reward_mean: 1076.1480165016717\n",
      "  episode_reward_min: -422.1655580086222\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 295\n",
      "  experiment_id: 2b7c5715c95c4828907cbc45e49df6f0\n",
      "  hostname: michael-home\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 3.637978807091713e-13\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -2.041754014628708\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010587584004594688\n",
      "          policy_loss: 0.0017092429647210298\n",
      "          total_loss: 122.55262743227041\n",
      "          vf_explained_var: -2.353814032574064e-09\n",
      "          vf_loss: 122.55091712975958\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1180000\n",
      "    num_steps_sampled: 1180000\n",
      "    num_steps_trained: 1180000\n",
      "  iterations_since_restore: 59\n",
      "  node_ip: 192.168.0.33\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 28.858004640371227\n",
      "    ram_util_percent: 61.62575406032483\n",
      "  pid: 25804\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09737936997187088\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.596677184441518\n",
      "    mean_inference_ms: 1.9667279001259539\n",
      "    mean_raw_obs_processing_ms: 21.68773759167688\n",
      "  time_since_restore: 17844.845892190933\n",
      "  time_this_iter_s: 301.47966170310974\n",
      "  time_total_s: 17844.845892190933\n",
      "  timers:\n",
      "    learn_throughput: 240.515\n",
      "    learn_time_ms: 83155.045\n",
      "    sample_throughput: 90.819\n",
      "    sample_time_ms: 220217.842\n",
      "    update_time_ms: 2.972\n",
      "  timestamp: 1659381436\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1180000\n",
      "  training_iteration: 59\n",
      "  trial_id: fd1ac_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 19.2/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/4.91 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_0_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_5_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 accelerator_type:G, 0.0/6.0 CPU_group_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_3_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_1_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_2_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_4_28b5456e04c30e30e910ccff5ed5840d)\n",
      "Result logdir: /home/michael/ray_results/stabilizing_the_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_fd1ac_00000 | RUNNING  | 192.168.0.33:25804 |     59 |          17844.8 | 1180000 |  1076.15 |              3007.72 |             -422.166 |               4000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m ring length: 222\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m v_max: 3.3329270738617227\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m ring length: 226\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m v_max: 3.5232840694769565\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m ring length: 245\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m v_max: 4.427046998576354\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m ring length: 225\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m v_max: 3.4756971311168834\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m ring length: 228\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m v_max: 3.618452967700356\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_fd1ac_00000:\n",
      "  agent_timesteps_total: 1200000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-01_14-22-21\n",
      "  done: false\n",
      "  episode_len_mean: 4000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3007.723107023102\n",
      "  episode_reward_mean: 1057.6654467499984\n",
      "  episode_reward_min: -422.1655580086222\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 300\n",
      "  experiment_id: 2b7c5715c95c4828907cbc45e49df6f0\n",
      "  hostname: michael-home\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 3.637978807091713e-13\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -2.090003162280769\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.00989629706732775\n",
      "          policy_loss: 0.001775800008297109\n",
      "          total_loss: 156.70150779432552\n",
      "          vf_explained_var: -2.012131350426216e-09\n",
      "          vf_loss: 156.69973199200479\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1200000\n",
      "    num_steps_sampled: 1200000\n",
      "    num_steps_trained: 1200000\n",
      "  iterations_since_restore: 60\n",
      "  node_ip: 192.168.0.33\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 28.390092165898615\n",
      "    ram_util_percent: 61.58317972350231\n",
      "  pid: 25804\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09750629591017819\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.58406495831043\n",
      "    mean_inference_ms: 1.9702127605161923\n",
      "    mean_raw_obs_processing_ms: 21.68766105531871\n",
      "  time_since_restore: 18149.43630504608\n",
      "  time_this_iter_s: 304.5904128551483\n",
      "  time_total_s: 18149.43630504608\n",
      "  timers:\n",
      "    learn_throughput: 240.349\n",
      "    learn_time_ms: 83212.306\n",
      "    sample_throughput: 90.815\n",
      "    sample_time_ms: 220228.195\n",
      "    update_time_ms: 2.958\n",
      "  timestamp: 1659381741\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1200000\n",
      "  training_iteration: 60\n",
      "  trial_id: fd1ac_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 19.2/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/4.91 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_0_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_4_28b5456e04c30e30e910ccff5ed5840d, 0.0/6.0 CPU_group_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_5_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_3_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_1_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_2_28b5456e04c30e30e910ccff5ed5840d)\n",
      "Result logdir: /home/michael/ray_results/stabilizing_the_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_fd1ac_00000 | RUNNING  | 192.168.0.33:25804 |     60 |          18149.4 | 1200000 |  1057.67 |              3007.72 |             -422.166 |               4000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m ring length: 257\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m v_max: 4.997304682316517\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m ring length: 234\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m v_max: 3.903915223349057\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m ring length: 259\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m v_max: 5.092292348291853\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m ring length: 251\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m v_max: 4.71224180704279\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m ring length: 220\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m v_max: 3.2377399006821497\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_fd1ac_00000:\n",
      "  agent_timesteps_total: 1220000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-01_14-27-23\n",
      "  done: false\n",
      "  episode_len_mean: 4000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2741.470531571095\n",
      "  episode_reward_mean: 1025.5084958684286\n",
      "  episode_reward_min: -422.1655580086222\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 305\n",
      "  experiment_id: 2b7c5715c95c4828907cbc45e49df6f0\n",
      "  hostname: michael-home\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.8189894035458566e-13\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -2.215423438199766\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016165146638224402\n",
      "          policy_loss: 0.0023539495162048915\n",
      "          total_loss: 149.51976071983387\n",
      "          vf_explained_var: -2.353814032574064e-09\n",
      "          vf_loss: 149.51740675640713\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1220000\n",
      "    num_steps_sampled: 1220000\n",
      "    num_steps_trained: 1220000\n",
      "  iterations_since_restore: 61\n",
      "  node_ip: 192.168.0.33\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 29.05823665893271\n",
      "    ram_util_percent: 61.96705336426915\n",
      "  pid: 25804\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09762597022778755\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.57204772299188\n",
      "    mean_inference_ms: 1.973480477026753\n",
      "    mean_raw_obs_processing_ms: 21.68758476119244\n",
      "  time_since_restore: 18451.04660797119\n",
      "  time_this_iter_s: 301.61030292510986\n",
      "  time_total_s: 18451.04660797119\n",
      "  timers:\n",
      "    learn_throughput: 240.757\n",
      "    learn_time_ms: 83071.145\n",
      "    sample_throughput: 90.807\n",
      "    sample_time_ms: 220247.041\n",
      "    update_time_ms: 2.982\n",
      "  timestamp: 1659382043\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1220000\n",
      "  training_iteration: 61\n",
      "  trial_id: fd1ac_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 19.3/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/4.91 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_1_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_2_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 accelerator_type:G, 0.0/6.0 CPU_group_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_5_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_0_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_4_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_3_28b5456e04c30e30e910ccff5ed5840d)\n",
      "Result logdir: /home/michael/ray_results/stabilizing_the_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_fd1ac_00000 | RUNNING  | 192.168.0.33:25804 |     61 |            18451 | 1220000 |  1025.51 |              2741.47 |             -422.166 |               4000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m ring length: 221\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m v_max: 3.285334164169417\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m ring length: 244\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m v_max: 4.379503211387676\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m ring length: 258\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m v_max: 5.044800727535787\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m ring length: 223\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m v_max: 3.3805185755587788\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m ring length: 268\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m v_max: 5.5194978538368025\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_fd1ac_00000:\n",
      "  agent_timesteps_total: 1240000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-01_14-32-25\n",
      "  done: false\n",
      "  episode_len_mean: 4000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2769.1120585222548\n",
      "  episode_reward_mean: 1014.3260582305858\n",
      "  episode_reward_min: -422.1655580086222\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 310\n",
      "  experiment_id: 2b7c5715c95c4828907cbc45e49df6f0\n",
      "  hostname: michael-home\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.8189894035458566e-13\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -2.1984772217501503\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01014310428887538\n",
      "          policy_loss: 0.001324024139814506\n",
      "          total_loss: 165.59800362070655\n",
      "          vf_explained_var: -3.910368473469816e-09\n",
      "          vf_loss: 165.59667922827848\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1240000\n",
      "    num_steps_sampled: 1240000\n",
      "    num_steps_trained: 1240000\n",
      "  iterations_since_restore: 62\n",
      "  node_ip: 192.168.0.33\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 27.341162790697677\n",
      "    ram_util_percent: 61.93279069767442\n",
      "  pid: 25804\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09774179125191358\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.560427612801092\n",
      "    mean_inference_ms: 1.9766768909867585\n",
      "    mean_raw_obs_processing_ms: 21.68746234656113\n",
      "  time_since_restore: 18752.781382083893\n",
      "  time_this_iter_s: 301.7347741127014\n",
      "  time_total_s: 18752.781382083893\n",
      "  timers:\n",
      "    learn_throughput: 240.389\n",
      "    learn_time_ms: 83198.518\n",
      "    sample_throughput: 90.809\n",
      "    sample_time_ms: 220242.596\n",
      "    update_time_ms: 3.001\n",
      "  timestamp: 1659382345\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1240000\n",
      "  training_iteration: 62\n",
      "  trial_id: fd1ac_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 19.3/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/4.91 GiB heap, 0.0/14.65 GiB objects (0.0/6.0 CPU_group_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_3_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_2_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_0_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_5_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_1_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_4_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 accelerator_type:G)\n",
      "Result logdir: /home/michael/ray_results/stabilizing_the_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_fd1ac_00000 | RUNNING  | 192.168.0.33:25804 |     62 |          18752.8 | 1240000 |  1014.33 |              2769.11 |             -422.166 |               4000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m ring length: 270\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m v_max: 5.6143732387852054\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m ring length: 250\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m v_max: 4.664717904914125\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m ring length: 226\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m v_max: 3.5232840694769565\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m ring length: 242\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m v_max: 4.2844067680020546\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m ring length: 247\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m v_max: 4.522125250095652\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_fd1ac_00000:\n",
      "  agent_timesteps_total: 1260000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-01_14-37-30\n",
      "  done: false\n",
      "  episode_len_mean: 4000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2769.1120585222548\n",
      "  episode_reward_mean: 1044.264751078144\n",
      "  episode_reward_min: -422.1655580086222\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 315\n",
      "  experiment_id: 2b7c5715c95c4828907cbc45e49df6f0\n",
      "  hostname: michael-home\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.8189894035458566e-13\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -2.219789448665206\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010314086882436832\n",
      "          policy_loss: 0.0015615715557756793\n",
      "          total_loss: 178.5213086097863\n",
      "          vf_explained_var: 1.252836501208776e-09\n",
      "          vf_loss: 178.51974569430018\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1260000\n",
      "    num_steps_sampled: 1260000\n",
      "    num_steps_trained: 1260000\n",
      "  iterations_since_restore: 63\n",
      "  node_ip: 192.168.0.33\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 31.40504587155963\n",
      "    ram_util_percent: 62.47339449541285\n",
      "  pid: 25804\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09785617772870453\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.549137230777998\n",
      "    mean_inference_ms: 1.9798640303844137\n",
      "    mean_raw_obs_processing_ms: 21.687381803985215\n",
      "  time_since_restore: 19058.142557621002\n",
      "  time_this_iter_s: 305.3611755371094\n",
      "  time_total_s: 19058.142557621002\n",
      "  timers:\n",
      "    learn_throughput: 239.407\n",
      "    learn_time_ms: 83539.787\n",
      "    sample_throughput: 90.808\n",
      "    sample_time_ms: 220244.294\n",
      "    update_time_ms: 3.022\n",
      "  timestamp: 1659382650\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1260000\n",
      "  training_iteration: 63\n",
      "  trial_id: fd1ac_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 19.5/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/4.91 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_5_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_0_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_2_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_1_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_4_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 accelerator_type:G, 0.0/6.0 CPU_group_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_3_28b5456e04c30e30e910ccff5ed5840d)\n",
      "Result logdir: /home/michael/ray_results/stabilizing_the_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_fd1ac_00000 | RUNNING  | 192.168.0.33:25804 |     63 |          19058.1 | 1260000 |  1044.26 |              2769.11 |             -422.166 |               4000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m ring length: 236\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m v_max: 3.999052674951912\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m ring length: 259\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m v_max: 5.092292348291853\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m ring length: 252\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m v_max: 4.759762049824181\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m ring length: 268\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m v_max: 5.5194978538368025\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m ring length: 259\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m v_max: 5.092292348291853\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_fd1ac_00000:\n",
      "  agent_timesteps_total: 1280000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-01_14-42-36\n",
      "  done: false\n",
      "  episode_len_mean: 4000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2769.1120585222548\n",
      "  episode_reward_mean: 1057.5250966027168\n",
      "  episode_reward_min: -422.1655580086222\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 320\n",
      "  experiment_id: 2b7c5715c95c4828907cbc45e49df6f0\n",
      "  hostname: michael-home\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.8189894035458566e-13\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -2.320694310498086\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012054104532488171\n",
      "          policy_loss: 0.002239440725285821\n",
      "          total_loss: 190.62733119551544\n",
      "          vf_explained_var: -2.7714261996436562e-09\n",
      "          vf_loss: 190.62509065615902\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1280000\n",
      "    num_steps_sampled: 1280000\n",
      "    num_steps_trained: 1280000\n",
      "  iterations_since_restore: 64\n",
      "  node_ip: 192.168.0.33\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 32.01949541284403\n",
      "    ram_util_percent: 62.50756880733945\n",
      "  pid: 25804\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09797106862147203\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.538089565274518\n",
      "    mean_inference_ms: 1.9830875753901085\n",
      "    mean_raw_obs_processing_ms: 21.687240761246287\n",
      "  time_since_restore: 19363.760095596313\n",
      "  time_this_iter_s: 305.6175379753113\n",
      "  time_total_s: 19363.760095596313\n",
      "  timers:\n",
      "    learn_throughput: 238.514\n",
      "    learn_time_ms: 83852.59\n",
      "    sample_throughput: 90.811\n",
      "    sample_time_ms: 220237.893\n",
      "    update_time_ms: 3.0\n",
      "  timestamp: 1659382956\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1280000\n",
      "  training_iteration: 64\n",
      "  trial_id: fd1ac_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 19.5/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/4.91 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_4_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_5_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_0_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_2_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_3_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 accelerator_type:G, 0.0/6.0 CPU_group_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_1_28b5456e04c30e30e910ccff5ed5840d)\n",
      "Result logdir: /home/michael/ray_results/stabilizing_the_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_fd1ac_00000 | RUNNING  | 192.168.0.33:25804 |     64 |          19363.8 | 1280000 |  1057.53 |              2769.11 |             -422.166 |               4000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m ring length: 239\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m v_max: 4.141741239205832\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m ring length: 264\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m v_max: 5.329679917416892\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m ring length: 250\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m v_max: 4.664717904914125\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m ring length: 222\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m v_max: 3.3329270738617227\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m ring length: 246\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m v_max: 4.474587708995648\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_fd1ac_00000:\n",
      "  agent_timesteps_total: 1300000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-01_14-47-41\n",
      "  done: false\n",
      "  episode_len_mean: 4000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2769.1120585222548\n",
      "  episode_reward_mean: 1092.2287006151932\n",
      "  episode_reward_min: -422.1655580086222\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 325\n",
      "  experiment_id: 2b7c5715c95c4828907cbc45e49df6f0\n",
      "  hostname: michael-home\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.8189894035458566e-13\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -2.4056462430650263\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01194835591907053\n",
      "          policy_loss: 0.0016838729089410727\n",
      "          total_loss: 205.73276725574664\n",
      "          vf_explained_var: -4.1761216706959203e-10\n",
      "          vf_loss: 205.7310829454167\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1300000\n",
      "    num_steps_sampled: 1300000\n",
      "    num_steps_trained: 1300000\n",
      "  iterations_since_restore: 65\n",
      "  node_ip: 192.168.0.33\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 29.20298850574713\n",
      "    ram_util_percent: 62.17724137931034\n",
      "  pid: 25804\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09807398660018762\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.527810074350867\n",
      "    mean_inference_ms: 1.985942309992262\n",
      "    mean_raw_obs_processing_ms: 21.6871867292366\n",
      "  time_since_restore: 19668.512484550476\n",
      "  time_this_iter_s: 304.7523889541626\n",
      "  time_total_s: 19668.512484550476\n",
      "  timers:\n",
      "    learn_throughput: 238.346\n",
      "    learn_time_ms: 83911.458\n",
      "    sample_throughput: 90.771\n",
      "    sample_time_ms: 220334.069\n",
      "    update_time_ms: 3.02\n",
      "  timestamp: 1659383261\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1300000\n",
      "  training_iteration: 65\n",
      "  trial_id: fd1ac_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 19.4/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/4.91 GiB heap, 0.0/14.65 GiB objects (0.0/6.0 CPU_group_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_2_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_0_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_5_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_4_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_3_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_1_28b5456e04c30e30e910ccff5ed5840d)\n",
      "Result logdir: /home/michael/ray_results/stabilizing_the_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_fd1ac_00000 | RUNNING  | 192.168.0.33:25804 |     65 |          19668.5 | 1300000 |  1092.23 |              2769.11 |             -422.166 |               4000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m ring length: 230\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m v_max: 3.7136148111012934\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m ring length: 232\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m v_max: 3.8087690783250063\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m ring length: 239\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m v_max: 4.141741239205832\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m ring length: 248\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m v_max: 4.569659527528101\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m ring length: 249\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m v_max: 4.617190445131122\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_fd1ac_00000:\n",
      "  agent_timesteps_total: 1320000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-01_14-52-44\n",
      "  done: false\n",
      "  episode_len_mean: 4000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2769.1120585222548\n",
      "  episode_reward_mean: 1145.2001655456443\n",
      "  episode_reward_min: -422.1655580086222\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 330\n",
      "  experiment_id: 2b7c5715c95c4828907cbc45e49df6f0\n",
      "  hostname: michael-home\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.8189894035458566e-13\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -2.309550709329593\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008846589429731901\n",
      "          policy_loss: 0.0013427928515715869\n",
      "          total_loss: 206.65419894175923\n",
      "          vf_explained_var: -2.391778775034936e-09\n",
      "          vf_loss: 206.6528564234448\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1320000\n",
      "    num_steps_sampled: 1320000\n",
      "    num_steps_trained: 1320000\n",
      "  iterations_since_restore: 66\n",
      "  node_ip: 192.168.0.33\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 29.338337182448036\n",
      "    ram_util_percent: 62.528637413394925\n",
      "  pid: 25804\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0981692884410819\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.518039747327965\n",
      "    mean_inference_ms: 1.9886005673196934\n",
      "    mean_raw_obs_processing_ms: 21.687105650874898\n",
      "  time_since_restore: 19971.973247528076\n",
      "  time_this_iter_s: 303.4607629776001\n",
      "  time_total_s: 19971.973247528076\n",
      "  timers:\n",
      "    learn_throughput: 239.539\n",
      "    learn_time_ms: 83493.638\n",
      "    sample_throughput: 90.778\n",
      "    sample_time_ms: 220318.548\n",
      "    update_time_ms: 3.016\n",
      "  timestamp: 1659383564\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1320000\n",
      "  training_iteration: 66\n",
      "  trial_id: fd1ac_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 19.6/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/4.91 GiB heap, 0.0/14.65 GiB objects (0.0/6.0 CPU_group_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_2_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_5_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_0_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_4_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_3_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_1_28b5456e04c30e30e910ccff5ed5840d)\n",
      "Result logdir: /home/michael/ray_results/stabilizing_the_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_fd1ac_00000 | RUNNING  | 192.168.0.33:25804 |     66 |            19972 | 1320000 |   1145.2 |              2769.11 |             -422.166 |               4000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m ring length: 260\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m v_max: 5.139779427502188\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m ring length: 258\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m v_max: 5.044800727535787\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m ring length: 224\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m v_max: 3.4281086136538996\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m ring length: 222\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m v_max: 3.3329270738617227\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m ring length: 233\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m v_max: 3.856343201216534\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_fd1ac_00000:\n",
      "  agent_timesteps_total: 1340000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-01_14-57-48\n",
      "  done: false\n",
      "  episode_len_mean: 4000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2769.1120585222548\n",
      "  episode_reward_mean: 1208.87371557317\n",
      "  episode_reward_min: -422.1655580086222\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 335\n",
      "  experiment_id: 2b7c5715c95c4828907cbc45e49df6f0\n",
      "  hostname: michael-home\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 9.094947017729283e-14\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -2.282612615178345\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012913678253550047\n",
      "          policy_loss: 0.000827219236826014\n",
      "          total_loss: 211.7504197539797\n",
      "          vf_explained_var: 1.13894227382616e-09\n",
      "          vf_loss: 211.7495932111315\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1340000\n",
      "    num_steps_sampled: 1340000\n",
      "    num_steps_trained: 1340000\n",
      "  iterations_since_restore: 67\n",
      "  node_ip: 192.168.0.33\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 29.96543778801843\n",
      "    ram_util_percent: 62.76175115207373\n",
      "  pid: 25804\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09826286841146635\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.508508221221927\n",
      "    mean_inference_ms: 1.9912504974569267\n",
      "    mean_raw_obs_processing_ms: 21.687040112151507\n",
      "  time_since_restore: 20276.254814624786\n",
      "  time_this_iter_s: 304.2815670967102\n",
      "  time_total_s: 20276.254814624786\n",
      "  timers:\n",
      "    learn_throughput: 239.665\n",
      "    learn_time_ms: 83449.893\n",
      "    sample_throughput: 90.787\n",
      "    sample_time_ms: 220295.497\n",
      "    update_time_ms: 3.158\n",
      "  timestamp: 1659383868\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1340000\n",
      "  training_iteration: 67\n",
      "  trial_id: fd1ac_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 19.7/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/4.91 GiB heap, 0.0/14.65 GiB objects (0.0/6.0 CPU_group_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_4_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_5_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_3_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_0_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_1_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_2_28b5456e04c30e30e910ccff5ed5840d)\n",
      "Result logdir: /home/michael/ray_results/stabilizing_the_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_fd1ac_00000 | RUNNING  | 192.168.0.33:25804 |     67 |          20276.3 | 1340000 |  1208.87 |              2769.11 |             -422.166 |               4000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m ring length: 234\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m v_max: 3.903915223349057\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m ring length: 266\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m v_max: 5.42459972166245\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m ring length: 227\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m v_max: 3.570869368805752\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m ring length: 230\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m v_max: 3.7136148111012934\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m ring length: 233\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m v_max: 3.856343201216534\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_fd1ac_00000:\n",
      "  agent_timesteps_total: 1360000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-01_15-02-49\n",
      "  done: false\n",
      "  episode_len_mean: 4000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3133.9809538101003\n",
      "  episode_reward_mean: 1316.863004373282\n",
      "  episode_reward_min: -396.95767167757674\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 340\n",
      "  experiment_id: 2b7c5715c95c4828907cbc45e49df6f0\n",
      "  hostname: michael-home\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 9.094947017729283e-14\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -2.2960050942791494\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012682138528240468\n",
      "          policy_loss: 0.0022231743095596883\n",
      "          total_loss: 237.97776147392904\n",
      "          vf_explained_var: -3.7964742460872e-09\n",
      "          vf_loss: 237.9755391090539\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1360000\n",
      "    num_steps_sampled: 1360000\n",
      "    num_steps_trained: 1360000\n",
      "  iterations_since_restore: 68\n",
      "  node_ip: 192.168.0.33\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 28.369230769230768\n",
      "    ram_util_percent: 62.717482517482516\n",
      "  pid: 25804\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09835396888023236\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.499287863920344\n",
      "    mean_inference_ms: 1.9938576420161176\n",
      "    mean_raw_obs_processing_ms: 21.686979355162222\n",
      "  time_since_restore: 20576.874281406403\n",
      "  time_this_iter_s: 300.6194667816162\n",
      "  time_total_s: 20576.874281406403\n",
      "  timers:\n",
      "    learn_throughput: 240.82\n",
      "    learn_time_ms: 83049.648\n",
      "    sample_throughput: 90.791\n",
      "    sample_time_ms: 220285.114\n",
      "    update_time_ms: 3.122\n",
      "  timestamp: 1659384169\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1360000\n",
      "  training_iteration: 68\n",
      "  trial_id: fd1ac_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 19.5/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/4.91 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 accelerator_type:G, 0.0/6.0 CPU_group_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_5_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_2_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_0_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_1_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_4_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_3_28b5456e04c30e30e910ccff5ed5840d)\n",
      "Result logdir: /home/michael/ray_results/stabilizing_the_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_fd1ac_00000 | RUNNING  | 192.168.0.33:25804 |     68 |          20576.9 | 1360000 |  1316.86 |              3133.98 |             -396.958 |               4000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m ring length: 241\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m v_max: 4.236854288051661\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m ring length: 251\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m v_max: 4.71224180704279\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m ring length: 250\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m v_max: 4.664717904914125\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m ring length: 227\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m v_max: 3.570869368805752\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m ring length: 253\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m v_max: 4.807278529691987\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_fd1ac_00000:\n",
      "  agent_timesteps_total: 1380000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-01_15-07-50\n",
      "  done: false\n",
      "  episode_len_mean: 4000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3133.9809538101003\n",
      "  episode_reward_mean: 1409.3253535286428\n",
      "  episode_reward_min: -300.54223570520634\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 345\n",
      "  experiment_id: 2b7c5715c95c4828907cbc45e49df6f0\n",
      "  hostname: michael-home\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 9.094947017729283e-14\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -2.3087594012545933\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010988033639208174\n",
      "          policy_loss: 0.0021285263152650683\n",
      "          total_loss: 229.855122899098\n",
      "          vf_explained_var: -5.6187818842090564e-09\n",
      "          vf_loss: 229.8529943915689\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1380000\n",
      "    num_steps_sampled: 1380000\n",
      "    num_steps_trained: 1380000\n",
      "  iterations_since_restore: 69\n",
      "  node_ip: 192.168.0.33\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 27.15034965034965\n",
      "    ram_util_percent: 62.586247086247084\n",
      "  pid: 25804\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09843568669886235\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.490539477438563\n",
      "    mean_inference_ms: 1.9962366678877101\n",
      "    mean_raw_obs_processing_ms: 21.68689966988037\n",
      "  time_since_restore: 20877.70009279251\n",
      "  time_this_iter_s: 300.8258113861084\n",
      "  time_total_s: 20877.70009279251\n",
      "  timers:\n",
      "    learn_throughput: 240.982\n",
      "    learn_time_ms: 82993.665\n",
      "    sample_throughput: 90.795\n",
      "    sample_time_ms: 220275.725\n",
      "    update_time_ms: 3.129\n",
      "  timestamp: 1659384470\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1380000\n",
      "  training_iteration: 69\n",
      "  trial_id: fd1ac_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 19.9/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/4.91 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_3_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_2_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_4_28b5456e04c30e30e910ccff5ed5840d, 0.0/6.0 CPU_group_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_0_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_5_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_1_28b5456e04c30e30e910ccff5ed5840d)\n",
      "Result logdir: /home/michael/ray_results/stabilizing_the_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_fd1ac_00000 | RUNNING  | 192.168.0.33:25804 |     69 |          20877.7 | 1380000 |  1409.33 |              3133.98 |             -300.542 |               4000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m ring length: 258\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m v_max: 5.044800727535787\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m ring length: 256\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m v_max: 4.949804327743507\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m ring length: 236\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m v_max: 3.999052674951912\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m ring length: 226\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m v_max: 3.5232840694769565\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m ring length: 221\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m v_max: 3.285334164169417\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_fd1ac_00000:\n",
      "  agent_timesteps_total: 1400000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-01_15-12-51\n",
      "  done: false\n",
      "  episode_len_mean: 4000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3133.9809538101003\n",
      "  episode_reward_mean: 1492.4057914097255\n",
      "  episode_reward_min: -191.4532965647793\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 350\n",
      "  experiment_id: 2b7c5715c95c4828907cbc45e49df6f0\n",
      "  hostname: michael-home\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 9.094947017729283e-14\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -2.3275071156252722\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014360106004170637\n",
      "          policy_loss: 0.0015837775750210284\n",
      "          total_loss: 248.829065180736\n",
      "          vf_explained_var: -4.1761216706959203e-10\n",
      "          vf_loss: 248.82748163794255\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1400000\n",
      "    num_steps_sampled: 1400000\n",
      "    num_steps_trained: 1400000\n",
      "  iterations_since_restore: 70\n",
      "  node_ip: 192.168.0.33\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 26.780652680652683\n",
      "    ram_util_percent: 62.605594405594395\n",
      "  pid: 25804\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09850899963385688\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.482254989395038\n",
      "    mean_inference_ms: 1.9983959066548662\n",
      "    mean_raw_obs_processing_ms: 21.6867621951412\n",
      "  time_since_restore: 21178.712629556656\n",
      "  time_this_iter_s: 301.0125367641449\n",
      "  time_total_s: 21178.712629556656\n",
      "  timers:\n",
      "    learn_throughput: 241.933\n",
      "    learn_time_ms: 82667.403\n",
      "    sample_throughput: 90.808\n",
      "    sample_time_ms: 220244.22\n",
      "    update_time_ms: 3.147\n",
      "  timestamp: 1659384771\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1400000\n",
      "  training_iteration: 70\n",
      "  trial_id: fd1ac_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 19.6/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/4.91 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_0_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_1_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_5_28b5456e04c30e30e910ccff5ed5840d, 0.0/6.0 CPU_group_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_2_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_4_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_3_28b5456e04c30e30e910ccff5ed5840d)\n",
      "Result logdir: /home/michael/ray_results/stabilizing_the_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_fd1ac_00000 | RUNNING  | 192.168.0.33:25804 |     70 |          21178.7 | 1400000 |  1492.41 |              3133.98 |             -191.453 |               4000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m ring length: 262\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m v_max: 5.234739483008334\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m ring length: 258\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m v_max: 5.044800727535787\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m ring length: 227\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m v_max: 3.570869368805752\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m ring length: 259\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m v_max: 5.092292348291853\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m ring length: 234\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m v_max: 3.903915223349057\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_fd1ac_00000:\n",
      "  agent_timesteps_total: 1420000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-01_15-17-55\n",
      "  done: false\n",
      "  episode_len_mean: 4000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3133.9809538101003\n",
      "  episode_reward_mean: 1511.452488768329\n",
      "  episode_reward_min: -191.4532965647793\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 355\n",
      "  experiment_id: 2b7c5715c95c4828907cbc45e49df6f0\n",
      "  hostname: michael-home\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 9.094947017729283e-14\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -2.3389381905270232\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01166759231219559\n",
      "          policy_loss: 0.0017122068298553491\n",
      "          total_loss: 259.34995038463813\n",
      "          vf_explained_var: -3.6825800187045843e-09\n",
      "          vf_loss: 259.3482392815268\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1420000\n",
      "    num_steps_sampled: 1420000\n",
      "    num_steps_trained: 1420000\n",
      "  iterations_since_restore: 71\n",
      "  node_ip: 192.168.0.33\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 30.962298850574715\n",
      "    ram_util_percent: 63.30850574712644\n",
      "  pid: 25804\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09858081900768091\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.47422358606447\n",
      "    mean_inference_ms: 2.000508461916425\n",
      "    mean_raw_obs_processing_ms: 21.68666521043576\n",
      "  time_since_restore: 21482.900668621063\n",
      "  time_this_iter_s: 304.18803906440735\n",
      "  time_total_s: 21482.900668621063\n",
      "  timers:\n",
      "    learn_throughput: 241.176\n",
      "    learn_time_ms: 82927.017\n",
      "    sample_throughput: 90.809\n",
      "    sample_time_ms: 220242.345\n",
      "    update_time_ms: 3.175\n",
      "  timestamp: 1659385075\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1420000\n",
      "  training_iteration: 71\n",
      "  trial_id: fd1ac_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 19.8/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/4.91 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_4_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_1_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_5_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_2_28b5456e04c30e30e910ccff5ed5840d, 0.0/6.0 CPU_group_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_0_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_3_28b5456e04c30e30e910ccff5ed5840d)\n",
      "Result logdir: /home/michael/ray_results/stabilizing_the_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_fd1ac_00000 | RUNNING  | 192.168.0.33:25804 |     71 |          21482.9 | 1420000 |  1511.45 |              3133.98 |             -191.453 |               4000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m ring length: 227\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m v_max: 3.570869368805752\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m ring length: 249\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m v_max: 4.617190445131122\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m ring length: 220\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m v_max: 3.2377399006821497\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m ring length: 234\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m v_max: 3.903915223349057\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m ring length: 269\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m v_max: 5.566938458220921\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_fd1ac_00000:\n",
      "  agent_timesteps_total: 1440000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-01_15-22-57\n",
      "  done: false\n",
      "  episode_len_mean: 4000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3133.9809538101003\n",
      "  episode_reward_mean: 1585.7656879133765\n",
      "  episode_reward_min: -191.4532965647793\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 360\n",
      "  experiment_id: 2b7c5715c95c4828907cbc45e49df6f0\n",
      "  hostname: michael-home\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 9.094947017729283e-14\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -2.3641460596376165\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010338688835271436\n",
      "          policy_loss: 0.0016909907695060228\n",
      "          total_loss: 278.4521478276344\n",
      "          vf_explained_var: -1.176907016287032e-09\n",
      "          vf_loss: 278.45045779039907\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1440000\n",
      "    num_steps_sampled: 1440000\n",
      "    num_steps_trained: 1440000\n",
      "  iterations_since_restore: 72\n",
      "  node_ip: 192.168.0.33\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 29.638139534883724\n",
      "    ram_util_percent: 65.04790697674419\n",
      "  pid: 25804\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09865103309468191\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.466474055254817\n",
      "    mean_inference_ms: 2.0025543469595704\n",
      "    mean_raw_obs_processing_ms: 21.68654852676252\n",
      "  time_since_restore: 21784.286506414413\n",
      "  time_this_iter_s: 301.3858377933502\n",
      "  time_total_s: 21784.286506414413\n",
      "  timers:\n",
      "    learn_throughput: 241.301\n",
      "    learn_time_ms: 82883.864\n",
      "    sample_throughput: 90.806\n",
      "    sample_time_ms: 220250.659\n",
      "    update_time_ms: 3.163\n",
      "  timestamp: 1659385377\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1440000\n",
      "  training_iteration: 72\n",
      "  trial_id: fd1ac_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 20.3/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/4.91 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_2_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_4_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_1_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_0_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_5_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_3_28b5456e04c30e30e910ccff5ed5840d, 0.0/6.0 CPU_group_28b5456e04c30e30e910ccff5ed5840d)\n",
      "Result logdir: /home/michael/ray_results/stabilizing_the_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_fd1ac_00000 | RUNNING  | 192.168.0.33:25804 |     72 |          21784.3 | 1440000 |  1585.77 |              3133.98 |             -191.453 |               4000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m ring length: 247\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m v_max: 4.522125250095652\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m ring length: 247\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m v_max: 4.522125250095652\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m ring length: 255\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m v_max: 4.902299776966978\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m ring length: 262\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m v_max: 5.234739483008334\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m ring length: 227\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m v_max: 3.570869368805752\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_fd1ac_00000:\n",
      "  agent_timesteps_total: 1460000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-01_15-27-59\n",
      "  done: false\n",
      "  episode_len_mean: 4000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3133.9809538101003\n",
      "  episode_reward_mean: 1636.2020208042782\n",
      "  episode_reward_min: -191.4532965647793\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 365\n",
      "  experiment_id: 2b7c5715c95c4828907cbc45e49df6f0\n",
      "  hostname: michael-home\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 9.094947017729283e-14\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -2.470476508444282\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015863871110205065\n",
      "          policy_loss: 0.0024844884531347046\n",
      "          total_loss: 281.212148977389\n",
      "          vf_explained_var: -3.4547915639393523e-09\n",
      "          vf_loss: 281.2096649728763\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1460000\n",
      "    num_steps_sampled: 1460000\n",
      "    num_steps_trained: 1460000\n",
      "  iterations_since_restore: 73\n",
      "  node_ip: 192.168.0.33\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 26.42088167053364\n",
      "    ram_util_percent: 65.05545243619488\n",
      "  pid: 25804\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09871889547825613\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.45912663298092\n",
      "    mean_inference_ms: 2.004513639196293\n",
      "    mean_raw_obs_processing_ms: 21.686448798191762\n",
      "  time_since_restore: 22086.36596608162\n",
      "  time_this_iter_s: 302.0794596672058\n",
      "  time_total_s: 22086.36596608162\n",
      "  timers:\n",
      "    learn_throughput: 242.507\n",
      "    learn_time_ms: 82471.715\n",
      "    sample_throughput: 90.771\n",
      "    sample_time_ms: 220334.801\n",
      "    update_time_ms: 3.159\n",
      "  timestamp: 1659385679\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1460000\n",
      "  training_iteration: 73\n",
      "  trial_id: fd1ac_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 20.3/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/4.91 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_1_28b5456e04c30e30e910ccff5ed5840d, 0.0/6.0 CPU_group_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_5_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_0_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_4_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_2_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_3_28b5456e04c30e30e910ccff5ed5840d)\n",
      "Result logdir: /home/michael/ray_results/stabilizing_the_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_fd1ac_00000 | RUNNING  | 192.168.0.33:25804 |     73 |          22086.4 | 1460000 |   1636.2 |              3133.98 |             -191.453 |               4000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m ring length: 235\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m v_max: 3.9514850725282584\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m ring length: 269\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m v_max: 5.566938458220921\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m ring length: 224\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m v_max: 3.4281086136538996\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m ring length: 265\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m v_max: 5.37714246265477\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m ring length: 255\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m v_max: 4.902299776966978\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_fd1ac_00000:\n",
      "  agent_timesteps_total: 1480000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-01_15-33-00\n",
      "  done: false\n",
      "  episode_len_mean: 4000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3133.9809538101003\n",
      "  episode_reward_mean: 1674.9254980287255\n",
      "  episode_reward_min: -191.4532965647793\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 370\n",
      "  experiment_id: 2b7c5715c95c4828907cbc45e49df6f0\n",
      "  hostname: michael-home\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 9.094947017729283e-14\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -2.494332676936107\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017701519074683723\n",
      "          policy_loss: 0.001466308097847423\n",
      "          total_loss: 287.5120712438207\n",
      "          vf_explained_var: -2.315849290113192e-09\n",
      "          vf_loss: 287.510606330823\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1480000\n",
      "    num_steps_sampled: 1480000\n",
      "    num_steps_trained: 1480000\n",
      "  iterations_since_restore: 74\n",
      "  node_ip: 192.168.0.33\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 27.15279069767442\n",
      "    ram_util_percent: 65.76697674418605\n",
      "  pid: 25804\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09878198280201068\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.4520882320639\n",
      "    mean_inference_ms: 2.0063749252027048\n",
      "    mean_raw_obs_processing_ms: 21.686342306426653\n",
      "  time_since_restore: 22387.870104789734\n",
      "  time_this_iter_s: 301.5041387081146\n",
      "  time_total_s: 22387.870104789734\n",
      "  timers:\n",
      "    learn_throughput: 243.771\n",
      "    learn_time_ms: 82044.14\n",
      "    sample_throughput: 90.764\n",
      "    sample_time_ms: 220351.003\n",
      "    update_time_ms: 3.145\n",
      "  timestamp: 1659385980\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1480000\n",
      "  training_iteration: 74\n",
      "  trial_id: fd1ac_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 20.8/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/4.91 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_3_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_4_28b5456e04c30e30e910ccff5ed5840d, 0.0/6.0 CPU_group_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_0_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_5_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_2_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_1_28b5456e04c30e30e910ccff5ed5840d)\n",
      "Result logdir: /home/michael/ray_results/stabilizing_the_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_fd1ac_00000 | RUNNING  | 192.168.0.33:25804 |     74 |          22387.9 | 1480000 |  1674.93 |              3133.98 |             -191.453 |               4000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m ring length: 247\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m v_max: 4.522125250095652\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m ring length: 253\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m v_max: 4.807278529691987\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m ring length: 220\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m v_max: 3.2377399006821497\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m ring length: 258\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m v_max: 5.044800727535787\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m ring length: 233\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m v_max: 3.856343201216534\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_fd1ac_00000:\n",
      "  agent_timesteps_total: 1500000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-01_15-38-04\n",
      "  done: false\n",
      "  episode_len_mean: 4000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3415.287774190107\n",
      "  episode_reward_mean: 1744.5915147923533\n",
      "  episode_reward_min: -191.4532965647793\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 375\n",
      "  experiment_id: 2b7c5715c95c4828907cbc45e49df6f0\n",
      "  hostname: michael-home\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 9.094947017729283e-14\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -2.5123274636116757\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013994329662387967\n",
      "          policy_loss: 0.0019257244861619488\n",
      "          total_loss: 326.1450829499846\n",
      "          vf_explained_var: -2.163990320269704e-09\n",
      "          vf_loss: 326.143157157169\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1500000\n",
      "    num_steps_sampled: 1500000\n",
      "    num_steps_trained: 1500000\n",
      "  iterations_since_restore: 75\n",
      "  node_ip: 192.168.0.33\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 28.464203233256352\n",
      "    ram_util_percent: 66.52655889145497\n",
      "  pid: 25804\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0988424590694282\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.445289364437322\n",
      "    mean_inference_ms: 2.008188656741496\n",
      "    mean_raw_obs_processing_ms: 21.686232118526053\n",
      "  time_since_restore: 22691.268361091614\n",
      "  time_this_iter_s: 303.3982563018799\n",
      "  time_total_s: 22691.268361091614\n",
      "  timers:\n",
      "    learn_throughput: 243.88\n",
      "    learn_time_ms: 82007.537\n",
      "    sample_throughput: 90.805\n",
      "    sample_time_ms: 220252.532\n",
      "    update_time_ms: 3.098\n",
      "  timestamp: 1659386284\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1500000\n",
      "  training_iteration: 75\n",
      "  trial_id: fd1ac_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 21.0/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/4.91 GiB heap, 0.0/14.65 GiB objects (0.0/6.0 CPU_group_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_4_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_1_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_5_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_0_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_3_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_2_28b5456e04c30e30e910ccff5ed5840d)\n",
      "Result logdir: /home/michael/ray_results/stabilizing_the_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_fd1ac_00000 | RUNNING  | 192.168.0.33:25804 |     75 |          22691.3 | 1500000 |  1744.59 |              3415.29 |             -191.453 |               4000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m ring length: 253\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m v_max: 4.807278529691987\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m ring length: 229\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m v_max: 3.666034803266416\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m ring length: 253\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m v_max: 4.807278529691987\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m ring length: 266\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m v_max: 5.42459972166245\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m ring length: 236\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m v_max: 3.999052674951912\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_fd1ac_00000:\n",
      "  agent_timesteps_total: 1520000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-01_15-43-09\n",
      "  done: false\n",
      "  episode_len_mean: 4000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3415.287774190107\n",
      "  episode_reward_mean: 1828.1071683286855\n",
      "  episode_reward_min: -12.759258838943998\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 380\n",
      "  experiment_id: 2b7c5715c95c4828907cbc45e49df6f0\n",
      "  hostname: michael-home\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 9.094947017729283e-14\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -2.5419192157733215\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014468748580561103\n",
      "          policy_loss: 0.0014821354563757303\n",
      "          total_loss: 310.53074796154243\n",
      "          vf_explained_var: -2.8093909421045282e-09\n",
      "          vf_loss: 310.5292654025327\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1520000\n",
      "    num_steps_sampled: 1520000\n",
      "    num_steps_trained: 1520000\n",
      "  iterations_since_restore: 76\n",
      "  node_ip: 192.168.0.33\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 30.689885057471262\n",
      "    ram_util_percent: 66.61632183908047\n",
      "  pid: 25804\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09889722954225641\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.438874385793024\n",
      "    mean_inference_ms: 2.0098238867491545\n",
      "    mean_raw_obs_processing_ms: 21.686061327875418\n",
      "  time_since_restore: 22996.03960108757\n",
      "  time_this_iter_s: 304.7712399959564\n",
      "  time_total_s: 22996.03960108757\n",
      "  timers:\n",
      "    learn_throughput: 243.522\n",
      "    learn_time_ms: 82127.965\n",
      "    sample_throughput: 90.801\n",
      "    sample_time_ms: 220263.044\n",
      "    update_time_ms: 3.096\n",
      "  timestamp: 1659386589\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1520000\n",
      "  training_iteration: 76\n",
      "  trial_id: fd1ac_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 20.6/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/4.91 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_1_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_0_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_4_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_2_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_3_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_5_28b5456e04c30e30e910ccff5ed5840d, 0.0/6.0 CPU_group_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 accelerator_type:G)\n",
      "Result logdir: /home/michael/ray_results/stabilizing_the_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_fd1ac_00000 | RUNNING  | 192.168.0.33:25804 |     76 |            22996 | 1520000 |  1828.11 |              3415.29 |             -12.7593 |               4000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m ring length: 238\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m v_max: 4.094180836186086\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m ring length: 244\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m v_max: 4.379503211387676\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m ring length: 270\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m v_max: 5.6143732387852054\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m ring length: 245\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m v_max: 4.427046998576354\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m ring length: 238\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m v_max: 4.094180836186086\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_fd1ac_00000:\n",
      "  agent_timesteps_total: 1540000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-01_15-48-12\n",
      "  done: false\n",
      "  episode_len_mean: 4000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3415.287774190107\n",
      "  episode_reward_mean: 1890.6843546287128\n",
      "  episode_reward_min: 627.8091599315763\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 385\n",
      "  experiment_id: 2b7c5715c95c4828907cbc45e49df6f0\n",
      "  hostname: michael-home\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 9.094947017729283e-14\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -2.5225538648617496\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016730167892198784\n",
      "          policy_loss: 0.0013323606149839226\n",
      "          total_loss: 306.0901547960415\n",
      "          vf_explained_var: -1.8982371230436e-09\n",
      "          vf_loss: 306.08882235508815\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1540000\n",
      "    num_steps_sampled: 1540000\n",
      "    num_steps_trained: 1540000\n",
      "  iterations_since_restore: 77\n",
      "  node_ip: 192.168.0.33\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 27.73333333333333\n",
      "    ram_util_percent: 65.90532407407407\n",
      "  pid: 25804\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09894636155521255\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.432778215761275\n",
      "    mean_inference_ms: 2.011306830687265\n",
      "    mean_raw_obs_processing_ms: 21.68586337472474\n",
      "  time_since_restore: 23298.8639690876\n",
      "  time_this_iter_s: 302.8243680000305\n",
      "  time_total_s: 23298.8639690876\n",
      "  timers:\n",
      "    learn_throughput: 243.979\n",
      "    learn_time_ms: 81974.213\n",
      "    sample_throughput: 90.797\n",
      "    sample_time_ms: 220271.509\n",
      "    update_time_ms: 3.0\n",
      "  timestamp: 1659386892\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1540000\n",
      "  training_iteration: 77\n",
      "  trial_id: fd1ac_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 20.7/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/4.91 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_4_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_1_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_2_28b5456e04c30e30e910ccff5ed5840d, 0.0/6.0 CPU_group_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_5_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_0_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_3_28b5456e04c30e30e910ccff5ed5840d)\n",
      "Result logdir: /home/michael/ray_results/stabilizing_the_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_fd1ac_00000 | RUNNING  | 192.168.0.33:25804 |     77 |          23298.9 | 1540000 |  1890.68 |              3415.29 |              627.809 |               4000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m ring length: 261\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m v_max: 5.187261846097525\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m ring length: 269\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m v_max: 5.566938458220921\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m ring length: 242\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m v_max: 4.2844067680020546\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m ring length: 235\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m v_max: 3.9514850725282584\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m ring length: 268\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m v_max: 5.5194978538368025\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_fd1ac_00000:\n",
      "  agent_timesteps_total: 1560000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-01_15-53-14\n",
      "  done: false\n",
      "  episode_len_mean: 4000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3415.287774190107\n",
      "  episode_reward_mean: 1873.641437654922\n",
      "  episode_reward_min: 516.5139903495376\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 390\n",
      "  experiment_id: 2b7c5715c95c4828907cbc45e49df6f0\n",
      "  hostname: michael-home\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 9.094947017729283e-14\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -2.538451498784837\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01532171108507103\n",
      "          policy_loss: 0.0018887745806721935\n",
      "          total_loss: 325.9060700993629\n",
      "          vf_explained_var: -2.201955062730576e-09\n",
      "          vf_loss: 325.9041813479867\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1560000\n",
      "    num_steps_sampled: 1560000\n",
      "    num_steps_trained: 1560000\n",
      "  iterations_since_restore: 78\n",
      "  node_ip: 192.168.0.33\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 32.346635730858466\n",
      "    ram_util_percent: 66.62389791183296\n",
      "  pid: 25804\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09899429752206869\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.426913250959196\n",
      "    mean_inference_ms: 2.012712796830714\n",
      "    mean_raw_obs_processing_ms: 21.685657867828628\n",
      "  time_since_restore: 23601.534106969833\n",
      "  time_this_iter_s: 302.67013788223267\n",
      "  time_total_s: 23601.534106969833\n",
      "  timers:\n",
      "    learn_throughput: 243.412\n",
      "    learn_time_ms: 82165.139\n",
      "    sample_throughput: 90.791\n",
      "    sample_time_ms: 220285.366\n",
      "    update_time_ms: 3.049\n",
      "  timestamp: 1659387194\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1560000\n",
      "  training_iteration: 78\n",
      "  trial_id: fd1ac_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 20.7/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/4.91 GiB heap, 0.0/14.65 GiB objects (0.0/6.0 CPU_group_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_2_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_0_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_3_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_5_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_4_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_1_28b5456e04c30e30e910ccff5ed5840d)\n",
      "Result logdir: /home/michael/ray_results/stabilizing_the_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_fd1ac_00000 | RUNNING  | 192.168.0.33:25804 |     78 |          23601.5 | 1560000 |  1873.64 |              3415.29 |              516.514 |               4000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m ring length: 243\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m v_max: 4.331956438196479\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m ring length: 226\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m v_max: 3.5232840694769565\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m ring length: 239\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m v_max: 4.141741239205832\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m ring length: 234\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m v_max: 3.903915223349057\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m ring length: 254\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m v_max: 4.854791141191876\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_fd1ac_00000:\n",
      "  agent_timesteps_total: 1580000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-01_15-58-17\n",
      "  done: false\n",
      "  episode_len_mean: 4000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3415.287774190107\n",
      "  episode_reward_mean: 1898.51574782981\n",
      "  episode_reward_min: 516.5139903495376\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 395\n",
      "  experiment_id: 2b7c5715c95c4828907cbc45e49df6f0\n",
      "  hostname: michael-home\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 9.094947017729283e-14\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -2.658231842290064\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01957254297905286\n",
      "          policy_loss: 0.00355236090624788\n",
      "          total_loss: 304.5779357363464\n",
      "          vf_explained_var: -8.352243341391841e-10\n",
      "          vf_loss: 304.57438242481015\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1580000\n",
      "    num_steps_sampled: 1580000\n",
      "    num_steps_trained: 1580000\n",
      "  iterations_since_restore: 79\n",
      "  node_ip: 192.168.0.33\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 29.601620370370373\n",
      "    ram_util_percent: 66.81157407407407\n",
      "  pid: 25804\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09903788479791074\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.421336569011046\n",
      "    mean_inference_ms: 2.013996790784259\n",
      "    mean_raw_obs_processing_ms: 21.68548747791065\n",
      "  time_since_restore: 23904.068538188934\n",
      "  time_this_iter_s: 302.53443121910095\n",
      "  time_total_s: 23904.068538188934\n",
      "  timers:\n",
      "    learn_throughput: 243.022\n",
      "    learn_time_ms: 82297.074\n",
      "    sample_throughput: 90.775\n",
      "    sample_time_ms: 220324.057\n",
      "    update_time_ms: 3.105\n",
      "  timestamp: 1659387497\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1580000\n",
      "  training_iteration: 79\n",
      "  trial_id: fd1ac_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 21.0/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/4.91 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_0_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_4_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_3_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_2_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_5_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_1_28b5456e04c30e30e910ccff5ed5840d, 0.0/6.0 CPU_group_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 accelerator_type:G)\n",
      "Result logdir: /home/michael/ray_results/stabilizing_the_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_fd1ac_00000 | RUNNING  | 192.168.0.33:25804 |     79 |          23904.1 | 1580000 |  1898.52 |              3415.29 |              516.514 |               4000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m ring length: 233\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m v_max: 3.856343201216534\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m ring length: 262\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m v_max: 5.234739483008334\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m ring length: 259\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m v_max: 5.092292348291853\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m ring length: 241\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m v_max: 4.236854288051661\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m ring length: 266\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m v_max: 5.42459972166245\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_fd1ac_00000:\n",
      "  agent_timesteps_total: 1600000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-01_16-03-18\n",
      "  done: false\n",
      "  episode_len_mean: 4000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3415.287774190107\n",
      "  episode_reward_mean: 1895.9456807912456\n",
      "  episode_reward_min: 516.5139903495376\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 400\n",
      "  experiment_id: 2b7c5715c95c4828907cbc45e49df6f0\n",
      "  hostname: michael-home\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 9.094947017729283e-14\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -2.6914268428352988\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015907207116148\n",
      "          policy_loss: 0.0019211264159589721\n",
      "          total_loss: 303.7625168235439\n",
      "          vf_explained_var: -5.4289581719046964e-09\n",
      "          vf_loss: 303.7605957225629\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1600000\n",
      "    num_steps_sampled: 1600000\n",
      "    num_steps_trained: 1600000\n",
      "  iterations_since_restore: 80\n",
      "  node_ip: 192.168.0.33\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 27.239767441860465\n",
      "    ram_util_percent: 67.04255813953488\n",
      "  pid: 25804\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09908068127423314\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.4159556237071\n",
      "    mean_inference_ms: 2.015231754526624\n",
      "    mean_raw_obs_processing_ms: 21.685264589026204\n",
      "  time_since_restore: 24205.122613430023\n",
      "  time_this_iter_s: 301.05407524108887\n",
      "  time_total_s: 24205.122613430023\n",
      "  timers:\n",
      "    learn_throughput: 243.049\n",
      "    learn_time_ms: 82287.92\n",
      "    sample_throughput: 90.77\n",
      "    sample_time_ms: 220337.382\n",
      "    update_time_ms: 3.086\n",
      "  timestamp: 1659387798\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1600000\n",
      "  training_iteration: 80\n",
      "  trial_id: fd1ac_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 20.9/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/4.91 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_1_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_2_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_0_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_3_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 accelerator_type:G, 0.0/6.0 CPU_group_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_5_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_4_28b5456e04c30e30e910ccff5ed5840d)\n",
      "Result logdir: /home/michael/ray_results/stabilizing_the_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_fd1ac_00000 | RUNNING  | 192.168.0.33:25804 |     80 |          24205.1 | 1600000 |  1895.95 |              3415.29 |              516.514 |               4000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m ring length: 268\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m v_max: 5.5194978538368025\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m ring length: 248\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m v_max: 4.569659527528101\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m ring length: 249\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m v_max: 4.617190445131122\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m ring length: 239\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m v_max: 4.141741239205832\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m ring length: 251\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m v_max: 4.71224180704279\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_fd1ac_00000:\n",
      "  agent_timesteps_total: 1620000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-01_16-08-18\n",
      "  done: false\n",
      "  episode_len_mean: 4000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3415.287774190107\n",
      "  episode_reward_mean: 1930.5387300940085\n",
      "  episode_reward_min: 516.5139903495376\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 405\n",
      "  experiment_id: 2b7c5715c95c4828907cbc45e49df6f0\n",
      "  hostname: michael-home\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 9.094947017729283e-14\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -2.738918813019042\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0181152947545667\n",
      "          policy_loss: 0.00363110271979503\n",
      "          total_loss: 343.7565207560351\n",
      "          vf_explained_var: -6.07435879373952e-10\n",
      "          vf_loss: 343.75288744397983\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1620000\n",
      "    num_steps_sampled: 1620000\n",
      "    num_steps_trained: 1620000\n",
      "  iterations_since_restore: 81\n",
      "  node_ip: 192.168.0.33\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 26.186682242990653\n",
      "    ram_util_percent: 67.08504672897196\n",
      "  pid: 25804\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09911652589498698\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.410861993273397\n",
      "    mean_inference_ms: 2.016274853393876\n",
      "    mean_raw_obs_processing_ms: 21.685055766192587\n",
      "  time_since_restore: 24505.180095672607\n",
      "  time_this_iter_s: 300.05748224258423\n",
      "  time_total_s: 24505.180095672607\n",
      "  timers:\n",
      "    learn_throughput: 244.195\n",
      "    learn_time_ms: 81901.917\n",
      "    sample_throughput: 90.781\n",
      "    sample_time_ms: 220310.353\n",
      "    update_time_ms: 3.059\n",
      "  timestamp: 1659388098\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1620000\n",
      "  training_iteration: 81\n",
      "  trial_id: fd1ac_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 21.0/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/4.91 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_5_28b5456e04c30e30e910ccff5ed5840d, 0.0/6.0 CPU_group_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_1_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_3_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_4_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_2_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_0_28b5456e04c30e30e910ccff5ed5840d)\n",
      "Result logdir: /home/michael/ray_results/stabilizing_the_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_fd1ac_00000 | RUNNING  | 192.168.0.33:25804 |     81 |          24505.2 | 1620000 |  1930.54 |              3415.29 |              516.514 |               4000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m ring length: 270\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m v_max: 5.6143732387852054\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m ring length: 269\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m v_max: 5.566938458220921\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m ring length: 257\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m v_max: 4.997304682316517\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m ring length: 221\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m v_max: 3.285334164169417\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m ring length: 253\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m v_max: 4.807278529691987\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_fd1ac_00000:\n",
      "  agent_timesteps_total: 1640000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-01_16-13-21\n",
      "  done: false\n",
      "  episode_len_mean: 4000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3415.287774190107\n",
      "  episode_reward_mean: 1956.511095134253\n",
      "  episode_reward_min: 516.5139903495376\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 410\n",
      "  experiment_id: 2b7c5715c95c4828907cbc45e49df6f0\n",
      "  hostname: michael-home\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 9.094947017729283e-14\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -2.823484354717716\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.020549791455150367\n",
      "          policy_loss: 0.003441675737423075\n",
      "          total_loss: 347.43632343802483\n",
      "          vf_explained_var: -3.834438988548072e-09\n",
      "          vf_loss: 347.43288222877845\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1640000\n",
      "    num_steps_sampled: 1640000\n",
      "    num_steps_trained: 1640000\n",
      "  iterations_since_restore: 82\n",
      "  node_ip: 192.168.0.33\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 27.95578703703704\n",
      "    ram_util_percent: 66.71412037037037\n",
      "  pid: 25804\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09915016423198092\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.405984819108394\n",
      "    mean_inference_ms: 2.017258263583793\n",
      "    mean_raw_obs_processing_ms: 21.684801843958166\n",
      "  time_since_restore: 24808.108407258987\n",
      "  time_this_iter_s: 302.92831158638\n",
      "  time_total_s: 24808.108407258987\n",
      "  timers:\n",
      "    learn_throughput: 243.727\n",
      "    learn_time_ms: 82059.149\n",
      "    sample_throughput: 90.782\n",
      "    sample_time_ms: 220307.305\n",
      "    update_time_ms: 3.048\n",
      "  timestamp: 1659388401\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1640000\n",
      "  training_iteration: 82\n",
      "  trial_id: fd1ac_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 20.6/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/4.91 GiB heap, 0.0/14.65 GiB objects (0.0/6.0 CPU_group_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_1_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_5_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_0_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_3_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_4_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_2_28b5456e04c30e30e910ccff5ed5840d)\n",
      "Result logdir: /home/michael/ray_results/stabilizing_the_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_fd1ac_00000 | RUNNING  | 192.168.0.33:25804 |     82 |          24808.1 | 1640000 |  1956.51 |              3415.29 |              516.514 |               4000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m ring length: 223\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m v_max: 3.3805185755587788\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m ring length: 232\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m v_max: 3.8087690783250063\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m ring length: 223\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m v_max: 3.3805185755587788\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m ring length: 254\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m v_max: 4.854791141191876\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m ring length: 262\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m v_max: 5.234739483008334\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_fd1ac_00000:\n",
      "  agent_timesteps_total: 1660000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-01_16-18-21\n",
      "  done: false\n",
      "  episode_len_mean: 4000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3415.287774190107\n",
      "  episode_reward_mean: 1982.737796957983\n",
      "  episode_reward_min: 516.5139903495376\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 415\n",
      "  experiment_id: 2b7c5715c95c4828907cbc45e49df6f0\n",
      "  hostname: michael-home\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 9.094947017729283e-14\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -2.7661249418926848\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01713644842326917\n",
      "          policy_loss: 0.002736216498491396\n",
      "          total_loss: 370.31958604314525\n",
      "          vf_explained_var: 3.03717939686976e-10\n",
      "          vf_loss: 370.3168501106797\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1660000\n",
      "    num_steps_sampled: 1660000\n",
      "    num_steps_trained: 1660000\n",
      "  iterations_since_restore: 83\n",
      "  node_ip: 192.168.0.33\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 26.692757009345794\n",
      "    ram_util_percent: 65.98037383177571\n",
      "  pid: 25804\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09917842338025835\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.401396049506385\n",
      "    mean_inference_ms: 2.0180916726421314\n",
      "    mean_raw_obs_processing_ms: 21.684531154432804\n",
      "  time_since_restore: 25108.05655813217\n",
      "  time_this_iter_s: 299.9481508731842\n",
      "  time_total_s: 25108.05655813217\n",
      "  timers:\n",
      "    learn_throughput: 244.105\n",
      "    learn_time_ms: 81932.095\n",
      "    sample_throughput: 90.818\n",
      "    sample_time_ms: 220221.148\n",
      "    update_time_ms: 3.055\n",
      "  timestamp: 1659388701\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1660000\n",
      "  training_iteration: 83\n",
      "  trial_id: fd1ac_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 20.6/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/4.91 GiB heap, 0.0/14.65 GiB objects (0.0/6.0 CPU_group_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_1_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_0_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_3_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_4_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_2_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_5_28b5456e04c30e30e910ccff5ed5840d)\n",
      "Result logdir: /home/michael/ray_results/stabilizing_the_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_fd1ac_00000 | RUNNING  | 192.168.0.33:25804 |     83 |          25108.1 | 1660000 |  1982.74 |              3415.29 |              516.514 |               4000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m ring length: 245\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m v_max: 4.427046998576354\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m ring length: 253\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m v_max: 4.807278529691987\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m ring length: 268\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m v_max: 5.5194978538368025\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m ring length: 226\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m v_max: 3.5232840694769565\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m ring length: 256\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m v_max: 4.949804327743507\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_fd1ac_00000:\n",
      "  agent_timesteps_total: 1680000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-01_16-23-22\n",
      "  done: false\n",
      "  episode_len_mean: 4000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3415.287774190107\n",
      "  episode_reward_mean: 1979.4599917866958\n",
      "  episode_reward_min: 516.5139903495376\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 420\n",
      "  experiment_id: 2b7c5715c95c4828907cbc45e49df6f0\n",
      "  hostname: michael-home\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 9.094947017729283e-14\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -2.7742475210481388\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.020073692717480274\n",
      "          policy_loss: 0.003424272515105119\n",
      "          total_loss: 370.0504825300472\n",
      "          vf_explained_var: -7.592948492174401e-10\n",
      "          vf_loss: 370.04705749074367\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1680000\n",
      "    num_steps_sampled: 1680000\n",
      "    num_steps_trained: 1680000\n",
      "  iterations_since_restore: 84\n",
      "  node_ip: 192.168.0.33\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 26.51585081585082\n",
      "    ram_util_percent: 66.1\n",
      "  pid: 25804\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.099198504226193\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.397162794551882\n",
      "    mean_inference_ms: 2.018707868917775\n",
      "    mean_raw_obs_processing_ms: 21.684251121583166\n",
      "  time_since_restore: 25408.505558013916\n",
      "  time_this_iter_s: 300.4489998817444\n",
      "  time_total_s: 25408.505558013916\n",
      "  timers:\n",
      "    learn_throughput: 244.374\n",
      "    learn_time_ms: 81841.777\n",
      "    sample_throughput: 90.824\n",
      "    sample_time_ms: 220205.933\n",
      "    update_time_ms: 3.067\n",
      "  timestamp: 1659389002\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1680000\n",
      "  training_iteration: 84\n",
      "  trial_id: fd1ac_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 20.6/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/4.91 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_2_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_5_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_0_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_1_28b5456e04c30e30e910ccff5ed5840d, 0.0/6.0 CPU_group_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_4_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_3_28b5456e04c30e30e910ccff5ed5840d)\n",
      "Result logdir: /home/michael/ray_results/stabilizing_the_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_fd1ac_00000 | RUNNING  | 192.168.0.33:25804 |     84 |          25408.5 | 1680000 |  1979.46 |              3415.29 |              516.514 |               4000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m ring length: 263\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m v_max: 5.282212215151455\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m ring length: 262\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m v_max: 5.234739483008334\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m ring length: 252\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m v_max: 4.759762049824181\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m ring length: 222\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m v_max: 3.3329270738617227\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m ring length: 254\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m v_max: 4.854791141191876\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_fd1ac_00000:\n",
      "  agent_timesteps_total: 1700000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-01_16-28-23\n",
      "  done: false\n",
      "  episode_len_mean: 4000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3415.287774190107\n",
      "  episode_reward_mean: 1982.6722755831088\n",
      "  episode_reward_min: 516.5139903495376\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 425\n",
      "  experiment_id: 2b7c5715c95c4828907cbc45e49df6f0\n",
      "  hostname: michael-home\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 9.094947017729283e-14\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -2.817715877028787\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.018176007496747693\n",
      "          policy_loss: 0.003289576489804609\n",
      "          total_loss: 387.77987119832613\n",
      "          vf_explained_var: -1.8982371230436e-09\n",
      "          vf_loss: 387.77658290012624\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1700000\n",
      "    num_steps_sampled: 1700000\n",
      "    num_steps_trained: 1700000\n",
      "  iterations_since_restore: 85\n",
      "  node_ip: 192.168.0.33\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 27.08275058275058\n",
      "    ram_util_percent: 66.33240093240094\n",
      "  pid: 25804\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0992161730622569\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.39302623695379\n",
      "    mean_inference_ms: 2.019249921521263\n",
      "    mean_raw_obs_processing_ms: 21.683853551947486\n",
      "  time_since_restore: 25709.510143756866\n",
      "  time_this_iter_s: 301.00458574295044\n",
      "  time_total_s: 25709.510143756866\n",
      "  timers:\n",
      "    learn_throughput: 245.05\n",
      "    learn_time_ms: 81615.91\n",
      "    sample_throughput: 90.83\n",
      "    sample_time_ms: 220192.425\n",
      "    update_time_ms: 3.039\n",
      "  timestamp: 1659389303\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1700000\n",
      "  training_iteration: 85\n",
      "  trial_id: fd1ac_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 20.7/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/4.91 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_2_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_3_28b5456e04c30e30e910ccff5ed5840d, 0.0/6.0 CPU_group_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_4_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_1_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_0_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_5_28b5456e04c30e30e910ccff5ed5840d)\n",
      "Result logdir: /home/michael/ray_results/stabilizing_the_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_fd1ac_00000 | RUNNING  | 192.168.0.33:25804 |     85 |          25709.5 | 1700000 |  1982.67 |              3415.29 |              516.514 |               4000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m ring length: 242\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m v_max: 4.2844067680020546\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m ring length: 252\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m v_max: 4.759762049824181\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m ring length: 247\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m v_max: 4.522125250095652\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m ring length: 269\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m v_max: 5.566938458220921\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m ring length: 251\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m v_max: 4.71224180704279\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_fd1ac_00000:\n",
      "  agent_timesteps_total: 1720000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-01_16-33-25\n",
      "  done: false\n",
      "  episode_len_mean: 4000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3415.287774190107\n",
      "  episode_reward_mean: 1999.3731240327397\n",
      "  episode_reward_min: 516.5139903495376\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 430\n",
      "  experiment_id: 2b7c5715c95c4828907cbc45e49df6f0\n",
      "  hostname: michael-home\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 9.094947017729283e-14\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -2.798036224523168\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0164180889914259\n",
      "          policy_loss: 0.0025471079562179697\n",
      "          total_loss: 388.2600387111591\n",
      "          vf_explained_var: -9.073573004059199e-09\n",
      "          vf_loss: 388.2574914142584\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1720000\n",
      "    num_steps_sampled: 1720000\n",
      "    num_steps_trained: 1720000\n",
      "  iterations_since_restore: 86\n",
      "  node_ip: 192.168.0.33\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 26.700232018561483\n",
      "    ram_util_percent: 66.64547563805104\n",
      "  pid: 25804\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09922760459687441\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.38915636483039\n",
      "    mean_inference_ms: 2.019624335789166\n",
      "    mean_raw_obs_processing_ms: 21.68344111383569\n",
      "  time_since_restore: 26011.117345571518\n",
      "  time_this_iter_s: 301.6072018146515\n",
      "  time_total_s: 26011.117345571518\n",
      "  timers:\n",
      "    learn_throughput: 245.928\n",
      "    learn_time_ms: 81324.575\n",
      "    sample_throughput: 90.84\n",
      "    sample_time_ms: 220167.453\n",
      "    update_time_ms: 3.062\n",
      "  timestamp: 1659389605\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1720000\n",
      "  training_iteration: 86\n",
      "  trial_id: fd1ac_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 21.0/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/4.91 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_0_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_5_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_2_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_1_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_3_28b5456e04c30e30e910ccff5ed5840d, 0.0/6.0 CPU_group_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_4_28b5456e04c30e30e910ccff5ed5840d)\n",
      "Result logdir: /home/michael/ray_results/stabilizing_the_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_fd1ac_00000 | RUNNING  | 192.168.0.33:25804 |     86 |          26011.1 | 1720000 |  1999.37 |              3415.29 |              516.514 |               4000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m ring length: 240\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m v_max: 4.189299083856172\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m ring length: 224\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m v_max: 3.4281086136538996\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m ring length: 238\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m v_max: 4.094180836186086\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m ring length: 243\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m v_max: 4.331956438196479\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m ring length: 222\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m v_max: 3.3329270738617227\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_fd1ac_00000:\n",
      "  agent_timesteps_total: 1740000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-01_16-38-31\n",
      "  done: false\n",
      "  episode_len_mean: 4000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3415.287774190107\n",
      "  episode_reward_mean: 2035.0151769025856\n",
      "  episode_reward_min: 516.5139903495376\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 435\n",
      "  experiment_id: 2b7c5715c95c4828907cbc45e49df6f0\n",
      "  hostname: michael-home\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 9.094947017729283e-14\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -2.9101968652883152\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.024011927219716355\n",
      "          policy_loss: 0.0040804129779030375\n",
      "          total_loss: 404.08465404996446\n",
      "          vf_explained_var: -3.4168268214784803e-09\n",
      "          vf_loss: 404.08057676060184\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1740000\n",
      "    num_steps_sampled: 1740000\n",
      "    num_steps_trained: 1740000\n",
      "  iterations_since_restore: 87\n",
      "  node_ip: 192.168.0.33\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 29.116513761467893\n",
      "    ram_util_percent: 66.99266055045872\n",
      "  pid: 25804\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09923133229983128\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.38564585749776\n",
      "    mean_inference_ms: 2.019763143193059\n",
      "    mean_raw_obs_processing_ms: 21.683030159208748\n",
      "  time_since_restore: 26317.030930042267\n",
      "  time_this_iter_s: 305.9135844707489\n",
      "  time_total_s: 26317.030930042267\n",
      "  timers:\n",
      "    learn_throughput: 245.008\n",
      "    learn_time_ms: 81630.152\n",
      "    sample_throughput: 90.839\n",
      "    sample_time_ms: 220170.937\n",
      "    update_time_ms: 3.005\n",
      "  timestamp: 1659389911\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1740000\n",
      "  training_iteration: 87\n",
      "  trial_id: fd1ac_00000\n",
      "  \n",
      "\u001b[2m\u001b[36m(pid=25804)\u001b[0m 2022-08-01 16:38:31,027\tWARNING ppo.py:242 -- The magnitude of your environment rewards are more than 204.0x the scale of `vf_clip_param`. This means that it will take more than 204.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "== Status ==\n",
      "Memory usage on this node: 20.9/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/4.91 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_5_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_2_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_4_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_1_28b5456e04c30e30e910ccff5ed5840d, 0.0/6.0 CPU_group_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_0_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_3_28b5456e04c30e30e910ccff5ed5840d)\n",
      "Result logdir: /home/michael/ray_results/stabilizing_the_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_fd1ac_00000 | RUNNING  | 192.168.0.33:25804 |     87 |            26317 | 1740000 |  2035.02 |              3415.29 |              516.514 |               4000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m ring length: 258\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m v_max: 5.044800727535787\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m ring length: 267\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m v_max: 5.472051563171826\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m ring length: 252\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m v_max: 4.759762049824181\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m ring length: 254\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m v_max: 4.854791141191876\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m ring length: 261\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m v_max: 5.187261846097525\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_fd1ac_00000:\n",
      "  agent_timesteps_total: 1760000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-01_16-43-34\n",
      "  done: false\n",
      "  episode_len_mean: 4000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3415.287774190107\n",
      "  episode_reward_mean: 2010.6864886170852\n",
      "  episode_reward_min: 516.5139903495376\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 440\n",
      "  experiment_id: 2b7c5715c95c4828907cbc45e49df6f0\n",
      "  hostname: michael-home\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 9.094947017729283e-14\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -2.897121669987964\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.02681879687012161\n",
      "          policy_loss: 0.003092663308344544\n",
      "          total_loss: 407.3128925129107\n",
      "          vf_explained_var: -2.9992146544088882e-09\n",
      "          vf_loss: 407.30980069348766\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1760000\n",
      "    num_steps_sampled: 1760000\n",
      "    num_steps_trained: 1760000\n",
      "  iterations_since_restore: 88\n",
      "  node_ip: 192.168.0.33\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 28.75565819861432\n",
      "    ram_util_percent: 67.13972286374134\n",
      "  pid: 25804\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09923078622105351\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.382326363735146\n",
      "    mean_inference_ms: 2.019807383850239\n",
      "    mean_raw_obs_processing_ms: 21.68261769609968\n",
      "  time_since_restore: 26620.500834465027\n",
      "  time_this_iter_s: 303.46990442276\n",
      "  time_total_s: 26620.500834465027\n",
      "  timers:\n",
      "    learn_throughput: 244.717\n",
      "    learn_time_ms: 81726.942\n",
      "    sample_throughput: 90.845\n",
      "    sample_time_ms: 220154.087\n",
      "    update_time_ms: 3.002\n",
      "  timestamp: 1659390214\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1760000\n",
      "  training_iteration: 88\n",
      "  trial_id: fd1ac_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 20.9/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/4.91 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_1_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_2_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_0_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_3_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 accelerator_type:G, 0.0/6.0 CPU_group_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_4_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_5_28b5456e04c30e30e910ccff5ed5840d)\n",
      "Result logdir: /home/michael/ray_results/stabilizing_the_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_fd1ac_00000 | RUNNING  | 192.168.0.33:25804 |     88 |          26620.5 | 1760000 |  2010.69 |              3415.29 |              516.514 |               4000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=25804)\u001b[0m 2022-08-01 16:43:34,569\tWARNING ppo.py:242 -- The magnitude of your environment rewards are more than 201.0x the scale of `vf_clip_param`. This means that it will take more than 201.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m ring length: 220\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m v_max: 3.2377399006821497\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m ring length: 234\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m v_max: 3.903915223349057\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m ring length: 220\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m v_max: 3.2377399006821497\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m ring length: 260\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m v_max: 5.139779427502188\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m ring length: 256\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m v_max: 4.949804327743507\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_fd1ac_00000:\n",
      "  agent_timesteps_total: 1780000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-01_16-48-34\n",
      "  done: false\n",
      "  episode_len_mean: 4000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3415.287774190107\n",
      "  episode_reward_mean: 2044.0254344227205\n",
      "  episode_reward_min: 516.5139903495376\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 445\n",
      "  experiment_id: 2b7c5715c95c4828907cbc45e49df6f0\n",
      "  hostname: michael-home\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 9.094947017729283e-14\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -2.875780330342092\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.020957947947151077\n",
      "          policy_loss: 0.003092911940477314\n",
      "          total_loss: 432.4697479077965\n",
      "          vf_explained_var: -6.3780767334264965e-09\n",
      "          vf_loss: 432.4666566569334\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1780000\n",
      "    num_steps_sampled: 1780000\n",
      "    num_steps_trained: 1780000\n",
      "  iterations_since_restore: 89\n",
      "  node_ip: 192.168.0.33\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 26.86682242990654\n",
      "    ram_util_percent: 66.7282710280374\n",
      "  pid: 25804\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09923022609967443\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.379094850018884\n",
      "    mean_inference_ms: 2.0198461183243417\n",
      "    mean_raw_obs_processing_ms: 21.682226286110097\n",
      "  time_since_restore: 26920.482657670975\n",
      "  time_this_iter_s: 299.9818232059479\n",
      "  time_total_s: 26920.482657670975\n",
      "  timers:\n",
      "    learn_throughput: 245.368\n",
      "    learn_time_ms: 81510.31\n",
      "    sample_throughput: 90.861\n",
      "    sample_time_ms: 220115.594\n",
      "    update_time_ms: 2.947\n",
      "  timestamp: 1659390514\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1780000\n",
      "  training_iteration: 89\n",
      "  trial_id: fd1ac_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 20.8/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/4.91 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_4_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_5_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_1_28b5456e04c30e30e910ccff5ed5840d, 0.0/6.0 CPU_group_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_0_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_2_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_3_28b5456e04c30e30e910ccff5ed5840d)\n",
      "Result logdir: /home/michael/ray_results/stabilizing_the_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_fd1ac_00000 | RUNNING  | 192.168.0.33:25804 |     89 |          26920.5 | 1780000 |  2044.03 |              3415.29 |              516.514 |               4000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=25804)\u001b[0m 2022-08-01 16:48:34,631\tWARNING ppo.py:242 -- The magnitude of your environment rewards are more than 204.0x the scale of `vf_clip_param`. This means that it will take more than 204.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m ring length: 239\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m v_max: 4.141741239205832\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m ring length: 239\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m v_max: 4.141741239205832\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m ring length: 250\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m v_max: 4.664717904914125\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m ring length: 245\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m v_max: 4.427046998576354\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m ring length: 247\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m v_max: 4.522125250095652\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m  Retrying in 1 seconds\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_fd1ac_00000:\n",
      "  agent_timesteps_total: 1800000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-01_16-53-35\n",
      "  done: false\n",
      "  episode_len_mean: 4000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3415.287774190107\n",
      "  episode_reward_mean: 2043.891978340717\n",
      "  episode_reward_min: 516.5139903495376\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 450\n",
      "  experiment_id: 2b7c5715c95c4828907cbc45e49df6f0\n",
      "  hostname: michael-home\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 9.094947017729283e-14\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -2.935108118452084\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0234402672317879\n",
      "          policy_loss: 0.0041302906085327745\n",
      "          total_loss: 442.3772153149745\n",
      "          vf_explained_var: -1.176907016287032e-09\n",
      "          vf_loss: 442.3730851495342\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1800000\n",
      "    num_steps_sampled: 1800000\n",
      "    num_steps_trained: 1800000\n",
      "  iterations_since_restore: 90\n",
      "  node_ip: 192.168.0.33\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 26.453488372093023\n",
      "    ram_util_percent: 66.67953488372092\n",
      "  pid: 25804\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09923004623640098\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.375928203006147\n",
      "    mean_inference_ms: 2.0198860313022915\n",
      "    mean_raw_obs_processing_ms: 21.68199303077147\n",
      "  time_since_restore: 27221.424557447433\n",
      "  time_this_iter_s: 300.94189977645874\n",
      "  time_total_s: 27221.424557447433\n",
      "  timers:\n",
      "    learn_throughput: 245.641\n",
      "    learn_time_ms: 81419.645\n",
      "    sample_throughput: 90.829\n",
      "    sample_time_ms: 220194.898\n",
      "    update_time_ms: 2.952\n",
      "  timestamp: 1659390815\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1800000\n",
      "  training_iteration: 90\n",
      "  trial_id: fd1ac_00000\n",
      "  \n",
      "\u001b[2m\u001b[36m(pid=25804)\u001b[0m 2022-08-01 16:53:35,649\tWARNING ppo.py:242 -- The magnitude of your environment rewards are more than 204.0x the scale of `vf_clip_param`. This means that it will take more than 204.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Memory usage on this node: 20.8/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/4.91 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_2_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_4_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_5_28b5456e04c30e30e910ccff5ed5840d, 0.0/6.0 CPU_group_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_0_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_3_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_1_28b5456e04c30e30e910ccff5ed5840d)\n",
      "Result logdir: /home/michael/ray_results/stabilizing_the_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_fd1ac_00000 | RUNNING  | 192.168.0.33:25804 |     90 |          27221.4 | 1800000 |  2043.89 |              3415.29 |              516.514 |               4000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m ring length: 239\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m v_max: 4.141741239205832\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m ring length: 233\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m v_max: 3.856343201216534\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m ring length: 234\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m v_max: 3.903915223349057\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m ring length: 256\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m v_max: 4.949804327743507\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m ring length: 261\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m v_max: 5.187261846097525\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_fd1ac_00000:\n",
      "  agent_timesteps_total: 1820000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-01_16-58-36\n",
      "  done: false\n",
      "  episode_len_mean: 4000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3415.287774190107\n",
      "  episode_reward_mean: 2058.354985583076\n",
      "  episode_reward_min: 516.5139903495376\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 455\n",
      "  experiment_id: 2b7c5715c95c4828907cbc45e49df6f0\n",
      "  hostname: michael-home\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 9.094947017729283e-14\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -2.9270795251153836\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.025409549984589924\n",
      "          policy_loss: 0.004630820581965314\n",
      "          total_loss: 440.34619028613827\n",
      "          vf_explained_var: -1.4046954710522641e-09\n",
      "          vf_loss: 440.34155894115474\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1820000\n",
      "    num_steps_sampled: 1820000\n",
      "    num_steps_trained: 1820000\n",
      "  iterations_since_restore: 91\n",
      "  node_ip: 192.168.0.33\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 26.66799065420561\n",
      "    ram_util_percent: 67.02686915887851\n",
      "  pid: 25804\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09922497782356515\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.37298055497328\n",
      "    mean_inference_ms: 2.019791290093268\n",
      "    mean_raw_obs_processing_ms: 21.681720415583506\n",
      "  time_since_restore: 27521.788953065872\n",
      "  time_this_iter_s: 300.3643956184387\n",
      "  time_total_s: 27521.788953065872\n",
      "  timers:\n",
      "    learn_throughput: 245.527\n",
      "    learn_time_ms: 81457.457\n",
      "    sample_throughput: 90.832\n",
      "    sample_time_ms: 220187.818\n",
      "    update_time_ms: 2.951\n",
      "  timestamp: 1659391116\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1820000\n",
      "  training_iteration: 91\n",
      "  trial_id: fd1ac_00000\n",
      "  \n",
      "\u001b[2m\u001b[36m(pid=25804)\u001b[0m 2022-08-01 16:58:36,087\tWARNING ppo.py:242 -- The magnitude of your environment rewards are more than 206.0x the scale of `vf_clip_param`. This means that it will take more than 206.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "== Status ==\n",
      "Memory usage on this node: 20.9/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/4.91 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 accelerator_type:G, 0.0/6.0 CPU_group_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_1_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_4_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_0_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_2_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_3_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_5_28b5456e04c30e30e910ccff5ed5840d)\n",
      "Result logdir: /home/michael/ray_results/stabilizing_the_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_fd1ac_00000 | RUNNING  | 192.168.0.33:25804 |     91 |          27521.8 | 1820000 |  2058.35 |              3415.29 |              516.514 |               4000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m ring length: 251\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m v_max: 4.71224180704279\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m ring length: 265\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m v_max: 5.37714246265477\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m ring length: 257\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m v_max: 4.997304682316517\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m ring length: 243\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m v_max: 4.331956438196479\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m ring length: 251\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m v_max: 4.71224180704279\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_fd1ac_00000:\n",
      "  agent_timesteps_total: 1840000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-01_17-03-39\n",
      "  done: false\n",
      "  episode_len_mean: 4000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3415.287774190107\n",
      "  episode_reward_mean: 2077.2015797540994\n",
      "  episode_reward_min: 516.5139903495376\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 460\n",
      "  experiment_id: 2b7c5715c95c4828907cbc45e49df6f0\n",
      "  hostname: michael-home\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 9.094947017729283e-14\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -2.919917724998134\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.021338425103740923\n",
      "          policy_loss: 0.0027578449637516386\n",
      "          total_loss: 456.58690988941555\n",
      "          vf_explained_var: -5.6567466266699284e-09\n",
      "          vf_loss: 456.58415031190134\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1840000\n",
      "    num_steps_sampled: 1840000\n",
      "    num_steps_trained: 1840000\n",
      "  iterations_since_restore: 92\n",
      "  node_ip: 192.168.0.33\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 28.241339491916857\n",
      "    ram_util_percent: 66.98637413394918\n",
      "  pid: 25804\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09921536955728921\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.370194203831375\n",
      "    mean_inference_ms: 2.0196019327385804\n",
      "    mean_raw_obs_processing_ms: 21.68146099030287\n",
      "  time_since_restore: 27825.38761639595\n",
      "  time_this_iter_s: 303.5986633300781\n",
      "  time_total_s: 27825.38761639595\n",
      "  timers:\n",
      "    learn_throughput: 245.357\n",
      "    learn_time_ms: 81513.873\n",
      "    sample_throughput: 90.827\n",
      "    sample_time_ms: 220198.321\n",
      "    update_time_ms: 2.947\n",
      "  timestamp: 1659391419\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1840000\n",
      "  training_iteration: 92\n",
      "  trial_id: fd1ac_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=25804)\u001b[0m 2022-08-01 17:03:39,763\tWARNING ppo.py:242 -- The magnitude of your environment rewards are more than 208.0x the scale of `vf_clip_param`. This means that it will take more than 208.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "== Status ==\n",
      "Memory usage on this node: 20.5/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/4.91 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_1_28b5456e04c30e30e910ccff5ed5840d, 0.0/6.0 CPU_group_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_4_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_2_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_0_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_5_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_3_28b5456e04c30e30e910ccff5ed5840d)\n",
      "Result logdir: /home/michael/ray_results/stabilizing_the_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_fd1ac_00000 | RUNNING  | 192.168.0.33:25804 |     92 |          27825.4 | 1840000 |   2077.2 |              3415.29 |              516.514 |               4000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m ring length: 246\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m v_max: 4.474587708995648\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m ring length: 237\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m v_max: 4.04661795519353\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m ring length: 230\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m v_max: 3.7136148111012934\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m ring length: 260\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m v_max: 5.139779427502188\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m ring length: 238\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m v_max: 4.094180836186086\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_fd1ac_00000:\n",
      "  agent_timesteps_total: 1860000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-01_17-08-41\n",
      "  done: false\n",
      "  episode_len_mean: 4000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3415.287774190107\n",
      "  episode_reward_mean: 2107.9809339077306\n",
      "  episode_reward_min: 516.5139903495376\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 465\n",
      "  experiment_id: 2b7c5715c95c4828907cbc45e49df6f0\n",
      "  hostname: michael-home\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 9.094947017729283e-14\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -2.960186805239149\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.020348687405893864\n",
      "          policy_loss: 0.0029005474695686703\n",
      "          total_loss: 474.30520253029596\n",
      "          vf_explained_var: -1.3667307285913921e-09\n",
      "          vf_loss: 474.30230527987146\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1860000\n",
      "    num_steps_sampled: 1860000\n",
      "    num_steps_trained: 1860000\n",
      "  iterations_since_restore: 93\n",
      "  node_ip: 192.168.0.33\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 29.349187935034802\n",
      "    ram_util_percent: 65.41461716937354\n",
      "  pid: 25804\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09921267605217529\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.36713122581079\n",
      "    mean_inference_ms: 2.0196176428644557\n",
      "    mean_raw_obs_processing_ms: 21.681189294815464\n",
      "  time_since_restore: 28126.87088084221\n",
      "  time_this_iter_s: 301.48326444625854\n",
      "  time_total_s: 28126.87088084221\n",
      "  timers:\n",
      "    learn_throughput: 244.887\n",
      "    learn_time_ms: 81670.403\n",
      "    sample_throughput: 90.828\n",
      "    sample_time_ms: 220195.387\n",
      "    update_time_ms: 2.943\n",
      "  timestamp: 1659391721\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1860000\n",
      "  training_iteration: 93\n",
      "  trial_id: fd1ac_00000\n",
      "  \n",
      "\u001b[2m\u001b[36m(pid=25804)\u001b[0m 2022-08-01 17:08:41,316\tWARNING ppo.py:242 -- The magnitude of your environment rewards are more than 211.0x the scale of `vf_clip_param`. This means that it will take more than 211.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "== Status ==\n",
      "Memory usage on this node: 20.6/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/4.91 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_4_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_0_28b5456e04c30e30e910ccff5ed5840d, 0.0/6.0 CPU_group_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_2_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_1_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_5_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_3_28b5456e04c30e30e910ccff5ed5840d)\n",
      "Result logdir: /home/michael/ray_results/stabilizing_the_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_fd1ac_00000 | RUNNING  | 192.168.0.33:25804 |     93 |          28126.9 | 1860000 |  2107.98 |              3415.29 |              516.514 |               4000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m ring length: 247\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m v_max: 4.522125250095652\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m ring length: 252\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m v_max: 4.759762049824181\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m ring length: 240\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m v_max: 4.189299083856172\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m ring length: 248\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m v_max: 4.569659527528101\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m ring length: 266\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m v_max: 5.42459972166245\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_fd1ac_00000:\n",
      "  agent_timesteps_total: 1880000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-01_17-13-43\n",
      "  done: false\n",
      "  episode_len_mean: 4000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3415.287774190107\n",
      "  episode_reward_mean: 2103.1814484857173\n",
      "  episode_reward_min: 516.5139903495376\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 470\n",
      "  experiment_id: 2b7c5715c95c4828907cbc45e49df6f0\n",
      "  hostname: michael-home\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 9.094947017729283e-14\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -2.9971925252562115\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.02331792643413505\n",
      "          policy_loss: 0.003475210703182728\n",
      "          total_loss: 476.21889329290696\n",
      "          vf_explained_var: -3.2649678516349923e-09\n",
      "          vf_loss: 476.21541967695686\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1880000\n",
      "    num_steps_sampled: 1880000\n",
      "    num_steps_trained: 1880000\n",
      "  iterations_since_restore: 94\n",
      "  node_ip: 192.168.0.33\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 28.874883720930235\n",
      "    ram_util_percent: 65.86418604651162\n",
      "  pid: 25804\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09921151730045175\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.364101989784814\n",
      "    mean_inference_ms: 2.0196789932261185\n",
      "    mean_raw_obs_processing_ms: 21.68091558234321\n",
      "  time_since_restore: 28428.77443933487\n",
      "  time_this_iter_s: 301.9035584926605\n",
      "  time_total_s: 28428.77443933487\n",
      "  timers:\n",
      "    learn_throughput: 244.52\n",
      "    learn_time_ms: 81793.045\n",
      "    sample_throughput: 90.819\n",
      "    sample_time_ms: 220218.221\n",
      "    update_time_ms: 2.929\n",
      "  timestamp: 1659392023\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1880000\n",
      "  training_iteration: 94\n",
      "  trial_id: fd1ac_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=25804)\u001b[0m 2022-08-01 17:13:43,293\tWARNING ppo.py:242 -- The magnitude of your environment rewards are more than 210.0x the scale of `vf_clip_param`. This means that it will take more than 210.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "== Status ==\n",
      "Memory usage on this node: 20.5/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/4.91 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_2_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_5_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_1_28b5456e04c30e30e910ccff5ed5840d, 0.0/6.0 CPU_group_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_0_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_4_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_3_28b5456e04c30e30e910ccff5ed5840d)\n",
      "Result logdir: /home/michael/ray_results/stabilizing_the_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_fd1ac_00000 | RUNNING  | 192.168.0.33:25804 |     94 |          28428.8 | 1880000 |  2103.18 |              3415.29 |              516.514 |               4000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m ring length: 256\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m v_max: 4.949804327743507\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m ring length: 267\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m v_max: 5.472051563171826\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m ring length: 243\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m v_max: 4.331956438196479\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m ring length: 232\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m v_max: 3.8087690783250063\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m ring length: 259\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m v_max: 5.092292348291853\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_fd1ac_00000:\n",
      "  agent_timesteps_total: 1900000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-01_17-18-48\n",
      "  done: false\n",
      "  episode_len_mean: 4000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3319.8176460712125\n",
      "  episode_reward_mean: 2099.4118036818477\n",
      "  episode_reward_min: 516.5139903495376\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 475\n",
      "  experiment_id: 2b7c5715c95c4828907cbc45e49df6f0\n",
      "  hostname: michael-home\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 9.094947017729283e-14\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -2.9379945513549126\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017366813714298227\n",
      "          policy_loss: 0.0015041141201308959\n",
      "          total_loss: 488.0665440504718\n",
      "          vf_explained_var: -5.201169717139464e-09\n",
      "          vf_loss: 488.0650405956681\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1900000\n",
      "    num_steps_sampled: 1900000\n",
      "    num_steps_trained: 1900000\n",
      "  iterations_since_restore: 95\n",
      "  node_ip: 192.168.0.33\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 29.60666666666667\n",
      "    ram_util_percent: 66.0096551724138\n",
      "  pid: 25804\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09920880109302406\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.361196566532456\n",
      "    mean_inference_ms: 2.01969023410295\n",
      "    mean_raw_obs_processing_ms: 21.680640962919608\n",
      "  time_since_restore: 28733.490874052048\n",
      "  time_this_iter_s: 304.71643471717834\n",
      "  time_total_s: 28733.490874052048\n",
      "  timers:\n",
      "    learn_throughput: 243.461\n",
      "    learn_time_ms: 82148.667\n",
      "    sample_throughput: 90.813\n",
      "    sample_time_ms: 220233.663\n",
      "    update_time_ms: 2.983\n",
      "  timestamp: 1659392328\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1900000\n",
      "  training_iteration: 95\n",
      "  trial_id: fd1ac_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 20.7/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/4.91 GiB heap, 0.0/14.65 GiB objects (0.0/6.0 CPU_group_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_4_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_5_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_3_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_0_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_1_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_2_28b5456e04c30e30e910ccff5ed5840d)\n",
      "Result logdir: /home/michael/ray_results/stabilizing_the_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_fd1ac_00000 | RUNNING  | 192.168.0.33:25804 |     95 |          28733.5 | 1900000 |  2099.41 |              3319.82 |              516.514 |               4000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=25804)\u001b[0m 2022-08-01 17:18:48,097\tWARNING ppo.py:242 -- The magnitude of your environment rewards are more than 210.0x the scale of `vf_clip_param`. This means that it will take more than 210.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m ring length: 226\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m v_max: 3.5232840694769565\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m ring length: 266\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m v_max: 5.42459972166245\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m ring length: 230\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m v_max: 3.7136148111012934\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m ring length: 248\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m v_max: 4.569659527528101\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m ring length: 237\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m v_max: 4.04661795519353\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_fd1ac_00000:\n",
      "  agent_timesteps_total: 1920000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-01_17-23-49\n",
      "  done: false\n",
      "  episode_len_mean: 4000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3319.8176460712125\n",
      "  episode_reward_mean: 2125.478301804345\n",
      "  episode_reward_min: 516.5139903495376\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 480\n",
      "  experiment_id: 2b7c5715c95c4828907cbc45e49df6f0\n",
      "  hostname: michael-home\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 9.094947017729283e-14\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -3.0717717910268503\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.022812402064616124\n",
      "          policy_loss: 0.0032392845191002175\n",
      "          total_loss: 510.29256431190834\n",
      "          vf_explained_var: -7.592948492174401e-10\n",
      "          vf_loss: 510.28932453568575\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1920000\n",
      "    num_steps_sampled: 1920000\n",
      "    num_steps_trained: 1920000\n",
      "  iterations_since_restore: 96\n",
      "  node_ip: 192.168.0.33\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 29.046976744186047\n",
      "    ram_util_percent: 66.21302325581395\n",
      "  pid: 25804\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09920450948204287\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.358398615838393\n",
      "    mean_inference_ms: 2.019655546403101\n",
      "    mean_raw_obs_processing_ms: 21.68039942081194\n",
      "  time_since_restore: 29035.124310016632\n",
      "  time_this_iter_s: 301.63343596458435\n",
      "  time_total_s: 29035.124310016632\n",
      "  timers:\n",
      "    learn_throughput: 243.571\n",
      "    learn_time_ms: 82111.583\n",
      "    sample_throughput: 90.796\n",
      "    sample_time_ms: 220273.346\n",
      "    update_time_ms: 2.953\n",
      "  timestamp: 1659392629\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1920000\n",
      "  training_iteration: 96\n",
      "  trial_id: fd1ac_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=25804)\u001b[0m 2022-08-01 17:23:49,812\tWARNING ppo.py:242 -- The magnitude of your environment rewards are more than 213.0x the scale of `vf_clip_param`. This means that it will take more than 213.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "== Status ==\n",
      "Memory usage on this node: 20.6/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/4.91 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_1_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_2_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_4_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_5_28b5456e04c30e30e910ccff5ed5840d, 0.0/6.0 CPU_group_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_3_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_0_28b5456e04c30e30e910ccff5ed5840d)\n",
      "Result logdir: /home/michael/ray_results/stabilizing_the_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_fd1ac_00000 | RUNNING  | 192.168.0.33:25804 |     96 |          29035.1 | 1920000 |  2125.48 |              3319.82 |              516.514 |               4000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m ring length: 248\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m v_max: 4.569659527528101\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m ring length: 268\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m v_max: 5.5194978538368025\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m ring length: 230\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m v_max: 3.7136148111012934\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m ring length: 270\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m v_max: 5.6143732387852054\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m ring length: 229\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m v_max: 3.666034803266416\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_fd1ac_00000:\n",
      "  agent_timesteps_total: 1940000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-01_17-28-51\n",
      "  done: false\n",
      "  episode_len_mean: 4000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3319.8176460712125\n",
      "  episode_reward_mean: 2150.3092682772544\n",
      "  episode_reward_min: 516.5139903495376\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 485\n",
      "  experiment_id: 2b7c5715c95c4828907cbc45e49df6f0\n",
      "  hostname: michael-home\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 9.094947017729283e-14\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -3.1416062780246614\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.027763985420785797\n",
      "          policy_loss: 0.0037589943034954036\n",
      "          total_loss: 506.54250337880126\n",
      "          vf_explained_var: -4.707628065148128e-09\n",
      "          vf_loss: 506.53874464338753\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1940000\n",
      "    num_steps_sampled: 1940000\n",
      "    num_steps_trained: 1940000\n",
      "  iterations_since_restore: 97\n",
      "  node_ip: 192.168.0.33\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 27.17186046511628\n",
      "    ram_util_percent: 66.24674418604651\n",
      "  pid: 25804\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09919993758070529\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.355693574998266\n",
      "    mean_inference_ms: 2.019598656702069\n",
      "    mean_raw_obs_processing_ms: 21.680163374892242\n",
      "  time_since_restore: 29336.29975414276\n",
      "  time_this_iter_s: 301.17544412612915\n",
      "  time_total_s: 29336.29975414276\n",
      "  timers:\n",
      "    learn_throughput: 244.967\n",
      "    learn_time_ms: 81643.527\n",
      "    sample_throughput: 90.799\n",
      "    sample_time_ms: 220267.501\n",
      "    update_time_ms: 3.012\n",
      "  timestamp: 1659392931\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1940000\n",
      "  training_iteration: 97\n",
      "  trial_id: fd1ac_00000\n",
      "  \n",
      "\u001b[2m\u001b[36m(pid=25804)\u001b[0m 2022-08-01 17:28:51,052\tWARNING ppo.py:242 -- The magnitude of your environment rewards are more than 215.0x the scale of `vf_clip_param`. This means that it will take more than 215.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "== Status ==\n",
      "Memory usage on this node: 20.7/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/4.91 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_5_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_3_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_2_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_1_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_0_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_4_28b5456e04c30e30e910ccff5ed5840d, 0.0/6.0 CPU_group_28b5456e04c30e30e910ccff5ed5840d)\n",
      "Result logdir: /home/michael/ray_results/stabilizing_the_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_fd1ac_00000 | RUNNING  | 192.168.0.33:25804 |     97 |          29336.3 | 1940000 |  2150.31 |              3319.82 |              516.514 |               4000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m ring length: 220\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m v_max: 3.2377399006821497\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m ring length: 268\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m v_max: 5.5194978538368025\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m ring length: 244\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m v_max: 4.379503211387676\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m ring length: 265\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m v_max: 5.37714246265477\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m ring length: 222\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m v_max: 3.3329270738617227\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_fd1ac_00000:\n",
      "  agent_timesteps_total: 1960000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-01_17-33-57\n",
      "  done: false\n",
      "  episode_len_mean: 4000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3319.8176460712125\n",
      "  episode_reward_mean: 2190.2830497158766\n",
      "  episode_reward_min: 835.9606573830774\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 490\n",
      "  experiment_id: 2b7c5715c95c4828907cbc45e49df6f0\n",
      "  hostname: michael-home\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 9.094947017729283e-14\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -3.125575438426558\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.021988740873172437\n",
      "          policy_loss: 0.004542546033871117\n",
      "          total_loss: 524.0576714922668\n",
      "          vf_explained_var: -3.1890383667132483e-09\n",
      "          vf_loss: 524.0531288195567\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1960000\n",
      "    num_steps_sampled: 1960000\n",
      "    num_steps_trained: 1960000\n",
      "  iterations_since_restore: 98\n",
      "  node_ip: 192.168.0.33\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 29.0675057208238\n",
      "    ram_util_percent: 66.63180778032036\n",
      "  pid: 25804\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09918773597001235\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.353283073783683\n",
      "    mean_inference_ms: 2.0193193857828735\n",
      "    mean_raw_obs_processing_ms: 21.67990765070827\n",
      "  time_since_restore: 29642.38682103157\n",
      "  time_this_iter_s: 306.0870668888092\n",
      "  time_total_s: 29642.38682103157\n",
      "  timers:\n",
      "    learn_throughput: 244.206\n",
      "    learn_time_ms: 81898.205\n",
      "    sample_throughput: 90.796\n",
      "    sample_time_ms: 220274.774\n",
      "    update_time_ms: 3.023\n",
      "  timestamp: 1659393237\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1960000\n",
      "  training_iteration: 98\n",
      "  trial_id: fd1ac_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Memory usage on this node: 20.8/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/4.91 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_1_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_5_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_2_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_3_28b5456e04c30e30e910ccff5ed5840d, 0.0/6.0 CPU_group_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_4_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_0_28b5456e04c30e30e910ccff5ed5840d)\n",
      "Result logdir: /home/michael/ray_results/stabilizing_the_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_fd1ac_00000 | RUNNING  | 192.168.0.33:25804 |     98 |          29642.4 | 1960000 |  2190.28 |              3319.82 |              835.961 |               4000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=25804)\u001b[0m 2022-08-01 17:33:57,221\tWARNING ppo.py:242 -- The magnitude of your environment rewards are more than 219.0x the scale of `vf_clip_param`. This means that it will take more than 219.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m ring length: 234\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m v_max: 3.903915223349057\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m ring length: 259\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m v_max: 5.092292348291853\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m ring length: 243\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m v_max: 4.331956438196479\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m ring length: 248\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m v_max: 4.569659527528101\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m ring length: 250\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m v_max: 4.664717904914125\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_fd1ac_00000:\n",
      "  agent_timesteps_total: 1980000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-01_17-39-03\n",
      "  done: false\n",
      "  episode_len_mean: 4000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3319.8176460712125\n",
      "  episode_reward_mean: 2224.288310708894\n",
      "  episode_reward_min: 1297.7837157306046\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 495\n",
      "  experiment_id: 2b7c5715c95c4828907cbc45e49df6f0\n",
      "  hostname: michael-home\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 9.094947017729283e-14\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -3.0680077009140305\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.06307454673261222\n",
      "          policy_loss: 0.006806251253179352\n",
      "          total_loss: 530.9972539209257\n",
      "          vf_explained_var: -2.7714261996436562e-09\n",
      "          vf_loss: 530.9904494000089\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1980000\n",
      "    num_steps_sampled: 1980000\n",
      "    num_steps_trained: 1980000\n",
      "  iterations_since_restore: 99\n",
      "  node_ip: 192.168.0.33\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 31.04403669724771\n",
      "    ram_util_percent: 67.04747706422019\n",
      "  pid: 25804\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09917616906055166\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.35090120597584\n",
      "    mean_inference_ms: 2.0190392410508577\n",
      "    mean_raw_obs_processing_ms: 21.679634172287802\n",
      "  time_since_restore: 29948.2255525589\n",
      "  time_this_iter_s: 305.8387315273285\n",
      "  time_total_s: 29948.2255525589\n",
      "  timers:\n",
      "    learn_throughput: 242.472\n",
      "    learn_time_ms: 82483.633\n",
      "    sample_throughput: 90.796\n",
      "    sample_time_ms: 220274.897\n",
      "    update_time_ms: 3.038\n",
      "  timestamp: 1659393543\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1980000\n",
      "  training_iteration: 99\n",
      "  trial_id: fd1ac_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 21.0/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/4.91 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_2_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_3_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_1_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_5_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_4_28b5456e04c30e30e910ccff5ed5840d, 0.0/6.0 CPU_group_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_0_28b5456e04c30e30e910ccff5ed5840d)\n",
      "Result logdir: /home/michael/ray_results/stabilizing_the_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_fd1ac_00000 | RUNNING  | 192.168.0.33:25804 |     99 |          29948.2 | 1980000 |  2224.29 |              3319.82 |              1297.78 |               4000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=25804)\u001b[0m 2022-08-01 17:39:03,139\tWARNING ppo.py:242 -- The magnitude of your environment rewards are more than 222.0x the scale of `vf_clip_param`. This means that it will take more than 222.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m ring length: 241\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m v_max: 4.236854288051661\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m ring length: 247\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m v_max: 4.522125250095652\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m ring length: 242\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m v_max: 4.2844067680020546\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m ring length: 248\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m v_max: 4.569659527528101\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m ring length: 245\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m v_max: 4.427046998576354\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_fd1ac_00000:\n",
      "  agent_timesteps_total: 2000000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-01_17-44-09\n",
      "  done: false\n",
      "  episode_len_mean: 4000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3319.8176460712125\n",
      "  episode_reward_mean: 2267.2201991244765\n",
      "  episode_reward_min: 1377.4501664711158\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 500\n",
      "  experiment_id: 2b7c5715c95c4828907cbc45e49df6f0\n",
      "  hostname: michael-home\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.3642420526593923e-13\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -3.061614361538249\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.019724618871857714\n",
      "          policy_loss: 0.003497569264167813\n",
      "          total_loss: 532.1054715296266\n",
      "          vf_explained_var: -9.111538190609281e-10\n",
      "          vf_loss: 532.1019732092597\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 2000000\n",
      "    num_steps_sampled: 2000000\n",
      "    num_steps_trained: 2000000\n",
      "  iterations_since_restore: 100\n",
      "  node_ip: 192.168.0.33\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 29.60091533180778\n",
      "    ram_util_percent: 67.07048054919908\n",
      "  pid: 25804\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09916397036459661\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.348562234302673\n",
      "    mean_inference_ms: 2.0187589026759487\n",
      "    mean_raw_obs_processing_ms: 21.67939766955075\n",
      "  time_since_restore: 30254.163664102554\n",
      "  time_this_iter_s: 305.9381115436554\n",
      "  time_total_s: 30254.163664102554\n",
      "  timers:\n",
      "    learn_throughput: 240.796\n",
      "    learn_time_ms: 83057.781\n",
      "    sample_throughput: 90.827\n",
      "    sample_time_ms: 220199.882\n",
      "    update_time_ms: 3.043\n",
      "  timestamp: 1659393849\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2000000\n",
      "  training_iteration: 100\n",
      "  trial_id: fd1ac_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Memory usage on this node: 20.8/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/4.91 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_4_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_0_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_2_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_3_28b5456e04c30e30e910ccff5ed5840d, 0.0/6.0 CPU_group_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_1_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_5_28b5456e04c30e30e910ccff5ed5840d)\n",
      "Result logdir: /home/michael/ray_results/stabilizing_the_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_fd1ac_00000 | RUNNING  | 192.168.0.33:25804 |    100 |          30254.2 | 2000000 |  2267.22 |              3319.82 |              1377.45 |               4000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=25804)\u001b[0m 2022-08-01 17:44:09,147\tWARNING ppo.py:242 -- The magnitude of your environment rewards are more than 227.0x the scale of `vf_clip_param`. This means that it will take more than 227.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m ring length: 240\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m v_max: 4.189299083856172\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m ring length: 240\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m v_max: 4.189299083856172\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m ring length: 242\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m v_max: 4.2844067680020546\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m ring length: 253\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m v_max: 4.807278529691987\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m ring length: 250\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m v_max: 4.664717904914125\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_fd1ac_00000:\n",
      "  agent_timesteps_total: 2020000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-01_17-49-12\n",
      "  done: false\n",
      "  episode_len_mean: 4000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3319.8176460712125\n",
      "  episode_reward_mean: 2247.4820068856898\n",
      "  episode_reward_min: 1377.4501664711158\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 505\n",
      "  experiment_id: 2b7c5715c95c4828907cbc45e49df6f0\n",
      "  hostname: michael-home\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.3642420526593923e-13\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -3.075304164856103\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.024742158626634064\n",
      "          policy_loss: 0.00422780888628179\n",
      "          total_loss: 538.2134159719868\n",
      "          vf_explained_var: -5.163204974678592e-09\n",
      "          vf_loss: 538.2091898243898\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 2020000\n",
      "    num_steps_sampled: 2020000\n",
      "    num_steps_trained: 2020000\n",
      "  iterations_since_restore: 101\n",
      "  node_ip: 192.168.0.33\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 29.278240740740742\n",
      "    ram_util_percent: 66.78750000000001\n",
      "  pid: 25804\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09915674985808348\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.346170210913975\n",
      "    mean_inference_ms: 2.018598380801422\n",
      "    mean_raw_obs_processing_ms: 21.6791566843024\n",
      "  time_since_restore: 30557.20175933838\n",
      "  time_this_iter_s: 303.0380952358246\n",
      "  time_total_s: 30557.20175933838\n",
      "  timers:\n",
      "    learn_throughput: 240.056\n",
      "    learn_time_ms: 83313.761\n",
      "    sample_throughput: 90.822\n",
      "    sample_time_ms: 220211.063\n",
      "    update_time_ms: 3.055\n",
      "  timestamp: 1659394152\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2020000\n",
      "  training_iteration: 101\n",
      "  trial_id: fd1ac_00000\n",
      "  \n",
      "\u001b[2m\u001b[36m(pid=25804)\u001b[0m 2022-08-01 17:49:12,264\tWARNING ppo.py:242 -- The magnitude of your environment rewards are more than 225.0x the scale of `vf_clip_param`. This means that it will take more than 225.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "== Status ==\n",
      "Memory usage on this node: 20.8/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/4.91 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_2_28b5456e04c30e30e910ccff5ed5840d, 0.0/6.0 CPU_group_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_1_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_4_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_3_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_5_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_0_28b5456e04c30e30e910ccff5ed5840d)\n",
      "Result logdir: /home/michael/ray_results/stabilizing_the_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_fd1ac_00000 | RUNNING  | 192.168.0.33:25804 |    101 |          30557.2 | 2020000 |  2247.48 |              3319.82 |              1377.45 |               4000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m ring length: 223\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m v_max: 3.3805185755587788\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m ring length: 247\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m v_max: 4.522125250095652\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m ring length: 239\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m v_max: 4.141741239205832\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m ring length: 249\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m v_max: 4.617190445131122\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m ring length: 224\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m v_max: 3.4281086136538996\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_fd1ac_00000:\n",
      "  agent_timesteps_total: 2040000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-01_17-54-16\n",
      "  done: false\n",
      "  episode_len_mean: 4000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3319.8176460712125\n",
      "  episode_reward_mean: 2248.097517040667\n",
      "  episode_reward_min: 1377.4501664711158\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 510\n",
      "  experiment_id: 2b7c5715c95c4828907cbc45e49df6f0\n",
      "  hostname: michael-home\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.3642420526593923e-13\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -3.1167431217849635\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.022903822013881546\n",
      "          policy_loss: 0.0037945711241507084\n",
      "          total_loss: 541.8377292049918\n",
      "          vf_explained_var: -4.669663322687256e-09\n",
      "          vf_loss: 541.8339350584965\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 2040000\n",
      "    num_steps_sampled: 2040000\n",
      "    num_steps_trained: 2040000\n",
      "  iterations_since_restore: 102\n",
      "  node_ip: 192.168.0.33\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 29.997241379310342\n",
      "    ram_util_percent: 66.923908045977\n",
      "  pid: 25804\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09915360972077629\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.343728324028852\n",
      "    mean_inference_ms: 2.018544376803045\n",
      "    mean_raw_obs_processing_ms: 21.67895398005414\n",
      "  time_since_restore: 30861.665380716324\n",
      "  time_this_iter_s: 304.46362137794495\n",
      "  time_total_s: 30861.665380716324\n",
      "  timers:\n",
      "    learn_throughput: 239.859\n",
      "    learn_time_ms: 83382.273\n",
      "    sample_throughput: 90.815\n",
      "    sample_time_ms: 220229.102\n",
      "    update_time_ms: 3.072\n",
      "  timestamp: 1659394456\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2040000\n",
      "  training_iteration: 102\n",
      "  trial_id: fd1ac_00000\n",
      "  \n",
      "\u001b[2m\u001b[36m(pid=25804)\u001b[0m 2022-08-01 17:54:16,801\tWARNING ppo.py:242 -- The magnitude of your environment rewards are more than 225.0x the scale of `vf_clip_param`. This means that it will take more than 225.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Memory usage on this node: 20.9/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/4.91 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_5_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_1_28b5456e04c30e30e910ccff5ed5840d, 0.0/6.0 CPU_group_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_4_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_3_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_2_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_0_28b5456e04c30e30e910ccff5ed5840d)\n",
      "Result logdir: /home/michael/ray_results/stabilizing_the_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_fd1ac_00000 | RUNNING  | 192.168.0.33:25804 |    102 |          30861.7 | 2040000 |   2248.1 |              3319.82 |              1377.45 |               4000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m ring length: 233\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m v_max: 3.856343201216534\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m ring length: 247\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m v_max: 4.522125250095652\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m ring length: 270\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m v_max: 5.6143732387852054\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m ring length: 226\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m v_max: 3.5232840694769565\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m ring length: 244\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m v_max: 4.379503211387676\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_fd1ac_00000:\n",
      "  agent_timesteps_total: 2060000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-01_17-59-18\n",
      "  done: false\n",
      "  episode_len_mean: 4000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3319.8176460712125\n",
      "  episode_reward_mean: 2213.3564623910306\n",
      "  episode_reward_min: 1377.4501664711158\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 515\n",
      "  experiment_id: 2b7c5715c95c4828907cbc45e49df6f0\n",
      "  hostname: michael-home\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.3642420526593923e-13\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -3.041550891566428\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.019883664469722326\n",
      "          policy_loss: 0.0030147126588116217\n",
      "          total_loss: 540.9509082867082\n",
      "          vf_explained_var: -4.138156928235048e-09\n",
      "          vf_loss: 540.9478952784447\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 2060000\n",
      "    num_steps_sampled: 2060000\n",
      "    num_steps_trained: 2060000\n",
      "  iterations_since_restore: 103\n",
      "  node_ip: 192.168.0.33\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 28.01740139211137\n",
      "    ram_util_percent: 67.03642691415313\n",
      "  pid: 25804\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09915326910021785\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.34125030455134\n",
      "    mean_inference_ms: 2.0185640948132155\n",
      "    mean_raw_obs_processing_ms: 21.67877870941441\n",
      "  time_since_restore: 31163.54970765114\n",
      "  time_this_iter_s: 301.88432693481445\n",
      "  time_total_s: 31163.54970765114\n",
      "  timers:\n",
      "    learn_throughput: 239.77\n",
      "    learn_time_ms: 83413.193\n",
      "    sample_throughput: 90.811\n",
      "    sample_time_ms: 220237.907\n",
      "    update_time_ms: 3.125\n",
      "  timestamp: 1659394758\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2060000\n",
      "  training_iteration: 103\n",
      "  trial_id: fd1ac_00000\n",
      "  \n",
      "\u001b[2m\u001b[36m(pid=25804)\u001b[0m 2022-08-01 17:59:18,774\tWARNING ppo.py:242 -- The magnitude of your environment rewards are more than 221.0x the scale of `vf_clip_param`. This means that it will take more than 221.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "== Status ==\n",
      "Memory usage on this node: 21.0/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/4.91 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_3_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_0_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_2_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_4_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_5_28b5456e04c30e30e910ccff5ed5840d, 0.0/6.0 CPU_group_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_1_28b5456e04c30e30e910ccff5ed5840d)\n",
      "Result logdir: /home/michael/ray_results/stabilizing_the_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_fd1ac_00000 | RUNNING  | 192.168.0.33:25804 |    103 |          31163.5 | 2060000 |  2213.36 |              3319.82 |              1377.45 |               4000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m ring length: 253\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m v_max: 4.807278529691987\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m ring length: 259\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m v_max: 5.092292348291853\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m ring length: 267\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m v_max: 5.472051563171826\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m ring length: 221\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m v_max: 3.285334164169417\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m ring length: 256\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m v_max: 4.949804327743507\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_fd1ac_00000:\n",
      "  agent_timesteps_total: 2080000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-01_18-04-21\n",
      "  done: false\n",
      "  episode_len_mean: 4000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3319.8176460712125\n",
      "  episode_reward_mean: 2223.4346348726085\n",
      "  episode_reward_min: 1377.4501664711158\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 520\n",
      "  experiment_id: 2b7c5715c95c4828907cbc45e49df6f0\n",
      "  hostname: michael-home\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.3642420526593923e-13\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -3.050597250537508\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.02264730977876115\n",
      "          policy_loss: 0.003634297183883869\n",
      "          total_loss: 548.740640354764\n",
      "          vf_explained_var: 2.9992146544088882e-09\n",
      "          vf_loss: 548.737005618879\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 2080000\n",
      "    num_steps_sampled: 2080000\n",
      "    num_steps_trained: 2080000\n",
      "  iterations_since_restore: 104\n",
      "  node_ip: 192.168.0.33\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 27.67401392111369\n",
      "    ram_util_percent: 67.16566125290024\n",
      "  pid: 25804\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0991546164017259\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.338783499194527\n",
      "    mean_inference_ms: 2.0186192785276194\n",
      "    mean_raw_obs_processing_ms: 21.678623946379403\n",
      "  time_since_restore: 31466.0724337101\n",
      "  time_this_iter_s: 302.52272605895996\n",
      "  time_total_s: 31466.0724337101\n",
      "  timers:\n",
      "    learn_throughput: 239.552\n",
      "    learn_time_ms: 83489.209\n",
      "    sample_throughput: 90.817\n",
      "    sample_time_ms: 220223.601\n",
      "    update_time_ms: 3.129\n",
      "  timestamp: 1659395061\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2080000\n",
      "  training_iteration: 104\n",
      "  trial_id: fd1ac_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Memory usage on this node: 20.9/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/4.91 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 accelerator_type:G, 0.0/6.0 CPU_group_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_3_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_1_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_2_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_4_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_5_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_0_28b5456e04c30e30e910ccff5ed5840d)\n",
      "Result logdir: /home/michael/ray_results/stabilizing_the_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_fd1ac_00000 | RUNNING  | 192.168.0.33:25804 |    104 |          31466.1 | 2080000 |  2223.43 |              3319.82 |              1377.45 |               4000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=25804)\u001b[0m 2022-08-01 18:04:21,378\tWARNING ppo.py:242 -- The magnitude of your environment rewards are more than 222.0x the scale of `vf_clip_param`. This means that it will take more than 222.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m ring length: 265\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m v_max: 5.37714246265477\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m ring length: 228\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m v_max: 3.618452967700356\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m ring length: 256\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m v_max: 4.949804327743507\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m ring length: 248\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m v_max: 4.569659527528101\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m ring length: 226\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m v_max: 3.5232840694769565\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_fd1ac_00000:\n",
      "  agent_timesteps_total: 2100000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-01_18-09-25\n",
      "  done: false\n",
      "  episode_len_mean: 4000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3319.8176460712125\n",
      "  episode_reward_mean: 2235.9297761372723\n",
      "  episode_reward_min: 1377.4501664711158\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 525\n",
      "  experiment_id: 2b7c5715c95c4828907cbc45e49df6f0\n",
      "  hostname: michael-home\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.3642420526593923e-13\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -3.074337559596748\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0265627720119442\n",
      "          policy_loss: 0.003939160120311626\n",
      "          total_loss: 566.1621227167215\n",
      "          vf_explained_var: -1.7463781532001121e-09\n",
      "          vf_loss: 566.1581860487628\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 2100000\n",
      "    num_steps_sampled: 2100000\n",
      "    num_steps_trained: 2100000\n",
      "  iterations_since_restore: 105\n",
      "  node_ip: 192.168.0.33\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 28.88298850574713\n",
      "    ram_util_percent: 67.06965517241379\n",
      "  pid: 25804\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09915734557057458\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.33634584175512\n",
      "    mean_inference_ms: 2.0187086034979003\n",
      "    mean_raw_obs_processing_ms: 21.678486466694753\n",
      "  time_since_restore: 31770.413240671158\n",
      "  time_this_iter_s: 304.34080696105957\n",
      "  time_total_s: 31770.413240671158\n",
      "  timers:\n",
      "    learn_throughput: 239.681\n",
      "    learn_time_ms: 83444.413\n",
      "    sample_throughput: 90.814\n",
      "    sample_time_ms: 220230.85\n",
      "    update_time_ms: 3.101\n",
      "  timestamp: 1659395365\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2100000\n",
      "  training_iteration: 105\n",
      "  trial_id: fd1ac_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 20.9/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/4.91 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_3_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_5_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_2_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 accelerator_type:G, 0.0/6.0 CPU_group_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_4_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_1_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_0_28b5456e04c30e30e910ccff5ed5840d)\n",
      "Result logdir: /home/michael/ray_results/stabilizing_the_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_fd1ac_00000 | RUNNING  | 192.168.0.33:25804 |    105 |          31770.4 | 2100000 |  2235.93 |              3319.82 |              1377.45 |               4000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=25804)\u001b[0m 2022-08-01 18:09:25,799\tWARNING ppo.py:242 -- The magnitude of your environment rewards are more than 224.0x the scale of `vf_clip_param`. This means that it will take more than 224.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m ring length: 246\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m v_max: 4.474587708995648\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m ring length: 227\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m v_max: 3.570869368805752\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m ring length: 236\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m v_max: 3.999052674951912\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m ring length: 248\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m v_max: 4.569659527528101\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m ring length: 231\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m v_max: 3.761192925277308\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_fd1ac_00000:\n",
      "  agent_timesteps_total: 2120000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-01_18-14-30\n",
      "  done: false\n",
      "  episode_len_mean: 4000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3319.8176460712125\n",
      "  episode_reward_mean: 2232.9542018793118\n",
      "  episode_reward_min: 1377.4501664711158\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 530\n",
      "  experiment_id: 2b7c5715c95c4828907cbc45e49df6f0\n",
      "  hostname: michael-home\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.3642420526593923e-13\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -2.9794953695528066\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015721063196357445\n",
      "          policy_loss: 0.002016707216122538\n",
      "          total_loss: 567.586154778596\n",
      "          vf_explained_var: -1.290801243669648e-09\n",
      "          vf_loss: 567.5841391630233\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 2120000\n",
      "    num_steps_sampled: 2120000\n",
      "    num_steps_trained: 2120000\n",
      "  iterations_since_restore: 106\n",
      "  node_ip: 192.168.0.33\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 28.680459770114943\n",
      "    ram_util_percent: 67.07379310344827\n",
      "  pid: 25804\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09916454803459075\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.333833420446854\n",
      "    mean_inference_ms: 2.018912981864496\n",
      "    mean_raw_obs_processing_ms: 21.678382992709274\n",
      "  time_since_restore: 32075.476270914078\n",
      "  time_this_iter_s: 305.0630302429199\n",
      "  time_total_s: 32075.476270914078\n",
      "  timers:\n",
      "    learn_throughput: 238.627\n",
      "    learn_time_ms: 83812.786\n",
      "    sample_throughput: 90.824\n",
      "    sample_time_ms: 220205.097\n",
      "    update_time_ms: 3.16\n",
      "  timestamp: 1659395670\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2120000\n",
      "  training_iteration: 106\n",
      "  trial_id: fd1ac_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=25804)\u001b[0m 2022-08-01 18:14:30,936\tWARNING ppo.py:242 -- The magnitude of your environment rewards are more than 223.0x the scale of `vf_clip_param`. This means that it will take more than 223.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "== Status ==\n",
      "Memory usage on this node: 21.0/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/4.91 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_3_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_0_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_1_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_5_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_4_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_2_28b5456e04c30e30e910ccff5ed5840d, 0.0/6.0 CPU_group_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 accelerator_type:G)\n",
      "Result logdir: /home/michael/ray_results/stabilizing_the_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_fd1ac_00000 | RUNNING  | 192.168.0.33:25804 |    106 |          32075.5 | 2120000 |  2232.95 |              3319.82 |              1377.45 |               4000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m ring length: 261\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m v_max: 5.187261846097525\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m ring length: 244\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m v_max: 4.379503211387676\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m ring length: 243\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m v_max: 4.331956438196479\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m ring length: 239\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m v_max: 4.141741239205832\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m ring length: 237\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m v_max: 4.04661795519353\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_fd1ac_00000:\n",
      "  agent_timesteps_total: 2140000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-01_18-19-32\n",
      "  done: false\n",
      "  episode_len_mean: 4000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3319.8176460712125\n",
      "  episode_reward_mean: 2192.214628977695\n",
      "  episode_reward_min: 1377.4501664711158\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 535\n",
      "  experiment_id: 2b7c5715c95c4828907cbc45e49df6f0\n",
      "  hostname: michael-home\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.3642420526593923e-13\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -2.9760506118179126\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.024768540949364228\n",
      "          policy_loss: 0.0032127466862487374\n",
      "          total_loss: 563.712092921992\n",
      "          vf_explained_var: -1.4426602135131361e-09\n",
      "          vf_loss: 563.7088839087517\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 2140000\n",
      "    num_steps_sampled: 2140000\n",
      "    num_steps_trained: 2140000\n",
      "  iterations_since_restore: 107\n",
      "  node_ip: 192.168.0.33\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 28.51786542923434\n",
      "    ram_util_percent: 67.34663573085848\n",
      "  pid: 25804\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09917499040841127\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.33126554354106\n",
      "    mean_inference_ms: 2.0192149782927555\n",
      "    mean_raw_obs_processing_ms: 21.67828238512914\n",
      "  time_since_restore: 32377.392428398132\n",
      "  time_this_iter_s: 301.91615748405457\n",
      "  time_total_s: 32377.392428398132\n",
      "  timers:\n",
      "    learn_throughput: 238.395\n",
      "    learn_time_ms: 83894.475\n",
      "    sample_throughput: 90.828\n",
      "    sample_time_ms: 220197.514\n",
      "    update_time_ms: 3.088\n",
      "  timestamp: 1659395972\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2140000\n",
      "  training_iteration: 107\n",
      "  trial_id: fd1ac_00000\n",
      "  \n",
      "\u001b[2m\u001b[36m(pid=25804)\u001b[0m 2022-08-01 18:19:32,940\tWARNING ppo.py:242 -- The magnitude of your environment rewards are more than 219.0x the scale of `vf_clip_param`. This means that it will take more than 219.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "== Status ==\n",
      "Memory usage on this node: 21.1/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/4.91 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_3_28b5456e04c30e30e910ccff5ed5840d, 0.0/6.0 CPU_group_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_2_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_0_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_5_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_1_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_4_28b5456e04c30e30e910ccff5ed5840d)\n",
      "Result logdir: /home/michael/ray_results/stabilizing_the_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_fd1ac_00000 | RUNNING  | 192.168.0.33:25804 |    107 |          32377.4 | 2140000 |  2192.21 |              3319.82 |              1377.45 |               4000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m ring length: 231\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m v_max: 3.761192925277308\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m ring length: 259\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m v_max: 5.092292348291853\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m ring length: 270\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m v_max: 5.6143732387852054\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m ring length: 249\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m v_max: 4.617190445131122\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m ring length: 259\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m v_max: 5.092292348291853\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_fd1ac_00000:\n",
      "  agent_timesteps_total: 2160000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-01_18-24-33\n",
      "  done: false\n",
      "  episode_len_mean: 4000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3319.8176460712125\n",
      "  episode_reward_mean: 2214.17668800718\n",
      "  episode_reward_min: 1377.4501664711158\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 540\n",
      "  experiment_id: 2b7c5715c95c4828907cbc45e49df6f0\n",
      "  hostname: michael-home\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.3642420526593923e-13\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -2.9760011335846723\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.022104750541472696\n",
      "          policy_loss: 0.0035338926332213793\n",
      "          total_loss: 572.0618954263675\n",
      "          vf_explained_var: -6.45400621834824e-10\n",
      "          vf_loss: 572.0583617422991\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 2160000\n",
      "    num_steps_sampled: 2160000\n",
      "    num_steps_trained: 2160000\n",
      "  iterations_since_restore: 108\n",
      "  node_ip: 192.168.0.33\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 27.385514018691588\n",
      "    ram_util_percent: 66.64158878504672\n",
      "  pid: 25804\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09918497555040799\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.32879245341949\n",
      "    mean_inference_ms: 2.019486971985535\n",
      "    mean_raw_obs_processing_ms: 21.678162219307705\n",
      "  time_since_restore: 32677.88338804245\n",
      "  time_this_iter_s: 300.4909596443176\n",
      "  time_total_s: 32677.88338804245\n",
      "  timers:\n",
      "    learn_throughput: 239.976\n",
      "    learn_time_ms: 83341.746\n",
      "    sample_throughput: 90.83\n",
      "    sample_time_ms: 220190.47\n",
      "    update_time_ms: 3.049\n",
      "  timestamp: 1659396273\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2160000\n",
      "  training_iteration: 108\n",
      "  trial_id: fd1ac_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=25804)\u001b[0m 2022-08-01 18:24:33,506\tWARNING ppo.py:242 -- The magnitude of your environment rewards are more than 221.0x the scale of `vf_clip_param`. This means that it will take more than 221.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "== Status ==\n",
      "Memory usage on this node: 20.4/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/4.91 GiB heap, 0.0/14.65 GiB objects (0.0/6.0 CPU_group_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_3_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_0_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_1_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_4_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_5_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_2_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 accelerator_type:G)\n",
      "Result logdir: /home/michael/ray_results/stabilizing_the_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_fd1ac_00000 | RUNNING  | 192.168.0.33:25804 |    108 |          32677.9 | 2160000 |  2214.18 |              3319.82 |              1377.45 |               4000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m ring length: 261\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m v_max: 5.187261846097525\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m ring length: 245\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m v_max: 4.427046998576354\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m ring length: 234\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m v_max: 3.903915223349057\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m ring length: 254\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m v_max: 4.854791141191876\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m ring length: 247\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m v_max: 4.522125250095652\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m Could not connect to TraCI server at localhost:35657 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m Could not connect to TraCI server at localhost:41611 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m Could not connect to TraCI server at localhost:45453 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m Could not connect to TraCI server at localhost:45851 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m Could not connect to TraCI server at localhost:57823 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m Could not connect to TraCI server at localhost:35657 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m  Retrying in 3 seconds\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m Could not connect to TraCI server at localhost:41611 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m  Retrying in 3 seconds\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m Could not connect to TraCI server at localhost:45453 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m  Retrying in 3 seconds\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m Could not connect to TraCI server at localhost:45851 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m  Retrying in 3 seconds\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m Could not connect to TraCI server at localhost:57823 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m  Retrying in 3 seconds\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_fd1ac_00000:\n",
      "  agent_timesteps_total: 2180000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-01_18-29-40\n",
      "  done: false\n",
      "  episode_len_mean: 4000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3319.8176460712125\n",
      "  episode_reward_mean: 2206.773085539426\n",
      "  episode_reward_min: 1377.4501664711158\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 545\n",
      "  experiment_id: 2b7c5715c95c4828907cbc45e49df6f0\n",
      "  hostname: michael-home\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.3642420526593923e-13\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -3.105751174118868\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.02556705730364821\n",
      "          policy_loss: 0.003040927411857874\n",
      "          total_loss: 590.1428549262369\n",
      "          vf_explained_var: -9.87083303982672e-10\n",
      "          vf_loss: 590.1398116069234\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 2180000\n",
      "    num_steps_sampled: 2180000\n",
      "    num_steps_trained: 2180000\n",
      "  iterations_since_restore: 109\n",
      "  node_ip: 192.168.0.33\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 26.732801822323463\n",
      "    ram_util_percent: 65.5487471526196\n",
      "  pid: 25804\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09919580983751246\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.326371133372472\n",
      "    mean_inference_ms: 2.0197515443231246\n",
      "    mean_raw_obs_processing_ms: 21.678725573996527\n",
      "  time_since_restore: 32985.041399002075\n",
      "  time_this_iter_s: 307.15801095962524\n",
      "  time_total_s: 32985.041399002075\n",
      "  timers:\n",
      "    learn_throughput: 241.299\n",
      "    learn_time_ms: 82884.741\n",
      "    sample_throughput: 90.588\n",
      "    sample_time_ms: 220779.552\n",
      "    update_time_ms: 3.071\n",
      "  timestamp: 1659396580\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2180000\n",
      "  training_iteration: 109\n",
      "  trial_id: fd1ac_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 20.5/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/4.91 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_5_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_1_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_3_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_0_28b5456e04c30e30e910ccff5ed5840d, 0.0/6.0 CPU_group_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_4_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_2_28b5456e04c30e30e910ccff5ed5840d)\n",
      "Result logdir: /home/michael/ray_results/stabilizing_the_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_fd1ac_00000 | RUNNING  | 192.168.0.33:25804 |    109 |            32985 | 2180000 |  2206.77 |              3319.82 |              1377.45 |               4000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=25804)\u001b[0m 2022-08-01 18:29:40,736\tWARNING ppo.py:242 -- The magnitude of your environment rewards are more than 221.0x the scale of `vf_clip_param`. This means that it will take more than 221.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m ring length: 238\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m v_max: 4.094180836186086\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m ring length: 262\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m v_max: 5.234739483008334\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m ring length: 222\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m v_max: 3.3329270738617227\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m ring length: 235\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m v_max: 3.9514850725282584\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m ring length: 241\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m v_max: 4.236854288051661\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m  Retrying in 1 seconds\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_fd1ac_00000:\n",
      "  agent_timesteps_total: 2200000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-01_18-34-46\n",
      "  done: false\n",
      "  episode_len_mean: 4000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3319.8176460712125\n",
      "  episode_reward_mean: 2219.1756598932852\n",
      "  episode_reward_min: 1467.9568558195476\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 550\n",
      "  experiment_id: 2b7c5715c95c4828907cbc45e49df6f0\n",
      "  hostname: michael-home\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.3642420526593923e-13\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -3.120423062895514\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.025538621744622278\n",
      "          policy_loss: 0.0038850443716499077\n",
      "          total_loss: 589.7792284424897\n",
      "          vf_explained_var: -7.0234773552613206e-09\n",
      "          vf_loss: 589.7753425488806\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 2200000\n",
      "    num_steps_sampled: 2200000\n",
      "    num_steps_trained: 2200000\n",
      "  iterations_since_restore: 110\n",
      "  node_ip: 192.168.0.33\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 27.25632183908046\n",
      "    ram_util_percent: 65.7751724137931\n",
      "  pid: 25804\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09920784326016925\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.323952127333232\n",
      "    mean_inference_ms: 2.02005488713191\n",
      "    mean_raw_obs_processing_ms: 21.67926155184218\n",
      "  time_since_restore: 33290.43390965462\n",
      "  time_this_iter_s: 305.3925106525421\n",
      "  time_total_s: 33290.43390965462\n",
      "  timers:\n",
      "    learn_throughput: 241.652\n",
      "    learn_time_ms: 82763.791\n",
      "    sample_throughput: 90.561\n",
      "    sample_time_ms: 220846.639\n",
      "    update_time_ms: 3.076\n",
      "  timestamp: 1659396886\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2200000\n",
      "  training_iteration: 110\n",
      "  trial_id: fd1ac_00000\n",
      "  \n",
      "\u001b[2m\u001b[36m(pid=25804)\u001b[0m 2022-08-01 18:34:46,200\tWARNING ppo.py:242 -- The magnitude of your environment rewards are more than 222.0x the scale of `vf_clip_param`. This means that it will take more than 222.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "== Status ==\n",
      "Memory usage on this node: 20.5/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/4.91 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_3_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_4_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_0_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_1_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_2_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_5_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 accelerator_type:G, 0.0/6.0 CPU_group_28b5456e04c30e30e910ccff5ed5840d)\n",
      "Result logdir: /home/michael/ray_results/stabilizing_the_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_fd1ac_00000 | RUNNING  | 192.168.0.33:25804 |    110 |          33290.4 | 2200000 |  2219.18 |              3319.82 |              1467.96 |               4000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m ring length: 230\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m v_max: 3.7136148111012934\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m ring length: 236\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m v_max: 3.999052674951912\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m ring length: 227\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m v_max: 3.570869368805752\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m ring length: 256\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m v_max: 4.949804327743507\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m ring length: 264\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m v_max: 5.329679917416892\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_fd1ac_00000:\n",
      "  agent_timesteps_total: 2220000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-01_18-39-54\n",
      "  done: false\n",
      "  episode_len_mean: 4000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3319.8176460712125\n",
      "  episode_reward_mean: 2211.512530903541\n",
      "  episode_reward_min: 1467.9568558195476\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 555\n",
      "  experiment_id: 2b7c5715c95c4828907cbc45e49df6f0\n",
      "  hostname: michael-home\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.3642420526593923e-13\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -3.1503481608287545\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.023263641907976255\n",
      "          policy_loss: 0.003997562309003369\n",
      "          total_loss: 602.407314254372\n",
      "          vf_explained_var: -3.948333215930688e-09\n",
      "          vf_loss: 602.4033149597751\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 2220000\n",
      "    num_steps_sampled: 2220000\n",
      "    num_steps_trained: 2220000\n",
      "  iterations_since_restore: 111\n",
      "  node_ip: 192.168.0.33\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 27.955909090909092\n",
      "    ram_util_percent: 65.89522727272728\n",
      "  pid: 25804\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09922277511628982\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.3215173278336\n",
      "    mean_inference_ms: 2.0204253827272707\n",
      "    mean_raw_obs_processing_ms: 21.67980974624783\n",
      "  time_since_restore: 33598.385407447815\n",
      "  time_this_iter_s: 307.95149779319763\n",
      "  time_total_s: 33598.385407447815\n",
      "  timers:\n",
      "    learn_throughput: 240.258\n",
      "    learn_time_ms: 83243.698\n",
      "    sample_throughput: 90.556\n",
      "    sample_time_ms: 220858.061\n",
      "    update_time_ms: 3.123\n",
      "  timestamp: 1659397194\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2220000\n",
      "  training_iteration: 111\n",
      "  trial_id: fd1ac_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=25804)\u001b[0m 2022-08-01 18:39:54,226\tWARNING ppo.py:242 -- The magnitude of your environment rewards are more than 221.0x the scale of `vf_clip_param`. This means that it will take more than 221.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "== Status ==\n",
      "Memory usage on this node: 20.5/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/4.91 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_1_28b5456e04c30e30e910ccff5ed5840d, 0.0/6.0 CPU_group_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_0_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_4_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_2_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_5_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_3_28b5456e04c30e30e910ccff5ed5840d)\n",
      "Result logdir: /home/michael/ray_results/stabilizing_the_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_fd1ac_00000 | RUNNING  | 192.168.0.33:25804 |    111 |          33598.4 | 2220000 |  2211.51 |              3319.82 |              1467.96 |               4000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m ring length: 238\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m v_max: 4.094180836186086\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m ring length: 233\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m v_max: 3.856343201216534\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m ring length: 256\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m v_max: 4.949804327743507\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m ring length: 263\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m v_max: 5.282212215151455\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m ring length: 222\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m v_max: 3.3329270738617227\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m  Retrying in 1 seconds\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_fd1ac_00000:\n",
      "  agent_timesteps_total: 2240000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-01_18-45-07\n",
      "  done: false\n",
      "  episode_len_mean: 4000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3319.8176460712125\n",
      "  episode_reward_mean: 2204.195619927916\n",
      "  episode_reward_min: 1467.9568558195476\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 560\n",
      "  experiment_id: 2b7c5715c95c4828907cbc45e49df6f0\n",
      "  hostname: michael-home\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.3642420526593923e-13\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -3.2687509949799556\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.026382739278380925\n",
      "          policy_loss: 0.002897410591097347\n",
      "          total_loss: 605.294939796788\n",
      "          vf_explained_var: -2.6575319722610402e-09\n",
      "          vf_loss: 605.2920433190218\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 2240000\n",
      "    num_steps_sampled: 2240000\n",
      "    num_steps_trained: 2240000\n",
      "  iterations_since_restore: 112\n",
      "  node_ip: 192.168.0.33\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 29.789910313901345\n",
      "    ram_util_percent: 66.67511210762332\n",
      "  pid: 25804\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09923948913283494\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.319051677728442\n",
      "    mean_inference_ms: 2.020840780481016\n",
      "    mean_raw_obs_processing_ms: 21.68045680182486\n",
      "  time_since_restore: 33911.1399474144\n",
      "  time_this_iter_s: 312.75453996658325\n",
      "  time_total_s: 33911.1399474144\n",
      "  timers:\n",
      "    learn_throughput: 237.995\n",
      "    learn_time_ms: 84035.41\n",
      "    sample_throughput: 90.541\n",
      "    sample_time_ms: 220895.469\n",
      "    update_time_ms: 3.089\n",
      "  timestamp: 1659397507\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2240000\n",
      "  training_iteration: 112\n",
      "  trial_id: fd1ac_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 21.3/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/4.91 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_2_28b5456e04c30e30e910ccff5ed5840d, 0.0/6.0 CPU_group_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_3_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_4_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_0_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_5_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_1_28b5456e04c30e30e910ccff5ed5840d)\n",
      "Result logdir: /home/michael/ray_results/stabilizing_the_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_fd1ac_00000 | RUNNING  | 192.168.0.33:25804 |    112 |          33911.1 | 2240000 |   2204.2 |              3319.82 |              1467.96 |               4000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=25804)\u001b[0m 2022-08-01 18:45:07,061\tWARNING ppo.py:242 -- The magnitude of your environment rewards are more than 220.0x the scale of `vf_clip_param`. This means that it will take more than 220.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m ring length: 237\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m v_max: 4.04661795519353\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m ring length: 264\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m v_max: 5.329679917416892\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m ring length: 240\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m v_max: 4.189299083856172\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m ring length: 247\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m v_max: 4.522125250095652\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m ring length: 262\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m v_max: 5.234739483008334\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_WaveAttenuationPOEnv-v0_fd1ac_00000:\n",
      "  agent_timesteps_total: 2260000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-01_18-50-18\n",
      "  done: false\n",
      "  episode_len_mean: 4000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3239.789815882068\n",
      "  episode_reward_mean: 2179.3027679274414\n",
      "  episode_reward_min: 1467.9568558195476\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 565\n",
      "  experiment_id: 2b7c5715c95c4828907cbc45e49df6f0\n",
      "  hostname: michael-home\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.3642420526593923e-13\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -3.2212021132183684\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.024105917219484855\n",
      "          policy_loss: 0.004208711303094295\n",
      "          total_loss: 607.3494284368625\n",
      "          vf_explained_var: 3.7964742460872003e-10\n",
      "          vf_loss: 607.3452207698944\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 2260000\n",
      "    num_steps_sampled: 2260000\n",
      "    num_steps_trained: 2260000\n",
      "  iterations_since_restore: 113\n",
      "  node_ip: 192.168.0.33\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 31.504719101123598\n",
      "    ram_util_percent: 68.49955056179776\n",
      "  pid: 25804\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09925555456082373\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.316687216532664\n",
      "    mean_inference_ms: 2.0212044210285867\n",
      "    mean_raw_obs_processing_ms: 21.681123005720917\n",
      "  time_since_restore: 34222.862550497055\n",
      "  time_this_iter_s: 311.72260308265686\n",
      "  time_total_s: 34222.862550497055\n",
      "  timers:\n",
      "    learn_throughput: 235.288\n",
      "    learn_time_ms: 85002.328\n",
      "    sample_throughput: 90.534\n",
      "    sample_time_ms: 220912.478\n",
      "    update_time_ms: 3.023\n",
      "  timestamp: 1659397818\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2260000\n",
      "  training_iteration: 113\n",
      "  trial_id: fd1ac_00000\n",
      "  \n",
      "\u001b[2m\u001b[36m(pid=25804)\u001b[0m 2022-08-01 18:50:18,863\tWARNING ppo.py:242 -- The magnitude of your environment rewards are more than 218.0x the scale of `vf_clip_param`. This means that it will take more than 218.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "== Status ==\n",
      "Memory usage on this node: 21.5/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/4.91 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_5_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_0_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_4_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_2_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_3_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_1_28b5456e04c30e30e910ccff5ed5840d, 0.0/6.0 CPU_group_28b5456e04c30e30e910ccff5ed5840d)\n",
      "Result logdir: /home/michael/ray_results/stabilizing_the_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_fd1ac_00000 | RUNNING  | 192.168.0.33:25804 |    113 |          34222.9 | 2260000 |   2179.3 |              3239.79 |              1467.96 |               4000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m ring length: 256\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m v_max: 4.949804327743507\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m ring length: 235\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m v_max: 3.9514850725282584\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m ring length: 235\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m v_max: 3.9514850725282584\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m ring length: 234\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m v_max: 3.903915223349057\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m ring length: 235\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m v_max: 3.9514850725282584\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_fd1ac_00000:\n",
      "  agent_timesteps_total: 2280000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-01_18-55-30\n",
      "  done: false\n",
      "  episode_len_mean: 4000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3239.789815882068\n",
      "  episode_reward_mean: 2179.9538969196224\n",
      "  episode_reward_min: 1467.9568558195476\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 570\n",
      "  experiment_id: 2b7c5715c95c4828907cbc45e49df6f0\n",
      "  hostname: michael-home\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.3642420526593923e-13\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -3.263295124746432\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.023744198121964797\n",
      "          policy_loss: 0.002259397721748538\n",
      "          total_loss: 611.1406449627724\n",
      "          vf_explained_var: -4.783557550069872e-09\n",
      "          vf_loss: 611.1383855005738\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 2280000\n",
      "    num_steps_sampled: 2280000\n",
      "    num_steps_trained: 2280000\n",
      "  iterations_since_restore: 114\n",
      "  node_ip: 192.168.0.33\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 31.064864864864866\n",
      "    ram_util_percent: 69.53581081081082\n",
      "  pid: 25804\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09927425620840165\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.314254296426935\n",
      "    mean_inference_ms: 2.021636062017916\n",
      "    mean_raw_obs_processing_ms: 21.681855840199315\n",
      "  time_since_restore: 34533.9515068531\n",
      "  time_this_iter_s: 311.0889563560486\n",
      "  time_total_s: 34533.9515068531\n",
      "  timers:\n",
      "    learn_throughput: 233.061\n",
      "    learn_time_ms: 85814.532\n",
      "    sample_throughput: 90.515\n",
      "    sample_time_ms: 220956.823\n",
      "    update_time_ms: 3.033\n",
      "  timestamp: 1659398130\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2280000\n",
      "  training_iteration: 114\n",
      "  trial_id: fd1ac_00000\n",
      "  \n",
      "\u001b[2m\u001b[36m(pid=25804)\u001b[0m 2022-08-01 18:55:30,032\tWARNING ppo.py:242 -- The magnitude of your environment rewards are more than 218.0x the scale of `vf_clip_param`. This means that it will take more than 218.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "== Status ==\n",
      "Memory usage on this node: 21.8/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/4.91 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_3_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_2_28b5456e04c30e30e910ccff5ed5840d, 0.0/6.0 CPU_group_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_4_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_1_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_0_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_5_28b5456e04c30e30e910ccff5ed5840d)\n",
      "Result logdir: /home/michael/ray_results/stabilizing_the_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_fd1ac_00000 | RUNNING  | 192.168.0.33:25804 |    114 |            34534 | 2280000 |  2179.95 |              3239.79 |              1467.96 |               4000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m ring length: 255\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m v_max: 4.902299776966978\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m ring length: 264\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m v_max: 5.329679917416892\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m ring length: 267\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m v_max: 5.472051563171826\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m ring length: 253\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m v_max: 4.807278529691987\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m ring length: 237\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m v_max: 4.04661795519353\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_fd1ac_00000:\n",
      "  agent_timesteps_total: 2300000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-01_19-00-40\n",
      "  done: false\n",
      "  episode_len_mean: 4000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3239.789815882068\n",
      "  episode_reward_mean: 2166.3427961212865\n",
      "  episode_reward_min: 1467.9568558195476\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 575\n",
      "  experiment_id: 2b7c5715c95c4828907cbc45e49df6f0\n",
      "  hostname: michael-home\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.3642420526593923e-13\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -3.1893221771641143\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.02221741963920901\n",
      "          policy_loss: 0.003619771115897093\n",
      "          total_loss: 602.9545235141827\n",
      "          vf_explained_var: -4.783557550069872e-09\n",
      "          vf_loss: 602.9509058618241\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 2300000\n",
      "    num_steps_sampled: 2300000\n",
      "    num_steps_trained: 2300000\n",
      "  iterations_since_restore: 115\n",
      "  node_ip: 192.168.0.33\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 31.752821670428894\n",
      "    ram_util_percent: 70.31286681715575\n",
      "  pid: 25804\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09929644239214877\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.31174830253407\n",
      "    mean_inference_ms: 2.0221640908268164\n",
      "    mean_raw_obs_processing_ms: 21.68260244291732\n",
      "  time_since_restore: 34844.65844082832\n",
      "  time_this_iter_s: 310.7069339752197\n",
      "  time_total_s: 34844.65844082832\n",
      "  timers:\n",
      "    learn_throughput: 231.33\n",
      "    learn_time_ms: 86456.457\n",
      "    sample_throughput: 90.518\n",
      "    sample_time_ms: 220950.701\n",
      "    update_time_ms: 3.059\n",
      "  timestamp: 1659398440\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2300000\n",
      "  training_iteration: 115\n",
      "  trial_id: fd1ac_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 22.0/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/4.91 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_2_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_0_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_4_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_5_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 accelerator_type:G, 0.0/6.0 CPU_group_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_3_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_1_28b5456e04c30e30e910ccff5ed5840d)\n",
      "Result logdir: /home/michael/ray_results/stabilizing_the_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_fd1ac_00000 | RUNNING  | 192.168.0.33:25804 |    115 |          34844.7 | 2300000 |  2166.34 |              3239.79 |              1467.96 |               4000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=25804)\u001b[0m 2022-08-01 19:00:40,817\tWARNING ppo.py:242 -- The magnitude of your environment rewards are more than 217.0x the scale of `vf_clip_param`. This means that it will take more than 217.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m ring length: 258\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m v_max: 5.044800727535787\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m ring length: 258\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m v_max: 5.044800727535787\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m ring length: 247\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m v_max: 4.522125250095652\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m ring length: 262\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m v_max: 5.234739483008334\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m ring length: 223\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m v_max: 3.3805185755587788\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_fd1ac_00000:\n",
      "  agent_timesteps_total: 2320000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-01_19-05-53\n",
      "  done: false\n",
      "  episode_len_mean: 4000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3239.789815882068\n",
      "  episode_reward_mean: 2173.786694120873\n",
      "  episode_reward_min: 1467.9568558195476\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 580\n",
      "  experiment_id: 2b7c5715c95c4828907cbc45e49df6f0\n",
      "  hostname: michael-home\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.3642420526593923e-13\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -3.164309071735212\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.026430384296209215\n",
      "          policy_loss: 0.004879644643633988\n",
      "          total_loss: 624.6408469910834\n",
      "          vf_explained_var: -1.13894227382616e-09\n",
      "          vf_loss: 624.6359695580355\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 2320000\n",
      "    num_steps_sampled: 2320000\n",
      "    num_steps_trained: 2320000\n",
      "  iterations_since_restore: 116\n",
      "  node_ip: 192.168.0.33\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.59910313901346\n",
      "    ram_util_percent: 70.71659192825112\n",
      "  pid: 25804\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09931972878446427\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.30930741864316\n",
      "    mean_inference_ms: 2.022717456152449\n",
      "    mean_raw_obs_processing_ms: 21.683351828259394\n",
      "  time_since_restore: 35157.207953453064\n",
      "  time_this_iter_s: 312.5495126247406\n",
      "  time_total_s: 35157.207953453064\n",
      "  timers:\n",
      "    learn_throughput: 229.59\n",
      "    learn_time_ms: 87111.955\n",
      "    sample_throughput: 90.48\n",
      "    sample_time_ms: 221044.147\n",
      "    update_time_ms: 3.018\n",
      "  timestamp: 1659398753\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2320000\n",
      "  training_iteration: 116\n",
      "  trial_id: fd1ac_00000\n",
      "  \n",
      "\u001b[2m\u001b[36m(pid=25804)\u001b[0m 2022-08-01 19:05:53,451\tWARNING ppo.py:242 -- The magnitude of your environment rewards are more than 217.0x the scale of `vf_clip_param`. This means that it will take more than 217.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Memory usage on this node: 22.1/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/4.91 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_5_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_0_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_4_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_1_28b5456e04c30e30e910ccff5ed5840d, 0.0/6.0 CPU_group_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_3_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_2_28b5456e04c30e30e910ccff5ed5840d)\n",
      "Result logdir: /home/michael/ray_results/stabilizing_the_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_fd1ac_00000 | RUNNING  | 192.168.0.33:25804 |    116 |          35157.2 | 2320000 |  2173.79 |              3239.79 |              1467.96 |               4000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m ring length: 256\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m v_max: 4.949804327743507\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m ring length: 250\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m v_max: 4.664717904914125\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m ring length: 267\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m v_max: 5.472051563171826\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m ring length: 267\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m v_max: 5.472051563171826\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m ring length: 254\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m v_max: 4.854791141191876\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_fd1ac_00000:\n",
      "  agent_timesteps_total: 2340000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-01_19-11-04\n",
      "  done: false\n",
      "  episode_len_mean: 4000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3239.789815882068\n",
      "  episode_reward_mean: 2188.3534801012725\n",
      "  episode_reward_min: 1467.9568558195476\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 585\n",
      "  experiment_id: 2b7c5715c95c4828907cbc45e49df6f0\n",
      "  hostname: michael-home\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.3642420526593923e-13\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -3.1846824254199957\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.02096849641806993\n",
      "          policy_loss: 0.0036119231412922786\n",
      "          total_loss: 632.7465037327663\n",
      "          vf_explained_var: -1.214871758747904e-09\n",
      "          vf_loss: 632.7428892147769\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 2340000\n",
      "    num_steps_sampled: 2340000\n",
      "    num_steps_trained: 2340000\n",
      "  iterations_since_restore: 117\n",
      "  node_ip: 192.168.0.33\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 31.67882882882883\n",
      "    ram_util_percent: 70.78626126126126\n",
      "  pid: 25804\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09934565899110337\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.306810879116533\n",
      "    mean_inference_ms: 2.0233503690587518\n",
      "    mean_raw_obs_processing_ms: 21.68415248479377\n",
      "  time_since_restore: 35468.325050115585\n",
      "  time_this_iter_s: 311.11709666252136\n",
      "  time_total_s: 35468.325050115585\n",
      "  timers:\n",
      "    learn_throughput: 227.314\n",
      "    learn_time_ms: 87983.844\n",
      "    sample_throughput: 90.46\n",
      "    sample_time_ms: 221092.157\n",
      "    update_time_ms: 2.986\n",
      "  timestamp: 1659399064\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2340000\n",
      "  training_iteration: 117\n",
      "  trial_id: fd1ac_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 22.1/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/4.91 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_2_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_4_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_3_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_0_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_5_28b5456e04c30e30e910ccff5ed5840d, 0.0/6.0 CPU_group_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_1_28b5456e04c30e30e910ccff5ed5840d)\n",
      "Result logdir: /home/michael/ray_results/stabilizing_the_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_fd1ac_00000 | RUNNING  | 192.168.0.33:25804 |    117 |          35468.3 | 2340000 |  2188.35 |              3239.79 |              1467.96 |               4000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=25804)\u001b[0m 2022-08-01 19:11:04,662\tWARNING ppo.py:242 -- The magnitude of your environment rewards are more than 219.0x the scale of `vf_clip_param`. This means that it will take more than 219.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m ring length: 249\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m v_max: 4.617190445131122\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m ring length: 229\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m v_max: 3.666034803266416\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m ring length: 261\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m v_max: 5.187261846097525\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m ring length: 262\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m v_max: 5.234739483008334\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m ring length: 266\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m v_max: 5.42459972166245\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_fd1ac_00000:\n",
      "  agent_timesteps_total: 2360000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-01_19-16-11\n",
      "  done: false\n",
      "  episode_len_mean: 4000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3239.789815882068\n",
      "  episode_reward_mean: 2211.03932205329\n",
      "  episode_reward_min: 1467.9568558195476\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 590\n",
      "  experiment_id: 2b7c5715c95c4828907cbc45e49df6f0\n",
      "  hostname: michael-home\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.3642420526593923e-13\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -3.2122822890615765\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.023480384250475272\n",
      "          policy_loss: 0.003583993128453423\n",
      "          total_loss: 643.6295594743863\n",
      "          vf_explained_var: -2.7714261996436562e-09\n",
      "          vf_loss: 643.6259745628212\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 2360000\n",
      "    num_steps_sampled: 2360000\n",
      "    num_steps_trained: 2360000\n",
      "  iterations_since_restore: 118\n",
      "  node_ip: 192.168.0.33\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 29.472374429223738\n",
      "    ram_util_percent: 70.16438356164383\n",
      "  pid: 25804\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09937518288830233\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.304234726737437\n",
      "    mean_inference_ms: 2.0240753161134433\n",
      "    mean_raw_obs_processing_ms: 21.684945852721057\n",
      "  time_since_restore: 35774.84409427643\n",
      "  time_this_iter_s: 306.5190441608429\n",
      "  time_total_s: 35774.84409427643\n",
      "  timers:\n",
      "    learn_throughput: 225.741\n",
      "    learn_time_ms: 88597.242\n",
      "    sample_throughput: 90.464\n",
      "    sample_time_ms: 221081.765\n",
      "    update_time_ms: 2.984\n",
      "  timestamp: 1659399371\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2360000\n",
      "  training_iteration: 118\n",
      "  trial_id: fd1ac_00000\n",
      "  \u001b[2m\u001b[36m(pid=25804)\u001b[0m 2022-08-01 19:16:11,279\tWARNING ppo.py:242 -- The magnitude of your environment rewards are more than 221.0x the scale of `vf_clip_param`. This means that it will take more than 221.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Memory usage on this node: 21.9/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/4.91 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_2_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_5_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_1_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_4_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_0_28b5456e04c30e30e910ccff5ed5840d, 0.0/6.0 CPU_group_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_3_28b5456e04c30e30e910ccff5ed5840d)\n",
      "Result logdir: /home/michael/ray_results/stabilizing_the_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_fd1ac_00000 | RUNNING  | 192.168.0.33:25804 |    118 |          35774.8 | 2360000 |  2211.04 |              3239.79 |              1467.96 |               4000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m ring length: 253\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m v_max: 4.807278529691987\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m ring length: 246\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m v_max: 4.474587708995648\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m ring length: 245\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m v_max: 4.427046998576354\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m ring length: 261\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m v_max: 5.187261846097525\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m ring length: 250\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m v_max: 4.664717904914125\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_fd1ac_00000:\n",
      "  agent_timesteps_total: 2380000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-01_19-21-16\n",
      "  done: false\n",
      "  episode_len_mean: 4000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3236.142395283845\n",
      "  episode_reward_mean: 2229.9460427241734\n",
      "  episode_reward_min: 1487.881728051722\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 595\n",
      "  experiment_id: 2b7c5715c95c4828907cbc45e49df6f0\n",
      "  hostname: michael-home\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.3642420526593923e-13\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -3.1902078756101573\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.022387584373064453\n",
      "          policy_loss: 0.003159761694141302\n",
      "          total_loss: 651.953224180914\n",
      "          vf_explained_var: -3.7205447611654563e-09\n",
      "          vf_loss: 651.9500633276192\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 2380000\n",
      "    num_steps_sampled: 2380000\n",
      "    num_steps_trained: 2380000\n",
      "  iterations_since_restore: 119\n",
      "  node_ip: 192.168.0.33\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 27.509655172413794\n",
      "    ram_util_percent: 70.04206896551725\n",
      "  pid: 25804\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09940144739392114\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.301791945794225\n",
      "    mean_inference_ms: 2.024707663401188\n",
      "    mean_raw_obs_processing_ms: 21.685743258090397\n",
      "  time_since_restore: 36079.74415516853\n",
      "  time_this_iter_s: 304.9000608921051\n",
      "  time_total_s: 36079.74415516853\n",
      "  timers:\n",
      "    learn_throughput: 224.837\n",
      "    learn_time_ms: 88953.277\n",
      "    sample_throughput: 90.703\n",
      "    sample_time_ms: 220499.825\n",
      "    update_time_ms: 2.938\n",
      "  timestamp: 1659399676\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2380000\n",
      "  training_iteration: 119\n",
      "  trial_id: fd1ac_00000\n",
      "  \n",
      "\u001b[2m\u001b[36m(pid=25804)\u001b[0m 2022-08-01 19:21:16,257\tWARNING ppo.py:242 -- The magnitude of your environment rewards are more than 223.0x the scale of `vf_clip_param`. This means that it will take more than 223.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "== Status ==\n",
      "Memory usage on this node: 21.9/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/4.91 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_3_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_4_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_5_28b5456e04c30e30e910ccff5ed5840d, 0.0/6.0 CPU_group_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_0_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_1_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_2_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 accelerator_type:G)\n",
      "Result logdir: /home/michael/ray_results/stabilizing_the_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_fd1ac_00000 | RUNNING  | 192.168.0.33:25804 |    119 |          36079.7 | 2380000 |  2229.95 |              3236.14 |              1487.88 |               4000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m ring length: 241\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m v_max: 4.236854288051661\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m ring length: 225\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m v_max: 3.4756971311168834\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m ring length: 245\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m v_max: 4.427046998576354\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m ring length: 234\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m v_max: 3.903915223349057\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m ring length: 246\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m v_max: 4.474587708995648\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_fd1ac_00000:\n",
      "  agent_timesteps_total: 2400000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-01_19-26-20\n",
      "  done: false\n",
      "  episode_len_mean: 4000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3236.142395283845\n",
      "  episode_reward_mean: 2232.6495147079727\n",
      "  episode_reward_min: 1487.881728051722\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 600\n",
      "  experiment_id: 2b7c5715c95c4828907cbc45e49df6f0\n",
      "  hostname: michael-home\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.3642420526593923e-13\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -3.216274451753896\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.02702301944754135\n",
      "          policy_loss: 0.003841147109848346\n",
      "          total_loss: 663.66838821727\n",
      "          vf_explained_var: -7.213301067565681e-10\n",
      "          vf_loss: 663.6645457358877\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 2400000\n",
      "    num_steps_sampled: 2400000\n",
      "    num_steps_trained: 2400000\n",
      "  iterations_since_restore: 120\n",
      "  node_ip: 192.168.0.33\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 27.795622119815665\n",
      "    ram_util_percent: 70.13294930875576\n",
      "  pid: 25804\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09942744682932059\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.299387584946533\n",
      "    mean_inference_ms: 2.0253202226638023\n",
      "    mean_raw_obs_processing_ms: 21.686511225263168\n",
      "  time_since_restore: 36384.23333358765\n",
      "  time_this_iter_s: 304.48917841911316\n",
      "  time_total_s: 36384.23333358765\n",
      "  timers:\n",
      "    learn_throughput: 224.817\n",
      "    learn_time_ms: 88961.237\n",
      "    sample_throughput: 90.744\n",
      "    sample_time_ms: 220401.371\n",
      "    update_time_ms: 2.974\n",
      "  timestamp: 1659399980\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2400000\n",
      "  training_iteration: 120\n",
      "  trial_id: fd1ac_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Memory usage on this node: 21.8/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/4.91 GiB heap, 0.0/14.65 GiB objects (0.0/6.0 CPU_group_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_5_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_0_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_2_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_4_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_3_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_1_28b5456e04c30e30e910ccff5ed5840d)\n",
      "Result logdir: /home/michael/ray_results/stabilizing_the_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_fd1ac_00000 | RUNNING  | 192.168.0.33:25804 |    120 |          36384.2 | 2400000 |  2232.65 |              3236.14 |              1487.88 |               4000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=25804)\u001b[0m 2022-08-01 19:26:20,846\tWARNING ppo.py:242 -- The magnitude of your environment rewards are more than 223.0x the scale of `vf_clip_param`. This means that it will take more than 223.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m ring length: 223\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m v_max: 3.3805185755587788\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m ring length: 229\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m v_max: 3.666034803266416\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m ring length: 251\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m v_max: 4.71224180704279\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m ring length: 261\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m v_max: 5.187261846097525\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m ring length: 247\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m v_max: 4.522125250095652\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_fd1ac_00000:\n",
      "  agent_timesteps_total: 2420000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-01_19-31-25\n",
      "  done: false\n",
      "  episode_len_mean: 4000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3236.142395283845\n",
      "  episode_reward_mean: 2234.916016205338\n",
      "  episode_reward_min: 1487.881728051722\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 605\n",
      "  experiment_id: 2b7c5715c95c4828907cbc45e49df6f0\n",
      "  hostname: michael-home\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.3642420526593923e-13\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -3.192360408746513\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.021979014593258923\n",
      "          policy_loss: 0.0025955676331012776\n",
      "          total_loss: 657.8786999295471\n",
      "          vf_explained_var: -3.7205447611654563e-09\n",
      "          vf_loss: 657.8761060981993\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 2420000\n",
      "    num_steps_sampled: 2420000\n",
      "    num_steps_trained: 2420000\n",
      "  iterations_since_restore: 121\n",
      "  node_ip: 192.168.0.33\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 27.710574712643673\n",
      "    ram_util_percent: 70.203908045977\n",
      "  pid: 25804\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0994503562705101\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.29711064791039\n",
      "    mean_inference_ms: 2.025839713346548\n",
      "    mean_raw_obs_processing_ms: 21.687295564326963\n",
      "  time_since_restore: 36689.046392679214\n",
      "  time_this_iter_s: 304.813059091568\n",
      "  time_total_s: 36689.046392679214\n",
      "  timers:\n",
      "    learn_throughput: 225.603\n",
      "    learn_time_ms: 88651.371\n",
      "    sample_throughput: 90.745\n",
      "    sample_time_ms: 220397.275\n",
      "    update_time_ms: 2.952\n",
      "  timestamp: 1659400285\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2420000\n",
      "  training_iteration: 121\n",
      "  trial_id: fd1ac_00000\n",
      "  \n",
      "\u001b[2m\u001b[36m(pid=25804)\u001b[0m 2022-08-01 19:31:25,752\tWARNING ppo.py:242 -- The magnitude of your environment rewards are more than 223.0x the scale of `vf_clip_param`. This means that it will take more than 223.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "== Status ==\n",
      "Memory usage on this node: 21.9/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/4.91 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_4_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_1_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_3_28b5456e04c30e30e910ccff5ed5840d, 0.0/6.0 CPU_group_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_2_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_5_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_0_28b5456e04c30e30e910ccff5ed5840d)\n",
      "Result logdir: /home/michael/ray_results/stabilizing_the_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_fd1ac_00000 | RUNNING  | 192.168.0.33:25804 |    121 |            36689 | 2420000 |  2234.92 |              3236.14 |              1487.88 |               4000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m ring length: 242\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m v_max: 4.2844067680020546\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m ring length: 265\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m v_max: 5.37714246265477\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m ring length: 238\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m v_max: 4.094180836186086\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m ring length: 234\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m v_max: 3.903915223349057\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m ring length: 257\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m v_max: 4.997304682316517\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_fd1ac_00000:\n",
      "  agent_timesteps_total: 2440000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-01_19-36-31\n",
      "  done: false\n",
      "  episode_len_mean: 4000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3236.142395283845\n",
      "  episode_reward_mean: 2234.975519706984\n",
      "  episode_reward_min: 1483.2724940032845\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 610\n",
      "  experiment_id: 2b7c5715c95c4828907cbc45e49df6f0\n",
      "  hostname: michael-home\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.3642420526593923e-13\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -3.2926636982875266\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.039318961586864744\n",
      "          policy_loss: 0.004390486945386288\n",
      "          total_loss: 663.4596396051395\n",
      "          vf_explained_var: -4.327980640539408e-09\n",
      "          vf_loss: 663.4552497171293\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 2440000\n",
      "    num_steps_sampled: 2440000\n",
      "    num_steps_trained: 2440000\n",
      "  iterations_since_restore: 122\n",
      "  node_ip: 192.168.0.33\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 27.67545871559633\n",
      "    ram_util_percent: 70.37270642201835\n",
      "  pid: 25804\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09947005924916975\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.29491469081915\n",
      "    mean_inference_ms: 2.0262719023399565\n",
      "    mean_raw_obs_processing_ms: 21.688047217838307\n",
      "  time_since_restore: 36994.223235845566\n",
      "  time_this_iter_s: 305.1768431663513\n",
      "  time_total_s: 36994.223235845566\n",
      "  timers:\n",
      "    learn_throughput: 227.281\n",
      "    learn_time_ms: 87996.808\n",
      "    sample_throughput: 90.788\n",
      "    sample_time_ms: 220293.82\n",
      "    update_time_ms: 3.056\n",
      "  timestamp: 1659400591\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2440000\n",
      "  training_iteration: 122\n",
      "  trial_id: fd1ac_00000\n",
      "  \n",
      "\u001b[2m\u001b[36m(pid=25804)\u001b[0m 2022-08-01 19:36:31,019\tWARNING ppo.py:242 -- The magnitude of your environment rewards are more than 223.0x the scale of `vf_clip_param`. This means that it will take more than 223.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Memory usage on this node: 21.9/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/4.91 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_0_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_4_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_1_28b5456e04c30e30e910ccff5ed5840d, 0.0/6.0 CPU_group_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_3_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_2_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_5_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 accelerator_type:G)\n",
      "Result logdir: /home/michael/ray_results/stabilizing_the_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_fd1ac_00000 | RUNNING  | 192.168.0.33:25804 |    122 |          36994.2 | 2440000 |  2234.98 |              3236.14 |              1483.27 |               4000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m ring length: 230\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m v_max: 3.7136148111012934\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m ring length: 261\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m v_max: 5.187261846097525\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m ring length: 237\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m v_max: 4.04661795519353\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m ring length: 256\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m v_max: 4.949804327743507\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m ring length: 221\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m v_max: 3.285334164169417\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_fd1ac_00000:\n",
      "  agent_timesteps_total: 2460000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-01_19-41-36\n",
      "  done: false\n",
      "  episode_len_mean: 4000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3236.142395283845\n",
      "  episode_reward_mean: 2261.600505252058\n",
      "  episode_reward_min: 1483.2724940032845\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 615\n",
      "  experiment_id: 2b7c5715c95c4828907cbc45e49df6f0\n",
      "  hostname: michael-home\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.3642420526593923e-13\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -3.3008996507924073\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.026075601555773494\n",
      "          policy_loss: 0.004173295005005162\n",
      "          total_loss: 665.4411367343489\n",
      "          vf_explained_var: -1.100977531365288e-09\n",
      "          vf_loss: 665.4369639548527\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 2460000\n",
      "    num_steps_sampled: 2460000\n",
      "    num_steps_trained: 2460000\n",
      "  iterations_since_restore: 123\n",
      "  node_ip: 192.168.0.33\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 27.65126436781609\n",
      "    ram_util_percent: 70.52551724137932\n",
      "  pid: 25804\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09948671332072094\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.292834888723533\n",
      "    mean_inference_ms: 2.0266236829262927\n",
      "    mean_raw_obs_processing_ms: 21.68878590431941\n",
      "  time_since_restore: 37299.5067384243\n",
      "  time_this_iter_s: 305.28350257873535\n",
      "  time_total_s: 37299.5067384243\n",
      "  timers:\n",
      "    learn_throughput: 228.876\n",
      "    learn_time_ms: 87383.391\n",
      "    sample_throughput: 90.8\n",
      "    sample_time_ms: 220263.42\n",
      "    update_time_ms: 3.047\n",
      "  timestamp: 1659400896\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2460000\n",
      "  training_iteration: 123\n",
      "  trial_id: fd1ac_00000\n",
      "  \n",
      "\u001b[2m\u001b[36m(pid=25804)\u001b[0m 2022-08-01 19:41:36,388\tWARNING ppo.py:242 -- The magnitude of your environment rewards are more than 226.0x the scale of `vf_clip_param`. This means that it will take more than 226.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "== Status ==\n",
      "Memory usage on this node: 22.0/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/4.91 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_5_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_4_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_1_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_0_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_3_28b5456e04c30e30e910ccff5ed5840d, 0.0/6.0 CPU_group_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_2_28b5456e04c30e30e910ccff5ed5840d)\n",
      "Result logdir: /home/michael/ray_results/stabilizing_the_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_fd1ac_00000 | RUNNING  | 192.168.0.33:25804 |    123 |          37299.5 | 2460000 |   2261.6 |              3236.14 |              1483.27 |               4000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m ring length: 244\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m v_max: 4.379503211387676\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m ring length: 250\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m v_max: 4.664717904914125\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m ring length: 266\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m v_max: 5.42459972166245\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m ring length: 244\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m v_max: 4.379503211387676\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m ring length: 260\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m v_max: 5.139779427502188\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_fd1ac_00000:\n",
      "  agent_timesteps_total: 2480000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-01_19-46-41\n",
      "  done: false\n",
      "  episode_len_mean: 4000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3236.142395283845\n",
      "  episode_reward_mean: 2260.6614109764837\n",
      "  episode_reward_min: 1441.3803462337996\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 620\n",
      "  experiment_id: 2b7c5715c95c4828907cbc45e49df6f0\n",
      "  hostname: michael-home\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.3642420526593923e-13\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -3.3256681301031903\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.026468343225661602\n",
      "          policy_loss: 0.004584530793771053\n",
      "          total_loss: 677.440253697535\n",
      "          vf_explained_var: -2.8473556845654002e-09\n",
      "          vf_loss: 677.4356683135792\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 2480000\n",
      "    num_steps_sampled: 2480000\n",
      "    num_steps_trained: 2480000\n",
      "  iterations_since_restore: 124\n",
      "  node_ip: 192.168.0.33\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 27.657241379310346\n",
      "    ram_util_percent: 70.52206896551724\n",
      "  pid: 25804\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09950361477442952\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.290786127228493\n",
      "    mean_inference_ms: 2.0269672919432056\n",
      "    mean_raw_obs_processing_ms: 21.68950848681795\n",
      "  time_since_restore: 37604.174228191376\n",
      "  time_this_iter_s: 304.6674897670746\n",
      "  time_total_s: 37604.174228191376\n",
      "  timers:\n",
      "    learn_throughput: 230.401\n",
      "    learn_time_ms: 86805.189\n",
      "    sample_throughput: 90.827\n",
      "    sample_time_ms: 220199.638\n",
      "    update_time_ms: 3.032\n",
      "  timestamp: 1659401201\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2480000\n",
      "  training_iteration: 124\n",
      "  trial_id: fd1ac_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=25804)\u001b[0m 2022-08-01 19:46:41,156\tWARNING ppo.py:242 -- The magnitude of your environment rewards are more than 226.0x the scale of `vf_clip_param`. This means that it will take more than 226.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "== Status ==\n",
      "Memory usage on this node: 22.0/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/4.91 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_2_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_5_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_0_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_1_28b5456e04c30e30e910ccff5ed5840d, 0.0/6.0 CPU_group_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_3_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_4_28b5456e04c30e30e910ccff5ed5840d)\n",
      "Result logdir: /home/michael/ray_results/stabilizing_the_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_fd1ac_00000 | RUNNING  | 192.168.0.33:25804 |    124 |          37604.2 | 2480000 |  2260.66 |              3236.14 |              1441.38 |               4000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m ring length: 244\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m v_max: 4.379503211387676\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m ring length: 265\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m v_max: 5.37714246265477\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m ring length: 238\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m v_max: 4.094180836186086\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m ring length: 240\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m v_max: 4.189299083856172\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m ring length: 233\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m v_max: 3.856343201216534\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_fd1ac_00000:\n",
      "  agent_timesteps_total: 2500000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-01_19-51-45\n",
      "  done: false\n",
      "  episode_len_mean: 4000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3236.142395283845\n",
      "  episode_reward_mean: 2262.1614748706256\n",
      "  episode_reward_min: 1441.3803462337996\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 625\n",
      "  experiment_id: 2b7c5715c95c4828907cbc45e49df6f0\n",
      "  hostname: michael-home\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.3642420526593923e-13\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -3.320333419787656\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.025049803235399966\n",
      "          policy_loss: 0.002972595002887782\n",
      "          total_loss: 675.9380802689085\n",
      "          vf_explained_var: -3.6066505337828403e-09\n",
      "          vf_loss: 675.9351068448109\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 2500000\n",
      "    num_steps_sampled: 2500000\n",
      "    num_steps_trained: 2500000\n",
      "  iterations_since_restore: 125\n",
      "  node_ip: 192.168.0.33\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 27.664285714285718\n",
      "    ram_util_percent: 70.56382488479262\n",
      "  pid: 25804\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09951875006536852\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.288820458431843\n",
      "    mean_inference_ms: 2.027248613951318\n",
      "    mean_raw_obs_processing_ms: 21.69022637298992\n",
      "  time_since_restore: 37908.72818160057\n",
      "  time_this_iter_s: 304.55395340919495\n",
      "  time_total_s: 37908.72818160057\n",
      "  timers:\n",
      "    learn_throughput: 232.016\n",
      "    learn_time_ms: 86200.851\n",
      "    sample_throughput: 90.831\n",
      "    sample_time_ms: 220189.589\n",
      "    update_time_ms: 2.989\n",
      "  timestamp: 1659401505\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2500000\n",
      "  training_iteration: 125\n",
      "  trial_id: fd1ac_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 22.0/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/4.91 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_0_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_4_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_3_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_1_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_2_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_5_28b5456e04c30e30e910ccff5ed5840d, 0.0/6.0 CPU_group_28b5456e04c30e30e910ccff5ed5840d)\n",
      "Result logdir: /home/michael/ray_results/stabilizing_the_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_fd1ac_00000 | RUNNING  | 192.168.0.33:25804 |    125 |          37908.7 | 2500000 |  2262.16 |              3236.14 |              1441.38 |               4000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=25804)\u001b[0m 2022-08-01 19:51:45,790\tWARNING ppo.py:242 -- The magnitude of your environment rewards are more than 226.0x the scale of `vf_clip_param`. This means that it will take more than 226.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m ring length: 239\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m v_max: 4.141741239205832\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m ring length: 260\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m v_max: 5.139779427502188\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m ring length: 268\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m v_max: 5.5194978538368025\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m ring length: 245\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m v_max: 4.427046998576354\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m ring length: 244\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m v_max: 4.379503211387676\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_fd1ac_00000:\n",
      "  agent_timesteps_total: 2520000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-01_19-56-50\n",
      "  done: false\n",
      "  episode_len_mean: 4000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3236.142395283845\n",
      "  episode_reward_mean: 2259.433511464264\n",
      "  episode_reward_min: 1441.3803462337996\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 630\n",
      "  experiment_id: 2b7c5715c95c4828907cbc45e49df6f0\n",
      "  hostname: michael-home\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.3642420526593923e-13\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -3.299525550216626\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.03481443147244993\n",
      "          policy_loss: 0.00467800229511989\n",
      "          total_loss: 676.3080220252845\n",
      "          vf_explained_var: -3.1890383667132483e-09\n",
      "          vf_loss: 676.3033446123645\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 2520000\n",
      "    num_steps_sampled: 2520000\n",
      "    num_steps_trained: 2520000\n",
      "  iterations_since_restore: 126\n",
      "  node_ip: 192.168.0.33\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 27.72712643678161\n",
      "    ram_util_percent: 70.66482758620688\n",
      "  pid: 25804\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09953192730520173\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.286936561812382\n",
      "    mean_inference_ms: 2.02746606927189\n",
      "    mean_raw_obs_processing_ms: 21.690925235959764\n",
      "  time_since_restore: 38213.20894098282\n",
      "  time_this_iter_s: 304.4807593822479\n",
      "  time_total_s: 38213.20894098282\n",
      "  timers:\n",
      "    learn_throughput: 233.877\n",
      "    learn_time_ms: 85515.102\n",
      "    sample_throughput: 90.881\n",
      "    sample_time_ms: 220068.291\n",
      "    update_time_ms: 3.032\n",
      "  timestamp: 1659401810\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2520000\n",
      "  training_iteration: 126\n",
      "  trial_id: fd1ac_00000\n",
      "  \n",
      "\u001b[2m\u001b[36m(pid=25804)\u001b[0m 2022-08-01 19:56:50,358\tWARNING ppo.py:242 -- The magnitude of your environment rewards are more than 226.0x the scale of `vf_clip_param`. This means that it will take more than 226.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Memory usage on this node: 22.1/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/4.91 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_1_28b5456e04c30e30e910ccff5ed5840d, 0.0/6.0 CPU_group_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_5_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_3_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_0_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_4_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_2_28b5456e04c30e30e910ccff5ed5840d)\n",
      "Result logdir: /home/michael/ray_results/stabilizing_the_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_fd1ac_00000 | RUNNING  | 192.168.0.33:25804 |    126 |          38213.2 | 2520000 |  2259.43 |              3236.14 |              1441.38 |               4000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m ring length: 250\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m v_max: 4.664717904914125\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m ring length: 238\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m v_max: 4.094180836186086\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m ring length: 226\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m v_max: 3.5232840694769565\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m ring length: 250\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m v_max: 4.664717904914125\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m ring length: 233\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m v_max: 3.856343201216534\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_fd1ac_00000:\n",
      "  agent_timesteps_total: 2540000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-01_20-01-55\n",
      "  done: false\n",
      "  episode_len_mean: 4000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3236.142395283845\n",
      "  episode_reward_mean: 2283.0550853422005\n",
      "  episode_reward_min: 1441.3803462337996\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 635\n",
      "  experiment_id: 2b7c5715c95c4828907cbc45e49df6f0\n",
      "  hostname: michael-home\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.3642420526593923e-13\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -3.320472539610164\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.025571247734126463\n",
      "          policy_loss: 0.0047863985186407144\n",
      "          total_loss: 691.0489736544858\n",
      "          vf_explained_var: -2.543637744878424e-09\n",
      "          vf_loss: 691.0441894555547\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 2540000\n",
      "    num_steps_sampled: 2540000\n",
      "    num_steps_trained: 2540000\n",
      "  iterations_since_restore: 127\n",
      "  node_ip: 192.168.0.33\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 27.642889908256883\n",
      "    ram_util_percent: 70.75298165137613\n",
      "  pid: 25804\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09954366363303006\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.285126565488998\n",
      "    mean_inference_ms: 2.0276340887354682\n",
      "    mean_raw_obs_processing_ms: 21.69164237095463\n",
      "  time_since_restore: 38518.479199409485\n",
      "  time_this_iter_s: 305.27025842666626\n",
      "  time_total_s: 38518.479199409485\n",
      "  timers:\n",
      "    learn_throughput: 235.39\n",
      "    learn_time_ms: 84965.268\n",
      "    sample_throughput: 90.895\n",
      "    sample_time_ms: 220033.341\n",
      "    update_time_ms: 3.034\n",
      "  timestamp: 1659402115\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2540000\n",
      "  training_iteration: 127\n",
      "  trial_id: fd1ac_00000\n",
      "  \n",
      "\u001b[2m\u001b[36m(pid=25804)\u001b[0m 2022-08-01 20:01:55,710\tWARNING ppo.py:242 -- The magnitude of your environment rewards are more than 228.0x the scale of `vf_clip_param`. This means that it will take more than 228.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "== Status ==\n",
      "Memory usage on this node: 22.1/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/4.91 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_1_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_4_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_0_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_2_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_3_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_5_28b5456e04c30e30e910ccff5ed5840d, 0.0/6.0 CPU_group_28b5456e04c30e30e910ccff5ed5840d)\n",
      "Result logdir: /home/michael/ray_results/stabilizing_the_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_fd1ac_00000 | RUNNING  | 192.168.0.33:25804 |    127 |          38518.5 | 2540000 |  2283.06 |              3236.14 |              1441.38 |               4000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m ring length: 260\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m v_max: 5.139779427502188\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m ring length: 233\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m v_max: 3.856343201216534\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m ring length: 256\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m v_max: 4.949804327743507\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m ring length: 270\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m v_max: 5.6143732387852054\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m ring length: 224\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m v_max: 3.4281086136538996\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_fd1ac_00000:\n",
      "  agent_timesteps_total: 2560000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-01_20-07-00\n",
      "  done: false\n",
      "  episode_len_mean: 4000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3236.142395283845\n",
      "  episode_reward_mean: 2281.525201481443\n",
      "  episode_reward_min: 1441.3803462337996\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 640\n",
      "  experiment_id: 2b7c5715c95c4828907cbc45e49df6f0\n",
      "  hostname: michael-home\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.3642420526593923e-13\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -3.410448714881946\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.03843371987715067\n",
      "          policy_loss: 0.005791536433574785\n",
      "          total_loss: 685.2210122515442\n",
      "          vf_explained_var: -4.707628065148128e-09\n",
      "          vf_loss: 685.2152243122174\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 2560000\n",
      "    num_steps_sampled: 2560000\n",
      "    num_steps_trained: 2560000\n",
      "  iterations_since_restore: 128\n",
      "  node_ip: 192.168.0.33\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 27.672643678160917\n",
      "    ram_util_percent: 70.8216091954023\n",
      "  pid: 25804\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09955501967202526\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.28333722732046\n",
      "    mean_inference_ms: 2.0277777143872537\n",
      "    mean_raw_obs_processing_ms: 21.692358699930647\n",
      "  time_since_restore: 38823.56533622742\n",
      "  time_this_iter_s: 305.08613681793213\n",
      "  time_total_s: 38823.56533622742\n",
      "  timers:\n",
      "    learn_throughput: 235.738\n",
      "    learn_time_ms: 84839.892\n",
      "    sample_throughput: 90.903\n",
      "    sample_time_ms: 220015.298\n",
      "    update_time_ms: 3.032\n",
      "  timestamp: 1659402420\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2560000\n",
      "  training_iteration: 128\n",
      "  trial_id: fd1ac_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=25804)\u001b[0m 2022-08-01 20:07:00,881\tWARNING ppo.py:242 -- The magnitude of your environment rewards are more than 228.0x the scale of `vf_clip_param`. This means that it will take more than 228.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "== Status ==\n",
      "Memory usage on this node: 22.1/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/4.91 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_2_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_3_28b5456e04c30e30e910ccff5ed5840d, 0.0/6.0 CPU_group_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_0_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_4_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_5_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_1_28b5456e04c30e30e910ccff5ed5840d)\n",
      "Result logdir: /home/michael/ray_results/stabilizing_the_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_fd1ac_00000 | RUNNING  | 192.168.0.33:25804 |    128 |          38823.6 | 2560000 |  2281.53 |              3236.14 |              1441.38 |               4000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m ring length: 243\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m v_max: 4.331956438196479\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m ring length: 268\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m v_max: 5.5194978538368025\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m ring length: 254\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m v_max: 4.854791141191876\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m ring length: 234\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m v_max: 3.903915223349057\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m ring length: 267\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m v_max: 5.472051563171826\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_fd1ac_00000:\n",
      "  agent_timesteps_total: 2580000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-01_20-12-05\n",
      "  done: false\n",
      "  episode_len_mean: 4000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3275.946860697429\n",
      "  episode_reward_mean: 2283.3604142545005\n",
      "  episode_reward_min: 1441.3803462337996\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 645\n",
      "  experiment_id: 2b7c5715c95c4828907cbc45e49df6f0\n",
      "  hostname: michael-home\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.3642420526593923e-13\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -3.426328880164274\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.03710123891309159\n",
      "          policy_loss: 0.004794212749856672\n",
      "          total_loss: 701.9976067731335\n",
      "          vf_explained_var: -1.100977531365288e-09\n",
      "          vf_loss: 701.9928143592397\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 2580000\n",
      "    num_steps_sampled: 2580000\n",
      "    num_steps_trained: 2580000\n",
      "  iterations_since_restore: 129\n",
      "  node_ip: 192.168.0.33\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 27.69494252873563\n",
      "    ram_util_percent: 70.93471264367817\n",
      "  pid: 25804\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09956671057463279\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.281552085597387\n",
      "    mean_inference_ms: 2.0279469131570718\n",
      "    mean_raw_obs_processing_ms: 21.692393422635327\n",
      "  time_since_restore: 39128.284022808075\n",
      "  time_this_iter_s: 304.71868658065796\n",
      "  time_total_s: 39128.284022808075\n",
      "  timers:\n",
      "    learn_throughput: 235.771\n",
      "    learn_time_ms: 84828.25\n",
      "    sample_throughput: 90.905\n",
      "    sample_time_ms: 220008.898\n",
      "    update_time_ms: 3.057\n",
      "  timestamp: 1659402725\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2580000\n",
      "  training_iteration: 129\n",
      "  trial_id: fd1ac_00000\n",
      "  \n",
      "\u001b[2m\u001b[36m(pid=25804)\u001b[0m 2022-08-01 20:12:05,688\tWARNING ppo.py:242 -- The magnitude of your environment rewards are more than 228.0x the scale of `vf_clip_param`. This means that it will take more than 228.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "== Status ==\n",
      "Memory usage on this node: 22.1/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/4.91 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_4_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_2_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_5_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_1_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_0_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_3_28b5456e04c30e30e910ccff5ed5840d, 0.0/6.0 CPU_group_28b5456e04c30e30e910ccff5ed5840d)\n",
      "Result logdir: /home/michael/ray_results/stabilizing_the_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_fd1ac_00000 | RUNNING  | 192.168.0.33:25804 |    129 |          39128.3 | 2580000 |  2283.36 |              3275.95 |              1441.38 |               4000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m ring length: 229\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m v_max: 3.666034803266416\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m ring length: 236\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m v_max: 3.999052674951912\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m ring length: 238\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m v_max: 4.094180836186086\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m ring length: 226\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m v_max: 3.5232840694769565\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m ring length: 257\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m v_max: 4.997304682316517\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m  Retrying in 1 seconds\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_fd1ac_00000:\n",
      "  agent_timesteps_total: 2600000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-01_20-17-11\n",
      "  done: false\n",
      "  episode_len_mean: 4000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3275.946860697429\n",
      "  episode_reward_mean: 2289.0926855256585\n",
      "  episode_reward_min: 1441.3803462337996\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 650\n",
      "  experiment_id: 2b7c5715c95c4828907cbc45e49df6f0\n",
      "  hostname: michael-home\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.3642420526593923e-13\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -3.3628476545309565\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.021465469571497812\n",
      "          policy_loss: 0.0029004914947091394\n",
      "          total_loss: 707.5961665183876\n",
      "          vf_explained_var: -1.025048046443544e-09\n",
      "          vf_loss: 707.5932641873694\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 2600000\n",
      "    num_steps_sampled: 2600000\n",
      "    num_steps_trained: 2600000\n",
      "  iterations_since_restore: 130\n",
      "  node_ip: 192.168.0.33\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 27.492889908256885\n",
      "    ram_util_percent: 71.09036697247706\n",
      "  pid: 25804\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09957840162279546\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.279830881311433\n",
      "    mean_inference_ms: 2.0281092177995514\n",
      "    mean_raw_obs_processing_ms: 21.692375951225216\n",
      "  time_since_restore: 39433.74330496788\n",
      "  time_this_iter_s: 305.4592821598053\n",
      "  time_total_s: 39433.74330496788\n",
      "  timers:\n",
      "    learn_throughput: 235.761\n",
      "    learn_time_ms: 84831.838\n",
      "    sample_throughput: 90.867\n",
      "    sample_time_ms: 220102.377\n",
      "    update_time_ms: 3.015\n",
      "  timestamp: 1659403031\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2600000\n",
      "  training_iteration: 130\n",
      "  trial_id: fd1ac_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=25804)\u001b[0m 2022-08-01 20:17:11,242\tWARNING ppo.py:242 -- The magnitude of your environment rewards are more than 229.0x the scale of `vf_clip_param`. This means that it will take more than 229.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "== Status ==\n",
      "Memory usage on this node: 22.2/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/4.91 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_5_28b5456e04c30e30e910ccff5ed5840d, 0.0/6.0 CPU_group_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_1_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_0_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_3_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_2_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_4_28b5456e04c30e30e910ccff5ed5840d)\n",
      "Result logdir: /home/michael/ray_results/stabilizing_the_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_fd1ac_00000 | RUNNING  | 192.168.0.33:25804 |    130 |          39433.7 | 2600000 |  2289.09 |              3275.95 |              1441.38 |               4000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m ring length: 236\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m v_max: 3.999052674951912\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m ring length: 249\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m v_max: 4.617190445131122\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m ring length: 225\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m v_max: 3.4756971311168834\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m ring length: 260\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m v_max: 5.139779427502188\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m ring length: 269\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m v_max: 5.566938458220921\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_fd1ac_00000:\n",
      "  agent_timesteps_total: 2620000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-01_20-22-18\n",
      "  done: false\n",
      "  episode_len_mean: 4000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3275.946860697429\n",
      "  episode_reward_mean: 2278.8471568327695\n",
      "  episode_reward_min: 1441.3803462337996\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 655\n",
      "  experiment_id: 2b7c5715c95c4828907cbc45e49df6f0\n",
      "  hostname: michael-home\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.3642420526593923e-13\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -3.3534134085770626\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.03295927723060717\n",
      "          policy_loss: 0.003783048674832009\n",
      "          total_loss: 695.7745625149672\n",
      "          vf_explained_var: -2.6954967147219122e-09\n",
      "          vf_loss: 695.770781942234\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 2620000\n",
      "    num_steps_sampled: 2620000\n",
      "    num_steps_trained: 2620000\n",
      "  iterations_since_restore: 131\n",
      "  node_ip: 192.168.0.33\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 28.221004566210045\n",
      "    ram_util_percent: 71.21643835616439\n",
      "  pid: 25804\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09958886369463212\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.278164275588566\n",
      "    mean_inference_ms: 2.02823522565259\n",
      "    mean_raw_obs_processing_ms: 21.692364688594665\n",
      "  time_since_restore: 39741.27827167511\n",
      "  time_this_iter_s: 307.5349667072296\n",
      "  time_total_s: 39741.27827167511\n",
      "  timers:\n",
      "    learn_throughput: 235.011\n",
      "    learn_time_ms: 85102.516\n",
      "    sample_throughput: 90.866\n",
      "    sample_time_ms: 220103.802\n",
      "    update_time_ms: 2.959\n",
      "  timestamp: 1659403338\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2620000\n",
      "  training_iteration: 131\n",
      "  trial_id: fd1ac_00000\n",
      "  \n",
      "\u001b[2m\u001b[36m(pid=25804)\u001b[0m 2022-08-01 20:22:18,864\tWARNING ppo.py:242 -- The magnitude of your environment rewards are more than 228.0x the scale of `vf_clip_param`. This means that it will take more than 228.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "== Status ==\n",
      "Memory usage on this node: 22.3/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/4.91 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_2_28b5456e04c30e30e910ccff5ed5840d, 0.0/6.0 CPU_group_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_3_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_4_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_5_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_0_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_1_28b5456e04c30e30e910ccff5ed5840d)\n",
      "Result logdir: /home/michael/ray_results/stabilizing_the_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_fd1ac_00000 | RUNNING  | 192.168.0.33:25804 |    131 |          39741.3 | 2620000 |  2278.85 |              3275.95 |              1441.38 |               4000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m ring length: 220\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m v_max: 3.2377399006821497\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m ring length: 258\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m v_max: 5.044800727535787\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m ring length: 268\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m v_max: 5.5194978538368025\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m ring length: 231\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m v_max: 3.761192925277308\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m ring length: 226\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m v_max: 3.5232840694769565\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_fd1ac_00000:\n",
      "  agent_timesteps_total: 2640000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-01_20-27-24\n",
      "  done: false\n",
      "  episode_len_mean: 4000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3275.946860697429\n",
      "  episode_reward_mean: 2287.166065413263\n",
      "  episode_reward_min: 1441.3803462337996\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 660\n",
      "  experiment_id: 2b7c5715c95c4828907cbc45e49df6f0\n",
      "  hostname: michael-home\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.3642420526593923e-13\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -3.1971224941265812\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.021837709718428338\n",
      "          policy_loss: 0.0020923517472377627\n",
      "          total_loss: 709.3213021563877\n",
      "          vf_explained_var: 0.0\n",
      "          vf_loss: 709.319210878603\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 2640000\n",
      "    num_steps_sampled: 2640000\n",
      "    num_steps_trained: 2640000\n",
      "  iterations_since_restore: 132\n",
      "  node_ip: 192.168.0.33\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 27.702059496567504\n",
      "    ram_util_percent: 71.99061784897025\n",
      "  pid: 25804\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0995979305822061\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.276559631774234\n",
      "    mean_inference_ms: 2.0283218560458725\n",
      "    mean_raw_obs_processing_ms: 21.692253124770524\n",
      "  time_since_restore: 40046.89961767197\n",
      "  time_this_iter_s: 305.6213459968567\n",
      "  time_total_s: 40046.89961767197\n",
      "  timers:\n",
      "    learn_throughput: 234.917\n",
      "    learn_time_ms: 85136.299\n",
      "    sample_throughput: 90.862\n",
      "    sample_time_ms: 220114.578\n",
      "    update_time_ms: 2.887\n",
      "  timestamp: 1659403644\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2640000\n",
      "  training_iteration: 132\n",
      "  trial_id: fd1ac_00000\n",
      "  \n",
      "\u001b[2m\u001b[36m(pid=25804)\u001b[0m 2022-08-01 20:27:24,573\tWARNING ppo.py:242 -- The magnitude of your environment rewards are more than 229.0x the scale of `vf_clip_param`. This means that it will take more than 229.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Memory usage on this node: 22.7/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/4.91 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_2_28b5456e04c30e30e910ccff5ed5840d, 0.0/6.0 CPU_group_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_0_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_1_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_3_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_5_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_4_28b5456e04c30e30e910ccff5ed5840d)\n",
      "Result logdir: /home/michael/ray_results/stabilizing_the_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_fd1ac_00000 | RUNNING  | 192.168.0.33:25804 |    132 |          40046.9 | 2640000 |  2287.17 |              3275.95 |              1441.38 |               4000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m ring length: 251\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m v_max: 4.71224180704279\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m ring length: 230\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m v_max: 3.7136148111012934\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m ring length: 250\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m v_max: 4.664717904914125\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m ring length: 270\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m v_max: 5.6143732387852054\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m ring length: 258\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m v_max: 5.044800727535787\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_fd1ac_00000:\n",
      "  agent_timesteps_total: 2660000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-01_20-32-27\n",
      "  done: false\n",
      "  episode_len_mean: 4000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3275.946860697429\n",
      "  episode_reward_mean: 2286.8477088840523\n",
      "  episode_reward_min: 1441.3803462337996\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 665\n",
      "  experiment_id: 2b7c5715c95c4828907cbc45e49df6f0\n",
      "  hostname: michael-home\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.3642420526593923e-13\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -3.2578782897086658\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.02887344439769813\n",
      "          policy_loss: 0.0019184928185715796\n",
      "          total_loss: 709.6921822274567\n",
      "          vf_explained_var: -7.2133010675656806e-09\n",
      "          vf_loss: 709.6902641393576\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 2660000\n",
      "    num_steps_sampled: 2660000\n",
      "    num_steps_trained: 2660000\n",
      "  iterations_since_restore: 133\n",
      "  node_ip: 192.168.0.33\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 26.73796296296296\n",
      "    ram_util_percent: 72.68912037037036\n",
      "  pid: 25804\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09960413941197968\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.275086274390482\n",
      "    mean_inference_ms: 2.028344614367065\n",
      "    mean_raw_obs_processing_ms: 21.692118402744356\n",
      "  time_since_restore: 40350.03473210335\n",
      "  time_this_iter_s: 303.1351144313812\n",
      "  time_total_s: 40350.03473210335\n",
      "  timers:\n",
      "    learn_throughput: 235.68\n",
      "    learn_time_ms: 84860.86\n",
      "    sample_throughput: 90.837\n",
      "    sample_time_ms: 220175.262\n",
      "    update_time_ms: 2.92\n",
      "  timestamp: 1659403947\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2660000\n",
      "  training_iteration: 133\n",
      "  trial_id: fd1ac_00000\n",
      "  \n",
      "\u001b[2m\u001b[36m(pid=25804)\u001b[0m 2022-08-01 20:32:27,795\tWARNING ppo.py:242 -- The magnitude of your environment rewards are more than 229.0x the scale of `vf_clip_param`. This means that it will take more than 229.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "== Status ==\n",
      "Memory usage on this node: 22.7/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/4.91 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_2_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_0_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_1_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_5_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_3_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_4_28b5456e04c30e30e910ccff5ed5840d, 0.0/6.0 CPU_group_28b5456e04c30e30e910ccff5ed5840d)\n",
      "Result logdir: /home/michael/ray_results/stabilizing_the_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_fd1ac_00000 | RUNNING  | 192.168.0.33:25804 |    133 |            40350 | 2660000 |  2286.85 |              3275.95 |              1441.38 |               4000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m ring length: 221\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m v_max: 3.285334164169417\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m ring length: 248\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m v_max: 4.569659527528101\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m ring length: 265\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m v_max: 5.37714246265477\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m ring length: 232\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m v_max: 3.8087690783250063\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m ring length: 250\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m v_max: 4.664717904914125\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_fd1ac_00000:\n",
      "  agent_timesteps_total: 2680000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-01_20-37-38\n",
      "  done: false\n",
      "  episode_len_mean: 4000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3275.946860697429\n",
      "  episode_reward_mean: 2297.8860010795524\n",
      "  episode_reward_min: 1441.3803462337996\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 670\n",
      "  experiment_id: 2b7c5715c95c4828907cbc45e49df6f0\n",
      "  hostname: michael-home\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.3642420526593923e-13\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -3.1957745275679668\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.021936679060731268\n",
      "          policy_loss: 0.0036531168292721463\n",
      "          total_loss: 707.0519191401779\n",
      "          vf_explained_var: -2.581602487339296e-09\n",
      "          vf_loss: 707.0482662856958\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 2680000\n",
      "    num_steps_sampled: 2680000\n",
      "    num_steps_trained: 2680000\n",
      "  iterations_since_restore: 134\n",
      "  node_ip: 192.168.0.33\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 28.272460496614\n",
      "    ram_util_percent: 72.2234762979684\n",
      "  pid: 25804\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0996070017137805\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.273763207564834\n",
      "    mean_inference_ms: 2.028276670415079\n",
      "    mean_raw_obs_processing_ms: 21.691976697940234\n",
      "  time_since_restore: 40660.52780556679\n",
      "  time_this_iter_s: 310.49307346343994\n",
      "  time_total_s: 40660.52780556679\n",
      "  timers:\n",
      "    learn_throughput: 234.448\n",
      "    learn_time_ms: 85306.843\n",
      "    sample_throughput: 90.781\n",
      "    sample_time_ms: 220311.116\n",
      "    update_time_ms: 3.024\n",
      "  timestamp: 1659404258\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2680000\n",
      "  training_iteration: 134\n",
      "  trial_id: fd1ac_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=25804)\u001b[0m 2022-08-01 20:37:38,380\tWARNING ppo.py:242 -- The magnitude of your environment rewards are more than 230.0x the scale of `vf_clip_param`. This means that it will take more than 230.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "== Status ==\n",
      "Memory usage on this node: 22.5/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/4.91 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_2_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_1_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_4_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_0_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_5_28b5456e04c30e30e910ccff5ed5840d, 0.0/6.0 CPU_group_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_3_28b5456e04c30e30e910ccff5ed5840d)\n",
      "Result logdir: /home/michael/ray_results/stabilizing_the_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_fd1ac_00000 | RUNNING  | 192.168.0.33:25804 |    134 |          40660.5 | 2680000 |  2297.89 |              3275.95 |              1441.38 |               4000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m ring length: 257\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m v_max: 4.997304682316517\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m ring length: 254\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m v_max: 4.854791141191876\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m ring length: 269\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m v_max: 5.566938458220921\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m ring length: 270\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m v_max: 5.6143732387852054\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m ring length: 262\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m v_max: 5.234739483008334\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_fd1ac_00000:\n",
      "  agent_timesteps_total: 2700000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-01_20-42-49\n",
      "  done: false\n",
      "  episode_len_mean: 4000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3275.946860697429\n",
      "  episode_reward_mean: 2304.0616446216304\n",
      "  episode_reward_min: 1441.3803462337996\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 675\n",
      "  experiment_id: 2b7c5715c95c4828907cbc45e49df6f0\n",
      "  hostname: michael-home\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.3642420526593923e-13\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -3.341891511686289\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.03489110148704635\n",
      "          policy_loss: 0.004080149148602726\n",
      "          total_loss: 712.4692509997423\n",
      "          vf_explained_var: -3.4168268214784803e-09\n",
      "          vf_loss: 712.465168327283\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 2700000\n",
      "    num_steps_sampled: 2700000\n",
      "    num_steps_trained: 2700000\n",
      "  iterations_since_restore: 135\n",
      "  node_ip: 192.168.0.33\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 30.97155756207675\n",
      "    ram_util_percent: 73.0277652370203\n",
      "  pid: 25804\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0996099251796607\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.272488346594177\n",
      "    mean_inference_ms: 2.028205744726073\n",
      "    mean_raw_obs_processing_ms: 21.69182302097886\n",
      "  time_since_restore: 40971.10027909279\n",
      "  time_this_iter_s: 310.572473526001\n",
      "  time_total_s: 40971.10027909279\n",
      "  timers:\n",
      "    learn_throughput: 233.048\n",
      "    learn_time_ms: 85819.155\n",
      "    sample_throughput: 90.744\n",
      "    sample_time_ms: 220399.485\n",
      "    update_time_ms: 3.027\n",
      "  timestamp: 1659404569\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2700000\n",
      "  training_iteration: 135\n",
      "  trial_id: fd1ac_00000\n",
      "  \n",
      "\u001b[2m\u001b[36m(pid=25804)\u001b[0m 2022-08-01 20:42:49,056\tWARNING ppo.py:242 -- The magnitude of your environment rewards are more than 230.0x the scale of `vf_clip_param`. This means that it will take more than 230.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "== Status ==\n",
      "Memory usage on this node: 22.8/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/4.91 GiB heap, 0.0/14.65 GiB objects (0.0/6.0 CPU_group_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_5_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_0_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_3_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_4_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_2_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_1_28b5456e04c30e30e910ccff5ed5840d)\n",
      "Result logdir: /home/michael/ray_results/stabilizing_the_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_fd1ac_00000 | RUNNING  | 192.168.0.33:25804 |    135 |          40971.1 | 2700000 |  2304.06 |              3275.95 |              1441.38 |               4000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m ring length: 235\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m v_max: 3.9514850725282584\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m ring length: 265\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m v_max: 5.37714246265477\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m ring length: 240\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m v_max: 4.189299083856172\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m ring length: 226\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m v_max: 3.5232840694769565\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m ring length: 222\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m v_max: 3.3329270738617227\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m  Retrying in 1 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m Could not connect to TraCI server at localhost:57823 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m Could not connect to TraCI server at localhost:35657 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m Could not connect to TraCI server at localhost:41611 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m Could not connect to TraCI server at localhost:45453 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m Could not connect to TraCI server at localhost:45851 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m Could not connect to TraCI server at localhost:57823 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m  Retrying in 3 seconds\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m Could not connect to TraCI server at localhost:35657 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m  Retrying in 3 seconds\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m Could not connect to TraCI server at localhost:41611 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m  Retrying in 3 seconds\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m Could not connect to TraCI server at localhost:45453 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m  Retrying in 3 seconds\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m Could not connect to TraCI server at localhost:45851 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m  Retrying in 3 seconds\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_fd1ac_00000:\n",
      "  agent_timesteps_total: 2720000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-01_20-47-59\n",
      "  done: false\n",
      "  episode_len_mean: 4000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3275.946860697429\n",
      "  episode_reward_mean: 2325.925514301807\n",
      "  episode_reward_min: 1441.3803462337996\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 680\n",
      "  experiment_id: 2b7c5715c95c4828907cbc45e49df6f0\n",
      "  hostname: michael-home\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.3642420526593923e-13\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -3.3742500838200757\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.030192319230287173\n",
      "          policy_loss: 0.004432206560270327\n",
      "          total_loss: 722.4372195735858\n",
      "          vf_explained_var: -1.7843428956609841e-09\n",
      "          vf_loss: 722.4327847486849\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 2720000\n",
      "    num_steps_sampled: 2720000\n",
      "    num_steps_trained: 2720000\n",
      "  iterations_since_restore: 136\n",
      "  node_ip: 192.168.0.33\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 27.78153153153153\n",
      "    ram_util_percent: 73.01936936936937\n",
      "  pid: 25804\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09960976281327312\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.271296855275146\n",
      "    mean_inference_ms: 2.0280491083736245\n",
      "    mean_raw_obs_processing_ms: 21.692190025670506\n",
      "  time_since_restore: 41281.89450907707\n",
      "  time_this_iter_s: 310.79422998428345\n",
      "  time_total_s: 41281.89450907707\n",
      "  timers:\n",
      "    learn_throughput: 233.281\n",
      "    learn_time_ms: 85733.408\n",
      "    sample_throughput: 90.45\n",
      "    sample_time_ms: 221115.644\n",
      "    update_time_ms: 3.038\n",
      "  timestamp: 1659404879\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2720000\n",
      "  training_iteration: 136\n",
      "  trial_id: fd1ac_00000\n",
      "  \n",
      "\u001b[2m\u001b[36m(pid=25804)\u001b[0m 2022-08-01 20:47:59,944\tWARNING ppo.py:242 -- The magnitude of your environment rewards are more than 233.0x the scale of `vf_clip_param`. This means that it will take more than 233.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "== Status ==\n",
      "Memory usage on this node: 22.8/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/4.91 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_3_28b5456e04c30e30e910ccff5ed5840d, 0.0/6.0 CPU_group_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_1_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_2_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_5_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_4_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_0_28b5456e04c30e30e910ccff5ed5840d)\n",
      "Result logdir: /home/michael/ray_results/stabilizing_the_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_fd1ac_00000 | RUNNING  | 192.168.0.33:25804 |    136 |          41281.9 | 2720000 |  2325.93 |              3275.95 |              1441.38 |               4000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m ring length: 221\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m v_max: 3.285334164169417\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m ring length: 226\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m v_max: 3.5232840694769565\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m ring length: 232\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m v_max: 3.8087690783250063\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m ring length: 263\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m v_max: 5.282212215151455\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m ring length: 265\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m v_max: 5.37714246265477\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_fd1ac_00000:\n",
      "  agent_timesteps_total: 2740000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-01_20-53-05\n",
      "  done: false\n",
      "  episode_len_mean: 4000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3275.946860697429\n",
      "  episode_reward_mean: 2307.395702801924\n",
      "  episode_reward_min: 1441.3803462337996\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 685\n",
      "  experiment_id: 2b7c5715c95c4828907cbc45e49df6f0\n",
      "  hostname: michael-home\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.3642420526593923e-13\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -3.3221052403662616\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.024600191066278126\n",
      "          policy_loss: 0.0033031694306309817\n",
      "          total_loss: 733.5888718987726\n",
      "          vf_explained_var: -2.353814032574064e-09\n",
      "          vf_loss: 733.5855662254771\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 2740000\n",
      "    num_steps_sampled: 2740000\n",
      "    num_steps_trained: 2740000\n",
      "  iterations_since_restore: 137\n",
      "  node_ip: 192.168.0.33\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 25.371264367816092\n",
      "    ram_util_percent: 73.11724137931034\n",
      "  pid: 25804\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09960536733397556\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.270270978207144\n",
      "    mean_inference_ms: 2.027765858833954\n",
      "    mean_raw_obs_processing_ms: 21.692501008415952\n",
      "  time_since_restore: 41586.914393901825\n",
      "  time_this_iter_s: 305.0198848247528\n",
      "  time_total_s: 41586.914393901825\n",
      "  timers:\n",
      "    learn_throughput: 233.445\n",
      "    learn_time_ms: 85673.35\n",
      "    sample_throughput: 90.436\n",
      "    sample_time_ms: 221150.355\n",
      "    update_time_ms: 3.033\n",
      "  timestamp: 1659405185\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2740000\n",
      "  training_iteration: 137\n",
      "  trial_id: fd1ac_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=25804)\u001b[0m 2022-08-01 20:53:05,061\tWARNING ppo.py:242 -- The magnitude of your environment rewards are more than 231.0x the scale of `vf_clip_param`. This means that it will take more than 231.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "== Status ==\n",
      "Memory usage on this node: 22.8/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/4.91 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_3_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_4_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_1_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 accelerator_type:G, 0.0/6.0 CPU_group_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_0_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_5_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_2_28b5456e04c30e30e910ccff5ed5840d)\n",
      "Result logdir: /home/michael/ray_results/stabilizing_the_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_fd1ac_00000 | RUNNING  | 192.168.0.33:25804 |    137 |          41586.9 | 2740000 |   2307.4 |              3275.95 |              1441.38 |               4000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m ring length: 238\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m v_max: 4.094180836186086\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m ring length: 264\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m v_max: 5.329679917416892\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m ring length: 248\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m v_max: 4.569659527528101\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m ring length: 256\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m v_max: 4.949804327743507\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m ring length: 264\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m v_max: 5.329679917416892\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_fd1ac_00000:\n",
      "  agent_timesteps_total: 2760000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-01_20-58-08\n",
      "  done: false\n",
      "  episode_len_mean: 4000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3275.946860697429\n",
      "  episode_reward_mean: 2280.5871449124575\n",
      "  episode_reward_min: 1441.3803462337996\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 690\n",
      "  experiment_id: 2b7c5715c95c4828907cbc45e49df6f0\n",
      "  hostname: michael-home\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.3642420526593923e-13\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -3.3191042222794453\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.02945505721039646\n",
      "          policy_loss: 0.004530993456860328\n",
      "          total_loss: 733.0823947602776\n",
      "          vf_explained_var: -3.758509503626328e-09\n",
      "          vf_loss: 733.0778658241223\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 2760000\n",
      "    num_steps_sampled: 2760000\n",
      "    num_steps_trained: 2760000\n",
      "  iterations_since_restore: 138\n",
      "  node_ip: 192.168.0.33\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 25.39976905311778\n",
      "    ram_util_percent: 73.27367205542726\n",
      "  pid: 25804\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09959669142556708\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.269369326243837\n",
      "    mean_inference_ms: 2.0273792953324037\n",
      "    mean_raw_obs_processing_ms: 21.69279706703794\n",
      "  time_since_restore: 41890.327278614044\n",
      "  time_this_iter_s: 303.41288471221924\n",
      "  time_total_s: 41890.327278614044\n",
      "  timers:\n",
      "    learn_throughput: 233.883\n",
      "    learn_time_ms: 85512.762\n",
      "    sample_throughput: 90.44\n",
      "    sample_time_ms: 221142.196\n",
      "    update_time_ms: 3.057\n",
      "  timestamp: 1659405488\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2760000\n",
      "  training_iteration: 138\n",
      "  trial_id: fd1ac_00000\n",
      "  \n",
      "\u001b[2m\u001b[36m(pid=25804)\u001b[0m 2022-08-01 20:58:08,561\tWARNING ppo.py:242 -- The magnitude of your environment rewards are more than 228.0x the scale of `vf_clip_param`. This means that it will take more than 228.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "== Status ==\n",
      "Memory usage on this node: 22.9/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/4.91 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_4_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_1_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_5_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_3_28b5456e04c30e30e910ccff5ed5840d, 0.0/6.0 CPU_group_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_0_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_2_28b5456e04c30e30e910ccff5ed5840d)\n",
      "Result logdir: /home/michael/ray_results/stabilizing_the_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_fd1ac_00000 | RUNNING  | 192.168.0.33:25804 |    138 |          41890.3 | 2760000 |  2280.59 |              3275.95 |              1441.38 |               4000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m ring length: 263\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m v_max: 5.282212215151455\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m ring length: 242\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m v_max: 4.2844067680020546\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m ring length: 266\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m v_max: 5.42459972166245\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m ring length: 226\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m v_max: 3.5232840694769565\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m ring length: 233\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m v_max: 3.856343201216534\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_fd1ac_00000:\n",
      "  agent_timesteps_total: 2780000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-01_21-03-12\n",
      "  done: false\n",
      "  episode_len_mean: 4000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3275.946860697429\n",
      "  episode_reward_mean: 2274.146131930865\n",
      "  episode_reward_min: 1441.3803462337996\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 695\n",
      "  experiment_id: 2b7c5715c95c4828907cbc45e49df6f0\n",
      "  hostname: michael-home\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.3642420526593923e-13\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -3.352401918210801\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.04186566447013593\n",
      "          policy_loss: 0.005585602541590572\n",
      "          total_loss: 721.8105094435869\n",
      "          vf_explained_var: -5.6567466266699284e-09\n",
      "          vf_loss: 721.8049279680678\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 2780000\n",
      "    num_steps_sampled: 2780000\n",
      "    num_steps_trained: 2780000\n",
      "  iterations_since_restore: 139\n",
      "  node_ip: 192.168.0.33\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 25.321428571428573\n",
      "    ram_util_percent: 73.35115207373272\n",
      "  pid: 25804\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09958684179428662\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.268518401760662\n",
      "    mean_inference_ms: 2.0269768114294635\n",
      "    mean_raw_obs_processing_ms: 21.69306721356459\n",
      "  time_since_restore: 42194.10927891731\n",
      "  time_this_iter_s: 303.78200030326843\n",
      "  time_total_s: 42194.10927891731\n",
      "  timers:\n",
      "    learn_throughput: 234.17\n",
      "    learn_time_ms: 85407.859\n",
      "    sample_throughput: 90.435\n",
      "    sample_time_ms: 221152.382\n",
      "    update_time_ms: 3.046\n",
      "  timestamp: 1659405792\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2780000\n",
      "  training_iteration: 139\n",
      "  trial_id: fd1ac_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=25804)\u001b[0m 2022-08-01 21:03:12,443\tWARNING ppo.py:242 -- The magnitude of your environment rewards are more than 227.0x the scale of `vf_clip_param`. This means that it will take more than 227.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "== Status ==\n",
      "Memory usage on this node: 22.9/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/4.91 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_0_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_1_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_3_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_5_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_4_28b5456e04c30e30e910ccff5ed5840d, 0.0/6.0 CPU_group_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_2_28b5456e04c30e30e910ccff5ed5840d)\n",
      "Result logdir: /home/michael/ray_results/stabilizing_the_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_fd1ac_00000 | RUNNING  | 192.168.0.33:25804 |    139 |          42194.1 | 2780000 |  2274.15 |              3275.95 |              1441.38 |               4000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m ring length: 232\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m v_max: 3.8087690783250063\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m ring length: 244\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m v_max: 4.379503211387676\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m ring length: 224\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m v_max: 3.4281086136538996\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m ring length: 244\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m v_max: 4.379503211387676\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m ring length: 262\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m v_max: 5.234739483008334\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_fd1ac_00000:\n",
      "  agent_timesteps_total: 2800000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-01_21-08-15\n",
      "  done: false\n",
      "  episode_len_mean: 4000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3275.946860697429\n",
      "  episode_reward_mean: 2265.8023014040677\n",
      "  episode_reward_min: 1441.3803462337996\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 700\n",
      "  experiment_id: 2b7c5715c95c4828907cbc45e49df6f0\n",
      "  hostname: michael-home\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.046363078989089e-13\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -3.3437608240516323\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.03151091323393563\n",
      "          policy_loss: 0.0035146138443949686\n",
      "          total_loss: 732.0772007255797\n",
      "          vf_explained_var: -6.0743587937395205e-09\n",
      "          vf_loss: 732.0736868196233\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 2800000\n",
      "    num_steps_sampled: 2800000\n",
      "    num_steps_trained: 2800000\n",
      "  iterations_since_restore: 140\n",
      "  node_ip: 192.168.0.33\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 25.415081206496524\n",
      "    ram_util_percent: 73.44965197215778\n",
      "  pid: 25804\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09957563615530185\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.26771032349136\n",
      "    mean_inference_ms: 2.026553671150667\n",
      "    mean_raw_obs_processing_ms: 21.6933336061607\n",
      "  time_since_restore: 42496.592343091965\n",
      "  time_this_iter_s: 302.4830641746521\n",
      "  time_total_s: 42496.592343091965\n",
      "  timers:\n",
      "    learn_throughput: 234.69\n",
      "    learn_time_ms: 85218.763\n",
      "    sample_throughput: 90.481\n",
      "    sample_time_ms: 221041.586\n",
      "    update_time_ms: 3.114\n",
      "  timestamp: 1659406095\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2800000\n",
      "  training_iteration: 140\n",
      "  trial_id: fd1ac_00000\n",
      "  \n",
      "\u001b[2m\u001b[36m(pid=25804)\u001b[0m 2022-08-01 21:08:15,019\tWARNING ppo.py:242 -- The magnitude of your environment rewards are more than 227.0x the scale of `vf_clip_param`. This means that it will take more than 227.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "== Status ==\n",
      "Memory usage on this node: 23.0/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/4.91 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_3_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_4_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_5_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_1_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_2_28b5456e04c30e30e910ccff5ed5840d, 0.0/6.0 CPU_group_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_0_28b5456e04c30e30e910ccff5ed5840d)\n",
      "Result logdir: /home/michael/ray_results/stabilizing_the_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_fd1ac_00000 | RUNNING  | 192.168.0.33:25804 |    140 |          42496.6 | 2800000 |   2265.8 |              3275.95 |              1441.38 |               4000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m ring length: 227\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m v_max: 3.570869368805752\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m ring length: 240\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m v_max: 4.189299083856172\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m ring length: 231\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m v_max: 3.761192925277308\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m ring length: 254\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m v_max: 4.854791141191876\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m ring length: 225\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m v_max: 3.4756971311168834\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_fd1ac_00000:\n",
      "  agent_timesteps_total: 2820000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-01_21-13-19\n",
      "  done: false\n",
      "  episode_len_mean: 4000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3275.946860697429\n",
      "  episode_reward_mean: 2269.0752706928893\n",
      "  episode_reward_min: 1441.3803462337996\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 705\n",
      "  experiment_id: 2b7c5715c95c4828907cbc45e49df6f0\n",
      "  hostname: michael-home\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.046363078989089e-13\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -3.3906352974047325\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.03068467767109848\n",
      "          policy_loss: 0.003293883290058773\n",
      "          total_loss: 733.9696996980412\n",
      "          vf_explained_var: -4.36594538300028e-09\n",
      "          vf_loss: 733.9664088109496\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 2820000\n",
      "    num_steps_sampled: 2820000\n",
      "    num_steps_trained: 2820000\n",
      "  iterations_since_restore: 141\n",
      "  node_ip: 192.168.0.33\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 25.340919540229887\n",
      "    ram_util_percent: 73.61126436781608\n",
      "  pid: 25804\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09956275462820648\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.266976256733983\n",
      "    mean_inference_ms: 2.026090149534787\n",
      "    mean_raw_obs_processing_ms: 21.693581609840717\n",
      "  time_since_restore: 42801.35040283203\n",
      "  time_this_iter_s: 304.7580597400665\n",
      "  time_total_s: 42801.35040283203\n",
      "  timers:\n",
      "    learn_throughput: 235.504\n",
      "    learn_time_ms: 84924.372\n",
      "    sample_throughput: 90.474\n",
      "    sample_time_ms: 221057.893\n",
      "    update_time_ms: 3.168\n",
      "  timestamp: 1659406399\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2820000\n",
      "  training_iteration: 141\n",
      "  trial_id: fd1ac_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=25804)\u001b[0m 2022-08-01 21:13:19,894\tWARNING ppo.py:242 -- The magnitude of your environment rewards are more than 227.0x the scale of `vf_clip_param`. This means that it will take more than 227.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "== Status ==\n",
      "Memory usage on this node: 23.0/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/4.91 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_1_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_4_28b5456e04c30e30e910ccff5ed5840d, 0.0/6.0 CPU_group_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_2_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_3_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_5_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_0_28b5456e04c30e30e910ccff5ed5840d)\n",
      "Result logdir: /home/michael/ray_results/stabilizing_the_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_fd1ac_00000 | RUNNING  | 192.168.0.33:25804 |    141 |          42801.4 | 2820000 |  2269.08 |              3275.95 |              1441.38 |               4000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m ring length: 255\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m v_max: 4.902299776966978\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m ring length: 270\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m v_max: 5.6143732387852054\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m ring length: 269\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m v_max: 5.566938458220921\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m ring length: 267\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m v_max: 5.472051563171826\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m ring length: 220\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m v_max: 3.2377399006821497\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m Could not connect to TraCI server at localhost:41611 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m Could not connect to TraCI server at localhost:45453 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m Could not connect to TraCI server at localhost:45851 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m Could not connect to TraCI server at localhost:57823 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m Could not connect to TraCI server at localhost:35657 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m Could not connect to TraCI server at localhost:57823 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m  Retrying in 3 seconds\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m Could not connect to TraCI server at localhost:41611 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m  Retrying in 3 seconds\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m Could not connect to TraCI server at localhost:45453 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m  Retrying in 3 seconds\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m Could not connect to TraCI server at localhost:45851 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m  Retrying in 3 seconds\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m Could not connect to TraCI server at localhost:35657 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m  Retrying in 3 seconds\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_fd1ac_00000:\n",
      "  agent_timesteps_total: 2840000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-01_21-18-28\n",
      "  done: false\n",
      "  episode_len_mean: 4000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3275.946860697429\n",
      "  episode_reward_mean: 2255.5007624859613\n",
      "  episode_reward_min: 1441.3803462337996\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 710\n",
      "  experiment_id: 2b7c5715c95c4828907cbc45e49df6f0\n",
      "  hostname: michael-home\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.046363078989089e-13\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -3.4521344452147273\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.03481086644681058\n",
      "          policy_loss: 0.003768023176439059\n",
      "          total_loss: 719.8473771903166\n",
      "          vf_explained_var: -3.7205447611654563e-09\n",
      "          vf_loss: 719.8436081041956\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 2840000\n",
      "    num_steps_sampled: 2840000\n",
      "    num_steps_trained: 2840000\n",
      "  iterations_since_restore: 142\n",
      "  node_ip: 192.168.0.33\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 24.884772727272725\n",
      "    ram_util_percent: 73.65840909090909\n",
      "  pid: 25804\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09954843867037859\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.266283281522874\n",
      "    mean_inference_ms: 2.0256082012824423\n",
      "    mean_raw_obs_processing_ms: 21.69435551073283\n",
      "  time_since_restore: 43109.446774721146\n",
      "  time_this_iter_s: 308.0963718891144\n",
      "  time_total_s: 43109.446774721146\n",
      "  timers:\n",
      "    learn_throughput: 236.453\n",
      "    learn_time_ms: 84583.348\n",
      "    sample_throughput: 90.234\n",
      "    sample_time_ms: 221644.78\n",
      "    update_time_ms: 3.225\n",
      "  timestamp: 1659406708\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2840000\n",
      "  training_iteration: 142\n",
      "  trial_id: fd1ac_00000\n",
      "  \n",
      "\u001b[2m\u001b[36m(pid=25804)\u001b[0m 2022-08-01 21:18:28,081\tWARNING ppo.py:242 -- The magnitude of your environment rewards are more than 226.0x the scale of `vf_clip_param`. This means that it will take more than 226.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "== Status ==\n",
      "Memory usage on this node: 23.0/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/4.91 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_1_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_5_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_3_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_4_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_0_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_2_28b5456e04c30e30e910ccff5ed5840d, 0.0/6.0 CPU_group_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 accelerator_type:G)\n",
      "Result logdir: /home/michael/ray_results/stabilizing_the_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_fd1ac_00000 | RUNNING  | 192.168.0.33:25804 |    142 |          43109.4 | 2840000 |   2255.5 |              3275.95 |              1441.38 |               4000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m ring length: 224\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m v_max: 3.4281086136538996\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m ring length: 237\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m v_max: 4.04661795519353\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m ring length: 259\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m v_max: 5.092292348291853\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m ring length: 262\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m v_max: 5.234739483008334\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m ring length: 266\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m v_max: 5.42459972166245\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_fd1ac_00000:\n",
      "  agent_timesteps_total: 2860000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-01_21-23-32\n",
      "  done: false\n",
      "  episode_len_mean: 4000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3275.946860697429\n",
      "  episode_reward_mean: 2277.3397219215985\n",
      "  episode_reward_min: 1441.3803462337996\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 715\n",
      "  experiment_id: 2b7c5715c95c4828907cbc45e49df6f0\n",
      "  hostname: michael-home\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.046363078989089e-13\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -3.497001225021994\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.03350176924249582\n",
      "          policy_loss: 0.004642341221730183\n",
      "          total_loss: 740.4307561364144\n",
      "          vf_explained_var: -4.973381262374232e-09\n",
      "          vf_loss: 740.4261134408841\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 2860000\n",
      "    num_steps_sampled: 2860000\n",
      "    num_steps_trained: 2860000\n",
      "  iterations_since_restore: 143\n",
      "  node_ip: 192.168.0.33\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 25.378801843317977\n",
      "    ram_util_percent: 73.87327188940093\n",
      "  pid: 25804\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09953255684559421\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.26566468384731\n",
      "    mean_inference_ms: 2.025084392024226\n",
      "    mean_raw_obs_processing_ms: 21.695113153651963\n",
      "  time_since_restore: 43413.92192554474\n",
      "  time_this_iter_s: 304.47515082359314\n",
      "  time_total_s: 43413.92192554474\n",
      "  timers:\n",
      "    learn_throughput: 235.995\n",
      "    learn_time_ms: 84747.728\n",
      "    sample_throughput: 90.247\n",
      "    sample_time_ms: 221614.019\n",
      "    update_time_ms: 3.211\n",
      "  timestamp: 1659407012\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2860000\n",
      "  training_iteration: 143\n",
      "  trial_id: fd1ac_00000\n",
      "  \n",
      "\u001b[2m\u001b[36m(pid=25804)\u001b[0m 2022-08-01 21:23:32,647\tWARNING ppo.py:242 -- The magnitude of your environment rewards are more than 228.0x the scale of `vf_clip_param`. This means that it will take more than 228.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "== Status ==\n",
      "Memory usage on this node: 23.1/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/4.91 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_0_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 accelerator_type:G, 0.0/6.0 CPU_group_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_5_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_2_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_3_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_4_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_1_28b5456e04c30e30e910ccff5ed5840d)\n",
      "Result logdir: /home/michael/ray_results/stabilizing_the_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_fd1ac_00000 | RUNNING  | 192.168.0.33:25804 |    143 |          43413.9 | 2860000 |  2277.34 |              3275.95 |              1441.38 |               4000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m ring length: 233\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m v_max: 3.856343201216534\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m ring length: 230\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m v_max: 3.7136148111012934\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m ring length: 261\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m v_max: 5.187261846097525\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m ring length: 258\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m v_max: 5.044800727535787\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m ring length: 229\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m v_max: 3.666034803266416\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_fd1ac_00000:\n",
      "  agent_timesteps_total: 2880000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-01_21-28-35\n",
      "  done: false\n",
      "  episode_len_mean: 4000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3275.946860697429\n",
      "  episode_reward_mean: 2292.4170692644275\n",
      "  episode_reward_min: 1472.0858601143846\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 720\n",
      "  experiment_id: 2b7c5715c95c4828907cbc45e49df6f0\n",
      "  hostname: michael-home\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.046363078989089e-13\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -3.5710075310081435\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.03668655406729656\n",
      "          policy_loss: 0.005415322628483224\n",
      "          total_loss: 743.9917470846966\n",
      "          vf_explained_var: -1.025048046443544e-09\n",
      "          vf_loss: 743.9863320283829\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 2880000\n",
      "    num_steps_sampled: 2880000\n",
      "    num_steps_trained: 2880000\n",
      "  iterations_since_restore: 144\n",
      "  node_ip: 192.168.0.33\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 25.438425925925927\n",
      "    ram_util_percent: 73.90856481481481\n",
      "  pid: 25804\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09951513579260668\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.265088390947568\n",
      "    mean_inference_ms: 2.0245328868215076\n",
      "    mean_raw_obs_processing_ms: 21.695867038673146\n",
      "  time_since_restore: 43716.314168691635\n",
      "  time_this_iter_s: 302.39224314689636\n",
      "  time_total_s: 43716.314168691635\n",
      "  timers:\n",
      "    learn_throughput: 237.857\n",
      "    learn_time_ms: 84084.058\n",
      "    sample_throughput: 90.307\n",
      "    sample_time_ms: 221466.832\n",
      "    update_time_ms: 3.136\n",
      "  timestamp: 1659407315\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2880000\n",
      "  training_iteration: 144\n",
      "  trial_id: fd1ac_00000\n",
      "  \n",
      "\u001b[2m\u001b[36m(pid=25804)\u001b[0m 2022-08-01 21:28:35,126\tWARNING ppo.py:242 -- The magnitude of your environment rewards are more than 229.0x the scale of `vf_clip_param`. This means that it will take more than 229.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "== Status ==\n",
      "Memory usage on this node: 23.1/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/4.91 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_4_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_5_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_0_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_2_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_1_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_3_28b5456e04c30e30e910ccff5ed5840d, 0.0/6.0 CPU_group_28b5456e04c30e30e910ccff5ed5840d)\n",
      "Result logdir: /home/michael/ray_results/stabilizing_the_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_fd1ac_00000 | RUNNING  | 192.168.0.33:25804 |    144 |          43716.3 | 2880000 |  2292.42 |              3275.95 |              1472.09 |               4000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m ring length: 245\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m v_max: 4.427046998576354\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m ring length: 243\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m v_max: 4.331956438196479\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m ring length: 249\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m v_max: 4.617190445131122\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m ring length: 261\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m v_max: 5.187261846097525\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m ring length: 235\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m v_max: 3.9514850725282584\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_fd1ac_00000:\n",
      "  agent_timesteps_total: 2900000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-01_21-33-40\n",
      "  done: false\n",
      "  episode_len_mean: 4000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3275.946860697429\n",
      "  episode_reward_mean: 2271.65660033562\n",
      "  episode_reward_min: 1472.0858601143846\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 725\n",
      "  experiment_id: 2b7c5715c95c4828907cbc45e49df6f0\n",
      "  hostname: michael-home\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.046363078989089e-13\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -3.562912962846695\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.030834339643942908\n",
      "          policy_loss: 0.004042100845420863\n",
      "          total_loss: 741.2715022749202\n",
      "          vf_explained_var: -1.8982371230436e-09\n",
      "          vf_loss: 741.2674609323975\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 2900000\n",
      "    num_steps_sampled: 2900000\n",
      "    num_steps_trained: 2900000\n",
      "  iterations_since_restore: 145\n",
      "  node_ip: 192.168.0.33\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 25.322298850574715\n",
      "    ram_util_percent: 73.9496551724138\n",
      "  pid: 25804\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09949659751734603\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.264561269456213\n",
      "    mean_inference_ms: 2.0239651799657374\n",
      "    mean_raw_obs_processing_ms: 21.696608690308576\n",
      "  time_since_restore: 44021.23482275009\n",
      "  time_this_iter_s: 304.9206540584564\n",
      "  time_total_s: 44021.23482275009\n",
      "  timers:\n",
      "    learn_throughput: 239.309\n",
      "    learn_time_ms: 83574.024\n",
      "    sample_throughput: 90.329\n",
      "    sample_time_ms: 221412.626\n",
      "    update_time_ms: 3.149\n",
      "  timestamp: 1659407620\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2900000\n",
      "  training_iteration: 145\n",
      "  trial_id: fd1ac_00000\n",
      "  \n",
      "\u001b[2m\u001b[36m(pid=25804)\u001b[0m 2022-08-01 21:33:40,141\tWARNING ppo.py:242 -- The magnitude of your environment rewards are more than 227.0x the scale of `vf_clip_param`. This means that it will take more than 227.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "== Status ==\n",
      "Memory usage on this node: 23.1/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/4.91 GiB heap, 0.0/14.65 GiB objects (0.0/6.0 CPU_group_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_0_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_4_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_5_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_3_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_2_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_1_28b5456e04c30e30e910ccff5ed5840d)\n",
      "Result logdir: /home/michael/ray_results/stabilizing_the_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_fd1ac_00000 | RUNNING  | 192.168.0.33:25804 |    145 |          44021.2 | 2900000 |  2271.66 |              3275.95 |              1472.09 |               4000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m ring length: 247\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m v_max: 4.522125250095652\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m ring length: 236\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m v_max: 3.999052674951912\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m ring length: 270\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m v_max: 5.6143732387852054\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m ring length: 245\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m v_max: 4.427046998576354\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m ring length: 264\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m v_max: 5.329679917416892\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_fd1ac_00000:\n",
      "  agent_timesteps_total: 2920000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-01_21-38-42\n",
      "  done: false\n",
      "  episode_len_mean: 4000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3275.946860697429\n",
      "  episode_reward_mean: 2270.0810575571295\n",
      "  episode_reward_min: 1472.0858601143846\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 730\n",
      "  experiment_id: 2b7c5715c95c4828907cbc45e49df6f0\n",
      "  hostname: michael-home\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.046363078989089e-13\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -3.5498422475377467\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.041882722281951855\n",
      "          policy_loss: 0.006029788857314047\n",
      "          total_loss: 735.8007680564929\n",
      "          vf_explained_var: -3.0751441393306322e-09\n",
      "          vf_loss: 735.7947354128406\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 2920000\n",
      "    num_steps_sampled: 2920000\n",
      "    num_steps_trained: 2920000\n",
      "  iterations_since_restore: 146\n",
      "  node_ip: 192.168.0.33\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 25.31090487238979\n",
      "    ram_util_percent: 73.95313225058005\n",
      "  pid: 25804\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0994764365255439\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.26409023905613\n",
      "    mean_inference_ms: 2.023362099532717\n",
      "    mean_raw_obs_processing_ms: 21.6973424245773\n",
      "  time_since_restore: 44323.45616698265\n",
      "  time_this_iter_s: 302.2213442325592\n",
      "  time_total_s: 44323.45616698265\n",
      "  timers:\n",
      "    learn_throughput: 239.69\n",
      "    learn_time_ms: 83441.044\n",
      "    sample_throughput: 90.626\n",
      "    sample_time_ms: 220687.823\n",
      "    update_time_ms: 3.131\n",
      "  timestamp: 1659407922\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2920000\n",
      "  training_iteration: 146\n",
      "  trial_id: fd1ac_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 23.1/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/4.91 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_3_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_5_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 accelerator_type:G, 0.0/6.0 CPU_group_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_0_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_4_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_1_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_2_28b5456e04c30e30e910ccff5ed5840d)\n",
      "Result logdir: /home/michael/ray_results/stabilizing_the_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc                |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_fd1ac_00000 | RUNNING  | 192.168.0.33:25804 |    146 |          44323.5 | 2920000 |  2270.08 |              3275.95 |              1472.09 |               4000 |\n",
      "+-----------------------------------------+----------+--------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=25804)\u001b[0m 2022-08-01 21:38:42,452\tWARNING ppo.py:242 -- The magnitude of your environment rewards are more than 227.0x the scale of `vf_clip_param`. This means that it will take more than 227.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m ring length: 265\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m v_max: 5.37714246265477\n",
      "\u001b[2m\u001b[36m(pid=25808)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m ring length: 250\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m v_max: 4.664717904914125\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m ring length: 250\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m v_max: 4.664717904914125\n",
      "\u001b[2m\u001b[36m(pid=25805)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m ring length: 229\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m v_max: 3.666034803266416\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m ring length: 258\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m v_max: 5.044800727535787\n",
      "\u001b[2m\u001b[36m(pid=25809)\u001b[0m -----------------------\n",
      "2022-08-01 21:40:58,842\tERROR trial_runner.py:773 -- Trial PPO_WaveAttenuationPOEnv-v0_fd1ac_00000: Error processing event.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/michael/anaconda3/envs/flow/lib/python3.7/site-packages/ray/tune/trial_runner.py\", line 739, in _process_trial\n",
      "    results = self.trial_executor.fetch_result(trial)\n",
      "  File \"/home/michael/anaconda3/envs/flow/lib/python3.7/site-packages/ray/tune/ray_trial_executor.py\", line 729, in fetch_result\n",
      "    result = ray.get(trial_future[0], timeout=DEFAULT_GET_TIMEOUT)\n",
      "  File \"/home/michael/anaconda3/envs/flow/lib/python3.7/site-packages/ray/_private/client_mode_hook.py\", line 82, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/michael/anaconda3/envs/flow/lib/python3.7/site-packages/ray/worker.py\", line 1564, in get\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(FatalFlowError): \u001b[36mray::PPO.train()\u001b[39m (pid=25804, ip=192.168.0.33)\n",
      "  File \"python/ray/_raylet.pyx\", line 534, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 484, in ray._raylet.execute_task.function_executor\n",
      "  File \"/home/michael/anaconda3/envs/flow/lib/python3.7/site-packages/ray/_private/function_manager.py\", line 563, in actor_method_executor\n",
      "    return method(__ray_actor, *args, **kwargs)\n",
      "  File \"/home/michael/anaconda3/envs/flow/lib/python3.7/site-packages/ray/rllib/agents/trainer.py\", line 640, in train\n",
      "    raise e\n",
      "  File \"/home/michael/anaconda3/envs/flow/lib/python3.7/site-packages/ray/rllib/agents/trainer.py\", line 629, in train\n",
      "    result = Trainable.train(self)\n",
      "  File \"/home/michael/anaconda3/envs/flow/lib/python3.7/site-packages/ray/tune/trainable.py\", line 237, in train\n",
      "    result = self.step()\n",
      "  File \"/home/michael/anaconda3/envs/flow/lib/python3.7/site-packages/ray/rllib/agents/trainer_template.py\", line 170, in step\n",
      "    res = next(self.train_exec_impl)\n",
      "  File \"/home/michael/anaconda3/envs/flow/lib/python3.7/site-packages/ray/util/iter.py\", line 756, in __next__\n",
      "    return next(self.built_iterator)\n",
      "  File \"/home/michael/anaconda3/envs/flow/lib/python3.7/site-packages/ray/util/iter.py\", line 783, in apply_foreach\n",
      "    for item in it:\n",
      "  File \"/home/michael/anaconda3/envs/flow/lib/python3.7/site-packages/ray/util/iter.py\", line 783, in apply_foreach\n",
      "    for item in it:\n",
      "  File \"/home/michael/anaconda3/envs/flow/lib/python3.7/site-packages/ray/util/iter.py\", line 843, in apply_filter\n",
      "    for item in it:\n",
      "  File \"/home/michael/anaconda3/envs/flow/lib/python3.7/site-packages/ray/util/iter.py\", line 843, in apply_filter\n",
      "    for item in it:\n",
      "  File \"/home/michael/anaconda3/envs/flow/lib/python3.7/site-packages/ray/util/iter.py\", line 783, in apply_foreach\n",
      "    for item in it:\n",
      "  File \"/home/michael/anaconda3/envs/flow/lib/python3.7/site-packages/ray/util/iter.py\", line 783, in apply_foreach\n",
      "    for item in it:\n",
      "  File \"/home/michael/anaconda3/envs/flow/lib/python3.7/site-packages/ray/util/iter.py\", line 783, in apply_foreach\n",
      "    for item in it:\n",
      "  [Previous line repeated 1 more time]\n",
      "  File \"/home/michael/anaconda3/envs/flow/lib/python3.7/site-packages/ray/util/iter.py\", line 876, in apply_flatten\n",
      "    for item in it:\n",
      "  File \"/home/michael/anaconda3/envs/flow/lib/python3.7/site-packages/ray/util/iter.py\", line 828, in add_wait_hooks\n",
      "    item = next(it)\n",
      "  File \"/home/michael/anaconda3/envs/flow/lib/python3.7/site-packages/ray/util/iter.py\", line 783, in apply_foreach\n",
      "    for item in it:\n",
      "  File \"/home/michael/anaconda3/envs/flow/lib/python3.7/site-packages/ray/util/iter.py\", line 783, in apply_foreach\n",
      "    for item in it:\n",
      "  File \"/home/michael/anaconda3/envs/flow/lib/python3.7/site-packages/ray/util/iter.py\", line 783, in apply_foreach\n",
      "    for item in it:\n",
      "  [Previous line repeated 1 more time]\n",
      "  File \"/home/michael/anaconda3/envs/flow/lib/python3.7/site-packages/ray/util/iter.py\", line 471, in base_iterator\n",
      "    yield ray.get(futures, timeout=timeout)\n",
      "  File \"/home/michael/anaconda3/envs/flow/lib/python3.7/site-packages/ray/_private/client_mode_hook.py\", line 82, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "ray.exceptions.RayTaskError(FatalFlowError): \u001b[36mray::RolloutWorker.par_iter_next()\u001b[39m (pid=25808, ip=192.168.0.33)\n",
      "  File \"python/ray/_raylet.pyx\", line 534, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 484, in ray._raylet.execute_task.function_executor\n",
      "  File \"/home/michael/anaconda3/envs/flow/lib/python3.7/site-packages/ray/_private/function_manager.py\", line 563, in actor_method_executor\n",
      "    return method(__ray_actor, *args, **kwargs)\n",
      "  File \"/home/michael/anaconda3/envs/flow/lib/python3.7/site-packages/ray/util/iter.py\", line 1151, in par_iter_next\n",
      "    return next(self.local_it)\n",
      "  File \"/home/michael/anaconda3/envs/flow/lib/python3.7/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 338, in gen_rollouts\n",
      "    yield self.sample()\n",
      "  File \"/home/michael/anaconda3/envs/flow/lib/python3.7/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 740, in sample\n",
      "    batches = [self.input_reader.next()]\n",
      "  File \"/home/michael/anaconda3/envs/flow/lib/python3.7/site-packages/ray/rllib/evaluation/sampler.py\", line 101, in next\n",
      "    batches = [self.get_data()]\n",
      "  File \"/home/michael/anaconda3/envs/flow/lib/python3.7/site-packages/ray/rllib/evaluation/sampler.py\", line 231, in get_data\n",
      "    item = next(self.rollout_provider)\n",
      "  File \"/home/michael/anaconda3/envs/flow/lib/python3.7/site-packages/ray/rllib/evaluation/sampler.py\", line 615, in _env_runner\n",
      "    sample_collector=sample_collector,\n",
      "  File \"/home/michael/anaconda3/envs/flow/lib/python3.7/site-packages/ray/rllib/evaluation/sampler.py\", line 934, in _process_observations\n",
      "    env_id)\n",
      "  File \"/home/michael/anaconda3/envs/flow/lib/python3.7/site-packages/ray/rllib/env/base_env.py\", line 368, in try_reset\n",
      "    return {_DUMMY_AGENT_ID: self.vector_env.reset_at(env_id)}\n",
      "  File \"/home/michael/anaconda3/envs/flow/lib/python3.7/site-packages/ray/rllib/env/vector_env.py\", line 167, in reset_at\n",
      "    return self.envs[index].reset()\n",
      "  File \"/home/michael/Desktop/flow/flow/envs/ring/wave_attenuation.py\", line 237, in reset\n",
      "    return super().reset()\n",
      "  File \"/home/michael/Desktop/flow/flow/envs/base.py\", line 544, in reset\n",
      "    raise FatalFlowError(msg=msg)\n",
      "flow.utils.exceptions.FatalFlowError: \n",
      "Not enough vehicles have spawned! Bad start?\n",
      "Missing vehicles / initial state:\n",
      "- rl_0: ('rl', 'left', 0, 53.904545454545314, 0)\n",
      "- human_11: ('human', 'right', 0, 66.15, 0)\n",
      "2022-08-01 21:40:58,845\tINFO trial_runner.py:1051 -- Trial PPO_WaveAttenuationPOEnv-v0_fd1ac_00000: Attempting to restore trial state from last checkpoint.\n",
      "== Status ==\n",
      "Memory usage on this node: 22.5/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 0/6 CPUs, 0/6 GPUs, 0.0/4.91 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_0_28b5456e04c30e30e910ccff5ed5840d, 0.0/6.0 CPU_group_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_2_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_4_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_3_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_1_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_5_28b5456e04c30e30e910ccff5ed5840d)\n",
      "Result logdir: /home/michael/ray_results/stabilizing_the_ring\n",
      "Number of trials: 1/1 (1 PENDING)\n",
      "+-----------------------------------------+----------+-------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc   |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+-------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_fd1ac_00000 | PENDING  |       |    146 |          44323.5 | 2920000 |  2270.08 |              3275.95 |              1472.09 |               4000 |\n",
      "+-----------------------------------------+----------+-------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "Number of errored trials: 1\n",
      "+-----------------------------------------+--------------+------------------------------------------------------------------------------------------------------------------------+\n",
      "| Trial name                              |   # failures | error file                                                                                                             |\n",
      "|-----------------------------------------+--------------+------------------------------------------------------------------------------------------------------------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_fd1ac_00000 |            1 | /home/michael/ray_results/stabilizing_the_ring/PPO_WaveAttenuationPOEnv-v0_fd1ac_00000_0_2022-08-01_09-19-43/error.txt |\n",
      "+-----------------------------------------+--------------+------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=25804)\u001b[0m Traceback (most recent call last):\r\n",
      "\u001b[2m\u001b[36m(pid=25804)\u001b[0m   File \"/home/michael/anaconda3/envs/flow/lib/python3.7/site-packages/ray/rllib/evaluation/worker_set.py\", line 155, in stop\r\n",
      "\u001b[2m\u001b[36m(pid=25804)\u001b[0m     ray.get(tids)\r\n",
      "\u001b[2m\u001b[36m(pid=25804)\u001b[0m   File \"/home/michael/anaconda3/envs/flow/lib/python3.7/site-packages/ray/_private/client_mode_hook.py\", line 82, in wrapper\r\n",
      "\u001b[2m\u001b[36m(pid=25804)\u001b[0m     return func(*args, **kwargs)\r\n",
      "\u001b[2m\u001b[36m(pid=25804)\u001b[0m   File \"/home/michael/anaconda3/envs/flow/lib/python3.7/site-packages/ray/worker.py\", line 1558, in get\r\n",
      "\u001b[2m\u001b[36m(pid=25804)\u001b[0m     object_refs, timeout=timeout)\r\n",
      "\u001b[2m\u001b[36m(pid=25804)\u001b[0m   File \"/home/michael/anaconda3/envs/flow/lib/python3.7/site-packages/ray/worker.py\", line 341, in get_objects\r\n",
      "\u001b[2m\u001b[36m(pid=25804)\u001b[0m     object_refs, self.current_task_id, timeout_ms)\r\n",
      "\u001b[2m\u001b[36m(pid=25804)\u001b[0m   File \"/home/michael/anaconda3/envs/flow/lib/python3.7/site-packages/ray/worker.py\", line 418, in sigterm_handler\r\n",
      "\u001b[2m\u001b[36m(pid=25804)\u001b[0m     sys.exit(1)\r\n",
      "\u001b[2m\u001b[36m(pid=25804)\u001b[0m SystemExit: 1\r\n",
      "\u001b[2m\u001b[36m(pid=25804)\u001b[0m \r\n",
      "\u001b[2m\u001b[36m(pid=25804)\u001b[0m During handling of the above exception, another exception occurred:\r\n",
      "\u001b[2m\u001b[36m(pid=25804)\u001b[0m \r\n",
      "\u001b[2m\u001b[36m(pid=25804)\u001b[0m Traceback (most recent call last):\r\n",
      "\u001b[2m\u001b[36m(pid=25804)\u001b[0m   File \"python/ray/_raylet.pyx\", line 523, in ray._raylet.execute_task\r\n",
      "\u001b[2m\u001b[36m(pid=25804)\u001b[0m   File \"python/ray/_raylet.pyx\", line 530, in ray._raylet.execute_task\r\n",
      "\u001b[2m\u001b[36m(pid=25804)\u001b[0m   File \"python/ray/_raylet.pyx\", line 534, in ray._raylet.execute_task\r\n",
      "\u001b[2m\u001b[36m(pid=25804)\u001b[0m   File \"python/ray/_raylet.pyx\", line 484, in ray._raylet.execute_task.function_executor\r\n",
      "\u001b[2m\u001b[36m(pid=25804)\u001b[0m   File \"/home/michael/anaconda3/envs/flow/lib/python3.7/site-packages/ray/_private/function_manager.py\", line 563, in actor_method_executor\r\n",
      "\u001b[2m\u001b[36m(pid=25804)\u001b[0m     return method(__ray_actor, *args, **kwargs)\r\n",
      "\u001b[2m\u001b[36m(pid=25804)\u001b[0m   File \"/home/michael/anaconda3/envs/flow/lib/python3.7/site-packages/ray/tune/trainable.py\", line 570, in stop\r\n",
      "\u001b[2m\u001b[36m(pid=25804)\u001b[0m     self.cleanup()\r\n",
      "\u001b[2m\u001b[36m(pid=25804)\u001b[0m   File \"/home/michael/anaconda3/envs/flow/lib/python3.7/site-packages/ray/rllib/agents/trainer.py\", line 770, in cleanup\r\n",
      "\u001b[2m\u001b[36m(pid=25804)\u001b[0m     self.workers.stop()\r\n",
      "\u001b[2m\u001b[36m(pid=25804)\u001b[0m   File \"/home/michael/anaconda3/envs/flow/lib/python3.7/site-packages/ray/rllib/evaluation/worker_set.py\", line 160, in stop\r\n",
      "\u001b[2m\u001b[36m(pid=25804)\u001b[0m     w.__ray_terminate__.remote()\r\n",
      "\u001b[2m\u001b[36m(pid=25804)\u001b[0m   File \"/home/michael/anaconda3/envs/flow/lib/python3.7/site-packages/ray/actor.py\", line 115, in remote\r\n",
      "\u001b[2m\u001b[36m(pid=25804)\u001b[0m     return self._remote(args, kwargs)\r\n",
      "\u001b[2m\u001b[36m(pid=25804)\u001b[0m   File \"/home/michael/anaconda3/envs/flow/lib/python3.7/site-packages/ray/util/tracing/tracing_helper.py\", line 403, in _start_span\r\n",
      "\u001b[2m\u001b[36m(pid=25804)\u001b[0m     return method(self, args, kwargs, *_args, **_kwargs)\r\n",
      "\u001b[2m\u001b[36m(pid=25804)\u001b[0m   File \"/home/michael/anaconda3/envs/flow/lib/python3.7/site-packages/ray/actor.py\", line 157, in _remote\r\n",
      "\u001b[2m\u001b[36m(pid=25804)\u001b[0m     return invocation(args, kwargs)\r\n",
      "\u001b[2m\u001b[36m(pid=25804)\u001b[0m   File \"/home/michael/anaconda3/envs/flow/lib/python3.7/site-packages/ray/actor.py\", line 151, in invocation\r\n",
      "\u001b[2m\u001b[36m(pid=25804)\u001b[0m     num_returns=num_returns)\r\n",
      "\u001b[2m\u001b[36m(pid=25804)\u001b[0m   File \"/home/michael/anaconda3/envs/flow/lib/python3.7/site-packages/ray/actor.py\", line 889, in _actor_method_call\r\n",
      "\u001b[2m\u001b[36m(pid=25804)\u001b[0m     object_refs = worker.core_worker.submit_actor_task(\r\n",
      "\u001b[2m\u001b[36m(pid=25804)\u001b[0m AttributeError: 'Worker' object has no attribute 'core_worker'\r\n",
      "\u001b[2m\u001b[36m(pid=25804)\u001b[0m \r\n",
      "\u001b[2m\u001b[36m(pid=25804)\u001b[0m During handling of the above exception, another exception occurred:\r\n",
      "\u001b[2m\u001b[36m(pid=25804)\u001b[0m \r\n",
      "\u001b[2m\u001b[36m(pid=25804)\u001b[0m Traceback (most recent call last):\r\n",
      "\u001b[2m\u001b[36m(pid=25804)\u001b[0m   File \"python/ray/_raylet.pyx\", line 632, in ray._raylet.task_execution_handler\r\n",
      "\u001b[2m\u001b[36m(pid=25804)\u001b[0m   File \"python/ray/_raylet.pyx\", line 486, in ray._raylet.execute_task\r\n",
      "\u001b[2m\u001b[36m(pid=25804)\u001b[0m   File \"python/ray/_raylet.pyx\", line 590, in ray._raylet.execute_task\r\n",
      "\u001b[2m\u001b[36m(pid=25804)\u001b[0m   File \"/home/michael/anaconda3/envs/flow/lib/python3.7/site-packages/ray/_private/utils.py\", line 103, in push_error_to_driver\r\n",
      "\u001b[2m\u001b[36m(pid=25804)\u001b[0m     worker.core_worker.push_error(job_id, error_type, message, time.time())\r\n",
      "\u001b[2m\u001b[36m(pid=25804)\u001b[0m AttributeError: 'Worker' object has no attribute 'core_worker'\r\n",
      "\u001b[2m\u001b[36m(pid=25804)\u001b[0m \r\n",
      "\u001b[2m\u001b[36m(pid=25804)\u001b[0m During handling of the above exception, another exception occurred:\r\n",
      "\u001b[2m\u001b[36m(pid=25804)\u001b[0m \r\n",
      "\u001b[2m\u001b[36m(pid=25804)\u001b[0m Traceback (most recent call last):\r\n",
      "\u001b[2m\u001b[36m(pid=25804)\u001b[0m   File \"python/ray/_raylet.pyx\", line 653, in ray._raylet.task_execution_handler\r\n",
      "\u001b[2m\u001b[36m(pid=25804)\u001b[0m   File \"/home/michael/anaconda3/envs/flow/lib/python3.7/site-packages/ray/_private/utils.py\", line 103, in push_error_to_driver\r\n",
      "\u001b[2m\u001b[36m(pid=25804)\u001b[0m     worker.core_worker.push_error(job_id, error_type, message, time.time())\r\n",
      "\u001b[2m\u001b[36m(pid=25804)\u001b[0m AttributeError: 'Worker' object has no attribute 'core_worker'\r\n",
      "\u001b[2m\u001b[36m(pid=25804)\u001b[0m Exception ignored in: 'ray._raylet.task_execution_handler'\r\n",
      "\u001b[2m\u001b[36m(pid=25804)\u001b[0m Traceback (most recent call last):\r\n",
      "\u001b[2m\u001b[36m(pid=25804)\u001b[0m   File \"python/ray/_raylet.pyx\", line 653, in ray._raylet.task_execution_handler\r\n",
      "\u001b[2m\u001b[36m(pid=25804)\u001b[0m   File \"/home/michael/anaconda3/envs/flow/lib/python3.7/site-packages/ray/_private/utils.py\", line 103, in push_error_to_driver\r\n",
      "\u001b[2m\u001b[36m(pid=25804)\u001b[0m     worker.core_worker.push_error(job_id, error_type, message, time.time())\r\n",
      "\u001b[2m\u001b[36m(pid=25804)\u001b[0m AttributeError: 'Worker' object has no attribute 'core_worker'\r\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m 2022-08-01 21:41:00,851\tERROR worker.py:421 -- SystemExit was raised from the worker\r\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m Traceback (most recent call last):\r\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m   File \"python/ray/_raylet.pyx\", line 632, in ray._raylet.task_execution_handler\r\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m   File \"python/ray/_raylet.pyx\", line 486, in ray._raylet.execute_task\r\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m   File \"python/ray/_raylet.pyx\", line 523, in ray._raylet.execute_task\r\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m   File \"python/ray/_raylet.pyx\", line 530, in ray._raylet.execute_task\r\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m   File \"python/ray/_raylet.pyx\", line 534, in ray._raylet.execute_task\r\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m   File \"python/ray/_raylet.pyx\", line 484, in ray._raylet.execute_task.function_executor\r\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m   File \"/home/michael/anaconda3/envs/flow/lib/python3.7/site-packages/ray/_private/function_manager.py\", line 563, in actor_method_executor\r\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m     return method(__ray_actor, *args, **kwargs)\r\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m   File \"/home/michael/anaconda3/envs/flow/lib/python3.7/site-packages/ray/util/iter.py\", line 1151, in par_iter_next\r\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m     return next(self.local_it)\r\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m   File \"/home/michael/anaconda3/envs/flow/lib/python3.7/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 338, in gen_rollouts\r\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m     yield self.sample()\r\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m   File \"/home/michael/anaconda3/envs/flow/lib/python3.7/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 740, in sample\r\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m     batches = [self.input_reader.next()]\r\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m   File \"/home/michael/anaconda3/envs/flow/lib/python3.7/site-packages/ray/rllib/evaluation/sampler.py\", line 101, in next\r\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m     batches = [self.get_data()]\r\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m   File \"/home/michael/anaconda3/envs/flow/lib/python3.7/site-packages/ray/rllib/evaluation/sampler.py\", line 231, in get_data\r\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m     item = next(self.rollout_provider)\r\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m   File \"/home/michael/anaconda3/envs/flow/lib/python3.7/site-packages/ray/rllib/evaluation/sampler.py\", line 615, in _env_runner\r\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m     sample_collector=sample_collector,\r\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m   File \"/home/michael/anaconda3/envs/flow/lib/python3.7/site-packages/ray/rllib/evaluation/sampler.py\", line 934, in _process_observations\r\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m     env_id)\r\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m   File \"/home/michael/anaconda3/envs/flow/lib/python3.7/site-packages/ray/rllib/env/base_env.py\", line 368, in try_reset\r\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m     return {_DUMMY_AGENT_ID: self.vector_env.reset_at(env_id)}\r\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m   File \"/home/michael/anaconda3/envs/flow/lib/python3.7/site-packages/ray/rllib/env/vector_env.py\", line 167, in reset_at\r\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m     return self.envs[index].reset()\r\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m   File \"/home/michael/Desktop/flow/flow/envs/ring/wave_attenuation.py\", line 237, in reset\r\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m     return super().reset()\r\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m   File \"/home/michael/Desktop/flow/flow/envs/base.py\", line 557, in reset\r\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m     observation, _, _, _ = self.step(rl_actions=None)\r\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m   File \"/home/michael/Desktop/flow/flow/envs/base.py\", line 338, in step\r\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m     self.k.vehicle.get_controlled_ids(), accel)\r\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m   File \"/home/michael/Desktop/flow/flow/core/kernel/vehicle/traci.py\", line 969, in apply_acceleration\r\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m     self.kernel_api.vehicle.slowDown(vid, next_vel, 1e-3)\r\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m   File \"/home/michael/anaconda3/envs/flow/lib/python3.7/site-packages/traci/_vehicle.py\", line 962, in slowDown\r\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m     self._connection._sendExact()\r\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m   File \"/home/michael/anaconda3/envs/flow/lib/python3.7/site-packages/traci/connection.py\", line 95, in _sendExact\r\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m     result = self._recvExact()\r\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m   File \"/home/michael/anaconda3/envs/flow/lib/python3.7/site-packages/traci/connection.py\", line 74, in _recvExact\r\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m     t = self._socket.recv(4 - len(result))\r\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m   File \"/home/michael/anaconda3/envs/flow/lib/python3.7/site-packages/ray/worker.py\", line 418, in sigterm_handler\r\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m     sys.exit(1)\r\n",
      "\u001b[2m\u001b[36m(pid=25806)\u001b[0m SystemExit: 1\r\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m 2022-08-01 21:41:00,850\tERROR worker.py:421 -- SystemExit was raised from the worker\r\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m Traceback (most recent call last):\r\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m   File \"python/ray/_raylet.pyx\", line 632, in ray._raylet.task_execution_handler\r\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m   File \"python/ray/_raylet.pyx\", line 486, in ray._raylet.execute_task\r\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m   File \"python/ray/_raylet.pyx\", line 523, in ray._raylet.execute_task\r\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m   File \"python/ray/_raylet.pyx\", line 530, in ray._raylet.execute_task\r\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m   File \"python/ray/_raylet.pyx\", line 534, in ray._raylet.execute_task\r\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m   File \"python/ray/_raylet.pyx\", line 484, in ray._raylet.execute_task.function_executor\r\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m   File \"/home/michael/anaconda3/envs/flow/lib/python3.7/site-packages/ray/_private/function_manager.py\", line 563, in actor_method_executor\r\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m     return method(__ray_actor, *args, **kwargs)\r\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m   File \"/home/michael/anaconda3/envs/flow/lib/python3.7/site-packages/ray/util/iter.py\", line 1151, in par_iter_next\r\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m     return next(self.local_it)\r\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m   File \"/home/michael/anaconda3/envs/flow/lib/python3.7/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 338, in gen_rollouts\r\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m     yield self.sample()\r\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m   File \"/home/michael/anaconda3/envs/flow/lib/python3.7/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 740, in sample\r\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m     batches = [self.input_reader.next()]\r\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m   File \"/home/michael/anaconda3/envs/flow/lib/python3.7/site-packages/ray/rllib/evaluation/sampler.py\", line 101, in next\r\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m     batches = [self.get_data()]\r\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m   File \"/home/michael/anaconda3/envs/flow/lib/python3.7/site-packages/ray/rllib/evaluation/sampler.py\", line 231, in get_data\r\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m     item = next(self.rollout_provider)\r\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m   File \"/home/michael/anaconda3/envs/flow/lib/python3.7/site-packages/ray/rllib/evaluation/sampler.py\", line 615, in _env_runner\r\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m     sample_collector=sample_collector,\r\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m   File \"/home/michael/anaconda3/envs/flow/lib/python3.7/site-packages/ray/rllib/evaluation/sampler.py\", line 934, in _process_observations\r\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m     env_id)\r\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m   File \"/home/michael/anaconda3/envs/flow/lib/python3.7/site-packages/ray/rllib/env/base_env.py\", line 368, in try_reset\r\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m     return {_DUMMY_AGENT_ID: self.vector_env.reset_at(env_id)}\r\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m   File \"/home/michael/anaconda3/envs/flow/lib/python3.7/site-packages/ray/rllib/env/vector_env.py\", line 167, in reset_at\r\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m     return self.envs[index].reset()\r\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m   File \"/home/michael/Desktop/flow/flow/envs/ring/wave_attenuation.py\", line 237, in reset\r\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m     return super().reset()\r\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m   File \"/home/michael/Desktop/flow/flow/envs/base.py\", line 557, in reset\r\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m     observation, _, _, _ = self.step(rl_actions=None)\r\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m   File \"/home/michael/Desktop/flow/flow/envs/base.py\", line 370, in step\r\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m     self.k.simulation.simulation_step()\r\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m   File \"/home/michael/Desktop/flow/flow/core/kernel/simulation/traci.py\", line 124, in simulation_step\r\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m     self.kernel_api.simulationStep()\r\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m   File \"/home/michael/anaconda3/envs/flow/lib/python3.7/site-packages/traci/connection.py\", line 323, in simulationStep\r\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m     result = self._sendExact()\r\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m   File \"/home/michael/anaconda3/envs/flow/lib/python3.7/site-packages/traci/connection.py\", line 95, in _sendExact\r\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m     result = self._recvExact()\r\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m   File \"/home/michael/anaconda3/envs/flow/lib/python3.7/site-packages/traci/connection.py\", line 74, in _recvExact\r\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m     t = self._socket.recv(4 - len(result))\r\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m   File \"/home/michael/anaconda3/envs/flow/lib/python3.7/site-packages/ray/worker.py\", line 418, in sigterm_handler\r\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m     sys.exit(1)\r\n",
      "\u001b[2m\u001b[36m(pid=25807)\u001b[0m SystemExit: 1\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-08-01 21:41:01,445\tINFO trainable.py:76 -- Checkpoint size is 44950582 bytes\n",
      "\u001b[2m\u001b[36m(pid=822)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=822)\u001b[0m Bad key text.latex.preview in file /home/michael/anaconda3/envs/flow/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 123 ('text.latex.preview : False')\n",
      "\u001b[2m\u001b[36m(pid=822)\u001b[0m You probably need to get an updated matplotlibrc file from\n",
      "\u001b[2m\u001b[36m(pid=822)\u001b[0m https://github.com/matplotlib/matplotlib/blob/v3.5.1/matplotlibrc.template\n",
      "\u001b[2m\u001b[36m(pid=822)\u001b[0m or from the matplotlib source distribution\n",
      "\u001b[2m\u001b[36m(pid=822)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=822)\u001b[0m Bad key mathtext.fallback_to_cm in file /home/michael/anaconda3/envs/flow/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 155 ('mathtext.fallback_to_cm : True  # When True, use symbols from the Computer Modern')\n",
      "\u001b[2m\u001b[36m(pid=822)\u001b[0m You probably need to get an updated matplotlibrc file from\n",
      "\u001b[2m\u001b[36m(pid=822)\u001b[0m https://github.com/matplotlib/matplotlib/blob/v3.5.1/matplotlibrc.template\n",
      "\u001b[2m\u001b[36m(pid=822)\u001b[0m or from the matplotlib source distribution\n",
      "\u001b[2m\u001b[36m(pid=822)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=822)\u001b[0m Bad key savefig.jpeg_quality in file /home/michael/anaconda3/envs/flow/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 418 ('savefig.jpeg_quality: 95       # when a jpeg is saved, the default quality parameter.')\n",
      "\u001b[2m\u001b[36m(pid=822)\u001b[0m You probably need to get an updated matplotlibrc file from\n",
      "\u001b[2m\u001b[36m(pid=822)\u001b[0m https://github.com/matplotlib/matplotlib/blob/v3.5.1/matplotlibrc.template\n",
      "\u001b[2m\u001b[36m(pid=822)\u001b[0m or from the matplotlib source distribution\n",
      "\u001b[2m\u001b[36m(pid=822)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=822)\u001b[0m Bad key keymap.all_axes in file /home/michael/anaconda3/envs/flow/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 466 ('keymap.all_axes : a                 # enable all axes')\n",
      "\u001b[2m\u001b[36m(pid=822)\u001b[0m You probably need to get an updated matplotlibrc file from\n",
      "\u001b[2m\u001b[36m(pid=822)\u001b[0m https://github.com/matplotlib/matplotlib/blob/v3.5.1/matplotlibrc.template\n",
      "\u001b[2m\u001b[36m(pid=822)\u001b[0m or from the matplotlib source distribution\n",
      "\u001b[2m\u001b[36m(pid=822)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=822)\u001b[0m Bad key animation.avconv_path in file /home/michael/anaconda3/envs/flow/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 477 ('animation.avconv_path: avconv     # Path to avconv binary. Without full path')\n",
      "\u001b[2m\u001b[36m(pid=822)\u001b[0m You probably need to get an updated matplotlibrc file from\n",
      "\u001b[2m\u001b[36m(pid=822)\u001b[0m https://github.com/matplotlib/matplotlib/blob/v3.5.1/matplotlibrc.template\n",
      "\u001b[2m\u001b[36m(pid=822)\u001b[0m or from the matplotlib source distribution\n",
      "\u001b[2m\u001b[36m(pid=822)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=822)\u001b[0m Bad key animation.avconv_args in file /home/michael/anaconda3/envs/flow/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 479 ('animation.avconv_args:            # Additional arguments to pass to avconv')\n",
      "\u001b[2m\u001b[36m(pid=822)\u001b[0m You probably need to get an updated matplotlibrc file from\n",
      "\u001b[2m\u001b[36m(pid=822)\u001b[0m https://github.com/matplotlib/matplotlib/blob/v3.5.1/matplotlibrc.template\n",
      "\u001b[2m\u001b[36m(pid=822)\u001b[0m or from the matplotlib source distribution\n",
      "\u001b[2m\u001b[36m(pid=822)\u001b[0m 2022-08-01 21:41:04,247\tINFO trainer.py:720 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=823)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=823)\u001b[0m Bad key text.latex.preview in file /home/michael/anaconda3/envs/flow/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 123 ('text.latex.preview : False')\n",
      "\u001b[2m\u001b[36m(pid=823)\u001b[0m You probably need to get an updated matplotlibrc file from\n",
      "\u001b[2m\u001b[36m(pid=823)\u001b[0m https://github.com/matplotlib/matplotlib/blob/v3.5.1/matplotlibrc.template\n",
      "\u001b[2m\u001b[36m(pid=823)\u001b[0m or from the matplotlib source distribution\n",
      "\u001b[2m\u001b[36m(pid=823)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=823)\u001b[0m Bad key mathtext.fallback_to_cm in file /home/michael/anaconda3/envs/flow/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 155 ('mathtext.fallback_to_cm : True  # When True, use symbols from the Computer Modern')\n",
      "\u001b[2m\u001b[36m(pid=823)\u001b[0m You probably need to get an updated matplotlibrc file from\n",
      "\u001b[2m\u001b[36m(pid=823)\u001b[0m https://github.com/matplotlib/matplotlib/blob/v3.5.1/matplotlibrc.template\n",
      "\u001b[2m\u001b[36m(pid=823)\u001b[0m or from the matplotlib source distribution\n",
      "\u001b[2m\u001b[36m(pid=823)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=823)\u001b[0m Bad key savefig.jpeg_quality in file /home/michael/anaconda3/envs/flow/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 418 ('savefig.jpeg_quality: 95       # when a jpeg is saved, the default quality parameter.')\n",
      "\u001b[2m\u001b[36m(pid=823)\u001b[0m You probably need to get an updated matplotlibrc file from\n",
      "\u001b[2m\u001b[36m(pid=823)\u001b[0m https://github.com/matplotlib/matplotlib/blob/v3.5.1/matplotlibrc.template\n",
      "\u001b[2m\u001b[36m(pid=823)\u001b[0m or from the matplotlib source distribution\n",
      "\u001b[2m\u001b[36m(pid=823)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=823)\u001b[0m Bad key keymap.all_axes in file /home/michael/anaconda3/envs/flow/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 466 ('keymap.all_axes : a                 # enable all axes')\n",
      "\u001b[2m\u001b[36m(pid=823)\u001b[0m You probably need to get an updated matplotlibrc file from\n",
      "\u001b[2m\u001b[36m(pid=823)\u001b[0m https://github.com/matplotlib/matplotlib/blob/v3.5.1/matplotlibrc.template\n",
      "\u001b[2m\u001b[36m(pid=823)\u001b[0m or from the matplotlib source distribution\n",
      "\u001b[2m\u001b[36m(pid=823)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=823)\u001b[0m Bad key animation.avconv_path in file /home/michael/anaconda3/envs/flow/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 477 ('animation.avconv_path: avconv     # Path to avconv binary. Without full path')\n",
      "\u001b[2m\u001b[36m(pid=823)\u001b[0m You probably need to get an updated matplotlibrc file from\n",
      "\u001b[2m\u001b[36m(pid=823)\u001b[0m https://github.com/matplotlib/matplotlib/blob/v3.5.1/matplotlibrc.template\n",
      "\u001b[2m\u001b[36m(pid=823)\u001b[0m or from the matplotlib source distribution\n",
      "\u001b[2m\u001b[36m(pid=823)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=823)\u001b[0m Bad key animation.avconv_args in file /home/michael/anaconda3/envs/flow/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 479 ('animation.avconv_args:            # Additional arguments to pass to avconv')\n",
      "\u001b[2m\u001b[36m(pid=823)\u001b[0m You probably need to get an updated matplotlibrc file from\n",
      "\u001b[2m\u001b[36m(pid=823)\u001b[0m https://github.com/matplotlib/matplotlib/blob/v3.5.1/matplotlibrc.template\n",
      "\u001b[2m\u001b[36m(pid=823)\u001b[0m or from the matplotlib source distribution\n",
      "\u001b[2m\u001b[36m(pid=821)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=821)\u001b[0m Bad key text.latex.preview in file /home/michael/anaconda3/envs/flow/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 123 ('text.latex.preview : False')\n",
      "\u001b[2m\u001b[36m(pid=821)\u001b[0m You probably need to get an updated matplotlibrc file from\n",
      "\u001b[2m\u001b[36m(pid=821)\u001b[0m https://github.com/matplotlib/matplotlib/blob/v3.5.1/matplotlibrc.template\n",
      "\u001b[2m\u001b[36m(pid=821)\u001b[0m or from the matplotlib source distribution\n",
      "\u001b[2m\u001b[36m(pid=821)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=821)\u001b[0m Bad key mathtext.fallback_to_cm in file /home/michael/anaconda3/envs/flow/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 155 ('mathtext.fallback_to_cm : True  # When True, use symbols from the Computer Modern')\n",
      "\u001b[2m\u001b[36m(pid=821)\u001b[0m You probably need to get an updated matplotlibrc file from\n",
      "\u001b[2m\u001b[36m(pid=821)\u001b[0m https://github.com/matplotlib/matplotlib/blob/v3.5.1/matplotlibrc.template\n",
      "\u001b[2m\u001b[36m(pid=821)\u001b[0m or from the matplotlib source distribution\n",
      "\u001b[2m\u001b[36m(pid=821)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=821)\u001b[0m Bad key savefig.jpeg_quality in file /home/michael/anaconda3/envs/flow/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 418 ('savefig.jpeg_quality: 95       # when a jpeg is saved, the default quality parameter.')\n",
      "\u001b[2m\u001b[36m(pid=821)\u001b[0m You probably need to get an updated matplotlibrc file from\n",
      "\u001b[2m\u001b[36m(pid=821)\u001b[0m https://github.com/matplotlib/matplotlib/blob/v3.5.1/matplotlibrc.template\n",
      "\u001b[2m\u001b[36m(pid=821)\u001b[0m or from the matplotlib source distribution\n",
      "\u001b[2m\u001b[36m(pid=821)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=821)\u001b[0m Bad key keymap.all_axes in file /home/michael/anaconda3/envs/flow/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 466 ('keymap.all_axes : a                 # enable all axes')\n",
      "\u001b[2m\u001b[36m(pid=821)\u001b[0m You probably need to get an updated matplotlibrc file from\n",
      "\u001b[2m\u001b[36m(pid=821)\u001b[0m https://github.com/matplotlib/matplotlib/blob/v3.5.1/matplotlibrc.template\n",
      "\u001b[2m\u001b[36m(pid=821)\u001b[0m or from the matplotlib source distribution\n",
      "\u001b[2m\u001b[36m(pid=821)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=821)\u001b[0m Bad key animation.avconv_path in file /home/michael/anaconda3/envs/flow/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 477 ('animation.avconv_path: avconv     # Path to avconv binary. Without full path')\n",
      "\u001b[2m\u001b[36m(pid=821)\u001b[0m You probably need to get an updated matplotlibrc file from\n",
      "\u001b[2m\u001b[36m(pid=821)\u001b[0m https://github.com/matplotlib/matplotlib/blob/v3.5.1/matplotlibrc.template\n",
      "\u001b[2m\u001b[36m(pid=821)\u001b[0m or from the matplotlib source distribution\n",
      "\u001b[2m\u001b[36m(pid=821)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=821)\u001b[0m Bad key animation.avconv_args in file /home/michael/anaconda3/envs/flow/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 479 ('animation.avconv_args:            # Additional arguments to pass to avconv')\n",
      "\u001b[2m\u001b[36m(pid=821)\u001b[0m You probably need to get an updated matplotlibrc file from\n",
      "\u001b[2m\u001b[36m(pid=821)\u001b[0m https://github.com/matplotlib/matplotlib/blob/v3.5.1/matplotlibrc.template\n",
      "\u001b[2m\u001b[36m(pid=821)\u001b[0m or from the matplotlib source distribution\n",
      "\u001b[2m\u001b[36m(pid=824)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=824)\u001b[0m Bad key text.latex.preview in file /home/michael/anaconda3/envs/flow/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 123 ('text.latex.preview : False')\n",
      "\u001b[2m\u001b[36m(pid=824)\u001b[0m You probably need to get an updated matplotlibrc file from\n",
      "\u001b[2m\u001b[36m(pid=824)\u001b[0m https://github.com/matplotlib/matplotlib/blob/v3.5.1/matplotlibrc.template\n",
      "\u001b[2m\u001b[36m(pid=824)\u001b[0m or from the matplotlib source distribution\n",
      "\u001b[2m\u001b[36m(pid=824)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=824)\u001b[0m Bad key mathtext.fallback_to_cm in file /home/michael/anaconda3/envs/flow/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 155 ('mathtext.fallback_to_cm : True  # When True, use symbols from the Computer Modern')\n",
      "\u001b[2m\u001b[36m(pid=824)\u001b[0m You probably need to get an updated matplotlibrc file from\n",
      "\u001b[2m\u001b[36m(pid=824)\u001b[0m https://github.com/matplotlib/matplotlib/blob/v3.5.1/matplotlibrc.template\n",
      "\u001b[2m\u001b[36m(pid=824)\u001b[0m or from the matplotlib source distribution\n",
      "\u001b[2m\u001b[36m(pid=824)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=824)\u001b[0m Bad key savefig.jpeg_quality in file /home/michael/anaconda3/envs/flow/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 418 ('savefig.jpeg_quality: 95       # when a jpeg is saved, the default quality parameter.')\n",
      "\u001b[2m\u001b[36m(pid=824)\u001b[0m You probably need to get an updated matplotlibrc file from\n",
      "\u001b[2m\u001b[36m(pid=824)\u001b[0m https://github.com/matplotlib/matplotlib/blob/v3.5.1/matplotlibrc.template\n",
      "\u001b[2m\u001b[36m(pid=824)\u001b[0m or from the matplotlib source distribution\n",
      "\u001b[2m\u001b[36m(pid=824)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=824)\u001b[0m Bad key keymap.all_axes in file /home/michael/anaconda3/envs/flow/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 466 ('keymap.all_axes : a                 # enable all axes')\n",
      "\u001b[2m\u001b[36m(pid=824)\u001b[0m You probably need to get an updated matplotlibrc file from\n",
      "\u001b[2m\u001b[36m(pid=824)\u001b[0m https://github.com/matplotlib/matplotlib/blob/v3.5.1/matplotlibrc.template\n",
      "\u001b[2m\u001b[36m(pid=824)\u001b[0m or from the matplotlib source distribution\n",
      "\u001b[2m\u001b[36m(pid=824)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=824)\u001b[0m Bad key animation.avconv_path in file /home/michael/anaconda3/envs/flow/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 477 ('animation.avconv_path: avconv     # Path to avconv binary. Without full path')\n",
      "\u001b[2m\u001b[36m(pid=824)\u001b[0m You probably need to get an updated matplotlibrc file from\n",
      "\u001b[2m\u001b[36m(pid=824)\u001b[0m https://github.com/matplotlib/matplotlib/blob/v3.5.1/matplotlibrc.template\n",
      "\u001b[2m\u001b[36m(pid=824)\u001b[0m or from the matplotlib source distribution\n",
      "\u001b[2m\u001b[36m(pid=824)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=824)\u001b[0m Bad key animation.avconv_args in file /home/michael/anaconda3/envs/flow/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 479 ('animation.avconv_args:            # Additional arguments to pass to avconv')\n",
      "\u001b[2m\u001b[36m(pid=824)\u001b[0m You probably need to get an updated matplotlibrc file from\n",
      "\u001b[2m\u001b[36m(pid=824)\u001b[0m https://github.com/matplotlib/matplotlib/blob/v3.5.1/matplotlibrc.template\n",
      "\u001b[2m\u001b[36m(pid=824)\u001b[0m or from the matplotlib source distribution\n",
      "\u001b[2m\u001b[36m(pid=826)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=826)\u001b[0m Bad key text.latex.preview in file /home/michael/anaconda3/envs/flow/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 123 ('text.latex.preview : False')\n",
      "\u001b[2m\u001b[36m(pid=826)\u001b[0m You probably need to get an updated matplotlibrc file from\n",
      "\u001b[2m\u001b[36m(pid=826)\u001b[0m https://github.com/matplotlib/matplotlib/blob/v3.5.1/matplotlibrc.template\n",
      "\u001b[2m\u001b[36m(pid=826)\u001b[0m or from the matplotlib source distribution\n",
      "\u001b[2m\u001b[36m(pid=826)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=826)\u001b[0m Bad key mathtext.fallback_to_cm in file /home/michael/anaconda3/envs/flow/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 155 ('mathtext.fallback_to_cm : True  # When True, use symbols from the Computer Modern')\n",
      "\u001b[2m\u001b[36m(pid=826)\u001b[0m You probably need to get an updated matplotlibrc file from\n",
      "\u001b[2m\u001b[36m(pid=826)\u001b[0m https://github.com/matplotlib/matplotlib/blob/v3.5.1/matplotlibrc.template\n",
      "\u001b[2m\u001b[36m(pid=826)\u001b[0m or from the matplotlib source distribution\n",
      "\u001b[2m\u001b[36m(pid=826)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=826)\u001b[0m Bad key savefig.jpeg_quality in file /home/michael/anaconda3/envs/flow/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 418 ('savefig.jpeg_quality: 95       # when a jpeg is saved, the default quality parameter.')\n",
      "\u001b[2m\u001b[36m(pid=826)\u001b[0m You probably need to get an updated matplotlibrc file from\n",
      "\u001b[2m\u001b[36m(pid=826)\u001b[0m https://github.com/matplotlib/matplotlib/blob/v3.5.1/matplotlibrc.template\n",
      "\u001b[2m\u001b[36m(pid=826)\u001b[0m or from the matplotlib source distribution\n",
      "\u001b[2m\u001b[36m(pid=826)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=826)\u001b[0m Bad key keymap.all_axes in file /home/michael/anaconda3/envs/flow/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 466 ('keymap.all_axes : a                 # enable all axes')\n",
      "\u001b[2m\u001b[36m(pid=826)\u001b[0m You probably need to get an updated matplotlibrc file from\n",
      "\u001b[2m\u001b[36m(pid=826)\u001b[0m https://github.com/matplotlib/matplotlib/blob/v3.5.1/matplotlibrc.template\n",
      "\u001b[2m\u001b[36m(pid=826)\u001b[0m or from the matplotlib source distribution\n",
      "\u001b[2m\u001b[36m(pid=826)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=826)\u001b[0m Bad key animation.avconv_path in file /home/michael/anaconda3/envs/flow/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 477 ('animation.avconv_path: avconv     # Path to avconv binary. Without full path')\n",
      "\u001b[2m\u001b[36m(pid=826)\u001b[0m You probably need to get an updated matplotlibrc file from\n",
      "\u001b[2m\u001b[36m(pid=826)\u001b[0m https://github.com/matplotlib/matplotlib/blob/v3.5.1/matplotlibrc.template\n",
      "\u001b[2m\u001b[36m(pid=826)\u001b[0m or from the matplotlib source distribution\n",
      "\u001b[2m\u001b[36m(pid=826)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=826)\u001b[0m Bad key animation.avconv_args in file /home/michael/anaconda3/envs/flow/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 479 ('animation.avconv_args:            # Additional arguments to pass to avconv')\n",
      "\u001b[2m\u001b[36m(pid=826)\u001b[0m You probably need to get an updated matplotlibrc file from\n",
      "\u001b[2m\u001b[36m(pid=826)\u001b[0m https://github.com/matplotlib/matplotlib/blob/v3.5.1/matplotlibrc.template\n",
      "\u001b[2m\u001b[36m(pid=826)\u001b[0m or from the matplotlib source distribution\n",
      "\u001b[2m\u001b[36m(pid=825)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=825)\u001b[0m Bad key text.latex.preview in file /home/michael/anaconda3/envs/flow/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 123 ('text.latex.preview : False')\n",
      "\u001b[2m\u001b[36m(pid=825)\u001b[0m You probably need to get an updated matplotlibrc file from\n",
      "\u001b[2m\u001b[36m(pid=825)\u001b[0m https://github.com/matplotlib/matplotlib/blob/v3.5.1/matplotlibrc.template\n",
      "\u001b[2m\u001b[36m(pid=825)\u001b[0m or from the matplotlib source distribution\n",
      "\u001b[2m\u001b[36m(pid=825)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=825)\u001b[0m Bad key mathtext.fallback_to_cm in file /home/michael/anaconda3/envs/flow/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 155 ('mathtext.fallback_to_cm : True  # When True, use symbols from the Computer Modern')\n",
      "\u001b[2m\u001b[36m(pid=825)\u001b[0m You probably need to get an updated matplotlibrc file from\n",
      "\u001b[2m\u001b[36m(pid=825)\u001b[0m https://github.com/matplotlib/matplotlib/blob/v3.5.1/matplotlibrc.template\n",
      "\u001b[2m\u001b[36m(pid=825)\u001b[0m or from the matplotlib source distribution\n",
      "\u001b[2m\u001b[36m(pid=825)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=825)\u001b[0m Bad key savefig.jpeg_quality in file /home/michael/anaconda3/envs/flow/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 418 ('savefig.jpeg_quality: 95       # when a jpeg is saved, the default quality parameter.')\n",
      "\u001b[2m\u001b[36m(pid=825)\u001b[0m You probably need to get an updated matplotlibrc file from\n",
      "\u001b[2m\u001b[36m(pid=825)\u001b[0m https://github.com/matplotlib/matplotlib/blob/v3.5.1/matplotlibrc.template\n",
      "\u001b[2m\u001b[36m(pid=825)\u001b[0m or from the matplotlib source distribution\n",
      "\u001b[2m\u001b[36m(pid=825)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=825)\u001b[0m Bad key keymap.all_axes in file /home/michael/anaconda3/envs/flow/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 466 ('keymap.all_axes : a                 # enable all axes')\n",
      "\u001b[2m\u001b[36m(pid=825)\u001b[0m You probably need to get an updated matplotlibrc file from\n",
      "\u001b[2m\u001b[36m(pid=825)\u001b[0m https://github.com/matplotlib/matplotlib/blob/v3.5.1/matplotlibrc.template\n",
      "\u001b[2m\u001b[36m(pid=825)\u001b[0m or from the matplotlib source distribution\n",
      "\u001b[2m\u001b[36m(pid=825)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=825)\u001b[0m Bad key animation.avconv_path in file /home/michael/anaconda3/envs/flow/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 477 ('animation.avconv_path: avconv     # Path to avconv binary. Without full path')\n",
      "\u001b[2m\u001b[36m(pid=825)\u001b[0m You probably need to get an updated matplotlibrc file from\n",
      "\u001b[2m\u001b[36m(pid=825)\u001b[0m https://github.com/matplotlib/matplotlib/blob/v3.5.1/matplotlibrc.template\n",
      "\u001b[2m\u001b[36m(pid=825)\u001b[0m or from the matplotlib source distribution\n",
      "\u001b[2m\u001b[36m(pid=825)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=825)\u001b[0m Bad key animation.avconv_args in file /home/michael/anaconda3/envs/flow/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 479 ('animation.avconv_args:            # Additional arguments to pass to avconv')\n",
      "\u001b[2m\u001b[36m(pid=825)\u001b[0m You probably need to get an updated matplotlibrc file from\n",
      "\u001b[2m\u001b[36m(pid=825)\u001b[0m https://github.com/matplotlib/matplotlib/blob/v3.5.1/matplotlibrc.template\n",
      "\u001b[2m\u001b[36m(pid=825)\u001b[0m or from the matplotlib source distribution\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=822)\u001b[0m 2022-08-01 21:41:07,663\tWARNING util.py:55 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(pid=822)\u001b[0m 2022-08-01 21:41:07,781\tINFO trainable.py:383 -- Restored on 192.168.0.33 from checkpoint: /home/michael/ray_results/stabilizing_the_ring/PPO_WaveAttenuationPOEnv-v0_fd1ac_00000_0_2022-08-01_09-19-43/tmpoz_p3ujcrestore_from_object/checkpoint-146\n",
      "\u001b[2m\u001b[36m(pid=822)\u001b[0m 2022-08-01 21:41:07,781\tINFO trainable.py:390 -- Current state after restoring: {'_iteration': 146, '_timesteps_total': None, '_time_total': 44323.45616698265, '_episodes_total': 730}\n",
      "== Status ==\n",
      "Memory usage on this node: 20.9/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/4.91 GiB heap, 0.0/14.65 GiB objects (0.0/6.0 CPU_group_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_1_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_5_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_3_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_0_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_2_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_4_28b5456e04c30e30e910ccff5ed5840d)\n",
      "Result logdir: /home/michael/ray_results/stabilizing_the_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+-------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc   |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+-------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_fd1ac_00000 | RUNNING  |       |    146 |          44323.5 | 2920000 |  2270.08 |              3275.95 |              1472.09 |               4000 |\n",
      "+-----------------------------------------+----------+-------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "Number of errored trials: 1\n",
      "+-----------------------------------------+--------------+------------------------------------------------------------------------------------------------------------------------+\n",
      "| Trial name                              |   # failures | error file                                                                                                             |\n",
      "|-----------------------------------------+--------------+------------------------------------------------------------------------------------------------------------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_fd1ac_00000 |            1 | /home/michael/ray_results/stabilizing_the_ring/PPO_WaveAttenuationPOEnv-v0_fd1ac_00000_0_2022-08-01_09-19-43/error.txt |\n",
      "+-----------------------------------------+--------------+------------------------------------------------------------------------------------------------------------------------+\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=821)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=821)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=821)\u001b[0m ring length: 247\n",
      "\u001b[2m\u001b[36m(pid=821)\u001b[0m v_max: 4.522125250095652\n",
      "\u001b[2m\u001b[36m(pid=821)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=825)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=825)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=825)\u001b[0m ring length: 270\n",
      "\u001b[2m\u001b[36m(pid=825)\u001b[0m v_max: 5.6143732387852054\n",
      "\u001b[2m\u001b[36m(pid=825)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=824)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=824)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=824)\u001b[0m ring length: 235\n",
      "\u001b[2m\u001b[36m(pid=824)\u001b[0m v_max: 3.9514850725282584\n",
      "\u001b[2m\u001b[36m(pid=824)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=826)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=826)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=826)\u001b[0m ring length: 268\n",
      "\u001b[2m\u001b[36m(pid=826)\u001b[0m v_max: 5.5194978538368025\n",
      "\u001b[2m\u001b[36m(pid=826)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=823)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=823)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=823)\u001b[0m ring length: 220\n",
      "\u001b[2m\u001b[36m(pid=823)\u001b[0m v_max: 3.2377399006821497\n",
      "\u001b[2m\u001b[36m(pid=823)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=821)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=821)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=821)\u001b[0m ring length: 237\n",
      "\u001b[2m\u001b[36m(pid=821)\u001b[0m v_max: 4.04661795519353\n",
      "\u001b[2m\u001b[36m(pid=821)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=825)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=825)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=825)\u001b[0m ring length: 259\n",
      "\u001b[2m\u001b[36m(pid=825)\u001b[0m v_max: 5.092292348291853\n",
      "\u001b[2m\u001b[36m(pid=825)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=824)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=824)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=824)\u001b[0m ring length: 222\n",
      "\u001b[2m\u001b[36m(pid=824)\u001b[0m v_max: 3.3329270738617227\n",
      "\u001b[2m\u001b[36m(pid=824)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=826)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=826)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=826)\u001b[0m ring length: 229\n",
      "\u001b[2m\u001b[36m(pid=826)\u001b[0m v_max: 3.666034803266416\n",
      "\u001b[2m\u001b[36m(pid=826)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=823)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=823)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=823)\u001b[0m ring length: 259\n",
      "\u001b[2m\u001b[36m(pid=823)\u001b[0m v_max: 5.092292348291853\n",
      "\u001b[2m\u001b[36m(pid=823)\u001b[0m -----------------------\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_fd1ac_00000:\n",
      "  agent_timesteps_total: 2940000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-01_21-47-33\n",
      "  done: false\n",
      "  episode_len_mean: 4000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3163.131906025613\n",
      "  episode_reward_mean: 2358.2952574466044\n",
      "  episode_reward_min: 1445.2146079285546\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 735\n",
      "  experiment_id: 2b7c5715c95c4828907cbc45e49df6f0\n",
      "  hostname: michael-home\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.2\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -3.484753327460805\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014044732482107298\n",
      "          policy_loss: 0.0015172398631360122\n",
      "          total_loss: 757.9906940605989\n",
      "          vf_explained_var: -9.491185615218e-10\n",
      "          vf_loss: 757.9863659974117\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 2940000\n",
      "    num_steps_sampled: 2940000\n",
      "    num_steps_trained: 2940000\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 192.168.0.33\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 26.3459165154265\n",
      "    ram_util_percent: 69.82032667876588\n",
      "  pid: 822\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09431630425142128\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 52.68618132703991\n",
      "    mean_inference_ms: 1.8766662294463614\n",
      "    mean_raw_obs_processing_ms: 21.613790332839002\n",
      "  time_since_restore: 385.78165578842163\n",
      "  time_this_iter_s: 385.78165578842163\n",
      "  time_total_s: 44709.23782277107\n",
      "  timers:\n",
      "    learn_throughput: 250.617\n",
      "    learn_time_ms: 79803.043\n",
      "    sample_throughput: 65.368\n",
      "    sample_time_ms: 305960.975\n",
      "    update_time_ms: 2.957\n",
      "  timestamp: 1659408453\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2940000\n",
      "  training_iteration: 147\n",
      "  trial_id: fd1ac_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 22.4/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/4.91 GiB heap, 0.0/14.65 GiB objects (0.0/6.0 CPU_group_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_1_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_5_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_3_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_0_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_2_28b5456e04c30e30e910ccff5ed5840d, 0.0/1.0 CPU_group_4_28b5456e04c30e30e910ccff5ed5840d)\n",
      "Result logdir: /home/michael/ray_results/stabilizing_the_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_fd1ac_00000 | RUNNING  | 192.168.0.33:822 |    147 |          44709.2 | 2940000 |   2358.3 |              3163.13 |              1445.21 |               4000 |\n",
      "+-----------------------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "Number of errored trials: 1\n",
      "+-----------------------------------------+--------------+------------------------------------------------------------------------------------------------------------------------+\n",
      "| Trial name                              |   # failures | error file                                                                                                             |\n",
      "|-----------------------------------------+--------------+------------------------------------------------------------------------------------------------------------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_fd1ac_00000 |            1 | /home/michael/ray_results/stabilizing_the_ring/PPO_WaveAttenuationPOEnv-v0_fd1ac_00000_0_2022-08-01_09-19-43/error.txt |\n",
      "+-----------------------------------------+--------------+------------------------------------------------------------------------------------------------------------------------+\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=822)\u001b[0m 2022-08-01 21:47:33,580\tWARNING ppo.py:242 -- The magnitude of your environment rewards are more than 236.0x the scale of `vf_clip_param`. This means that it will take more than 236.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=821)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=821)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=821)\u001b[0m ring length: 222\n",
      "\u001b[2m\u001b[36m(pid=821)\u001b[0m v_max: 3.3329270738617227\n",
      "\u001b[2m\u001b[36m(pid=821)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=825)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=825)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=825)\u001b[0m ring length: 230\n",
      "\u001b[2m\u001b[36m(pid=825)\u001b[0m v_max: 3.7136148111012934\n",
      "\u001b[2m\u001b[36m(pid=825)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=824)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=824)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=824)\u001b[0m ring length: 251\n",
      "\u001b[2m\u001b[36m(pid=824)\u001b[0m v_max: 4.71224180704279\n",
      "\u001b[2m\u001b[36m(pid=824)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=826)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=826)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=826)\u001b[0m ring length: 235\n",
      "\u001b[2m\u001b[36m(pid=826)\u001b[0m v_max: 3.9514850725282584\n",
      "\u001b[2m\u001b[36m(pid=826)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=823)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=823)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=823)\u001b[0m ring length: 242\n",
      "\u001b[2m\u001b[36m(pid=823)\u001b[0m v_max: 4.2844067680020546\n",
      "\u001b[2m\u001b[36m(pid=823)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=824)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=821)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=825)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=826)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=823)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=824)\u001b[0m Could not connect to TraCI server at localhost:53427 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=824)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=821)\u001b[0m Could not connect to TraCI server at localhost:58393 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=821)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=825)\u001b[0m Could not connect to TraCI server at localhost:46055 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=825)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=826)\u001b[0m Could not connect to TraCI server at localhost:45645 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=826)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=823)\u001b[0m Could not connect to TraCI server at localhost:58903 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=823)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=824)\u001b[0m Could not connect to TraCI server at localhost:53427 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=824)\u001b[0m  Retrying in 3 seconds\n",
      "\u001b[2m\u001b[36m(pid=826)\u001b[0m Could not connect to TraCI server at localhost:45645 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=826)\u001b[0m  Retrying in 3 seconds\n",
      "\u001b[2m\u001b[36m(pid=821)\u001b[0m Could not connect to TraCI server at localhost:58393 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=821)\u001b[0m  Retrying in 3 seconds\n",
      "\u001b[2m\u001b[36m(pid=825)\u001b[0m Could not connect to TraCI server at localhost:46055 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=825)\u001b[0m  Retrying in 3 seconds\n",
      "\u001b[2m\u001b[36m(pid=823)\u001b[0m Could not connect to TraCI server at localhost:58903 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=823)\u001b[0m  Retrying in 3 seconds\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_fd1ac_00000:\n",
      "  agent_timesteps_total: 2960000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-01_21-52-41\n",
      "  done: false\n",
      "  episode_len_mean: 4000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3163.131906025613\n",
      "  episode_reward_mean: 2241.8188830400986\n",
      "  episode_reward_min: 1445.2146079285546\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 740\n",
      "  experiment_id: 2b7c5715c95c4828907cbc45e49df6f0\n",
      "  hostname: michael-home\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.2\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -3.566587894129905\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017316386360040372\n",
      "          policy_loss: 0.0017939284936210533\n",
      "          total_loss: 747.7140665771095\n",
      "          vf_explained_var: -2.543637744878424e-09\n",
      "          vf_loss: 747.7088105487217\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 2960000\n",
      "    num_steps_sampled: 2960000\n",
      "    num_steps_trained: 2960000\n",
      "  iterations_since_restore: 2\n",
      "  node_ip: 192.168.0.33\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 25.08678815489749\n",
      "    ram_util_percent: 74.73052391799544\n",
      "  pid: 822\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09441303632849402\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 47.32959946516462\n",
      "    mean_inference_ms: 1.8849172243419248\n",
      "    mean_raw_obs_processing_ms: 21.992128480193703\n",
      "  time_since_restore: 693.587809085846\n",
      "  time_this_iter_s: 307.8061532974243\n",
      "  time_total_s: 45017.0439760685\n",
      "  timers:\n",
      "    learn_throughput: 248.33\n",
      "    learn_time_ms: 80538.013\n",
      "    sample_throughput: 75.12\n",
      "    sample_time_ms: 266239.466\n",
      "    update_time_ms: 3.202\n",
      "  timestamp: 1659408761\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2960000\n",
      "  training_iteration: 148\n",
      "  trial_id: fd1ac_00000\n",
      "  \n",
      "\u001b[2m\u001b[36m(pid=822)\u001b[0m 2022-08-01 21:52:41,479\tWARNING ppo.py:242 -- The magnitude of your environment rewards are more than 224.0x the scale of `vf_clip_param`. This means that it will take more than 224.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "== Status ==\n",
      "Memory usage on this node: 23.5/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/4.91 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_3_5f53d26ad78d84097dfda0a15e19ba2e, 0.0/6.0 CPU_group_5f53d26ad78d84097dfda0a15e19ba2e, 0.0/1.0 CPU_group_5_5f53d26ad78d84097dfda0a15e19ba2e, 0.0/1.0 CPU_group_1_5f53d26ad78d84097dfda0a15e19ba2e, 0.0/1.0 CPU_group_4_5f53d26ad78d84097dfda0a15e19ba2e, 0.0/1.0 CPU_group_0_5f53d26ad78d84097dfda0a15e19ba2e, 0.0/1.0 CPU_group_2_5f53d26ad78d84097dfda0a15e19ba2e)\n",
      "Result logdir: /home/michael/ray_results/stabilizing_the_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_fd1ac_00000 | RUNNING  | 192.168.0.33:822 |    148 |            45017 | 2960000 |  2241.82 |              3163.13 |              1445.21 |               4000 |\n",
      "+-----------------------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "Number of errored trials: 1\n",
      "+-----------------------------------------+--------------+------------------------------------------------------------------------------------------------------------------------+\n",
      "| Trial name                              |   # failures | error file                                                                                                             |\n",
      "|-----------------------------------------+--------------+------------------------------------------------------------------------------------------------------------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_fd1ac_00000 |            1 | /home/michael/ray_results/stabilizing_the_ring/PPO_WaveAttenuationPOEnv-v0_fd1ac_00000_0_2022-08-01_09-19-43/error.txt |\n",
      "+-----------------------------------------+--------------+------------------------------------------------------------------------------------------------------------------------+\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=821)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=821)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=821)\u001b[0m ring length: 240\n",
      "\u001b[2m\u001b[36m(pid=821)\u001b[0m v_max: 4.189299083856172\n",
      "\u001b[2m\u001b[36m(pid=821)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=825)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=825)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=825)\u001b[0m ring length: 250\n",
      "\u001b[2m\u001b[36m(pid=825)\u001b[0m v_max: 4.664717904914125\n",
      "\u001b[2m\u001b[36m(pid=825)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=824)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=824)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=824)\u001b[0m ring length: 251\n",
      "\u001b[2m\u001b[36m(pid=824)\u001b[0m v_max: 4.71224180704279\n",
      "\u001b[2m\u001b[36m(pid=824)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=826)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=826)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=826)\u001b[0m ring length: 247\n",
      "\u001b[2m\u001b[36m(pid=826)\u001b[0m v_max: 4.522125250095652\n",
      "\u001b[2m\u001b[36m(pid=826)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=823)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=823)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=823)\u001b[0m ring length: 241\n",
      "\u001b[2m\u001b[36m(pid=823)\u001b[0m v_max: 4.236854288051661\n",
      "\u001b[2m\u001b[36m(pid=823)\u001b[0m -----------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_WaveAttenuationPOEnv-v0_fd1ac_00000:\n",
      "  agent_timesteps_total: 2980000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-01_21-57-44\n",
      "  done: false\n",
      "  episode_len_mean: 4000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3163.131906025613\n",
      "  episode_reward_mean: 2165.6118445122715\n",
      "  episode_reward_min: 1445.2146079285546\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 745\n",
      "  experiment_id: 2b7c5715c95c4828907cbc45e49df6f0\n",
      "  hostname: michael-home\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.2\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -3.5189542864538303\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011833764802127673\n",
      "          policy_loss: 0.0011559433681937825\n",
      "          total_loss: 735.2892579279128\n",
      "          vf_explained_var: -2.6954967147219122e-09\n",
      "          vf_loss: 735.2857375236074\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 2980000\n",
      "    num_steps_sampled: 2980000\n",
      "    num_steps_trained: 2980000\n",
      "  iterations_since_restore: 3\n",
      "  node_ip: 192.168.0.33\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 26.38125\n",
      "    ram_util_percent: 74.92430555555555\n",
      "  pid: 822\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09443983825029133\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 44.351756447762845\n",
      "    mean_inference_ms: 1.8866650169508674\n",
      "    mean_raw_obs_processing_ms: 22.034154877297475\n",
      "  time_since_restore: 996.0851972103119\n",
      "  time_this_iter_s: 302.49738812446594\n",
      "  time_total_s: 45319.54136419296\n",
      "  timers:\n",
      "    learn_throughput: 246.348\n",
      "    learn_time_ms: 81185.871\n",
      "    sample_throughput: 79.738\n",
      "    sample_time_ms: 250822.332\n",
      "    update_time_ms: 3.084\n",
      "  timestamp: 1659409064\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2980000\n",
      "  training_iteration: 149\n",
      "  trial_id: fd1ac_00000\n",
      "  \n",
      "\u001b[2m\u001b[36m(pid=822)\u001b[0m 2022-08-01 21:57:44,071\tWARNING ppo.py:242 -- The magnitude of your environment rewards are more than 217.0x the scale of `vf_clip_param`. This means that it will take more than 217.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "== Status ==\n",
      "Memory usage on this node: 23.5/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/4.91 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_3_5f53d26ad78d84097dfda0a15e19ba2e, 0.0/6.0 CPU_group_5f53d26ad78d84097dfda0a15e19ba2e, 0.0/1.0 CPU_group_5_5f53d26ad78d84097dfda0a15e19ba2e, 0.0/1.0 CPU_group_2_5f53d26ad78d84097dfda0a15e19ba2e, 0.0/1.0 CPU_group_0_5f53d26ad78d84097dfda0a15e19ba2e, 0.0/1.0 CPU_group_4_5f53d26ad78d84097dfda0a15e19ba2e, 0.0/1.0 CPU_group_1_5f53d26ad78d84097dfda0a15e19ba2e, 0.0/1.0 accelerator_type:G)\n",
      "Result logdir: /home/michael/ray_results/stabilizing_the_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_fd1ac_00000 | RUNNING  | 192.168.0.33:822 |    149 |          45319.5 | 2980000 |  2165.61 |              3163.13 |              1445.21 |               4000 |\n",
      "+-----------------------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "Number of errored trials: 1\n",
      "+-----------------------------------------+--------------+------------------------------------------------------------------------------------------------------------------------+\n",
      "| Trial name                              |   # failures | error file                                                                                                             |\n",
      "|-----------------------------------------+--------------+------------------------------------------------------------------------------------------------------------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_fd1ac_00000 |            1 | /home/michael/ray_results/stabilizing_the_ring/PPO_WaveAttenuationPOEnv-v0_fd1ac_00000_0_2022-08-01_09-19-43/error.txt |\n",
      "+-----------------------------------------+--------------+------------------------------------------------------------------------------------------------------------------------+\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=821)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=821)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=821)\u001b[0m ring length: 243\n",
      "\u001b[2m\u001b[36m(pid=821)\u001b[0m v_max: 4.331956438196479\n",
      "\u001b[2m\u001b[36m(pid=821)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=825)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=825)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=825)\u001b[0m ring length: 264\n",
      "\u001b[2m\u001b[36m(pid=825)\u001b[0m v_max: 5.329679917416892\n",
      "\u001b[2m\u001b[36m(pid=825)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=824)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=824)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=824)\u001b[0m ring length: 243\n",
      "\u001b[2m\u001b[36m(pid=824)\u001b[0m v_max: 4.331956438196479\n",
      "\u001b[2m\u001b[36m(pid=824)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=826)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=826)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=826)\u001b[0m ring length: 230\n",
      "\u001b[2m\u001b[36m(pid=826)\u001b[0m v_max: 3.7136148111012934\n",
      "\u001b[2m\u001b[36m(pid=826)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=823)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=823)\u001b[0m -----------------------\n",
      "\u001b[2m\u001b[36m(pid=823)\u001b[0m ring length: 260\n",
      "\u001b[2m\u001b[36m(pid=823)\u001b[0m v_max: 5.139779427502188\n",
      "\u001b[2m\u001b[36m(pid=823)\u001b[0m -----------------------\n",
      "Result for PPO_WaveAttenuationPOEnv-v0_fd1ac_00000:\n",
      "  agent_timesteps_total: 3000000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-08-01_22-02-48\n",
      "  done: false\n",
      "  episode_len_mean: 4000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 3163.131906025613\n",
      "  episode_reward_mean: 2178.028898360287\n",
      "  episode_reward_min: 1445.2146079285546\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 750\n",
      "  experiment_id: 2b7c5715c95c4828907cbc45e49df6f0\n",
      "  hostname: michael-home\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.2\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -3.51535543924684\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014934370890796056\n",
      "          policy_loss: 0.0016021126224273804\n",
      "          total_loss: 734.2794996990519\n",
      "          vf_explained_var: -2.7334614571827842e-09\n",
      "          vf_loss: 734.2749101687389\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 3000000\n",
      "    num_steps_sampled: 3000000\n",
      "    num_steps_trained: 3000000\n",
      "  iterations_since_restore: 4\n",
      "  node_ip: 192.168.0.33\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 26.831494252873565\n",
      "    ram_util_percent: 75.16919540229885\n",
      "  pid: 822\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09458718002805945\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 42.41293348425735\n",
      "    mean_inference_ms: 1.8919528169451294\n",
      "    mean_raw_obs_processing_ms: 22.02574528750794\n",
      "  time_since_restore: 1300.656197309494\n",
      "  time_this_iter_s: 304.57100009918213\n",
      "  time_total_s: 45624.112364292145\n",
      "  timers:\n",
      "    learn_throughput: 244.219\n",
      "    learn_time_ms: 81893.641\n",
      "    sample_throughput: 82.22\n",
      "    sample_time_ms: 243251.006\n",
      "    update_time_ms: 3.045\n",
      "  timestamp: 1659409368\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 3000000\n",
      "  training_iteration: 150\n",
      "  trial_id: fd1ac_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 23.5/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/4.91 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_0_5f53d26ad78d84097dfda0a15e19ba2e, 0.0/1.0 CPU_group_1_5f53d26ad78d84097dfda0a15e19ba2e, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_3_5f53d26ad78d84097dfda0a15e19ba2e, 0.0/1.0 CPU_group_2_5f53d26ad78d84097dfda0a15e19ba2e, 0.0/1.0 CPU_group_4_5f53d26ad78d84097dfda0a15e19ba2e, 0.0/1.0 CPU_group_5_5f53d26ad78d84097dfda0a15e19ba2e, 0.0/6.0 CPU_group_5f53d26ad78d84097dfda0a15e19ba2e)\n",
      "Result logdir: /home/michael/ray_results/stabilizing_the_ring\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_fd1ac_00000 | RUNNING  | 192.168.0.33:822 |    150 |          45624.1 | 3000000 |  2178.03 |              3163.13 |              1445.21 |               4000 |\n",
      "+-----------------------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "Number of errored trials: 1\n",
      "+-----------------------------------------+--------------+------------------------------------------------------------------------------------------------------------------------+\n",
      "| Trial name                              |   # failures | error file                                                                                                             |\n",
      "|-----------------------------------------+--------------+------------------------------------------------------------------------------------------------------------------------|\n",
      "| PPO_WaveAttenuationPOEnv-v0_fd1ac_00000 |            1 | /home/michael/ray_results/stabilizing_the_ring/PPO_WaveAttenuationPOEnv-v0_fd1ac_00000_0_2022-08-01_09-19-43/error.txt |\n",
      "+-----------------------------------------+--------------+------------------------------------------------------------------------------------------------------------------------+\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=822)\u001b[0m 2022-08-01 22:02:48,741\tWARNING ppo.py:242 -- The magnitude of your environment rewards are more than 218.0x the scale of `vf_clip_param`. This means that it will take more than 218.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\r\n"
     ]
    }
   ],
   "source": [
    "!python examples/train.py singleagent_ring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca1e948b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
