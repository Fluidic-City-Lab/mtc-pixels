{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9ae1177",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/michael/Desktop/flow/flow/utils/flow_warnings.py:26: PendingDeprecationWarning: The attribute departLane in InFlows is deprecated, use depart_lane instead.\n",
      "  PendingDeprecationWarning\n",
      "/home/michael/Desktop/flow/flow/utils/flow_warnings.py:26: PendingDeprecationWarning: The attribute departSpeed in InFlows is deprecated, use depart_speed instead.\n",
      "  PendingDeprecationWarning\n",
      "/home/michael/Desktop/flow/flow/utils/flow_warnings.py:26: PendingDeprecationWarning: The attribute departLane in InFlows is deprecated, use depart_lane instead.\n",
      "  PendingDeprecationWarning\n",
      "/home/michael/Desktop/flow/flow/utils/flow_warnings.py:26: PendingDeprecationWarning: The attribute departSpeed in InFlows is deprecated, use depart_speed instead.\n",
      "  PendingDeprecationWarning\n",
      "/home/michael/Desktop/flow/flow/utils/flow_warnings.py:26: PendingDeprecationWarning: The attribute departLane in InFlows is deprecated, use depart_lane instead.\n",
      "  PendingDeprecationWarning\n",
      "/home/michael/Desktop/flow/flow/utils/flow_warnings.py:26: PendingDeprecationWarning: The attribute departSpeed in InFlows is deprecated, use depart_speed instead.\n",
      "  PendingDeprecationWarning\n",
      "/home/michael/Desktop/flow/flow/utils/flow_warnings.py:26: PendingDeprecationWarning: The attribute departLane in InFlows is deprecated, use depart_lane instead.\n",
      "  PendingDeprecationWarning\n",
      "/home/michael/Desktop/flow/flow/utils/flow_warnings.py:26: PendingDeprecationWarning: The attribute departSpeed in InFlows is deprecated, use depart_speed instead.\n",
      "  PendingDeprecationWarning\n",
      "{'num_workers': 5, 'num_envs_per_worker': 1, 'create_env_on_driver': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'gamma': 0.999, 'lr': 5e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': [[16, [8, 8], 4], [32, [4, 4], 2], [256, [11, 11], 1]], 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'num_framestacks': 'auto', 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1, 'framestack': True}, 'optimizer': {}, 'horizon': 400, 'soft_horizon': False, 'no_done_at_end': False, 'env': None, 'observation_space': None, 'action_space': None, 'env_config': {}, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'torch', 'eager_tracing': False, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_num_episodes': 10, 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'collect_metrics_timeout': 180, 'metrics_smoothing_episodes': 100, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'min_iter_time_s': 0, 'timesteps_per_iteration': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {}, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, 'simple_optimizer': -1, 'monitor': -1, 'use_critic': True, 'use_gae': True, 'lambda': 0.97, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 10, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.02, 'vf_share_layers': -1}\n",
      "2022-10-31 23:03:04,978\tINFO services.py:1247 -- View the Ray dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265\u001b[39m\u001b[22m\n",
      "== Status ==\n",
      "Memory usage on this node: 5.4/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 0/6 CPUs, 0/6 GPUs, 0.0/9.6 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 PENDING)\n",
      "+-------------------------------------------------+----------+-------+\n",
      "| Trial name                                      | status   | loc   |\n",
      "|-------------------------------------------------+----------+-------|\n",
      "| PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000 | PENDING  |       |\n",
      "+-------------------------------------------------+----------+-------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=16335)\u001b[0m 2022-10-31 23:03:07,162\tINFO trainer.py:720 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=16335)\u001b[0m 2022-10-31 23:03:10,212\tWARNING util.py:55 -- Install gputil for GPU system monitoring.\n",
      "Result for PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000:\n",
      "  agent_timesteps_total: 4000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-31_23-04-09\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 153.14604555800602\n",
      "  episode_reward_mean: 148.38432607493723\n",
      "  episode_reward_min: 143.95193055043651\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 10\n",
      "  experiment_id: cf8409b3cbbc42979d4540c7e6fbf708\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.19999999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.4474090158939361\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0017812136409929747\n",
      "          policy_loss: -0.0025894309306750073\n",
      "          total_loss: 75.31813398599624\n",
      "          vf_explained_var: -1.011416301821555e-07\n",
      "          vf_loss: 75.32036712169648\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 4000\n",
      "    num_steps_sampled: 4000\n",
      "    num_steps_trained: 4000\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.78139534883721\n",
      "    ram_util_percent: 23.024418604651157\n",
      "  pid: 16335\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08431552501206988\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 34.38534617572837\n",
      "    mean_inference_ms: 1.7339152194438654\n",
      "    mean_raw_obs_processing_ms: 6.108034922091404\n",
      "  time_since_restore: 59.725807666778564\n",
      "  time_this_iter_s: 59.725807666778564\n",
      "  time_total_s: 59.725807666778564\n",
      "  timers:\n",
      "    learn_throughput: 156.963\n",
      "    learn_time_ms: 25483.78\n",
      "    sample_throughput: 116.864\n",
      "    sample_time_ms: 34227.698\n",
      "    update_time_ms: 3.033\n",
      "  timestamp: 1667275449\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 4000\n",
      "  training_iteration: 1\n",
      "  trial_id: 168ff_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Memory usage on this node: 7.4/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/9.6 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                                      | status   | loc                |   iter |   total time (s) |   ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------------------------------+----------+--------------------+--------+------------------+------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000 | RUNNING  | 192.168.1.65:16335 |      1 |          59.7258 | 4000 |  148.384 |              153.146 |              143.952 |                400 |\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000:\n",
      "  agent_timesteps_total: 8000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-31_23-05-07\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 155.95498632284566\n",
      "  episode_reward_mean: 147.8021672461894\n",
      "  episode_reward_min: 138.89557253304523\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 20\n",
      "  experiment_id: cf8409b3cbbc42979d4540c7e6fbf708\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.09999999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.425993514060974\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008397792375228618\n",
      "          policy_loss: -0.0027001122478395702\n",
      "          total_loss: 69.70037093162537\n",
      "          vf_explained_var: -6.146728903644316e-09\n",
      "          vf_loss: 69.70223141908646\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 8000\n",
      "    num_steps_sampled: 8000\n",
      "    num_steps_trained: 8000\n",
      "  iterations_since_restore: 2\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.514814814814812\n",
      "    ram_util_percent: 24.179012345679016\n",
      "  pid: 16335\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08472678179268682\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 33.62575825306074\n",
      "    mean_inference_ms: 1.7561427533112357\n",
      "    mean_raw_obs_processing_ms: 6.094355678848304\n",
      "  time_since_restore: 116.93574571609497\n",
      "  time_this_iter_s: 57.209938049316406\n",
      "  time_total_s: 116.93574571609497\n",
      "  timers:\n",
      "    learn_throughput: 156.746\n",
      "    learn_time_ms: 25519.051\n",
      "    sample_throughput: 121.452\n",
      "    sample_time_ms: 32934.702\n",
      "    update_time_ms: 3.268\n",
      "  timestamp: 1667275507\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 8000\n",
      "  training_iteration: 2\n",
      "  trial_id: 168ff_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 7.5/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/9.6 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_4_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_2_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_5_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_3_400b3145f046e80a4a7d53925cb6fe08, 0.0/6.0 CPU_group_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_1_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_0_400b3145f046e80a4a7d53925cb6fe08)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                                      | status   | loc                |   iter |   total time (s) |   ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------------------------------+----------+--------------------+--------+------------------+------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000 | RUNNING  | 192.168.1.65:16335 |      2 |          116.936 | 8000 |  147.802 |              155.955 |              138.896 |                400 |\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000:\n",
      "  agent_timesteps_total: 12000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-31_23-06-04\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 155.95498632284566\n",
      "  episode_reward_mean: 146.78532166584543\n",
      "  episode_reward_min: 134.34988832849953\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 30\n",
      "  experiment_id: cf8409b3cbbc42979d4540c7e6fbf708\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.049999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.4222957603633404\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.00735228716163192\n",
      "          policy_loss: -0.0031776368618011475\n",
      "          total_loss: 65.60768420696259\n",
      "          vf_explained_var: -8.75443184611413e-09\n",
      "          vf_loss: 65.61049474477768\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 12000\n",
      "    num_steps_sampled: 12000\n",
      "    num_steps_trained: 12000\n",
      "  iterations_since_restore: 3\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.22073170731708\n",
      "    ram_util_percent: 24.203658536585372\n",
      "  pid: 16335\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08484023893687086\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 33.2103528316808\n",
      "    mean_inference_ms: 1.7601440884650377\n",
      "    mean_raw_obs_processing_ms: 6.0897040723493205\n",
      "  time_since_restore: 173.79668736457825\n",
      "  time_this_iter_s: 56.860941648483276\n",
      "  time_total_s: 173.79668736457825\n",
      "  timers:\n",
      "    learn_throughput: 157.423\n",
      "    learn_time_ms: 25409.195\n",
      "    sample_throughput: 123.043\n",
      "    sample_time_ms: 32509.014\n",
      "    update_time_ms: 3.362\n",
      "  timestamp: 1667275564\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 12000\n",
      "  training_iteration: 3\n",
      "  trial_id: 168ff_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 7.6/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/9.6 GiB heap, 0.0/14.65 GiB objects (0.0/6.0 CPU_group_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_3_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_0_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_4_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_2_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_1_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_5_400b3145f046e80a4a7d53925cb6fe08)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                                      | status   | loc                |   iter |   total time (s) |    ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------------------------------+----------+--------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000 | RUNNING  | 192.168.1.65:16335 |      3 |          173.797 | 12000 |  146.785 |              155.955 |               134.35 |                400 |\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000:\n",
      "  agent_timesteps_total: 16000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-31_23-07-00\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 155.95498632284566\n",
      "  episode_reward_mean: 146.80896588933396\n",
      "  episode_reward_min: 134.34988832849953\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 40\n",
      "  experiment_id: cf8409b3cbbc42979d4540c7e6fbf708\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.024999999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.413072719424963\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.004457622387528204\n",
      "          policy_loss: 1.0354843834647908e-05\n",
      "          total_loss: 67.66746897697449\n",
      "          vf_explained_var: -4.284083754413359e-09\n",
      "          vf_loss: 67.66734728813171\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 16000\n",
      "    num_steps_sampled: 16000\n",
      "    num_steps_trained: 16000\n",
      "  iterations_since_restore: 4\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.415000000000003\n",
      "    ram_util_percent: 24.347500000000004\n",
      "  pid: 16335\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08493058145932177\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 32.94154361446473\n",
      "    mean_inference_ms: 1.762689878442573\n",
      "    mean_raw_obs_processing_ms: 6.083289559603454\n",
      "  time_since_restore: 230.4150035381317\n",
      "  time_this_iter_s: 56.61831617355347\n",
      "  time_total_s: 230.4150035381317\n",
      "  timers:\n",
      "    learn_throughput: 158.129\n",
      "    learn_time_ms: 25295.811\n",
      "    sample_throughput: 123.862\n",
      "    sample_time_ms: 32293.94\n",
      "    update_time_ms: 3.276\n",
      "  timestamp: 1667275620\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 16000\n",
      "  training_iteration: 4\n",
      "  trial_id: 168ff_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 7.6/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/9.6 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 accelerator_type:G, 0.0/6.0 CPU_group_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_3_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_0_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_4_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_1_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_5_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_2_400b3145f046e80a4a7d53925cb6fe08)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                                      | status   | loc                |   iter |   total time (s) |    ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------------------------------+----------+--------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000 | RUNNING  | 192.168.1.65:16335 |      4 |          230.415 | 16000 |  146.809 |              155.955 |               134.35 |                400 |\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=16334)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=16331)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=16333)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=16332)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=16330)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=16334)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=16331)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=16333)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=16331)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=16333)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=16334)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=16332)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=16330)\u001b[0m  Retrying in 1 seconds\n",
      "Result for PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000:\n",
      "  agent_timesteps_total: 20000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-31_23-08-00\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 155.95498632284566\n",
      "  episode_reward_mean: 147.12844795154808\n",
      "  episode_reward_min: 134.34988832849953\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 50\n",
      "  experiment_id: cf8409b3cbbc42979d4540c7e6fbf708\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.012499999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.3383203439414502\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.006986956166593927\n",
      "          policy_loss: 0.00034147634723922236\n",
      "          total_loss: 69.05265254974366\n",
      "          vf_explained_var: -2.0489097085629737e-09\n",
      "          vf_loss: 69.0522243142128\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 20000\n",
      "    num_steps_sampled: 20000\n",
      "    num_steps_trained: 20000\n",
      "  iterations_since_restore: 5\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.296511627906973\n",
      "    ram_util_percent: 24.272093023255817\n",
      "  pid: 16335\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08500009725171541\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 32.75147341949855\n",
      "    mean_inference_ms: 1.764291094415646\n",
      "    mean_raw_obs_processing_ms: 6.227090967037952\n",
      "  time_since_restore: 290.3190486431122\n",
      "  time_this_iter_s: 59.90404510498047\n",
      "  time_total_s: 290.3190486431122\n",
      "  timers:\n",
      "    learn_throughput: 158.21\n",
      "    learn_time_ms: 25282.847\n",
      "    sample_throughput: 122.074\n",
      "    sample_time_ms: 32766.989\n",
      "    update_time_ms: 3.234\n",
      "  timestamp: 1667275680\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 20000\n",
      "  training_iteration: 5\n",
      "  trial_id: 168ff_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 7.6/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/9.6 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_0_400b3145f046e80a4a7d53925cb6fe08, 0.0/6.0 CPU_group_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_1_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_3_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_2_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_4_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_5_400b3145f046e80a4a7d53925cb6fe08)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                                      | status   | loc                |   iter |   total time (s) |    ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------------------------------+----------+--------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000 | RUNNING  | 192.168.1.65:16335 |      5 |          290.319 | 20000 |  147.128 |              155.955 |               134.35 |                400 |\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000:\n",
      "  agent_timesteps_total: 24000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-31_23-08-57\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 155.95498632284566\n",
      "  episode_reward_mean: 147.2151062296162\n",
      "  episode_reward_min: 134.34988832849953\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 60\n",
      "  experiment_id: cf8409b3cbbc42979d4540c7e6fbf708\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.0062499999999999995\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.3174512356519699\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0077747626942766615\n",
      "          policy_loss: -0.001167524747143034\n",
      "          total_loss: 70.65714248418809\n",
      "          vf_explained_var: -1.3038515822572094e-09\n",
      "          vf_loss: 70.65826094150543\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 24000\n",
      "    num_steps_sampled: 24000\n",
      "    num_steps_trained: 24000\n",
      "  iterations_since_restore: 6\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.14320987654321\n",
      "    ram_util_percent: 24.404938271604934\n",
      "  pid: 16335\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08504314182680606\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 32.60886241123644\n",
      "    mean_inference_ms: 1.7650868534065032\n",
      "    mean_raw_obs_processing_ms: 6.30053014195599\n",
      "  time_since_restore: 347.31491351127625\n",
      "  time_this_iter_s: 56.99586486816406\n",
      "  time_total_s: 347.31491351127625\n",
      "  timers:\n",
      "    learn_throughput: 158.138\n",
      "    learn_time_ms: 25294.437\n",
      "    sample_throughput: 122.785\n",
      "    sample_time_ms: 32577.273\n",
      "    update_time_ms: 3.221\n",
      "  timestamp: 1667275737\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 24000\n",
      "  training_iteration: 6\n",
      "  trial_id: 168ff_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 7.6/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/9.6 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_4_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_0_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 accelerator_type:G, 0.0/6.0 CPU_group_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_3_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_2_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_5_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_1_400b3145f046e80a4a7d53925cb6fe08)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                                      | status   | loc                |   iter |   total time (s) |    ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------------------------------+----------+--------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000 | RUNNING  | 192.168.1.65:16335 |      6 |          347.315 | 24000 |  147.215 |              155.955 |               134.35 |                400 |\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000:\n",
      "  agent_timesteps_total: 28000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-31_23-09-54\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 156.5854138871197\n",
      "  episode_reward_mean: 147.37919026083375\n",
      "  episode_reward_min: 134.34988832849953\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 70\n",
      "  experiment_id: cf8409b3cbbc42979d4540c7e6fbf708\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.0031249999999999997\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.2808966059237719\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.003444761956141564\n",
      "          policy_loss: 0.0004105894389795139\n",
      "          total_loss: 73.8499188542366\n",
      "          vf_explained_var: 1.4901161415892261e-09\n",
      "          vf_loss: 73.84949748516082\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 28000\n",
      "    num_steps_sampled: 28000\n",
      "    num_steps_trained: 28000\n",
      "  iterations_since_restore: 7\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.320987654320984\n",
      "    ram_util_percent: 24.31975308641975\n",
      "  pid: 16335\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08508419893008541\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 32.49664272263444\n",
      "    mean_inference_ms: 1.7657292605311856\n",
      "    mean_raw_obs_processing_ms: 6.339999698384002\n",
      "  time_since_restore: 404.0391471385956\n",
      "  time_this_iter_s: 56.724233627319336\n",
      "  time_total_s: 404.0391471385956\n",
      "  timers:\n",
      "    learn_throughput: 158.357\n",
      "    learn_time_ms: 25259.414\n",
      "    sample_throughput: 123.281\n",
      "    sample_time_ms: 32446.315\n",
      "    update_time_ms: 3.186\n",
      "  timestamp: 1667275794\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 28000\n",
      "  training_iteration: 7\n",
      "  trial_id: 168ff_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 7.6/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/9.6 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_0_400b3145f046e80a4a7d53925cb6fe08, 0.0/6.0 CPU_group_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_2_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_1_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_3_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_4_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_5_400b3145f046e80a4a7d53925cb6fe08)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                                      | status   | loc                |   iter |   total time (s) |    ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------------------------------+----------+--------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000 | RUNNING  | 192.168.1.65:16335 |      7 |          404.039 | 28000 |  147.379 |              156.585 |               134.35 |                400 |\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=16333)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=16330)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=16331)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=16333)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=16332)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=16330)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=16334)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=16334)\u001b[0m Could not connect to TraCI server at localhost:47333 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=16334)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=16331)\u001b[0m Could not connect to TraCI server at localhost:45859 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=16331)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=16333)\u001b[0m Could not connect to TraCI server at localhost:60241 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=16333)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=16332)\u001b[0m Could not connect to TraCI server at localhost:41419 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=16332)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=16330)\u001b[0m Could not connect to TraCI server at localhost:44711 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=16330)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=16334)\u001b[0m Could not connect to TraCI server at localhost:47333 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=16334)\u001b[0m  Retrying in 3 seconds\n",
      "\u001b[2m\u001b[36m(pid=16331)\u001b[0m Could not connect to TraCI server at localhost:45859 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=16331)\u001b[0m  Retrying in 3 seconds\n",
      "\u001b[2m\u001b[36m(pid=16333)\u001b[0m Could not connect to TraCI server at localhost:60241 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=16333)\u001b[0m  Retrying in 3 seconds\n",
      "\u001b[2m\u001b[36m(pid=16332)\u001b[0m Could not connect to TraCI server at localhost:41419 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=16332)\u001b[0m  Retrying in 3 seconds\n",
      "\u001b[2m\u001b[36m(pid=16330)\u001b[0m Could not connect to TraCI server at localhost:44711 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=16330)\u001b[0m  Retrying in 3 seconds\n",
      "Result for PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000:\n",
      "  agent_timesteps_total: 32000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-31_23-10-58\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 157.3647702018468\n",
      "  episode_reward_mean: 147.70023223857157\n",
      "  episode_reward_min: 134.34988832849953\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 80\n",
      "  experiment_id: cf8409b3cbbc42979d4540c7e6fbf708\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.0015624999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.2271074034273624\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0028156607755633516\n",
      "          policy_loss: -0.001951426948653534\n",
      "          total_loss: 79.11978242397308\n",
      "          vf_explained_var: -0.004440173041075468\n",
      "          vf_loss: 79.12172966003418\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 32000\n",
      "    num_steps_sampled: 32000\n",
      "    num_steps_trained: 32000\n",
      "  iterations_since_restore: 8\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 18.534065934065936\n",
      "    ram_util_percent: 24.36813186813187\n",
      "  pid: 16335\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08512037216669979\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 32.41777630300716\n",
      "    mean_inference_ms: 1.766404695748895\n",
      "    mean_raw_obs_processing_ms: 6.486031643803339\n",
      "  time_since_restore: 467.8510842323303\n",
      "  time_this_iter_s: 63.81193709373474\n",
      "  time_total_s: 467.8510842323303\n",
      "  timers:\n",
      "    learn_throughput: 158.454\n",
      "    learn_time_ms: 25243.843\n",
      "    sample_throughput: 120.397\n",
      "    sample_time_ms: 33223.399\n",
      "    update_time_ms: 3.197\n",
      "  timestamp: 1667275858\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 32000\n",
      "  training_iteration: 8\n",
      "  trial_id: 168ff_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 7.6/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/9.6 GiB heap, 0.0/14.65 GiB objects (0.0/6.0 CPU_group_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_0_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_1_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_5_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_2_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_3_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_4_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 accelerator_type:G)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                                      | status   | loc                |   iter |   total time (s) |    ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------------------------------+----------+--------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000 | RUNNING  | 192.168.1.65:16335 |      8 |          467.851 | 32000 |    147.7 |              157.365 |               134.35 |                400 |\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000:\n",
      "  agent_timesteps_total: 36000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-31_23-11-55\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 157.3647702018468\n",
      "  episode_reward_mean: 147.88360757260025\n",
      "  episode_reward_min: 134.34988832849953\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 90\n",
      "  experiment_id: cf8409b3cbbc42979d4540c7e6fbf708\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.0007812499999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.2151079576462507\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009068831571054767\n",
      "          policy_loss: -0.004496502765687182\n",
      "          total_loss: 84.02415895462036\n",
      "          vf_explained_var: -2.9802322831784522e-09\n",
      "          vf_loss: 84.0286481142044\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 36000\n",
      "    num_steps_sampled: 36000\n",
      "    num_steps_trained: 36000\n",
      "  iterations_since_restore: 9\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.23048780487805\n",
      "    ram_util_percent: 24.524390243902438\n",
      "  pid: 16335\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0851582585839216\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 32.3506800780775\n",
      "    mean_inference_ms: 1.7670016782220923\n",
      "    mean_raw_obs_processing_ms: 6.581128810493277\n",
      "  time_since_restore: 524.7938976287842\n",
      "  time_this_iter_s: 56.94281339645386\n",
      "  time_total_s: 524.7938976287842\n",
      "  timers:\n",
      "    learn_throughput: 158.409\n",
      "    learn_time_ms: 25251.05\n",
      "    sample_throughput: 121.046\n",
      "    sample_time_ms: 33045.169\n",
      "    update_time_ms: 3.183\n",
      "  timestamp: 1667275915\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 36000\n",
      "  training_iteration: 9\n",
      "  trial_id: 168ff_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 7.7/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/9.6 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_5_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_1_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_2_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_0_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_3_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_4_400b3145f046e80a4a7d53925cb6fe08, 0.0/6.0 CPU_group_400b3145f046e80a4a7d53925cb6fe08)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                                      | status   | loc                |   iter |   total time (s) |    ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------------------------------+----------+--------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000 | RUNNING  | 192.168.1.65:16335 |      9 |          524.794 | 36000 |  147.884 |              157.365 |               134.35 |                400 |\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000:\n",
      "  agent_timesteps_total: 40000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-31_23-12-52\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 157.3647702018468\n",
      "  episode_reward_mean: 147.97200746328483\n",
      "  episode_reward_min: 134.34988832849953\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 100\n",
      "  experiment_id: cf8409b3cbbc42979d4540c7e6fbf708\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.00039062499999999997\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.1746144238859415\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.002339558637655877\n",
      "          policy_loss: 0.0007839677826268598\n",
      "          total_loss: 87.69082835912704\n",
      "          vf_explained_var: -6.705522359595761e-09\n",
      "          vf_loss: 87.69004360437393\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 40000\n",
      "    num_steps_sampled: 40000\n",
      "    num_steps_trained: 40000\n",
      "  iterations_since_restore: 10\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.190123456790126\n",
      "    ram_util_percent: 24.51358024691358\n",
      "  pid: 16335\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08519364210461522\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 32.29277044481535\n",
      "    mean_inference_ms: 1.7675834165812123\n",
      "    mean_raw_obs_processing_ms: 6.644918400079713\n",
      "  time_since_restore: 581.8097083568573\n",
      "  time_this_iter_s: 57.01581072807312\n",
      "  time_total_s: 581.8097083568573\n",
      "  timers:\n",
      "    learn_throughput: 158.403\n",
      "    learn_time_ms: 25252.075\n",
      "    sample_throughput: 121.526\n",
      "    sample_time_ms: 32914.698\n",
      "    update_time_ms: 3.159\n",
      "  timestamp: 1667275972\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 40000\n",
      "  training_iteration: 10\n",
      "  trial_id: 168ff_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 7.7/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/9.6 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_1_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_3_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_4_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_0_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_5_400b3145f046e80a4a7d53925cb6fe08, 0.0/6.0 CPU_group_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_2_400b3145f046e80a4a7d53925cb6fe08)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                                      | status   | loc                |   iter |   total time (s) |    ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------------------------------+----------+--------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000 | RUNNING  | 192.168.1.65:16335 |     10 |           581.81 | 40000 |  147.972 |              157.365 |               134.35 |                400 |\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=16334)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=16331)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=16333)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=16332)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=16330)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=16334)\u001b[0m Could not connect to TraCI server at localhost:47333 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=16334)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=16331)\u001b[0m Could not connect to TraCI server at localhost:45859 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=16331)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=16333)\u001b[0m Could not connect to TraCI server at localhost:60241 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=16333)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=16332)\u001b[0m Could not connect to TraCI server at localhost:41419 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=16332)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=16330)\u001b[0m Could not connect to TraCI server at localhost:44711 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=16330)\u001b[0m  Retrying in 2 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=16334)\u001b[0m Could not connect to TraCI server at localhost:47333 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=16334)\u001b[0m  Retrying in 3 seconds\n",
      "\u001b[2m\u001b[36m(pid=16331)\u001b[0m Could not connect to TraCI server at localhost:45859 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=16331)\u001b[0m  Retrying in 3 seconds\n",
      "\u001b[2m\u001b[36m(pid=16333)\u001b[0m Could not connect to TraCI server at localhost:60241 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=16333)\u001b[0m  Retrying in 3 seconds\n",
      "\u001b[2m\u001b[36m(pid=16332)\u001b[0m Could not connect to TraCI server at localhost:41419 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=16332)\u001b[0m  Retrying in 3 seconds\n",
      "\u001b[2m\u001b[36m(pid=16330)\u001b[0m Could not connect to TraCI server at localhost:44711 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=16330)\u001b[0m  Retrying in 3 seconds\n",
      "Result for PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000:\n",
      "  agent_timesteps_total: 44000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-31_23-13-55\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 157.3647702018468\n",
      "  episode_reward_mean: 147.83927020098002\n",
      "  episode_reward_min: 134.34988832849953\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 110\n",
      "  experiment_id: cf8409b3cbbc42979d4540c7e6fbf708\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.00019531249999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.1706939000636338\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0020654123141838504\n",
      "          policy_loss: 0.0006321952023427002\n",
      "          total_loss: 94.46463944911957\n",
      "          vf_explained_var: -1.1548399569960566e-08\n",
      "          vf_loss: 94.46400698423386\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 44000\n",
      "    num_steps_sampled: 44000\n",
      "    num_steps_trained: 44000\n",
      "  iterations_since_restore: 11\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 18.97888888888889\n",
      "    ram_util_percent: 24.56555555555555\n",
      "  pid: 16335\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08532192394965285\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 32.02874665345478\n",
      "    mean_inference_ms: 1.7726607838305823\n",
      "    mean_raw_obs_processing_ms: 6.813514844170807\n",
      "  time_since_restore: 644.6678700447083\n",
      "  time_this_iter_s: 62.85816168785095\n",
      "  time_total_s: 644.6678700447083\n",
      "  timers:\n",
      "    learn_throughput: 158.751\n",
      "    learn_time_ms: 25196.74\n",
      "    sample_throughput: 120.181\n",
      "    sample_time_ms: 33283.197\n",
      "    update_time_ms: 3.174\n",
      "  timestamp: 1667276035\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 44000\n",
      "  training_iteration: 11\n",
      "  trial_id: 168ff_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 7.7/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/9.6 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_0_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_3_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_2_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_1_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_4_400b3145f046e80a4a7d53925cb6fe08, 0.0/6.0 CPU_group_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_5_400b3145f046e80a4a7d53925cb6fe08)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                                      | status   | loc                |   iter |   total time (s) |    ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------------------------------+----------+--------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000 | RUNNING  | 192.168.1.65:16335 |     11 |          644.668 | 44000 |  147.839 |              157.365 |               134.35 |                400 |\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=16334)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=16331)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=16333)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=16332)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=16330)\u001b[0m  Retrying in 1 seconds\n",
      "Result for PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000:\n",
      "  agent_timesteps_total: 48000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-31_23-14-53\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 157.3647702018468\n",
      "  episode_reward_mean: 147.90279679600206\n",
      "  episode_reward_min: 134.34988832849953\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 120\n",
      "  experiment_id: cf8409b3cbbc42979d4540c7e6fbf708\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 9.765624999999999e-05\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.140355632826686\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.005364744992837709\n",
      "          policy_loss: 0.0013668380750459618\n",
      "          total_loss: 102.57283500432968\n",
      "          vf_explained_var: -8.009374496964483e-09\n",
      "          vf_loss: 102.57146735191345\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 48000\n",
      "    num_steps_sampled: 48000\n",
      "    num_steps_trained: 48000\n",
      "  iterations_since_restore: 12\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.73855421686747\n",
      "    ram_util_percent: 24.571084337349394\n",
      "  pid: 16335\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0853700806636942\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.913787940664115\n",
      "    mean_inference_ms: 1.77297540032216\n",
      "    mean_raw_obs_processing_ms: 6.980609624090502\n",
      "  time_since_restore: 702.6052532196045\n",
      "  time_this_iter_s: 57.93738317489624\n",
      "  time_total_s: 702.6052532196045\n",
      "  timers:\n",
      "    learn_throughput: 158.931\n",
      "    learn_time_ms: 25168.152\n",
      "    sample_throughput: 119.816\n",
      "    sample_time_ms: 33384.484\n",
      "    update_time_ms: 3.123\n",
      "  timestamp: 1667276093\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 48000\n",
      "  training_iteration: 12\n",
      "  trial_id: 168ff_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 7.6/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/9.6 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_2_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_5_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_3_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_0_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 accelerator_type:G, 0.0/6.0 CPU_group_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_1_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_4_400b3145f046e80a4a7d53925cb6fe08)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                                      | status   | loc                |   iter |   total time (s) |    ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------------------------------+----------+--------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000 | RUNNING  | 192.168.1.65:16335 |     12 |          702.605 | 48000 |  147.903 |              157.365 |               134.35 |                400 |\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000:\n",
      "  agent_timesteps_total: 52000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-31_23-15-50\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 157.3647702018468\n",
      "  episode_reward_mean: 148.13974282357736\n",
      "  episode_reward_min: 135.38741324929148\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 130\n",
      "  experiment_id: cf8409b3cbbc42979d4540c7e6fbf708\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 4.8828124999999996e-05\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.0641189400106668\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.004478433093889582\n",
      "          policy_loss: -0.0008243764226790517\n",
      "          total_loss: 109.84319869279861\n",
      "          vf_explained_var: 5.587935669737476e-10\n",
      "          vf_loss: 109.8440225481987\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 52000\n",
      "    num_steps_sampled: 52000\n",
      "    num_steps_trained: 52000\n",
      "  iterations_since_restore: 13\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.302469135802465\n",
      "    ram_util_percent: 24.658024691358023\n",
      "  pid: 16335\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0854241153123985\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.845030842721368\n",
      "    mean_inference_ms: 1.774406298769638\n",
      "    mean_raw_obs_processing_ms: 7.135003514093215\n",
      "  time_since_restore: 759.3075878620148\n",
      "  time_this_iter_s: 56.70233464241028\n",
      "  time_total_s: 759.3075878620148\n",
      "  timers:\n",
      "    learn_throughput: 159.053\n",
      "    learn_time_ms: 25148.825\n",
      "    sample_throughput: 119.804\n",
      "    sample_time_ms: 33387.849\n",
      "    update_time_ms: 3.112\n",
      "  timestamp: 1667276150\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 52000\n",
      "  training_iteration: 13\n",
      "  trial_id: 168ff_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 7.7/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/9.6 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_1_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_0_400b3145f046e80a4a7d53925cb6fe08, 0.0/6.0 CPU_group_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_2_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_4_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_3_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_5_400b3145f046e80a4a7d53925cb6fe08)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                                      | status   | loc                |   iter |   total time (s) |    ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------------------------------+----------+--------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000 | RUNNING  | 192.168.1.65:16335 |     13 |          759.308 | 52000 |   148.14 |              157.365 |              135.387 |                400 |\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000:\n",
      "  agent_timesteps_total: 56000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-31_23-16-46\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 157.3647702018468\n",
      "  episode_reward_mean: 148.2206430059906\n",
      "  episode_reward_min: 137.63841498566373\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 140\n",
      "  experiment_id: cf8409b3cbbc42979d4540c7e6fbf708\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.4414062499999998e-05\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.0858371086418628\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.000903191453324589\n",
      "          policy_loss: -0.0005182826847885735\n",
      "          total_loss: 118.14193742275238\n",
      "          vf_explained_var: 1.8626451769865326e-10\n",
      "          vf_loss: 118.14245510101318\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 56000\n",
      "    num_steps_sampled: 56000\n",
      "    num_steps_trained: 56000\n",
      "  iterations_since_restore: 14\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.45679012345679\n",
      "    ram_util_percent: 24.6320987654321\n",
      "  pid: 16335\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08546907492114322\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.79877663117597\n",
      "    mean_inference_ms: 1.7755669396689218\n",
      "    mean_raw_obs_processing_ms: 7.279935013675581\n",
      "  time_since_restore: 816.1145639419556\n",
      "  time_this_iter_s: 56.806976079940796\n",
      "  time_total_s: 816.1145639419556\n",
      "  timers:\n",
      "    learn_throughput: 158.968\n",
      "    learn_time_ms: 25162.263\n",
      "    sample_throughput: 119.785\n",
      "    sample_time_ms: 33393.224\n",
      "    update_time_ms: 3.104\n",
      "  timestamp: 1667276206\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 56000\n",
      "  training_iteration: 14\n",
      "  trial_id: 168ff_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 7.7/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/9.6 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_5_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_0_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_2_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_4_400b3145f046e80a4a7d53925cb6fe08, 0.0/6.0 CPU_group_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_1_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_3_400b3145f046e80a4a7d53925cb6fe08)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                                      | status   | loc                |   iter |   total time (s) |    ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------------------------------+----------+--------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000 | RUNNING  | 192.168.1.65:16335 |     14 |          816.115 | 56000 |  148.221 |              157.365 |              137.638 |                400 |\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=16333)\u001b[0m  Retrying in 1 seconds\n",
      "Result for PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000:\n",
      "  agent_timesteps_total: 60000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-31_23-17-45\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 157.3647702018468\n",
      "  episode_reward_mean: 147.9750415462064\n",
      "  episode_reward_min: 137.63841498566373\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 150\n",
      "  experiment_id: cf8409b3cbbc42979d4540c7e6fbf708\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.2207031249999999e-05\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.0850614793598652\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009371415479571973\n",
      "          policy_loss: -0.005199645229731686\n",
      "          total_loss: 127.14038716554641\n",
      "          vf_explained_var: -2.2351742678949904e-09\n",
      "          vf_loss: 127.14558701515197\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 60000\n",
      "    num_steps_sampled: 60000\n",
      "    num_steps_trained: 60000\n",
      "  iterations_since_restore: 15\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.912048192771085\n",
      "    ram_util_percent: 24.674698795180724\n",
      "  pid: 16335\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08550129960303243\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.771985732666618\n",
      "    mean_inference_ms: 1.7765075935898829\n",
      "    mean_raw_obs_processing_ms: 7.343168635569541\n",
      "  time_since_restore: 874.2389230728149\n",
      "  time_this_iter_s: 58.124359130859375\n",
      "  time_total_s: 874.2389230728149\n",
      "  timers:\n",
      "    learn_throughput: 158.845\n",
      "    learn_time_ms: 25181.777\n",
      "    sample_throughput: 120.498\n",
      "    sample_time_ms: 33195.58\n",
      "    update_time_ms: 3.109\n",
      "  timestamp: 1667276265\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 60000\n",
      "  training_iteration: 15\n",
      "  trial_id: 168ff_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Memory usage on this node: 7.7/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/9.6 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_5_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 accelerator_type:G, 0.0/6.0 CPU_group_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_4_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_2_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_3_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_1_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_0_400b3145f046e80a4a7d53925cb6fe08)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                                      | status   | loc                |   iter |   total time (s) |    ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------------------------------+----------+--------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000 | RUNNING  | 192.168.1.65:16335 |     15 |          874.239 | 60000 |  147.975 |              157.365 |              137.638 |                400 |\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=16334)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=16331)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=16333)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=16332)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=16330)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=16334)\u001b[0m Could not connect to TraCI server at localhost:47333 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=16334)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=16331)\u001b[0m Could not connect to TraCI server at localhost:45859 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=16331)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=16333)\u001b[0m Could not connect to TraCI server at localhost:60241 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=16333)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=16332)\u001b[0m Could not connect to TraCI server at localhost:41419 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=16332)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=16330)\u001b[0m Could not connect to TraCI server at localhost:44711 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=16330)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=16334)\u001b[0m Could not connect to TraCI server at localhost:47333 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=16334)\u001b[0m  Retrying in 3 seconds\n",
      "\u001b[2m\u001b[36m(pid=16331)\u001b[0m Could not connect to TraCI server at localhost:45859 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=16331)\u001b[0m  Retrying in 3 seconds\n",
      "\u001b[2m\u001b[36m(pid=16333)\u001b[0m Could not connect to TraCI server at localhost:60241 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=16333)\u001b[0m  Retrying in 3 seconds\n",
      "\u001b[2m\u001b[36m(pid=16332)\u001b[0m Could not connect to TraCI server at localhost:41419 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=16332)\u001b[0m  Retrying in 3 seconds\n",
      "\u001b[2m\u001b[36m(pid=16330)\u001b[0m Could not connect to TraCI server at localhost:44711 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=16330)\u001b[0m  Retrying in 3 seconds\n",
      "Result for PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000:\n",
      "  agent_timesteps_total: 64000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-31_23-18-47\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 157.3647702018468\n",
      "  episode_reward_mean: 147.87892323992372\n",
      "  episode_reward_min: 137.63841498566373\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 160\n",
      "  experiment_id: cf8409b3cbbc42979d4540c7e6fbf708\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 6.1035156249999995e-06\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.089292775467038\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0006402142051506264\n",
      "          policy_loss: 0.0004388235960504971\n",
      "          total_loss: 138.77065993547438\n",
      "          vf_explained_var: -3.5390257391298974e-09\n",
      "          vf_loss: 138.77022141218185\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 64000\n",
      "    num_steps_sampled: 64000\n",
      "    num_steps_trained: 64000\n",
      "  iterations_since_restore: 16\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.215730337078654\n",
      "    ram_util_percent: 24.54269662921348\n",
      "  pid: 16335\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08553824414436846\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.752762831254604\n",
      "    mean_inference_ms: 1.7776083893577508\n",
      "    mean_raw_obs_processing_ms: 7.458139965467334\n",
      "  time_since_restore: 937.0149805545807\n",
      "  time_this_iter_s: 62.77605748176575\n",
      "  time_total_s: 937.0149805545807\n",
      "  timers:\n",
      "    learn_throughput: 159.04\n",
      "    learn_time_ms: 25150.979\n",
      "    sample_throughput: 118.327\n",
      "    sample_time_ms: 33804.486\n",
      "    update_time_ms: 3.087\n",
      "  timestamp: 1667276327\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 64000\n",
      "  training_iteration: 16\n",
      "  trial_id: 168ff_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 7.7/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/9.6 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_4_400b3145f046e80a4a7d53925cb6fe08, 0.0/6.0 CPU_group_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_0_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_1_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_5_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_2_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_3_400b3145f046e80a4a7d53925cb6fe08)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                                      | status   | loc                |   iter |   total time (s) |    ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------------------------------+----------+--------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000 | RUNNING  | 192.168.1.65:16335 |     16 |          937.015 | 64000 |  147.879 |              157.365 |              137.638 |                400 |\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000:\n",
      "  agent_timesteps_total: 68000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-31_23-19-44\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 157.3647702018468\n",
      "  episode_reward_mean: 147.5879200499928\n",
      "  episode_reward_min: 137.63841498566373\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 170\n",
      "  experiment_id: cf8409b3cbbc42979d4540c7e6fbf708\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 3.0517578124999997e-06\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.0966572977602482\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0007148476768691247\n",
      "          policy_loss: -0.0007936021487694233\n",
      "          total_loss: 145.6050185918808\n",
      "          vf_explained_var: -2.0489097085629737e-09\n",
      "          vf_loss: 145.60581291913985\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 68000\n",
      "    num_steps_sampled: 68000\n",
      "    num_steps_trained: 68000\n",
      "  iterations_since_restore: 17\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.76341463414634\n",
      "    ram_util_percent: 24.68780487804878\n",
      "  pid: 16335\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08556769534416034\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.739311159655326\n",
      "    mean_inference_ms: 1.7783026604737375\n",
      "    mean_raw_obs_processing_ms: 7.571576806505011\n",
      "  time_since_restore: 993.8554284572601\n",
      "  time_this_iter_s: 56.84044790267944\n",
      "  time_total_s: 993.8554284572601\n",
      "  timers:\n",
      "    learn_throughput: 158.956\n",
      "    learn_time_ms: 25164.12\n",
      "    sample_throughput: 118.333\n",
      "    sample_time_ms: 33802.998\n",
      "    update_time_ms: 3.103\n",
      "  timestamp: 1667276384\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 68000\n",
      "  training_iteration: 17\n",
      "  trial_id: 168ff_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Memory usage on this node: 7.7/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/9.6 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_4_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_5_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_0_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_1_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_2_400b3145f046e80a4a7d53925cb6fe08, 0.0/6.0 CPU_group_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_3_400b3145f046e80a4a7d53925cb6fe08)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                                      | status   | loc                |   iter |   total time (s) |    ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------------------------------+----------+--------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000 | RUNNING  | 192.168.1.65:16335 |     17 |          993.855 | 68000 |  147.588 |              157.365 |              137.638 |                400 |\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000:\n",
      "  agent_timesteps_total: 72000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-31_23-20-41\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 160.0107818350927\n",
      "  episode_reward_mean: 147.37602292365852\n",
      "  episode_reward_min: 137.63841498566373\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 180\n",
      "  experiment_id: cf8409b3cbbc42979d4540c7e6fbf708\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5258789062499999e-06\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.0540901109576226\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0025596018794061663\n",
      "          policy_loss: -0.000376438868988771\n",
      "          total_loss: 159.8466420173645\n",
      "          vf_explained_var: -1.4901161415892261e-09\n",
      "          vf_loss: 159.84701845645904\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 72000\n",
      "    num_steps_sampled: 72000\n",
      "    num_steps_trained: 72000\n",
      "  iterations_since_restore: 18\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.49125\n",
      "    ram_util_percent: 24.73\n",
      "  pid: 16335\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08559434149437817\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.7199812260103\n",
      "    mean_inference_ms: 1.7790790230144373\n",
      "    mean_raw_obs_processing_ms: 7.58325713878273\n",
      "  time_since_restore: 1050.5354545116425\n",
      "  time_this_iter_s: 56.680026054382324\n",
      "  time_total_s: 1050.5354545116425\n",
      "  timers:\n",
      "    learn_throughput: 159.104\n",
      "    learn_time_ms: 25140.755\n",
      "    sample_throughput: 120.798\n",
      "    sample_time_ms: 33113.116\n",
      "    update_time_ms: 3.077\n",
      "  timestamp: 1667276441\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 72000\n",
      "  training_iteration: 18\n",
      "  trial_id: 168ff_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 7.7/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/9.6 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_1_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_4_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_5_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_3_400b3145f046e80a4a7d53925cb6fe08, 0.0/6.0 CPU_group_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_2_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_0_400b3145f046e80a4a7d53925cb6fe08)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                                      | status   | loc                |   iter |   total time (s) |    ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------------------------------+----------+--------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000 | RUNNING  | 192.168.1.65:16335 |     18 |          1050.54 | 72000 |  147.376 |              160.011 |              137.638 |                400 |\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=16334)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=16331)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=16333)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=16332)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=16330)\u001b[0m  Retrying in 1 seconds\n",
      "Result for PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000:\n",
      "  agent_timesteps_total: 76000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-31_23-21-39\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 160.0107818350927\n",
      "  episode_reward_mean: 147.0958493419424\n",
      "  episode_reward_min: 137.63841498566373\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 190\n",
      "  experiment_id: cf8409b3cbbc42979d4540c7e6fbf708\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 7.629394531249999e-07\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.9838942518457771\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.005193818385885171\n",
      "          policy_loss: -0.0015430753628606908\n",
      "          total_loss: 168.73915828466414\n",
      "          vf_explained_var: -7.450580707946131e-10\n",
      "          vf_loss: 168.74070131778717\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 76000\n",
      "    num_steps_sampled: 76000\n",
      "    num_steps_trained: 76000\n",
      "  iterations_since_restore: 19\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.79397590361446\n",
      "    ram_util_percent: 24.759036144578314\n",
      "  pid: 16335\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08561285747149712\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.7045691189503\n",
      "    mean_inference_ms: 1.7796614015761023\n",
      "    mean_raw_obs_processing_ms: 7.609630934466449\n",
      "  time_since_restore: 1108.4025340080261\n",
      "  time_this_iter_s: 57.86707949638367\n",
      "  time_total_s: 1108.4025340080261\n",
      "  timers:\n",
      "    learn_throughput: 159.169\n",
      "    learn_time_ms: 25130.485\n",
      "    sample_throughput: 120.424\n",
      "    sample_time_ms: 33215.948\n",
      "    update_time_ms: 3.076\n",
      "  timestamp: 1667276499\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 76000\n",
      "  training_iteration: 19\n",
      "  trial_id: 168ff_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 7.7/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/9.6 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_0_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_5_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_2_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_4_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_3_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_1_400b3145f046e80a4a7d53925cb6fe08, 0.0/6.0 CPU_group_400b3145f046e80a4a7d53925cb6fe08)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                                      | status   | loc                |   iter |   total time (s) |    ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------------------------------+----------+--------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000 | RUNNING  | 192.168.1.65:16335 |     19 |           1108.4 | 76000 |  147.096 |              160.011 |              137.638 |                400 |\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000:\n",
      "  agent_timesteps_total: 80000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-31_23-22-36\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 160.0107818350927\n",
      "  episode_reward_mean: 146.83573240792555\n",
      "  episode_reward_min: 133.6858748045255\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 200\n",
      "  experiment_id: cf8409b3cbbc42979d4540c7e6fbf708\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 3.8146972656249997e-07\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.978075573220849\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.00779036031627528\n",
      "          policy_loss: -0.0036672849746537395\n",
      "          total_loss: 177.42987902164458\n",
      "          vf_explained_var: -0.004900506231933832\n",
      "          vf_loss: 177.4335464954376\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 80000\n",
      "    num_steps_sampled: 80000\n",
      "    num_steps_trained: 80000\n",
      "  iterations_since_restore: 20\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.245679012345676\n",
      "    ram_util_percent: 24.723456790123457\n",
      "  pid: 16335\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08562843873687424\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.691849638814936\n",
      "    mean_inference_ms: 1.780391375674232\n",
      "    mean_raw_obs_processing_ms: 7.640668591087383\n",
      "  time_since_restore: 1165.1299011707306\n",
      "  time_this_iter_s: 56.72736716270447\n",
      "  time_total_s: 1165.1299011707306\n",
      "  timers:\n",
      "    learn_throughput: 159.296\n",
      "    learn_time_ms: 25110.497\n",
      "    sample_throughput: 120.456\n",
      "    sample_time_ms: 33207.071\n",
      "    update_time_ms: 3.081\n",
      "  timestamp: 1667276556\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 80000\n",
      "  training_iteration: 20\n",
      "  trial_id: 168ff_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 7.7/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/9.6 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_5_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_3_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_2_400b3145f046e80a4a7d53925cb6fe08, 0.0/6.0 CPU_group_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_0_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_4_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_1_400b3145f046e80a4a7d53925cb6fe08)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                                      | status   | loc                |   iter |   total time (s) |    ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------------------------------+----------+--------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000 | RUNNING  | 192.168.1.65:16335 |     20 |          1165.13 | 80000 |  146.836 |              160.011 |              133.686 |                400 |\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=16334)\u001b[0m  Retrying in 1 seconds\n",
      "Result for PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000:\n",
      "  agent_timesteps_total: 84000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-31_23-23-34\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 160.0107818350927\n",
      "  episode_reward_mean: 146.69820668109438\n",
      "  episode_reward_min: 133.6858748045255\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 210\n",
      "  experiment_id: cf8409b3cbbc42979d4540c7e6fbf708\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.9073486328124998e-07\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.961035473458469\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0073147548957649775\n",
      "          policy_loss: -0.0015793137572472915\n",
      "          total_loss: 188.7195320725441\n",
      "          vf_explained_var: -2.0489097085629737e-09\n",
      "          vf_loss: 188.72111151218414\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 84000\n",
      "    num_steps_sampled: 84000\n",
      "    num_steps_trained: 84000\n",
      "  iterations_since_restore: 21\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.909523809523808\n",
      "    ram_util_percent: 24.73214285714286\n",
      "  pid: 16335\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08564092205181492\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.686121246373094\n",
      "    mean_inference_ms: 1.7800111132024108\n",
      "    mean_raw_obs_processing_ms: 7.608779939627084\n",
      "  time_since_restore: 1223.6655712127686\n",
      "  time_this_iter_s: 58.535670042037964\n",
      "  time_total_s: 1223.6655712127686\n",
      "  timers:\n",
      "    learn_throughput: 158.853\n",
      "    learn_time_ms: 25180.502\n",
      "    sample_throughput: 122.306\n",
      "    sample_time_ms: 32704.926\n",
      "    update_time_ms: 3.091\n",
      "  timestamp: 1667276614\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 84000\n",
      "  training_iteration: 21\n",
      "  trial_id: 168ff_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 7.7/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/9.6 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_5_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_0_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_4_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_3_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_2_400b3145f046e80a4a7d53925cb6fe08, 0.0/6.0 CPU_group_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_1_400b3145f046e80a4a7d53925cb6fe08)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                                      | status   | loc                |   iter |   total time (s) |    ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------------------------------+----------+--------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000 | RUNNING  | 192.168.1.65:16335 |     21 |          1223.67 | 84000 |  146.698 |              160.011 |              133.686 |                400 |\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000:\n",
      "  agent_timesteps_total: 88000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-31_23-24-31\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 160.0107818350927\n",
      "  episode_reward_mean: 146.76733257961973\n",
      "  episode_reward_min: 133.6858748045255\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 220\n",
      "  experiment_id: cf8409b3cbbc42979d4540c7e6fbf708\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 9.536743164062499e-08\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.9583513768389821\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.007610828206804874\n",
      "          policy_loss: -0.003947493791929446\n",
      "          total_loss: 202.45649296045303\n",
      "          vf_explained_var: -1.862645149230957e-09\n",
      "          vf_loss: 202.46043992042542\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 88000\n",
      "    num_steps_sampled: 88000\n",
      "    num_steps_trained: 88000\n",
      "  iterations_since_restore: 22\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.296296296296298\n",
      "    ram_util_percent: 24.733333333333334\n",
      "  pid: 16335\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08565507188118822\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.681661082190093\n",
      "    mean_inference_ms: 1.7801363020577186\n",
      "    mean_raw_obs_processing_ms: 7.574929797675964\n",
      "  time_since_restore: 1280.737621307373\n",
      "  time_this_iter_s: 57.07205009460449\n",
      "  time_total_s: 1280.737621307373\n",
      "  timers:\n",
      "    learn_throughput: 158.789\n",
      "    learn_time_ms: 25190.599\n",
      "    sample_throughput: 122.668\n",
      "    sample_time_ms: 32608.276\n",
      "    update_time_ms: 3.164\n",
      "  timestamp: 1667276671\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 88000\n",
      "  training_iteration: 22\n",
      "  trial_id: 168ff_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 7.7/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/9.6 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_2_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_4_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_0_400b3145f046e80a4a7d53925cb6fe08, 0.0/6.0 CPU_group_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_3_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_5_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_1_400b3145f046e80a4a7d53925cb6fe08)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                                      | status   | loc                |   iter |   total time (s) |    ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------------------------------+----------+--------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000 | RUNNING  | 192.168.1.65:16335 |     22 |          1280.74 | 88000 |  146.767 |              160.011 |              133.686 |                400 |\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000:\n",
      "  agent_timesteps_total: 92000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-31_23-25-29\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 160.0107818350927\n",
      "  episode_reward_mean: 147.07044647771252\n",
      "  episode_reward_min: 133.6858748045255\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 230\n",
      "  experiment_id: cf8409b3cbbc42979d4540c7e6fbf708\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 4.7683715820312496e-08\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.9506364004686475\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01074474080568281\n",
      "          policy_loss: 0.0003314305911771953\n",
      "          total_loss: 216.2496102333069\n",
      "          vf_explained_var: -1.862645149230957e-09\n",
      "          vf_loss: 216.24927897453307\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 92000\n",
      "    num_steps_sampled: 92000\n",
      "    num_steps_trained: 92000\n",
      "  iterations_since_restore: 23\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.11829268292683\n",
      "    ram_util_percent: 24.701219512195124\n",
      "  pid: 16335\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08566974001023363\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.678243911549526\n",
      "    mean_inference_ms: 1.7802708037336372\n",
      "    mean_raw_obs_processing_ms: 7.548107825951193\n",
      "  time_since_restore: 1337.7911777496338\n",
      "  time_this_iter_s: 57.05355644226074\n",
      "  time_total_s: 1337.7911777496338\n",
      "  timers:\n",
      "    learn_throughput: 158.562\n",
      "    learn_time_ms: 25226.766\n",
      "    sample_throughput: 122.672\n",
      "    sample_time_ms: 32607.232\n",
      "    update_time_ms: 3.167\n",
      "  timestamp: 1667276729\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 92000\n",
      "  training_iteration: 23\n",
      "  trial_id: 168ff_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 7.7/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/9.6 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_0_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 accelerator_type:G, 0.0/6.0 CPU_group_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_3_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_5_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_1_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_2_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_4_400b3145f046e80a4a7d53925cb6fe08)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                                      | status   | loc                |   iter |   total time (s) |    ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------------------------------+----------+--------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000 | RUNNING  | 192.168.1.65:16335 |     23 |          1337.79 | 92000 |   147.07 |              160.011 |              133.686 |                400 |\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=16334)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=16331)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=16332)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=16330)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=16332)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=16334)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=16331)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=16333)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=16330)\u001b[0m  Retrying in 1 seconds\n",
      "Result for PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000:\n",
      "  agent_timesteps_total: 96000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-31_23-26-28\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 160.0107818350927\n",
      "  episode_reward_mean: 147.19762274152316\n",
      "  episode_reward_min: 133.6858748045255\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 240\n",
      "  experiment_id: cf8409b3cbbc42979d4540c7e6fbf708\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 4.7683715820312496e-08\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.9995148777961731\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.003143051635879618\n",
      "          policy_loss: 0.000369613581278827\n",
      "          total_loss: 224.8205218076706\n",
      "          vf_explained_var: -1.0058283983482852e-08\n",
      "          vf_loss: 224.82015144824982\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 96000\n",
      "    num_steps_sampled: 96000\n",
      "    num_steps_trained: 96000\n",
      "  iterations_since_restore: 24\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.625000000000004\n",
      "    ram_util_percent: 24.695238095238096\n",
      "  pid: 16335\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08568014750990209\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.67567963619438\n",
      "    mean_inference_ms: 1.7803683928938256\n",
      "    mean_raw_obs_processing_ms: 7.536118522428264\n",
      "  time_since_restore: 1396.7551720142365\n",
      "  time_this_iter_s: 58.96399426460266\n",
      "  time_total_s: 1396.7551720142365\n",
      "  timers:\n",
      "    learn_throughput: 158.415\n",
      "    learn_time_ms: 25250.175\n",
      "    sample_throughput: 121.953\n",
      "    sample_time_ms: 32799.557\n",
      "    update_time_ms: 3.202\n",
      "  timestamp: 1667276788\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 96000\n",
      "  training_iteration: 24\n",
      "  trial_id: 168ff_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 7.7/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/9.6 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_1_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_3_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_5_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_0_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_4_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_2_400b3145f046e80a4a7d53925cb6fe08, 0.0/6.0 CPU_group_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 accelerator_type:G)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                                      | status   | loc                |   iter |   total time (s) |    ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------------------------------+----------+--------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000 | RUNNING  | 192.168.1.65:16335 |     24 |          1396.76 | 96000 |  147.198 |              160.011 |              133.686 |                400 |\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000:\n",
      "  agent_timesteps_total: 100000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-31_23-27-25\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 160.0107818350927\n",
      "  episode_reward_mean: 147.57019322223195\n",
      "  episode_reward_min: 133.6858748045255\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 250\n",
      "  experiment_id: cf8409b3cbbc42979d4540c7e6fbf708\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.3841857910156248e-08\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.0115292679518462\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.006096215041020536\n",
      "          policy_loss: 0.0001757212623488158\n",
      "          total_loss: 238.18667793273926\n",
      "          vf_explained_var: -8.195638834251895e-09\n",
      "          vf_loss: 238.1865030527115\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 100000\n",
      "    num_steps_sampled: 100000\n",
      "    num_steps_trained: 100000\n",
      "  iterations_since_restore: 25\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.26666666666667\n",
      "    ram_util_percent: 24.853086419753087\n",
      "  pid: 16335\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08569534063710633\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.667104321519755\n",
      "    mean_inference_ms: 1.780652587213126\n",
      "    mean_raw_obs_processing_ms: 7.526579242545955\n",
      "  time_since_restore: 1453.7206213474274\n",
      "  time_this_iter_s: 56.96544933319092\n",
      "  time_total_s: 1453.7206213474274\n",
      "  timers:\n",
      "    learn_throughput: 158.506\n",
      "    learn_time_ms: 25235.591\n",
      "    sample_throughput: 122.33\n",
      "    sample_time_ms: 32698.308\n",
      "    update_time_ms: 3.207\n",
      "  timestamp: 1667276845\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 100000\n",
      "  training_iteration: 25\n",
      "  trial_id: 168ff_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 7.8/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/9.6 GiB heap, 0.0/14.65 GiB objects (0.0/6.0 CPU_group_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_2_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_4_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_0_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_1_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_3_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_5_400b3145f046e80a4a7d53925cb6fe08)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                                      | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000 | RUNNING  | 192.168.1.65:16335 |     25 |          1453.72 | 100000 |   147.57 |              160.011 |              133.686 |                400 |\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000:\n",
      "  agent_timesteps_total: 104000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-31_23-28-22\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 160.0107818350927\n",
      "  episode_reward_mean: 147.55817824066912\n",
      "  episode_reward_min: 133.6858748045255\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 260\n",
      "  experiment_id: cf8409b3cbbc42979d4540c7e6fbf708\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.1920928955078124e-08\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.9992132678627967\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.006539363151227928\n",
      "          policy_loss: -0.0011353321286151185\n",
      "          total_loss: 245.1928030014038\n",
      "          vf_explained_var: 3.7252903539730653e-10\n",
      "          vf_loss: 245.19393858909606\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 104000\n",
      "    num_steps_sampled: 104000\n",
      "    num_steps_trained: 104000\n",
      "  iterations_since_restore: 26\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.26341463414634\n",
      "    ram_util_percent: 24.86951219512195\n",
      "  pid: 16335\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08570977236914633\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.659516434932367\n",
      "    mean_inference_ms: 1.7809482769484322\n",
      "    mean_raw_obs_processing_ms: 7.473629415266742\n",
      "  time_since_restore: 1510.7246272563934\n",
      "  time_this_iter_s: 57.004005908966064\n",
      "  time_total_s: 1510.7246272563934\n",
      "  timers:\n",
      "    learn_throughput: 158.306\n",
      "    learn_time_ms: 25267.467\n",
      "    sample_throughput: 124.652\n",
      "    sample_time_ms: 32089.222\n",
      "    update_time_ms: 3.227\n",
      "  timestamp: 1667276902\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 104000\n",
      "  training_iteration: 26\n",
      "  trial_id: 168ff_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 7.8/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/9.6 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_3_400b3145f046e80a4a7d53925cb6fe08, 0.0/6.0 CPU_group_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_1_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_0_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_2_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_4_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_5_400b3145f046e80a4a7d53925cb6fe08)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                                      | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000 | RUNNING  | 192.168.1.65:16335 |     26 |          1510.72 | 104000 |  147.558 |              160.011 |              133.686 |                400 |\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=16334)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=16331)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=16332)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=16330)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=16333)\u001b[0m  Retrying in 1 seconds\n",
      "Result for PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000:\n",
      "  agent_timesteps_total: 108000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-31_23-29-19\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 160.0107818350927\n",
      "  episode_reward_mean: 147.71490422741684\n",
      "  episode_reward_min: 133.6858748045255\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 270\n",
      "  experiment_id: cf8409b3cbbc42979d4540c7e6fbf708\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 5.960464477539062e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.039771180972457\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.002795116607558168\n",
      "          policy_loss: -0.0012354837555903942\n",
      "          total_loss: 255.18878228664397\n",
      "          vf_explained_var: -7.63684493421124e-09\n",
      "          vf_loss: 255.1900175333023\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 108000\n",
      "    num_steps_sampled: 108000\n",
      "    num_steps_trained: 108000\n",
      "  iterations_since_restore: 27\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.05\n",
      "    ram_util_percent: 24.84146341463415\n",
      "  pid: 16335\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08572914227909187\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.652317018332106\n",
      "    mean_inference_ms: 1.7818127489092899\n",
      "    mean_raw_obs_processing_ms: 7.431232469098159\n",
      "  time_since_restore: 1568.4952352046967\n",
      "  time_this_iter_s: 57.77060794830322\n",
      "  time_total_s: 1568.4952352046967\n",
      "  timers:\n",
      "    learn_throughput: 158.355\n",
      "    learn_time_ms: 25259.694\n",
      "    sample_throughput: 124.262\n",
      "    sample_time_ms: 32190.026\n",
      "    update_time_ms: 3.216\n",
      "  timestamp: 1667276959\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 108000\n",
      "  training_iteration: 27\n",
      "  trial_id: 168ff_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 7.7/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/9.6 GiB heap, 0.0/14.65 GiB objects (0.0/6.0 CPU_group_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_0_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_1_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_5_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_4_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_2_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_3_400b3145f046e80a4a7d53925cb6fe08)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                                      | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000 | RUNNING  | 192.168.1.65:16335 |     27 |           1568.5 | 108000 |  147.715 |              160.011 |              133.686 |                400 |\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000:\n",
      "  agent_timesteps_total: 112000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-31_23-30-17\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 159.4963924264567\n",
      "  episode_reward_mean: 147.96765809864962\n",
      "  episode_reward_min: 133.6858748045255\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 280\n",
      "  experiment_id: cf8409b3cbbc42979d4540c7e6fbf708\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.980232238769531e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.9855114387348294\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0066752525995852465\n",
      "          policy_loss: -0.003596148843644187\n",
      "          total_loss: 262.25016363859174\n",
      "          vf_explained_var: -4.470348535789981e-09\n",
      "          vf_loss: 262.253759932518\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 112000\n",
      "    num_steps_sampled: 112000\n",
      "    num_steps_trained: 112000\n",
      "  iterations_since_restore: 28\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.148780487804878\n",
      "    ram_util_percent: 24.891463414634142\n",
      "  pid: 16335\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08574574131360027\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.645979591199556\n",
      "    mean_inference_ms: 1.7824347443533106\n",
      "    mean_raw_obs_processing_ms: 7.393001184997196\n",
      "  time_since_restore: 1625.596617937088\n",
      "  time_this_iter_s: 57.10138273239136\n",
      "  time_total_s: 1625.596617937088\n",
      "  timers:\n",
      "    learn_throughput: 158.047\n",
      "    learn_time_ms: 25309.0\n",
      "    sample_throughput: 124.289\n",
      "    sample_time_ms: 32182.948\n",
      "    update_time_ms: 3.219\n",
      "  timestamp: 1667277017\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 112000\n",
      "  training_iteration: 28\n",
      "  trial_id: 168ff_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Memory usage on this node: 7.8/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/9.6 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_4_400b3145f046e80a4a7d53925cb6fe08, 0.0/6.0 CPU_group_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_2_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_3_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_1_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_0_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_5_400b3145f046e80a4a7d53925cb6fe08)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                                      | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000 | RUNNING  | 192.168.1.65:16335 |     28 |           1625.6 | 112000 |  147.968 |              159.496 |              133.686 |                400 |\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000:\n",
      "  agent_timesteps_total: 116000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-31_23-31-14\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 159.4963924264567\n",
      "  episode_reward_mean: 147.98650532543107\n",
      "  episode_reward_min: 133.6858748045255\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 290\n",
      "  experiment_id: cf8409b3cbbc42979d4540c7e6fbf708\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.4901161193847655e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.9507686706259847\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012048589021594586\n",
      "          policy_loss: -0.005070102389436215\n",
      "          total_loss: 270.28951437473296\n",
      "          vf_explained_var: -2.9802322831784522e-09\n",
      "          vf_loss: 270.2945838689804\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 116000\n",
      "    num_steps_sampled: 116000\n",
      "    num_steps_trained: 116000\n",
      "  iterations_since_restore: 29\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.337037037037035\n",
      "    ram_util_percent: 24.837037037037035\n",
      "  pid: 16335\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08576185403030838\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.64008292466516\n",
      "    mean_inference_ms: 1.7832516566339536\n",
      "    mean_raw_obs_processing_ms: 7.35258202686754\n",
      "  time_since_restore: 1682.4647378921509\n",
      "  time_this_iter_s: 56.868119955062866\n",
      "  time_total_s: 1682.4647378921509\n",
      "  timers:\n",
      "    learn_throughput: 158.06\n",
      "    learn_time_ms: 25306.774\n",
      "    sample_throughput: 124.668\n",
      "    sample_time_ms: 32085.244\n",
      "    update_time_ms: 3.208\n",
      "  timestamp: 1667277074\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 116000\n",
      "  training_iteration: 29\n",
      "  trial_id: 168ff_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 7.8/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/9.6 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_5_400b3145f046e80a4a7d53925cb6fe08, 0.0/6.0 CPU_group_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_4_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_0_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_2_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_3_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_1_400b3145f046e80a4a7d53925cb6fe08)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                                      | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000 | RUNNING  | 192.168.1.65:16335 |     29 |          1682.46 | 116000 |  147.987 |              159.496 |              133.686 |                400 |\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000:\n",
      "  agent_timesteps_total: 120000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-31_23-32-11\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 159.4963924264567\n",
      "  episode_reward_mean: 147.87633419537286\n",
      "  episode_reward_min: 138.18132544177834\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 300\n",
      "  experiment_id: cf8409b3cbbc42979d4540c7e6fbf708\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.4901161193847655e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.9183613397181034\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.003441952742495881\n",
      "          policy_loss: -0.0025189434294588865\n",
      "          total_loss: 277.1397904872894\n",
      "          vf_explained_var: 1.4901161415892261e-09\n",
      "          vf_loss: 277.1423108100891\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 120000\n",
      "    num_steps_sampled: 120000\n",
      "    num_steps_trained: 120000\n",
      "  iterations_since_restore: 30\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.182926829268293\n",
      "    ram_util_percent: 24.92926829268293\n",
      "  pid: 16335\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08577778415687944\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.63496866047631\n",
      "    mean_inference_ms: 1.7839059603001306\n",
      "    mean_raw_obs_processing_ms: 7.316062313214161\n",
      "  time_since_restore: 1739.6647396087646\n",
      "  time_this_iter_s: 57.20000171661377\n",
      "  time_total_s: 1739.6647396087646\n",
      "  timers:\n",
      "    learn_throughput: 157.798\n",
      "    learn_time_ms: 25348.889\n",
      "    sample_throughput: 124.648\n",
      "    sample_time_ms: 32090.451\n",
      "    update_time_ms: 3.225\n",
      "  timestamp: 1667277131\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 120000\n",
      "  training_iteration: 30\n",
      "  trial_id: 168ff_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 7.7/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/9.6 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_5_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_1_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_0_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_4_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_3_400b3145f046e80a4a7d53925cb6fe08, 0.0/6.0 CPU_group_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_2_400b3145f046e80a4a7d53925cb6fe08)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                                      | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000 | RUNNING  | 192.168.1.65:16335 |     30 |          1739.66 | 120000 |  147.876 |              159.496 |              138.181 |                400 |\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000:\n",
      "  agent_timesteps_total: 124000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-31_23-33-08\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 159.4963924264567\n",
      "  episode_reward_mean: 147.84841442493956\n",
      "  episode_reward_min: 138.18132544177834\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 310\n",
      "  experiment_id: cf8409b3cbbc42979d4540c7e6fbf708\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 7.450580596923828e-10\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.8913875406607985\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.00719958741120766\n",
      "          policy_loss: 0.000473427191900555\n",
      "          total_loss: 285.19493596553804\n",
      "          vf_explained_var: -4.842877210364804e-09\n",
      "          vf_loss: 285.19446177482604\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 124000\n",
      "    num_steps_sampled: 124000\n",
      "    num_steps_trained: 124000\n",
      "  iterations_since_restore: 31\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.422222222222224\n",
      "    ram_util_percent: 25.00617283950617\n",
      "  pid: 16335\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0857855969528465\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.6249036276852\n",
      "    mean_inference_ms: 1.784385571122523\n",
      "    mean_raw_obs_processing_ms: 7.281239107269339\n",
      "  time_since_restore: 1796.831208229065\n",
      "  time_this_iter_s: 57.16646862030029\n",
      "  time_total_s: 1796.831208229065\n",
      "  timers:\n",
      "    learn_throughput: 157.864\n",
      "    learn_time_ms: 25338.326\n",
      "    sample_throughput: 125.14\n",
      "    sample_time_ms: 31964.093\n",
      "    update_time_ms: 3.284\n",
      "  timestamp: 1667277188\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 124000\n",
      "  training_iteration: 31\n",
      "  trial_id: 168ff_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 7.8/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/9.6 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_5_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_2_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_3_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_1_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_0_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_4_400b3145f046e80a4a7d53925cb6fe08, 0.0/6.0 CPU_group_400b3145f046e80a4a7d53925cb6fe08)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                                      | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000 | RUNNING  | 192.168.1.65:16335 |     31 |          1796.83 | 124000 |  147.848 |              159.496 |              138.181 |                400 |\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=16334)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=16331)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=16333)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=16332)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=16330)\u001b[0m  Retrying in 1 seconds\n",
      "Result for PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000:\n",
      "  agent_timesteps_total: 128000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-31_23-34-06\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 159.4963924264567\n",
      "  episode_reward_mean: 147.65021692879267\n",
      "  episode_reward_min: 138.18132544177834\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 320\n",
      "  experiment_id: cf8409b3cbbc42979d4540c7e6fbf708\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 3.725290298461914e-10\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.8368359811604023\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.006739710600045618\n",
      "          policy_loss: -0.0003970838588429615\n",
      "          total_loss: 291.3855575561523\n",
      "          vf_explained_var: -9.685754420729609e-09\n",
      "          vf_loss: 291.3859538793564\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 128000\n",
      "    num_steps_sampled: 128000\n",
      "    num_steps_trained: 128000\n",
      "  iterations_since_restore: 32\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.903614457831324\n",
      "    ram_util_percent: 24.9855421686747\n",
      "  pid: 16335\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08578965500202815\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.615825124469264\n",
      "    mean_inference_ms: 1.7846666878158992\n",
      "    mean_raw_obs_processing_ms: 7.253198777793761\n",
      "  time_since_restore: 1854.671995639801\n",
      "  time_this_iter_s: 57.840787410736084\n",
      "  time_total_s: 1854.671995639801\n",
      "  timers:\n",
      "    learn_throughput: 158.019\n",
      "    learn_time_ms: 25313.411\n",
      "    sample_throughput: 124.743\n",
      "    sample_time_ms: 32065.872\n",
      "    update_time_ms: 3.261\n",
      "  timestamp: 1667277246\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 128000\n",
      "  training_iteration: 32\n",
      "  trial_id: 168ff_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 7.7/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/9.6 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_3_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_5_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_0_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_1_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_4_400b3145f046e80a4a7d53925cb6fe08, 0.0/6.0 CPU_group_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_2_400b3145f046e80a4a7d53925cb6fe08)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                                      | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000 | RUNNING  | 192.168.1.65:16335 |     32 |          1854.67 | 128000 |   147.65 |              159.496 |              138.181 |                400 |\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=16334)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=16331)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=16333)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=16330)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=16332)\u001b[0m  Retrying in 1 seconds\n",
      "Result for PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000:\n",
      "  agent_timesteps_total: 132000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-31_23-35-04\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 158.00180823910372\n",
      "  episode_reward_mean: 147.3499640581845\n",
      "  episode_reward_min: 138.18132544177834\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 330\n",
      "  experiment_id: cf8409b3cbbc42979d4540c7e6fbf708\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.862645149230957e-10\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.8265301117673516\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.006705838797032897\n",
      "          policy_loss: -0.00119219556072494\n",
      "          total_loss: 299.3642858982086\n",
      "          vf_explained_var: -0.004588683135807514\n",
      "          vf_loss: 299.3654800891876\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 132000\n",
      "    num_steps_sampled: 132000\n",
      "    num_steps_trained: 132000\n",
      "  iterations_since_restore: 33\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.79036144578313\n",
      "    ram_util_percent: 25.015662650602412\n",
      "  pid: 16335\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08579315796510784\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.607647937776928\n",
      "    mean_inference_ms: 1.7847848097202308\n",
      "    mean_raw_obs_processing_ms: 7.231392043086653\n",
      "  time_since_restore: 1912.705988407135\n",
      "  time_this_iter_s: 58.033992767333984\n",
      "  time_total_s: 1912.705988407135\n",
      "  timers:\n",
      "    learn_throughput: 158.042\n",
      "    learn_time_ms: 25309.724\n",
      "    sample_throughput: 124.348\n",
      "    sample_time_ms: 32167.708\n",
      "    update_time_ms: 3.215\n",
      "  timestamp: 1667277304\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 132000\n",
      "  training_iteration: 33\n",
      "  trial_id: 168ff_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Memory usage on this node: 7.8/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/9.6 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_0_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_1_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_5_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 accelerator_type:G, 0.0/6.0 CPU_group_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_2_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_4_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_3_400b3145f046e80a4a7d53925cb6fe08)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                                      | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000 | RUNNING  | 192.168.1.65:16335 |     33 |          1912.71 | 132000 |   147.35 |              158.002 |              138.181 |                400 |\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000:\n",
      "  agent_timesteps_total: 136000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-31_23-36-01\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 158.00180823910372\n",
      "  episode_reward_mean: 147.29647640476563\n",
      "  episode_reward_min: 138.18132544177834\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 340\n",
      "  experiment_id: cf8409b3cbbc42979d4540c7e6fbf708\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 9.313225746154784e-11\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.8328849451616407\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.003958982833486867\n",
      "          policy_loss: -0.0009431036611204036\n",
      "          total_loss: 302.0661789894104\n",
      "          vf_explained_var: 1.8626451769865326e-10\n",
      "          vf_loss: 302.06712198257446\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 136000\n",
      "    num_steps_sampled: 136000\n",
      "    num_steps_trained: 136000\n",
      "  iterations_since_restore: 34\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.23333333333333\n",
      "    ram_util_percent: 24.99753086419753\n",
      "  pid: 16335\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0857970863670968\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.600027675922796\n",
      "    mean_inference_ms: 1.7849316319297726\n",
      "    mean_raw_obs_processing_ms: 7.2026322666167575\n",
      "  time_since_restore: 1969.656124830246\n",
      "  time_this_iter_s: 56.95013642311096\n",
      "  time_total_s: 1969.656124830246\n",
      "  timers:\n",
      "    learn_throughput: 158.062\n",
      "    learn_time_ms: 25306.51\n",
      "    sample_throughput: 125.119\n",
      "    sample_time_ms: 31969.512\n",
      "    update_time_ms: 3.203\n",
      "  timestamp: 1667277361\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 136000\n",
      "  training_iteration: 34\n",
      "  trial_id: 168ff_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 7.8/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/9.6 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_5_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_3_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_1_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_0_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 accelerator_type:G, 0.0/6.0 CPU_group_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_2_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_4_400b3145f046e80a4a7d53925cb6fe08)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                                      | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000 | RUNNING  | 192.168.1.65:16335 |     34 |          1969.66 | 136000 |  147.296 |              158.002 |              138.181 |                400 |\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000:\n",
      "  agent_timesteps_total: 140000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-31_23-36-58\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 158.00180823910372\n",
      "  episode_reward_mean: 146.95627461351762\n",
      "  episode_reward_min: 138.18132544177834\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 350\n",
      "  experiment_id: cf8409b3cbbc42979d4540c7e6fbf708\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 4.656612873077392e-11\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.8460034979507327\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0033234321606585237\n",
      "          policy_loss: 0.0005844769744726364\n",
      "          total_loss: 308.21688451766965\n",
      "          vf_explained_var: -1.3038515822572094e-09\n",
      "          vf_loss: 308.2163008451462\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 140000\n",
      "    num_steps_sampled: 140000\n",
      "    num_steps_trained: 140000\n",
      "  iterations_since_restore: 35\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.20987654320988\n",
      "    ram_util_percent: 25.02469135802469\n",
      "  pid: 16335\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08580204545698163\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.59289061003411\n",
      "    mean_inference_ms: 1.78509847742717\n",
      "    mean_raw_obs_processing_ms: 7.176282319094733\n",
      "  time_since_restore: 2026.7489006519318\n",
      "  time_this_iter_s: 57.09277582168579\n",
      "  time_total_s: 2026.7489006519318\n",
      "  timers:\n",
      "    learn_throughput: 157.99\n",
      "    learn_time_ms: 25318.126\n",
      "    sample_throughput: 125.115\n",
      "    sample_time_ms: 31970.651\n",
      "    update_time_ms: 3.181\n",
      "  timestamp: 1667277418\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 140000\n",
      "  training_iteration: 35\n",
      "  trial_id: 168ff_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 7.8/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/9.6 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_5_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_3_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_1_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_4_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_0_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_2_400b3145f046e80a4a7d53925cb6fe08, 0.0/6.0 CPU_group_400b3145f046e80a4a7d53925cb6fe08)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                                      | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000 | RUNNING  | 192.168.1.65:16335 |     35 |          2026.75 | 140000 |  146.956 |              158.002 |              138.181 |                400 |\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000:\n",
      "  agent_timesteps_total: 144000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-31_23-37-55\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 158.00180823910372\n",
      "  episode_reward_mean: 147.03354209993148\n",
      "  episode_reward_min: 140.7976457660308\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 360\n",
      "  experiment_id: cf8409b3cbbc42979d4540c7e6fbf708\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.328306436538696e-11\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.8594845129176975\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0009452221888466364\n",
      "          policy_loss: 0.0033794934075558557\n",
      "          total_loss: 313.2522806406021\n",
      "          vf_explained_var: -0.003957572393119335\n",
      "          vf_loss: 313.248898267746\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 144000\n",
      "    num_steps_sampled: 144000\n",
      "    num_steps_trained: 144000\n",
      "  iterations_since_restore: 36\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.287654320987656\n",
      "    ram_util_percent: 25.119753086419752\n",
      "  pid: 16335\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08580381812238112\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.586250335759374\n",
      "    mean_inference_ms: 1.7852336734684493\n",
      "    mean_raw_obs_processing_ms: 7.152254605174233\n",
      "  time_since_restore: 2083.42987203598\n",
      "  time_this_iter_s: 56.68097138404846\n",
      "  time_total_s: 2083.42987203598\n",
      "  timers:\n",
      "    learn_throughput: 158.195\n",
      "    learn_time_ms: 25285.315\n",
      "    sample_throughput: 125.113\n",
      "    sample_time_ms: 31971.027\n",
      "    update_time_ms: 3.192\n",
      "  timestamp: 1667277475\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 144000\n",
      "  training_iteration: 36\n",
      "  trial_id: 168ff_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 7.8/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/9.6 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_5_400b3145f046e80a4a7d53925cb6fe08, 0.0/6.0 CPU_group_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_3_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_0_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_2_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_4_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_1_400b3145f046e80a4a7d53925cb6fe08)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                                      | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000 | RUNNING  | 192.168.1.65:16335 |     36 |          2083.43 | 144000 |  147.034 |              158.002 |              140.798 |                400 |\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000:\n",
      "  agent_timesteps_total: 148000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-31_23-38-52\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 158.00180823910372\n",
      "  episode_reward_mean: 147.03462637863353\n",
      "  episode_reward_min: 140.7976457660308\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 370\n",
      "  experiment_id: cf8409b3cbbc42979d4540c7e6fbf708\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.164153218269348e-11\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.8973076660186052\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0060172268006596415\n",
      "          policy_loss: 4.5213774137664585e-05\n",
      "          total_loss: 317.74330645799637\n",
      "          vf_explained_var: -6.891787140972383e-09\n",
      "          vf_loss: 317.7432592153549\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 148000\n",
      "    num_steps_sampled: 148000\n",
      "    num_steps_trained: 148000\n",
      "  iterations_since_restore: 37\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.114634146341466\n",
      "    ram_util_percent: 25.192682926829264\n",
      "  pid: 16335\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08580015602394145\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.58031980174487\n",
      "    mean_inference_ms: 1.7850366213498625\n",
      "    mean_raw_obs_processing_ms: 7.125478147235017\n",
      "  time_since_restore: 2140.2963020801544\n",
      "  time_this_iter_s: 56.866430044174194\n",
      "  time_total_s: 2140.2963020801544\n",
      "  timers:\n",
      "    learn_throughput: 158.127\n",
      "    learn_time_ms: 25296.118\n",
      "    sample_throughput: 125.511\n",
      "    sample_time_ms: 31869.773\n",
      "    update_time_ms: 3.2\n",
      "  timestamp: 1667277532\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 148000\n",
      "  training_iteration: 37\n",
      "  trial_id: 168ff_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 7.8/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/9.6 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_3_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_0_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_5_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_2_400b3145f046e80a4a7d53925cb6fe08, 0.0/6.0 CPU_group_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_4_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_1_400b3145f046e80a4a7d53925cb6fe08)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                                      | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000 | RUNNING  | 192.168.1.65:16335 |     37 |           2140.3 | 148000 |  147.035 |              158.002 |              140.798 |                400 |\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000:\n",
      "  agent_timesteps_total: 152000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-31_23-39-49\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 154.2010909450118\n",
      "  episode_reward_mean: 146.65796144770144\n",
      "  episode_reward_min: 137.71081104876401\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 380\n",
      "  experiment_id: cf8409b3cbbc42979d4540c7e6fbf708\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 5.82076609134674e-12\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.9018789537250995\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.004621176829338968\n",
      "          policy_loss: 0.00226930058124708\n",
      "          total_loss: 323.08637218475343\n",
      "          vf_explained_var: 5.029141991741426e-09\n",
      "          vf_loss: 323.08410210609435\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 152000\n",
      "    num_steps_sampled: 152000\n",
      "    num_steps_trained: 152000\n",
      "  iterations_since_restore: 38\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.04567901234568\n",
      "    ram_util_percent: 25.080246913580243\n",
      "  pid: 16335\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08579485764126116\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.574902374945765\n",
      "    mean_inference_ms: 1.7846714621780575\n",
      "    mean_raw_obs_processing_ms: 7.100492830381536\n",
      "  time_since_restore: 2196.9992640018463\n",
      "  time_this_iter_s: 56.702961921691895\n",
      "  time_total_s: 2196.9992640018463\n",
      "  timers:\n",
      "    learn_throughput: 158.338\n",
      "    learn_time_ms: 25262.423\n",
      "    sample_throughput: 125.535\n",
      "    sample_time_ms: 31863.598\n",
      "    update_time_ms: 3.204\n",
      "  timestamp: 1667277589\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 152000\n",
      "  training_iteration: 38\n",
      "  trial_id: 168ff_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Memory usage on this node: 7.8/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/9.6 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 accelerator_type:G, 0.0/6.0 CPU_group_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_4_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_3_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_1_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_2_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_0_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_5_400b3145f046e80a4a7d53925cb6fe08)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                                      | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000 | RUNNING  | 192.168.1.65:16335 |     38 |             2197 | 152000 |  146.658 |              154.201 |              137.711 |                400 |\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000:\n",
      "  agent_timesteps_total: 156000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-31_23-40-45\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 154.2010909450118\n",
      "  episode_reward_mean: 146.6357890300594\n",
      "  episode_reward_min: 137.71081104876401\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 390\n",
      "  experiment_id: cf8409b3cbbc42979d4540c7e6fbf708\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.91038304567337e-12\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.8484444076195359\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.005421052739578187\n",
      "          policy_loss: -0.0025126183551037683\n",
      "          total_loss: 321.50303184986115\n",
      "          vf_explained_var: 1.3038515822572094e-09\n",
      "          vf_loss: 321.50554505586626\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 156000\n",
      "    num_steps_sampled: 156000\n",
      "    num_steps_trained: 156000\n",
      "  iterations_since_restore: 39\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.25\n",
      "    ram_util_percent: 25.140000000000004\n",
      "  pid: 16335\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08578834969808362\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.569950818368902\n",
      "    mean_inference_ms: 1.7842119901906646\n",
      "    mean_raw_obs_processing_ms: 7.077181747759423\n",
      "  time_since_restore: 2253.242623090744\n",
      "  time_this_iter_s: 56.243359088897705\n",
      "  time_total_s: 2253.242623090744\n",
      "  timers:\n",
      "    learn_throughput: 158.742\n",
      "    learn_time_ms: 25198.199\n",
      "    sample_throughput: 125.529\n",
      "    sample_time_ms: 31865.272\n",
      "    update_time_ms: 3.228\n",
      "  timestamp: 1667277645\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 156000\n",
      "  training_iteration: 39\n",
      "  trial_id: 168ff_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 7.9/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/9.6 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_2_400b3145f046e80a4a7d53925cb6fe08, 0.0/6.0 CPU_group_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_0_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_3_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_4_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_5_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_1_400b3145f046e80a4a7d53925cb6fe08)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                                      | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000 | RUNNING  | 192.168.1.65:16335 |     39 |          2253.24 | 156000 |  146.636 |              154.201 |              137.711 |                400 |\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=16334)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=16333)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=16330)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=16334)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=16333)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=16332)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=16330)\u001b[0m  Retrying in 1 seconds\n",
      "Result for PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000:\n",
      "  agent_timesteps_total: 160000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-31_23-41-44\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 154.2010909450118\n",
      "  episode_reward_mean: 147.0636826876112\n",
      "  episode_reward_min: 137.71081104876401\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 400\n",
      "  experiment_id: cf8409b3cbbc42979d4540c7e6fbf708\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.455191522836685e-12\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.8207767823711037\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008725435582644565\n",
      "          policy_loss: -0.0024315111731993966\n",
      "          total_loss: 325.67475094795225\n",
      "          vf_explained_var: 7.450580707946131e-10\n",
      "          vf_loss: 325.67718291282654\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 160000\n",
      "    num_steps_sampled: 160000\n",
      "    num_steps_trained: 160000\n",
      "  iterations_since_restore: 40\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.404761904761905\n",
      "    ram_util_percent: 25.169047619047618\n",
      "  pid: 16335\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08577751793184392\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.566743203820568\n",
      "    mean_inference_ms: 1.7835839437549086\n",
      "    mean_raw_obs_processing_ms: 7.060183401047498\n",
      "  time_since_restore: 2312.092916250229\n",
      "  time_this_iter_s: 58.85029315948486\n",
      "  time_total_s: 2312.092916250229\n",
      "  timers:\n",
      "    learn_throughput: 158.936\n",
      "    learn_time_ms: 25167.326\n",
      "    sample_throughput: 124.762\n",
      "    sample_time_ms: 32061.032\n",
      "    update_time_ms: 3.218\n",
      "  timestamp: 1667277704\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 160000\n",
      "  training_iteration: 40\n",
      "  trial_id: 168ff_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 7.8/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/9.6 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_2_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_1_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_4_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_5_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_0_400b3145f046e80a4a7d53925cb6fe08, 0.0/6.0 CPU_group_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_3_400b3145f046e80a4a7d53925cb6fe08)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                                      | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000 | RUNNING  | 192.168.1.65:16335 |     40 |          2312.09 | 160000 |  147.064 |              154.201 |              137.711 |                400 |\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000:\n",
      "  agent_timesteps_total: 164000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-31_23-42-41\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 154.2010909450118\n",
      "  episode_reward_mean: 147.09875168642654\n",
      "  episode_reward_min: 137.71081104876401\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 410\n",
      "  experiment_id: cf8409b3cbbc42979d4540c7e6fbf708\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 7.275957614183425e-13\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.8190352283418179\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008542526042509036\n",
      "          policy_loss: -0.001307459335657768\n",
      "          total_loss: 328.57116758823395\n",
      "          vf_explained_var: -1.862645149230957e-09\n",
      "          vf_loss: 328.57247426509855\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 164000\n",
      "    num_steps_sampled: 164000\n",
      "    num_steps_trained: 164000\n",
      "  iterations_since_restore: 41\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.280246913580246\n",
      "    ram_util_percent: 25.12222222222222\n",
      "  pid: 16335\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08577037995553471\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.563752664956347\n",
      "    mean_inference_ms: 1.7830230473911908\n",
      "    mean_raw_obs_processing_ms: 7.0446803190348195\n",
      "  time_since_restore: 2369.0374252796173\n",
      "  time_this_iter_s: 56.94450902938843\n",
      "  time_total_s: 2369.0374252796173\n",
      "  timers:\n",
      "    learn_throughput: 159.102\n",
      "    learn_time_ms: 25141.122\n",
      "    sample_throughput: 124.747\n",
      "    sample_time_ms: 32064.926\n",
      "    update_time_ms: 3.219\n",
      "  timestamp: 1667277761\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 164000\n",
      "  training_iteration: 41\n",
      "  trial_id: 168ff_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 7.8/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/9.6 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_2_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_4_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_3_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_1_400b3145f046e80a4a7d53925cb6fe08, 0.0/6.0 CPU_group_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_0_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_5_400b3145f046e80a4a7d53925cb6fe08)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                                      | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000 | RUNNING  | 192.168.1.65:16335 |     41 |          2369.04 | 164000 |  147.099 |              154.201 |              137.711 |                400 |\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000:\n",
      "  agent_timesteps_total: 168000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-31_23-43-38\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 156.03412284173842\n",
      "  episode_reward_mean: 147.22055133785386\n",
      "  episode_reward_min: 137.71081104876401\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 420\n",
      "  experiment_id: cf8409b3cbbc42979d4540c7e6fbf708\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 3.6379788070917126e-13\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.7622886221855879\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.00474762035385085\n",
      "          policy_loss: 0.0003307083869003691\n",
      "          total_loss: 328.34955830574035\n",
      "          vf_explained_var: -2.9802322831784522e-09\n",
      "          vf_loss: 328.3492290019989\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 168000\n",
      "    num_steps_sampled: 168000\n",
      "    num_steps_trained: 168000\n",
      "  iterations_since_restore: 42\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.217073170731705\n",
      "    ram_util_percent: 25.295121951219514\n",
      "  pid: 16335\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08576522871496639\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.561298269258206\n",
      "    mean_inference_ms: 1.7825323420869497\n",
      "    mean_raw_obs_processing_ms: 7.026404002140455\n",
      "  time_since_restore: 2426.2343142032623\n",
      "  time_this_iter_s: 57.19688892364502\n",
      "  time_total_s: 2426.2343142032623\n",
      "  timers:\n",
      "    learn_throughput: 158.905\n",
      "    learn_time_ms: 25172.27\n",
      "    sample_throughput: 125.12\n",
      "    sample_time_ms: 31969.36\n",
      "    update_time_ms: 3.182\n",
      "  timestamp: 1667277818\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 168000\n",
      "  training_iteration: 42\n",
      "  trial_id: 168ff_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 7.9/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/9.6 GiB heap, 0.0/14.65 GiB objects (0.0/6.0 CPU_group_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_5_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_3_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_1_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_2_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_4_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_0_400b3145f046e80a4a7d53925cb6fe08)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                                      | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000 | RUNNING  | 192.168.1.65:16335 |     42 |          2426.23 | 168000 |  147.221 |              156.034 |              137.711 |                400 |\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000:\n",
      "  agent_timesteps_total: 172000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-31_23-44-35\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 156.03412284173842\n",
      "  episode_reward_mean: 147.05855155666634\n",
      "  episode_reward_min: 137.71081104876401\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 430\n",
      "  experiment_id: cf8409b3cbbc42979d4540c7e6fbf708\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.8189894035458563e-13\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.7165681645274162\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.003362074025959333\n",
      "          policy_loss: 0.000831470062257722\n",
      "          total_loss: 332.25310735702516\n",
      "          vf_explained_var: -5.215406329028838e-09\n",
      "          vf_loss: 332.25227665901184\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 172000\n",
      "    num_steps_sampled: 172000\n",
      "    num_steps_trained: 172000\n",
      "  iterations_since_restore: 43\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.264197530864195\n",
      "    ram_util_percent: 25.176543209876545\n",
      "  pid: 16335\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08576112343649889\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.558966869582537\n",
      "    mean_inference_ms: 1.7821441739375223\n",
      "    mean_raw_obs_processing_ms: 7.005632077903171\n",
      "  time_since_restore: 2482.8575320243835\n",
      "  time_this_iter_s: 56.623217821121216\n",
      "  time_total_s: 2482.8575320243835\n",
      "  timers:\n",
      "    learn_throughput: 159.185\n",
      "    learn_time_ms: 25127.939\n",
      "    sample_throughput: 125.5\n",
      "    sample_time_ms: 31872.568\n",
      "    update_time_ms: 3.184\n",
      "  timestamp: 1667277875\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 172000\n",
      "  training_iteration: 43\n",
      "  trial_id: 168ff_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Memory usage on this node: 7.9/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/9.6 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_2_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_0_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_3_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_5_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_4_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_1_400b3145f046e80a4a7d53925cb6fe08, 0.0/6.0 CPU_group_400b3145f046e80a4a7d53925cb6fe08)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                                      | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000 | RUNNING  | 192.168.1.65:16335 |     43 |          2482.86 | 172000 |  147.059 |              156.034 |              137.711 |                400 |\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000:\n",
      "  agent_timesteps_total: 176000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-31_23-45-32\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 156.03412284173842\n",
      "  episode_reward_mean: 146.82911304551735\n",
      "  episode_reward_min: 137.71081104876401\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 440\n",
      "  experiment_id: cf8409b3cbbc42979d4540c7e6fbf708\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 9.094947017729282e-14\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.6174780759960413\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009259196890268138\n",
      "          policy_loss: -0.0012039973022183403\n",
      "          total_loss: 331.34550840854644\n",
      "          vf_explained_var: 1.1175871339474952e-09\n",
      "          vf_loss: 331.3467129707336\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 176000\n",
      "    num_steps_sampled: 176000\n",
      "    num_steps_trained: 176000\n",
      "  iterations_since_restore: 44\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.16049382716049\n",
      "    ram_util_percent: 25.24320987654321\n",
      "  pid: 16335\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08575704085912308\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.556793648022598\n",
      "    mean_inference_ms: 1.7818271867602553\n",
      "    mean_raw_obs_processing_ms: 6.986142110799836\n",
      "  time_since_restore: 2539.8218750953674\n",
      "  time_this_iter_s: 56.96434307098389\n",
      "  time_total_s: 2539.8218750953674\n",
      "  timers:\n",
      "    learn_throughput: 159.192\n",
      "    learn_time_ms: 25126.958\n",
      "    sample_throughput: 125.491\n",
      "    sample_time_ms: 31874.912\n",
      "    update_time_ms: 3.157\n",
      "  timestamp: 1667277932\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 176000\n",
      "  training_iteration: 44\n",
      "  trial_id: 168ff_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 7.9/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/9.6 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_0_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_2_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_3_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_1_400b3145f046e80a4a7d53925cb6fe08, 0.0/6.0 CPU_group_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_5_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_4_400b3145f046e80a4a7d53925cb6fe08)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                                      | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000 | RUNNING  | 192.168.1.65:16335 |     44 |          2539.82 | 176000 |  146.829 |              156.034 |              137.711 |                400 |\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000:\n",
      "  agent_timesteps_total: 180000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-31_23-46-29\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 156.03412284173842\n",
      "  episode_reward_mean: 146.82000079767732\n",
      "  episode_reward_min: 137.71081104876401\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 450\n",
      "  experiment_id: cf8409b3cbbc42979d4540c7e6fbf708\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 4.547473508864641e-14\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.5730129471048713\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0038684771679335867\n",
      "          policy_loss: 3.767882590182126e-05\n",
      "          total_loss: 334.10820055007935\n",
      "          vf_explained_var: -4.842877210364804e-09\n",
      "          vf_loss: 334.1081636428833\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 180000\n",
      "    num_steps_sampled: 180000\n",
      "    num_steps_trained: 180000\n",
      "  iterations_since_restore: 45\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.07560975609756\n",
      "    ram_util_percent: 25.284146341463416\n",
      "  pid: 16335\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0857513881077309\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.554715433811214\n",
      "    mean_inference_ms: 1.781478524734443\n",
      "    mean_raw_obs_processing_ms: 6.967733531805684\n",
      "  time_since_restore: 2596.809734106064\n",
      "  time_this_iter_s: 56.98785901069641\n",
      "  time_total_s: 2596.809734106064\n",
      "  timers:\n",
      "    learn_throughput: 159.227\n",
      "    learn_time_ms: 25121.41\n",
      "    sample_throughput: 125.509\n",
      "    sample_time_ms: 31870.113\n",
      "    update_time_ms: 3.158\n",
      "  timestamp: 1667277989\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 180000\n",
      "  training_iteration: 45\n",
      "  trial_id: 168ff_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 7.9/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/9.6 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_0_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_3_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_1_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_2_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 accelerator_type:G, 0.0/6.0 CPU_group_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_5_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_4_400b3145f046e80a4a7d53925cb6fe08)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                                      | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000 | RUNNING  | 192.168.1.65:16335 |     45 |          2596.81 | 180000 |   146.82 |              156.034 |              137.711 |                400 |\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=16334)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=16331)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=16333)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=16332)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=16330)\u001b[0m  Retrying in 1 seconds\n",
      "Result for PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000:\n",
      "  agent_timesteps_total: 184000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-31_23-47-27\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 156.03412284173842\n",
      "  episode_reward_mean: 146.75109745970812\n",
      "  episode_reward_min: 137.71081104876401\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 460\n",
      "  experiment_id: cf8409b3cbbc42979d4540c7e6fbf708\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.2737367544323204e-14\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.49402220379561185\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.007199904140307481\n",
      "          policy_loss: -0.0007124663679860532\n",
      "          total_loss: 336.8249233007431\n",
      "          vf_explained_var: -3.911554635749326e-09\n",
      "          vf_loss: 336.82563333511354\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 184000\n",
      "    num_steps_sampled: 184000\n",
      "    num_steps_trained: 184000\n",
      "  iterations_since_restore: 46\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.68192771084337\n",
      "    ram_util_percent: 25.366265060240966\n",
      "  pid: 16335\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08574627648404184\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.55277411499448\n",
      "    mean_inference_ms: 1.7811438118683178\n",
      "    mean_raw_obs_processing_ms: 6.953119413721236\n",
      "  time_since_restore: 2654.87246465683\n",
      "  time_this_iter_s: 58.06273055076599\n",
      "  time_total_s: 2654.87246465683\n",
      "  timers:\n",
      "    learn_throughput: 159.054\n",
      "    learn_time_ms: 25148.663\n",
      "    sample_throughput: 125.073\n",
      "    sample_time_ms: 31981.208\n",
      "    update_time_ms: 3.14\n",
      "  timestamp: 1667278047\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 184000\n",
      "  training_iteration: 46\n",
      "  trial_id: 168ff_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 7.9/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/9.6 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_2_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_0_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_4_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_3_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_5_400b3145f046e80a4a7d53925cb6fe08, 0.0/6.0 CPU_group_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_1_400b3145f046e80a4a7d53925cb6fe08)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                                      | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000 | RUNNING  | 192.168.1.65:16335 |     46 |          2654.87 | 184000 |  146.751 |              156.034 |              137.711 |                400 |\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000:\n",
      "  agent_timesteps_total: 188000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-31_23-48-24\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 156.03412284173842\n",
      "  episode_reward_mean: 146.74905558594094\n",
      "  episode_reward_min: 137.71081104876401\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 470\n",
      "  experiment_id: cf8409b3cbbc42979d4540c7e6fbf708\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.1368683772161602e-14\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.4562185127288103\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008549066095718445\n",
      "          policy_loss: -0.00025063580542337147\n",
      "          total_loss: 338.3486196041107\n",
      "          vf_explained_var: -6.51925802230835e-09\n",
      "          vf_loss: 338.3488704204559\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 188000\n",
      "    num_steps_sampled: 188000\n",
      "    num_steps_trained: 188000\n",
      "  iterations_since_restore: 47\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.223456790123457\n",
      "    ram_util_percent: 25.385185185185183\n",
      "  pid: 16335\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0857414661977543\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.55098883669125\n",
      "    mean_inference_ms: 1.7808966226947929\n",
      "    mean_raw_obs_processing_ms: 6.939666894309003\n",
      "  time_since_restore: 2711.8338344097137\n",
      "  time_this_iter_s: 56.96136975288391\n",
      "  time_total_s: 2711.8338344097137\n",
      "  timers:\n",
      "    learn_throughput: 159.048\n",
      "    learn_time_ms: 25149.592\n",
      "    sample_throughput: 125.04\n",
      "    sample_time_ms: 31989.779\n",
      "    update_time_ms: 3.141\n",
      "  timestamp: 1667278104\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 188000\n",
      "  training_iteration: 47\n",
      "  trial_id: 168ff_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 7.9/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/9.6 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_1_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_5_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_0_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_2_400b3145f046e80a4a7d53925cb6fe08, 0.0/6.0 CPU_group_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_4_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_3_400b3145f046e80a4a7d53925cb6fe08)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                                      | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000 | RUNNING  | 192.168.1.65:16335 |     47 |          2711.83 | 188000 |  146.749 |              156.034 |              137.711 |                400 |\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000:\n",
      "  agent_timesteps_total: 192000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-31_23-49-21\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 156.03412284173842\n",
      "  episode_reward_mean: 146.70774538895913\n",
      "  episode_reward_min: 138.03549596560154\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 480\n",
      "  experiment_id: cf8409b3cbbc42979d4540c7e6fbf708\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 5.684341886080801e-15\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.435852191131562\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0017346778150539783\n",
      "          policy_loss: 0.00018548471707617863\n",
      "          total_loss: 336.07158868312837\n",
      "          vf_explained_var: -6.891787140972383e-09\n",
      "          vf_loss: 336.0714044570923\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 192000\n",
      "    num_steps_sampled: 192000\n",
      "    num_steps_trained: 192000\n",
      "  iterations_since_restore: 48\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.420987654320985\n",
      "    ram_util_percent: 25.43086419753086\n",
      "  pid: 16335\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08573978490600162\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.549107036154556\n",
      "    mean_inference_ms: 1.7808468119205056\n",
      "    mean_raw_obs_processing_ms: 6.92709627439049\n",
      "  time_since_restore: 2768.491955280304\n",
      "  time_this_iter_s: 56.65812087059021\n",
      "  time_total_s: 2768.491955280304\n",
      "  timers:\n",
      "    learn_throughput: 159.074\n",
      "    learn_time_ms: 25145.599\n",
      "    sample_throughput: 125.042\n",
      "    sample_time_ms: 31989.306\n",
      "    update_time_ms: 3.143\n",
      "  timestamp: 1667278161\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 192000\n",
      "  training_iteration: 48\n",
      "  trial_id: 168ff_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Memory usage on this node: 7.9/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/9.6 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_5_400b3145f046e80a4a7d53925cb6fe08, 0.0/6.0 CPU_group_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_0_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_2_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_4_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_3_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_1_400b3145f046e80a4a7d53925cb6fe08)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                                      | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000 | RUNNING  | 192.168.1.65:16335 |     48 |          2768.49 | 192000 |  146.708 |              156.034 |              138.035 |                400 |\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000:\n",
      "  agent_timesteps_total: 196000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-31_23-50-18\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 156.03412284173842\n",
      "  episode_reward_mean: 146.8142034903763\n",
      "  episode_reward_min: 138.03549596560154\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 490\n",
      "  experiment_id: cf8409b3cbbc42979d4540c7e6fbf708\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.8421709430404005e-15\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.4704097703099251\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0034551424454548675\n",
      "          policy_loss: -0.0014370301505550742\n",
      "          total_loss: 339.2959547996521\n",
      "          vf_explained_var: -2.7939677238464355e-09\n",
      "          vf_loss: 339.2973922729492\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 196000\n",
      "    num_steps_sampled: 196000\n",
      "    num_steps_trained: 196000\n",
      "  iterations_since_restore: 49\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.39753086419753\n",
      "    ram_util_percent: 25.475308641975303\n",
      "  pid: 16335\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08574004155661061\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.547233672573395\n",
      "    mean_inference_ms: 1.7809127720240412\n",
      "    mean_raw_obs_processing_ms: 6.91524796314414\n",
      "  time_since_restore: 2825.2980501651764\n",
      "  time_this_iter_s: 56.80609488487244\n",
      "  time_total_s: 2825.2980501651764\n",
      "  timers:\n",
      "    learn_throughput: 158.669\n",
      "    learn_time_ms: 25209.655\n",
      "    sample_throughput: 125.072\n",
      "    sample_time_ms: 31981.487\n",
      "    update_time_ms: 3.127\n",
      "  timestamp: 1667278218\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 196000\n",
      "  training_iteration: 49\n",
      "  trial_id: 168ff_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 7.9/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/9.6 GiB heap, 0.0/14.65 GiB objects (0.0/6.0 CPU_group_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_4_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_3_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_0_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_1_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_5_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_2_400b3145f046e80a4a7d53925cb6fe08)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                                      | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000 | RUNNING  | 192.168.1.65:16335 |     49 |           2825.3 | 196000 |  146.814 |              156.034 |              138.035 |                400 |\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000:\n",
      "  agent_timesteps_total: 200000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-31_23-51-15\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 156.03412284173842\n",
      "  episode_reward_mean: 146.45894815584242\n",
      "  episode_reward_min: 138.03549596560154\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 500\n",
      "  experiment_id: cf8409b3cbbc42979d4540c7e6fbf708\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.4210854715202003e-15\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.4673482209444046\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.004176116959602937\n",
      "          policy_loss: -0.0005779473227448762\n",
      "          total_loss: 341.6766658067703\n",
      "          vf_explained_var: -1.0430812658057675e-08\n",
      "          vf_loss: 341.6772441983223\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 200000\n",
      "    num_steps_sampled: 200000\n",
      "    num_steps_trained: 200000\n",
      "  iterations_since_restore: 50\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.25679012345679\n",
      "    ram_util_percent: 25.38641975308642\n",
      "  pid: 16335\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08574238701988351\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.544160448050278\n",
      "    mean_inference_ms: 1.7809889364011433\n",
      "    mean_raw_obs_processing_ms: 6.899253445478414\n",
      "  time_since_restore: 2882.2102267742157\n",
      "  time_this_iter_s: 56.91217660903931\n",
      "  time_total_s: 2882.2102267742157\n",
      "  timers:\n",
      "    learn_throughput: 158.655\n",
      "    learn_time_ms: 25211.948\n",
      "    sample_throughput: 125.844\n",
      "    sample_time_ms: 31785.43\n",
      "    update_time_ms: 3.148\n",
      "  timestamp: 1667278275\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 200000\n",
      "  training_iteration: 50\n",
      "  trial_id: 168ff_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 7.9/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/9.6 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_4_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_0_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_5_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_3_400b3145f046e80a4a7d53925cb6fe08, 0.0/6.0 CPU_group_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_1_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_2_400b3145f046e80a4a7d53925cb6fe08)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                                      | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000 | RUNNING  | 192.168.1.65:16335 |     50 |          2882.21 | 200000 |  146.459 |              156.034 |              138.035 |                400 |\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=16334)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=16333)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=16332)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=16330)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=16331)\u001b[0m  Retrying in 1 seconds\n",
      "Result for PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000:\n",
      "  agent_timesteps_total: 204000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-31_23-52-13\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 156.03412284173842\n",
      "  episode_reward_mean: 146.5521965164004\n",
      "  episode_reward_min: 138.03549596560154\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 510\n",
      "  experiment_id: cf8409b3cbbc42979d4540c7e6fbf708\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 7.105427357601001e-16\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.4244793113321066\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0027245549316007047\n",
      "          policy_loss: -0.0014735900549567305\n",
      "          total_loss: 341.9448129177093\n",
      "          vf_explained_var: -1.3038515822572094e-09\n",
      "          vf_loss: 341.94628365039824\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 204000\n",
      "    num_steps_sampled: 204000\n",
      "    num_steps_trained: 204000\n",
      "  iterations_since_restore: 51\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.71428571428571\n",
      "    ram_util_percent: 25.316666666666666\n",
      "  pid: 16335\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08574334970926938\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.541235461620445\n",
      "    mean_inference_ms: 1.7810573040278797\n",
      "    mean_raw_obs_processing_ms: 6.886557396812991\n",
      "  time_since_restore: 2940.510903120041\n",
      "  time_this_iter_s: 58.300676345825195\n",
      "  time_total_s: 2940.510903120041\n",
      "  timers:\n",
      "    learn_throughput: 158.492\n",
      "    learn_time_ms: 25237.875\n",
      "    sample_throughput: 125.411\n",
      "    sample_time_ms: 31895.162\n",
      "    update_time_ms: 3.065\n",
      "  timestamp: 1667278333\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 204000\n",
      "  training_iteration: 51\n",
      "  trial_id: 168ff_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 7.9/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/9.6 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 accelerator_type:G, 0.0/6.0 CPU_group_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_4_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_1_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_0_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_5_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_2_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_3_400b3145f046e80a4a7d53925cb6fe08)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                                      | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000 | RUNNING  | 192.168.1.65:16335 |     51 |          2940.51 | 204000 |  146.552 |              156.034 |              138.035 |                400 |\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000:\n",
      "  agent_timesteps_total: 208000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-31_23-53-11\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 152.9401686419436\n",
      "  episode_reward_mean: 146.3061341675063\n",
      "  episode_reward_min: 132.95990177603522\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 520\n",
      "  experiment_id: cf8409b3cbbc42979d4540c7e6fbf708\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 3.5527136788005006e-16\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.4383837698958814\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.003033001234288699\n",
      "          policy_loss: 0.00122388812887948\n",
      "          total_loss: 345.11686434745786\n",
      "          vf_explained_var: -5.029141991741426e-09\n",
      "          vf_loss: 345.115637755394\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 208000\n",
      "    num_steps_sampled: 208000\n",
      "    num_steps_trained: 208000\n",
      "  iterations_since_restore: 52\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.417073170731708\n",
      "    ram_util_percent: 25.32439024390244\n",
      "  pid: 16335\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08574235667476703\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.53808363880729\n",
      "    mean_inference_ms: 1.7810379611865585\n",
      "    mean_raw_obs_processing_ms: 6.874616033583004\n",
      "  time_since_restore: 2998.013525724411\n",
      "  time_this_iter_s: 57.50262260437012\n",
      "  time_total_s: 2998.013525724411\n",
      "  timers:\n",
      "    learn_throughput: 158.231\n",
      "    learn_time_ms: 25279.51\n",
      "    sample_throughput: 125.454\n",
      "    sample_time_ms: 31884.163\n",
      "    update_time_ms: 3.1\n",
      "  timestamp: 1667278391\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 208000\n",
      "  training_iteration: 52\n",
      "  trial_id: 168ff_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 7.9/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/9.6 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_5_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_2_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_0_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_3_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_1_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_4_400b3145f046e80a4a7d53925cb6fe08, 0.0/6.0 CPU_group_400b3145f046e80a4a7d53925cb6fe08)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                                      | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000 | RUNNING  | 192.168.1.65:16335 |     52 |          2998.01 | 208000 |  146.306 |               152.94 |               132.96 |                400 |\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000:\n",
      "  agent_timesteps_total: 212000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-31_23-54-08\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 152.9401686419436\n",
      "  episode_reward_mean: 146.16907209215168\n",
      "  episode_reward_min: 132.95990177603522\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 530\n",
      "  experiment_id: cf8409b3cbbc42979d4540c7e6fbf708\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.7763568394002503e-16\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.41776935337111354\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0029056053681415775\n",
      "          policy_loss: 0.001669499666604679\n",
      "          total_loss: 346.26478917598723\n",
      "          vf_explained_var: 5.587935447692871e-09\n",
      "          vf_loss: 346.26312098503115\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 212000\n",
      "    num_steps_sampled: 212000\n",
      "    num_steps_trained: 212000\n",
      "  iterations_since_restore: 53\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.44320987654321\n",
      "    ram_util_percent: 25.466666666666665\n",
      "  pid: 16335\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08574056175602494\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.53507804161276\n",
      "    mean_inference_ms: 1.7810220064006272\n",
      "    mean_raw_obs_processing_ms: 6.8632965585652\n",
      "  time_since_restore: 3054.930109977722\n",
      "  time_this_iter_s: 56.91658425331116\n",
      "  time_total_s: 3054.930109977722\n",
      "  timers:\n",
      "    learn_throughput: 158.035\n",
      "    learn_time_ms: 25310.794\n",
      "    sample_throughput: 125.462\n",
      "    sample_time_ms: 31882.141\n",
      "    update_time_ms: 3.158\n",
      "  timestamp: 1667278448\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 212000\n",
      "  training_iteration: 53\n",
      "  trial_id: 168ff_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Memory usage on this node: 7.9/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/9.6 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_4_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_0_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_2_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_5_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_3_400b3145f046e80a4a7d53925cb6fe08, 0.0/6.0 CPU_group_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_1_400b3145f046e80a4a7d53925cb6fe08)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                                      | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000 | RUNNING  | 192.168.1.65:16335 |     53 |          3054.93 | 212000 |  146.169 |               152.94 |               132.96 |                400 |\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000:\n",
      "  agent_timesteps_total: 216000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-31_23-55-04\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 152.9401686419436\n",
      "  episode_reward_mean: 146.16041584457062\n",
      "  episode_reward_min: 132.95990177603522\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 540\n",
      "  experiment_id: cf8409b3cbbc42979d4540c7e6fbf708\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 8.881784197001252e-17\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.44708673376590014\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.006642173598808307\n",
      "          policy_loss: -0.002095605115755461\n",
      "          total_loss: 346.2336388587952\n",
      "          vf_explained_var: -3.5390257391298974e-09\n",
      "          vf_loss: 346.23573430776594\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 216000\n",
      "    num_steps_sampled: 216000\n",
      "    num_steps_trained: 216000\n",
      "  iterations_since_restore: 54\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.52716049382716\n",
      "    ram_util_percent: 25.467901234567904\n",
      "  pid: 16335\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08573940601425369\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.532073917701336\n",
      "    mean_inference_ms: 1.7810773591059368\n",
      "    mean_raw_obs_processing_ms: 6.8524804510476205\n",
      "  time_since_restore: 3111.58446764946\n",
      "  time_this_iter_s: 56.65435767173767\n",
      "  time_total_s: 3111.58446764946\n",
      "  timers:\n",
      "    learn_throughput: 158.234\n",
      "    learn_time_ms: 25279.015\n",
      "    sample_throughput: 125.458\n",
      "    sample_time_ms: 31883.058\n",
      "    update_time_ms: 3.165\n",
      "  timestamp: 1667278504\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 216000\n",
      "  training_iteration: 54\n",
      "  trial_id: 168ff_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 7.9/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/9.6 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_2_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_5_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_4_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_1_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_0_400b3145f046e80a4a7d53925cb6fe08, 0.0/6.0 CPU_group_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_3_400b3145f046e80a4a7d53925cb6fe08)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                                      | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000 | RUNNING  | 192.168.1.65:16335 |     54 |          3111.58 | 216000 |   146.16 |               152.94 |               132.96 |                400 |\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=16334)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=16331)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=16333)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=16332)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=16330)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=16333)\u001b[0m Could not connect to TraCI server at localhost:60241 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=16333)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=16334)\u001b[0m Could not connect to TraCI server at localhost:47333 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=16334)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=16331)\u001b[0m Could not connect to TraCI server at localhost:45859 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=16331)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=16332)\u001b[0m Could not connect to TraCI server at localhost:41419 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=16332)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=16330)\u001b[0m Could not connect to TraCI server at localhost:44711 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=16330)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=16333)\u001b[0m Could not connect to TraCI server at localhost:60241 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=16333)\u001b[0m  Retrying in 3 seconds\n",
      "\u001b[2m\u001b[36m(pid=16330)\u001b[0m Could not connect to TraCI server at localhost:44711 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=16330)\u001b[0m  Retrying in 3 seconds\n",
      "\u001b[2m\u001b[36m(pid=16334)\u001b[0m Could not connect to TraCI server at localhost:47333 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=16334)\u001b[0m  Retrying in 3 seconds\n",
      "\u001b[2m\u001b[36m(pid=16331)\u001b[0m Could not connect to TraCI server at localhost:45859 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=16331)\u001b[0m  Retrying in 3 seconds\n",
      "\u001b[2m\u001b[36m(pid=16332)\u001b[0m Could not connect to TraCI server at localhost:41419 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=16332)\u001b[0m  Retrying in 3 seconds\n",
      "Result for PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000:\n",
      "  agent_timesteps_total: 220000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-31_23-56-08\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 152.9401686419436\n",
      "  episode_reward_mean: 145.95758410224644\n",
      "  episode_reward_min: 132.95990177603522\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 550\n",
      "  experiment_id: cf8409b3cbbc42979d4540c7e6fbf708\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 4.440892098500626e-17\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.4514486910775304\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.004622844120402198\n",
      "          policy_loss: -0.0011510672469739803\n",
      "          total_loss: 340.71372761726377\n",
      "          vf_explained_var: -1.862645149230957e-09\n",
      "          vf_loss: 340.7148808717728\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 220000\n",
      "    num_steps_sampled: 220000\n",
      "    num_steps_trained: 220000\n",
      "  iterations_since_restore: 55\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 18.80777777777778\n",
      "    ram_util_percent: 25.39888888888889\n",
      "  pid: 16335\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0857368779367622\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.529189117495015\n",
      "    mean_inference_ms: 1.7811131277489827\n",
      "    mean_raw_obs_processing_ms: 6.855965894030371\n",
      "  time_since_restore: 3174.7059433460236\n",
      "  time_this_iter_s: 63.12147569656372\n",
      "  time_total_s: 3174.7059433460236\n",
      "  timers:\n",
      "    learn_throughput: 158.206\n",
      "    learn_time_ms: 25283.502\n",
      "    sample_throughput: 123.108\n",
      "    sample_time_ms: 32491.908\n",
      "    update_time_ms: 3.183\n",
      "  timestamp: 1667278568\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 220000\n",
      "  training_iteration: 55\n",
      "  trial_id: 168ff_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Memory usage on this node: 8.0/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/9.6 GiB heap, 0.0/14.65 GiB objects (0.0/6.0 CPU_group_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_4_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_2_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_5_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_3_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_0_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_1_400b3145f046e80a4a7d53925cb6fe08)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                                      | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000 | RUNNING  | 192.168.1.65:16335 |     55 |          3174.71 | 220000 |  145.958 |               152.94 |               132.96 |                400 |\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000:\n",
      "  agent_timesteps_total: 224000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-31_23-57-05\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 152.9401686419436\n",
      "  episode_reward_mean: 146.0668863040736\n",
      "  episode_reward_min: 132.95990177603522\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 560\n",
      "  experiment_id: cf8409b3cbbc42979d4540c7e6fbf708\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.220446049250313e-17\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.49791951235383747\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0036748049647599298\n",
      "          policy_loss: 0.0007120259193470702\n",
      "          total_loss: 344.2656139373779\n",
      "          vf_explained_var: -3.3527611797978807e-09\n",
      "          vf_loss: 344.26490261554716\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 224000\n",
      "    num_steps_sampled: 224000\n",
      "    num_steps_trained: 224000\n",
      "  iterations_since_restore: 56\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.15853658536585\n",
      "    ram_util_percent: 25.357317073170734\n",
      "  pid: 16335\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08573505404923569\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.52642311881511\n",
      "    mean_inference_ms: 1.7811963700343818\n",
      "    mean_raw_obs_processing_ms: 6.856942034914243\n",
      "  time_since_restore: 3231.906811952591\n",
      "  time_this_iter_s: 57.20086860656738\n",
      "  time_total_s: 3231.906811952591\n",
      "  timers:\n",
      "    learn_throughput: 158.099\n",
      "    learn_time_ms: 25300.577\n",
      "    sample_throughput: 123.5\n",
      "    sample_time_ms: 32388.565\n",
      "    update_time_ms: 3.215\n",
      "  timestamp: 1667278625\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 224000\n",
      "  training_iteration: 56\n",
      "  trial_id: 168ff_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 7.9/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/9.6 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_0_400b3145f046e80a4a7d53925cb6fe08, 0.0/6.0 CPU_group_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_3_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_4_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_2_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_5_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_1_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 accelerator_type:G)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                                      | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000 | RUNNING  | 192.168.1.65:16335 |     56 |          3231.91 | 224000 |  146.067 |               152.94 |               132.96 |                400 |\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000:\n",
      "  agent_timesteps_total: 228000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-31_23-58-02\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 152.9401686419436\n",
      "  episode_reward_mean: 146.08267538620422\n",
      "  episode_reward_min: 132.95990177603522\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 570\n",
      "  experiment_id: cf8409b3cbbc42979d4540c7e6fbf708\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.1102230246251564e-17\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.5573098849505186\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.00342770212398594\n",
      "          policy_loss: -0.003023384194239043\n",
      "          total_loss: 343.42268974781035\n",
      "          vf_explained_var: -5.587935447692871e-09\n",
      "          vf_loss: 343.4257166862488\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 228000\n",
      "    num_steps_sampled: 228000\n",
      "    num_steps_trained: 228000\n",
      "  iterations_since_restore: 57\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.306097560975612\n",
      "    ram_util_percent: 25.432926829268286\n",
      "  pid: 16335\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08573276411020228\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.523677634395625\n",
      "    mean_inference_ms: 1.781323918989746\n",
      "    mean_raw_obs_processing_ms: 6.858105343888877\n",
      "  time_since_restore: 3289.4335310459137\n",
      "  time_this_iter_s: 57.526719093322754\n",
      "  time_total_s: 3289.4335310459137\n",
      "  timers:\n",
      "    learn_throughput: 157.731\n",
      "    learn_time_ms: 25359.643\n",
      "    sample_throughput: 123.51\n",
      "    sample_time_ms: 32385.969\n",
      "    update_time_ms: 3.215\n",
      "  timestamp: 1667278682\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 228000\n",
      "  training_iteration: 57\n",
      "  trial_id: 168ff_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 8.0/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/9.6 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_4_400b3145f046e80a4a7d53925cb6fe08, 0.0/6.0 CPU_group_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_5_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_1_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_0_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_2_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_3_400b3145f046e80a4a7d53925cb6fe08)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                                      | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000 | RUNNING  | 192.168.1.65:16335 |     57 |          3289.43 | 228000 |  146.083 |               152.94 |               132.96 |                400 |\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000:\n",
      "  agent_timesteps_total: 232000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-31_23-59-00\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 153.55666742547234\n",
      "  episode_reward_mean: 146.30364518858494\n",
      "  episode_reward_min: 132.95990177603522\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 580\n",
      "  experiment_id: cf8409b3cbbc42979d4540c7e6fbf708\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 5.551115123125782e-18\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.5891756845638156\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0020087107913454444\n",
      "          policy_loss: -0.0010190889035584405\n",
      "          total_loss: 343.5575573921204\n",
      "          vf_explained_var: 7.450580707946131e-10\n",
      "          vf_loss: 343.55857570171355\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 232000\n",
      "    num_steps_sampled: 232000\n",
      "    num_steps_trained: 232000\n",
      "  iterations_since_restore: 58\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.05609756097561\n",
      "    ram_util_percent: 25.551219512195125\n",
      "  pid: 16335\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0857287083931177\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.52109762486913\n",
      "    mean_inference_ms: 1.7813908575534696\n",
      "    mean_raw_obs_processing_ms: 6.859706964341816\n",
      "  time_since_restore: 3346.6376719474792\n",
      "  time_this_iter_s: 57.20414090156555\n",
      "  time_total_s: 3346.6376719474792\n",
      "  timers:\n",
      "    learn_throughput: 157.433\n",
      "    learn_time_ms: 25407.67\n",
      "    sample_throughput: 123.486\n",
      "    sample_time_ms: 32392.436\n",
      "    update_time_ms: 3.28\n",
      "  timestamp: 1667278740\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 232000\n",
      "  training_iteration: 58\n",
      "  trial_id: 168ff_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 8.0/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/9.6 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_0_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_3_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_2_400b3145f046e80a4a7d53925cb6fe08, 0.0/6.0 CPU_group_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_5_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_4_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_1_400b3145f046e80a4a7d53925cb6fe08)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                                      | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000 | RUNNING  | 192.168.1.65:16335 |     58 |          3346.64 | 232000 |  146.304 |              153.557 |               132.96 |                400 |\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000:\n",
      "  agent_timesteps_total: 236000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-31_23-59-57\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 153.83197413275414\n",
      "  episode_reward_mean: 146.349790104411\n",
      "  episode_reward_min: 132.95990177603522\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 590\n",
      "  experiment_id: cf8409b3cbbc42979d4540c7e6fbf708\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.775557561562891e-18\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.5638196147978306\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.002804501514543766\n",
      "          policy_loss: 0.005389844880846794\n",
      "          total_loss: 353.91866502761843\n",
      "          vf_explained_var: -0.0043880268931388855\n",
      "          vf_loss: 353.91327624320985\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 236000\n",
      "    num_steps_sampled: 236000\n",
      "    num_steps_trained: 236000\n",
      "  iterations_since_restore: 59\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.16219512195122\n",
      "    ram_util_percent: 25.49756097560975\n",
      "  pid: 16335\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0857235054571243\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.518591250246057\n",
      "    mean_inference_ms: 1.7813909677010797\n",
      "    mean_raw_obs_processing_ms: 6.861674041442207\n",
      "  time_since_restore: 3404.3058490753174\n",
      "  time_this_iter_s: 57.668177127838135\n",
      "  time_total_s: 3404.3058490753174\n",
      "  timers:\n",
      "    learn_throughput: 156.931\n",
      "    learn_time_ms: 25488.905\n",
      "    sample_throughput: 123.466\n",
      "    sample_time_ms: 32397.465\n",
      "    update_time_ms: 3.32\n",
      "  timestamp: 1667278797\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 236000\n",
      "  training_iteration: 59\n",
      "  trial_id: 168ff_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 8.0/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/9.6 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_2_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_1_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_4_400b3145f046e80a4a7d53925cb6fe08, 0.0/6.0 CPU_group_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_3_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_5_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_0_400b3145f046e80a4a7d53925cb6fe08)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                                      | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000 | RUNNING  | 192.168.1.65:16335 |     59 |          3404.31 | 236000 |   146.35 |              153.832 |               132.96 |                400 |\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000:\n",
      "  agent_timesteps_total: 240000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-11-01_00-00-55\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 153.83197413275414\n",
      "  episode_reward_mean: 146.4289115808188\n",
      "  episode_reward_min: 132.95990177603522\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 600\n",
      "  experiment_id: cf8409b3cbbc42979d4540c7e6fbf708\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.3877787807814456e-18\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.539361172914505\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.007165670470917017\n",
      "          policy_loss: -0.00189889859611867\n",
      "          total_loss: 347.12386021614077\n",
      "          vf_explained_var: -1.1175871339474952e-09\n",
      "          vf_loss: 347.1257591366768\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 240000\n",
      "    num_steps_sampled: 240000\n",
      "    num_steps_trained: 240000\n",
      "  iterations_since_restore: 60\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.074390243902442\n",
      "    ram_util_percent: 25.491463414634143\n",
      "  pid: 16335\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08571748478123528\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.51611305685934\n",
      "    mean_inference_ms: 1.7814289545844884\n",
      "    mean_raw_obs_processing_ms: 6.863893893759801\n",
      "  time_since_restore: 3461.330859899521\n",
      "  time_this_iter_s: 57.02501082420349\n",
      "  time_total_s: 3461.330859899521\n",
      "  timers:\n",
      "    learn_throughput: 156.84\n",
      "    learn_time_ms: 25503.67\n",
      "    sample_throughput: 123.48\n",
      "    sample_time_ms: 32394.034\n",
      "    update_time_ms: 3.3\n",
      "  timestamp: 1667278855\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 240000\n",
      "  training_iteration: 60\n",
      "  trial_id: 168ff_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Memory usage on this node: 8.0/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/9.6 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_1_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 accelerator_type:G, 0.0/6.0 CPU_group_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_2_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_0_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_5_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_3_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_4_400b3145f046e80a4a7d53925cb6fe08)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                                      | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000 | RUNNING  | 192.168.1.65:16335 |     60 |          3461.33 | 240000 |  146.429 |              153.832 |               132.96 |                400 |\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000:\n",
      "  agent_timesteps_total: 244000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-11-01_00-01-51\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 153.83197413275414\n",
      "  episode_reward_mean: 146.5223467807693\n",
      "  episode_reward_min: 132.95990177603522\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 610\n",
      "  experiment_id: cf8409b3cbbc42979d4540c7e6fbf708\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 6.938893903907228e-19\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.575634166225791\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0038245350977035956\n",
      "          policy_loss: -0.00011807664996013045\n",
      "          total_loss: 349.47233419418336\n",
      "          vf_explained_var: 0.0\n",
      "          vf_loss: 349.4724524140358\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 244000\n",
      "    num_steps_sampled: 244000\n",
      "    num_steps_trained: 244000\n",
      "  iterations_since_restore: 61\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.27777777777778\n",
      "    ram_util_percent: 25.535802469135803\n",
      "  pid: 16335\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08571249452989804\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.513706382368596\n",
      "    mean_inference_ms: 1.7815027316920578\n",
      "    mean_raw_obs_processing_ms: 6.863729815073218\n",
      "  time_since_restore: 3518.0029385089874\n",
      "  time_this_iter_s: 56.67207860946655\n",
      "  time_total_s: 3518.0029385089874\n",
      "  timers:\n",
      "    learn_throughput: 157.167\n",
      "    learn_time_ms: 25450.587\n",
      "    sample_throughput: 123.899\n",
      "    sample_time_ms: 32284.231\n",
      "    update_time_ms: 3.354\n",
      "  timestamp: 1667278911\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 244000\n",
      "  training_iteration: 61\n",
      "  trial_id: 168ff_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 8.0/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/9.6 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_0_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_5_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_4_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_3_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_1_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_2_400b3145f046e80a4a7d53925cb6fe08, 0.0/6.0 CPU_group_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 accelerator_type:G)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                                      | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000 | RUNNING  | 192.168.1.65:16335 |     61 |             3518 | 244000 |  146.522 |              153.832 |               132.96 |                400 |\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000:\n",
      "  agent_timesteps_total: 248000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-11-01_00-02-48\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 153.83197413275414\n",
      "  episode_reward_mean: 146.70440002909902\n",
      "  episode_reward_min: 133.4489002748291\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 620\n",
      "  experiment_id: cf8409b3cbbc42979d4540c7e6fbf708\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 3.469446951953614e-19\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.5596228839829565\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0094355826867883\n",
      "          policy_loss: 0.0005082201285404153\n",
      "          total_loss: 350.3451525688171\n",
      "          vf_explained_var: 5.029141991741426e-09\n",
      "          vf_loss: 350.3446434020996\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 248000\n",
      "    num_steps_sampled: 248000\n",
      "    num_steps_trained: 248000\n",
      "  iterations_since_restore: 62\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.25925925925926\n",
      "    ram_util_percent: 25.567901234567902\n",
      "  pid: 16335\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08570704835593285\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.511411353899593\n",
      "    mean_inference_ms: 1.7815818534316121\n",
      "    mean_raw_obs_processing_ms: 6.8637383609583775\n",
      "  time_since_restore: 3575.044327735901\n",
      "  time_this_iter_s: 57.04138922691345\n",
      "  time_total_s: 3575.044327735901\n",
      "  timers:\n",
      "    learn_throughput: 157.483\n",
      "    learn_time_ms: 25399.594\n",
      "    sample_throughput: 123.881\n",
      "    sample_time_ms: 32289.126\n",
      "    update_time_ms: 3.317\n",
      "  timestamp: 1667278968\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 248000\n",
      "  training_iteration: 62\n",
      "  trial_id: 168ff_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 8.0/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/9.6 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_4_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_0_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_3_400b3145f046e80a4a7d53925cb6fe08, 0.0/6.0 CPU_group_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_1_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_2_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_5_400b3145f046e80a4a7d53925cb6fe08)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                                      | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000 | RUNNING  | 192.168.1.65:16335 |     62 |          3575.04 | 248000 |  146.704 |              153.832 |              133.449 |                400 |\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000:\n",
      "  agent_timesteps_total: 252000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-11-01_00-03-46\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 153.83197413275414\n",
      "  episode_reward_mean: 147.06385488218092\n",
      "  episode_reward_min: 135.89401530949203\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 630\n",
      "  experiment_id: cf8409b3cbbc42979d4540c7e6fbf708\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.734723475976807e-19\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.5207881281152368\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.00467519568970546\n",
      "          policy_loss: 0.0003851655943435617\n",
      "          total_loss: 347.48530397415163\n",
      "          vf_explained_var: -6.51925802230835e-09\n",
      "          vf_loss: 347.4849191427231\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 252000\n",
      "    num_steps_sampled: 252000\n",
      "    num_steps_trained: 252000\n",
      "  iterations_since_restore: 63\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.16463414634146\n",
      "    ram_util_percent: 25.637804878048783\n",
      "  pid: 16335\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08570293133813968\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.509439450530227\n",
      "    mean_inference_ms: 1.7816045854911673\n",
      "    mean_raw_obs_processing_ms: 6.8638273983945295\n",
      "  time_since_restore: 3632.2203850746155\n",
      "  time_this_iter_s: 57.1760573387146\n",
      "  time_total_s: 3632.2203850746155\n",
      "  timers:\n",
      "    learn_throughput: 157.35\n",
      "    learn_time_ms: 25421.034\n",
      "    sample_throughput: 123.863\n",
      "    sample_time_ms: 32293.789\n",
      "    update_time_ms: 3.22\n",
      "  timestamp: 1667279026\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 252000\n",
      "  training_iteration: 63\n",
      "  trial_id: 168ff_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 8.0/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/9.6 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_5_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_3_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_1_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_4_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_0_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_2_400b3145f046e80a4a7d53925cb6fe08, 0.0/6.0 CPU_group_400b3145f046e80a4a7d53925cb6fe08)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                                      | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000 | RUNNING  | 192.168.1.65:16335 |     63 |          3632.22 | 252000 |  147.064 |              153.832 |              135.894 |                400 |\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000:\n",
      "  agent_timesteps_total: 256000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-11-01_00-04-42\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 154.31329399249174\n",
      "  episode_reward_mean: 147.49183987815184\n",
      "  episode_reward_min: 135.89401530949203\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 640\n",
      "  experiment_id: cf8409b3cbbc42979d4540c7e6fbf708\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 8.673617379884035e-20\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.48928202521055936\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.002125872278863049\n",
      "          policy_loss: -0.0005921304866205901\n",
      "          total_loss: 349.57721182107923\n",
      "          vf_explained_var: -3.725290298461914e-09\n",
      "          vf_loss: 349.57780467271806\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 256000\n",
      "    num_steps_sampled: 256000\n",
      "    num_steps_trained: 256000\n",
      "  iterations_since_restore: 64\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.335\n",
      "    ram_util_percent: 25.660000000000004\n",
      "  pid: 16335\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08569818877306876\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.50763933836185\n",
      "    mean_inference_ms: 1.7815571309597913\n",
      "    mean_raw_obs_processing_ms: 6.864211586130437\n",
      "  time_since_restore: 3688.482050895691\n",
      "  time_this_iter_s: 56.26166582107544\n",
      "  time_total_s: 3688.482050895691\n",
      "  timers:\n",
      "    learn_throughput: 157.618\n",
      "    learn_time_ms: 25377.834\n",
      "    sample_throughput: 123.848\n",
      "    sample_time_ms: 32297.636\n",
      "    update_time_ms: 3.275\n",
      "  timestamp: 1667279082\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 256000\n",
      "  training_iteration: 64\n",
      "  trial_id: 168ff_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 8.0/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/9.6 GiB heap, 0.0/14.65 GiB objects (0.0/6.0 CPU_group_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_3_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_2_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_5_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_1_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_4_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_0_400b3145f046e80a4a7d53925cb6fe08)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                                      | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000 | RUNNING  | 192.168.1.65:16335 |     64 |          3688.48 | 256000 |  147.492 |              154.313 |              135.894 |                400 |\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000:\n",
      "  agent_timesteps_total: 260000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-11-01_00-05-40\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 154.31329399249174\n",
      "  episode_reward_mean: 147.7947470645252\n",
      "  episode_reward_min: 140.65665299634838\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 650\n",
      "  experiment_id: cf8409b3cbbc42979d4540c7e6fbf708\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 4.3368086899420174e-20\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.44317802870646117\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.00369059451305135\n",
      "          policy_loss: 0.0011763696573325432\n",
      "          total_loss: 354.6303055524826\n",
      "          vf_explained_var: 1.1175871339474952e-09\n",
      "          vf_loss: 354.62912774086\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 260000\n",
      "    num_steps_sampled: 260000\n",
      "    num_steps_trained: 260000\n",
      "  iterations_since_restore: 65\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.00121951219512\n",
      "    ram_util_percent: 25.82439024390244\n",
      "  pid: 16335\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0856947626836319\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.50596828603235\n",
      "    mean_inference_ms: 1.7814647013125127\n",
      "    mean_raw_obs_processing_ms: 6.85109295855707\n",
      "  time_since_restore: 3745.9476668834686\n",
      "  time_this_iter_s: 57.46561598777771\n",
      "  time_total_s: 3745.9476668834686\n",
      "  timers:\n",
      "    learn_throughput: 157.357\n",
      "    learn_time_ms: 25419.974\n",
      "    sample_throughput: 126.223\n",
      "    sample_time_ms: 31689.904\n",
      "    update_time_ms: 3.276\n",
      "  timestamp: 1667279140\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 260000\n",
      "  training_iteration: 65\n",
      "  trial_id: 168ff_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Memory usage on this node: 8.0/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/9.6 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_5_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_1_400b3145f046e80a4a7d53925cb6fe08, 0.0/6.0 CPU_group_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_3_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_4_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_0_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_2_400b3145f046e80a4a7d53925cb6fe08)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                                      | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000 | RUNNING  | 192.168.1.65:16335 |     65 |          3745.95 | 260000 |  147.795 |              154.313 |              140.657 |                400 |\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=16332)\u001b[0m  Retrying in 1 seconds\n",
      "Result for PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000:\n",
      "  agent_timesteps_total: 264000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-11-01_00-06-37\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 154.31329399249174\n",
      "  episode_reward_mean: 147.66878136084568\n",
      "  episode_reward_min: 140.65665299634838\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 660\n",
      "  experiment_id: cf8409b3cbbc42979d4540c7e6fbf708\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.1684043449710087e-20\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.41883985986933114\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.00932979050976428\n",
      "          policy_loss: -0.0023867528099799527\n",
      "          total_loss: 346.92089104652405\n",
      "          vf_explained_var: -3.725290298461914e-09\n",
      "          vf_loss: 346.9232766389847\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 264000\n",
      "    num_steps_sampled: 264000\n",
      "    num_steps_trained: 264000\n",
      "  iterations_since_restore: 66\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.053012048192773\n",
      "    ram_util_percent: 25.77349397590362\n",
      "  pid: 16335\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08569161189098408\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.505881094448043\n",
      "    mean_inference_ms: 1.7813112319587032\n",
      "    mean_raw_obs_processing_ms: 6.838772247045647\n",
      "  time_since_restore: 3803.6621119976044\n",
      "  time_this_iter_s: 57.71444511413574\n",
      "  time_total_s: 3803.6621119976044\n",
      "  timers:\n",
      "    learn_throughput: 157.641\n",
      "    learn_time_ms: 25374.161\n",
      "    sample_throughput: 125.837\n",
      "    sample_time_ms: 31787.102\n",
      "    update_time_ms: 3.244\n",
      "  timestamp: 1667279197\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 264000\n",
      "  training_iteration: 66\n",
      "  trial_id: 168ff_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 8.0/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/9.6 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_5_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_2_400b3145f046e80a4a7d53925cb6fe08, 0.0/6.0 CPU_group_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_3_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_0_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_4_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_1_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 accelerator_type:G)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                                      | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000 | RUNNING  | 192.168.1.65:16335 |     66 |          3803.66 | 264000 |  147.669 |              154.313 |              140.657 |                400 |\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000:\n",
      "  agent_timesteps_total: 268000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-11-01_00-07-34\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 154.31329399249174\n",
      "  episode_reward_mean: 147.94784173417807\n",
      "  episode_reward_min: 140.65665299634838\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 670\n",
      "  experiment_id: cf8409b3cbbc42979d4540c7e6fbf708\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0842021724855043e-20\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.3852696382440627\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.003954800015230855\n",
      "          policy_loss: -0.0006651148782111704\n",
      "          total_loss: 348.9866717338562\n",
      "          vf_explained_var: -8.381903171539307e-09\n",
      "          vf_loss: 348.98733472824097\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 268000\n",
      "    num_steps_sampled: 268000\n",
      "    num_steps_trained: 268000\n",
      "  iterations_since_restore: 67\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.245679012345676\n",
      "    ram_util_percent: 25.730864197530867\n",
      "  pid: 16335\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08569006021379387\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.505752758186855\n",
      "    mean_inference_ms: 1.781196879507063\n",
      "    mean_raw_obs_processing_ms: 6.826847471381435\n",
      "  time_since_restore: 3860.649490594864\n",
      "  time_this_iter_s: 56.98737859725952\n",
      "  time_total_s: 3860.649490594864\n",
      "  timers:\n",
      "    learn_throughput: 157.975\n",
      "    learn_time_ms: 25320.39\n",
      "    sample_throughput: 125.838\n",
      "    sample_time_ms: 31786.98\n",
      "    update_time_ms: 3.246\n",
      "  timestamp: 1667279254\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 268000\n",
      "  training_iteration: 67\n",
      "  trial_id: 168ff_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 8.0/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/9.6 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_4_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_0_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_5_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_2_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_3_400b3145f046e80a4a7d53925cb6fe08, 0.0/6.0 CPU_group_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_1_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 accelerator_type:G)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                                      | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000 | RUNNING  | 192.168.1.65:16335 |     67 |          3860.65 | 268000 |  147.948 |              154.313 |              140.657 |                400 |\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000:\n",
      "  agent_timesteps_total: 272000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-11-01_00-08-32\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 155.00039480470204\n",
      "  episode_reward_mean: 147.89816657241258\n",
      "  episode_reward_min: 140.65665299634838\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 680\n",
      "  experiment_id: cf8409b3cbbc42979d4540c7e6fbf708\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 5.421010862427522e-21\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.38324514897540213\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.00928597652879274\n",
      "          policy_loss: -0.0028210316959302872\n",
      "          total_loss: 350.22627387046816\n",
      "          vf_explained_var: -7.450580707946131e-10\n",
      "          vf_loss: 350.22909586429597\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 272000\n",
      "    num_steps_sampled: 272000\n",
      "    num_steps_trained: 272000\n",
      "  iterations_since_restore: 68\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.14512195121951\n",
      "    ram_util_percent: 25.814634146341465\n",
      "  pid: 16335\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08569086295101921\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.505632941284784\n",
      "    mean_inference_ms: 1.7811504180631976\n",
      "    mean_raw_obs_processing_ms: 6.815167141652394\n",
      "  time_since_restore: 3918.0014078617096\n",
      "  time_this_iter_s: 57.3519172668457\n",
      "  time_total_s: 3918.0014078617096\n",
      "  timers:\n",
      "    learn_throughput: 157.922\n",
      "    learn_time_ms: 25328.912\n",
      "    sample_throughput: 125.813\n",
      "    sample_time_ms: 31793.256\n",
      "    update_time_ms: 3.178\n",
      "  timestamp: 1667279312\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 272000\n",
      "  training_iteration: 68\n",
      "  trial_id: 168ff_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 8.0/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/9.6 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_1_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 accelerator_type:G, 0.0/6.0 CPU_group_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_0_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_2_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_5_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_3_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_4_400b3145f046e80a4a7d53925cb6fe08)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                                      | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000 | RUNNING  | 192.168.1.65:16335 |     68 |             3918 | 272000 |  147.898 |                  155 |              140.657 |                400 |\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000:\n",
      "  agent_timesteps_total: 276000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-11-01_00-09-29\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 155.4227023916287\n",
      "  episode_reward_mean: 147.90439812596892\n",
      "  episode_reward_min: 140.65665299634838\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 690\n",
      "  experiment_id: cf8409b3cbbc42979d4540c7e6fbf708\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.710505431213761e-21\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.3577842256985605\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0053855322667168805\n",
      "          policy_loss: -0.001351433234231081\n",
      "          total_loss: 349.7867028236389\n",
      "          vf_explained_var: -3.7252903539730653e-10\n",
      "          vf_loss: 349.78805379867555\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 276000\n",
      "    num_steps_sampled: 276000\n",
      "    num_steps_trained: 276000\n",
      "  iterations_since_restore: 69\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.248148148148147\n",
      "    ram_util_percent: 25.862962962962964\n",
      "  pid: 16335\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08569097390229335\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.50566007481827\n",
      "    mean_inference_ms: 1.7810517151235041\n",
      "    mean_raw_obs_processing_ms: 6.803927278400365\n",
      "  time_since_restore: 3974.975175857544\n",
      "  time_this_iter_s: 56.97376799583435\n",
      "  time_total_s: 3974.975175857544\n",
      "  timers:\n",
      "    learn_throughput: 158.394\n",
      "    learn_time_ms: 25253.417\n",
      "    sample_throughput: 125.789\n",
      "    sample_time_ms: 31799.309\n",
      "    update_time_ms: 3.157\n",
      "  timestamp: 1667279369\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 276000\n",
      "  training_iteration: 69\n",
      "  trial_id: 168ff_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 8.1/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/9.6 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_4_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_3_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_5_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_1_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_0_400b3145f046e80a4a7d53925cb6fe08, 0.0/6.0 CPU_group_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_2_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 accelerator_type:G)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                                      | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000 | RUNNING  | 192.168.1.65:16335 |     69 |          3974.98 | 276000 |  147.904 |              155.423 |              140.657 |                400 |\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000:\n",
      "  agent_timesteps_total: 280000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-11-01_00-10-26\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 155.4227023916287\n",
      "  episode_reward_mean: 147.91487224219276\n",
      "  episode_reward_min: 141.28240094167876\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 700\n",
      "  experiment_id: cf8409b3cbbc42979d4540c7e6fbf708\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.3552527156068804e-21\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.33979505905881524\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009655378913771544\n",
      "          policy_loss: -0.002836962847504765\n",
      "          total_loss: 353.68554251194\n",
      "          vf_explained_var: -5.774199784980283e-09\n",
      "          vf_loss: 353.688382434845\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 280000\n",
      "    num_steps_sampled: 280000\n",
      "    num_steps_trained: 280000\n",
      "  iterations_since_restore: 70\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.12560975609756\n",
      "    ram_util_percent: 25.978048780487804\n",
      "  pid: 16335\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08569197214709817\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.50565678601326\n",
      "    mean_inference_ms: 1.7810222352812202\n",
      "    mean_raw_obs_processing_ms: 6.79297703202202\n",
      "  time_since_restore: 4032.1963305473328\n",
      "  time_this_iter_s: 57.22115468978882\n",
      "  time_total_s: 4032.1963305473328\n",
      "  timers:\n",
      "    learn_throughput: 158.266\n",
      "    learn_time_ms: 25273.924\n",
      "    sample_throughput: 125.793\n",
      "    sample_time_ms: 31798.28\n",
      "    update_time_ms: 3.22\n",
      "  timestamp: 1667279426\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 280000\n",
      "  training_iteration: 70\n",
      "  trial_id: 168ff_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Memory usage on this node: 8.1/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/9.6 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 accelerator_type:G, 0.0/6.0 CPU_group_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_5_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_2_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_3_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_4_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_0_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_1_400b3145f046e80a4a7d53925cb6fe08)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                                      | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000 | RUNNING  | 192.168.1.65:16335 |     70 |           4032.2 | 280000 |  147.915 |              155.423 |              141.282 |                400 |\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000:\n",
      "  agent_timesteps_total: 284000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-11-01_00-11-23\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 155.4227023916287\n",
      "  episode_reward_mean: 147.99736049272644\n",
      "  episode_reward_min: 141.5046047491151\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 710\n",
      "  experiment_id: cf8409b3cbbc42979d4540c7e6fbf708\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 6.776263578034402e-22\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.3200047302991152\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.00753951103077668\n",
      "          policy_loss: -0.0015493546263314784\n",
      "          total_loss: 355.6591100692749\n",
      "          vf_explained_var: 3.7252903539730653e-10\n",
      "          vf_loss: 355.6606599569321\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 284000\n",
      "    num_steps_sampled: 284000\n",
      "    num_steps_trained: 284000\n",
      "  iterations_since_restore: 71\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.147560975609757\n",
      "    ram_util_percent: 25.819512195121956\n",
      "  pid: 16335\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08569203926124466\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.50562722828972\n",
      "    mean_inference_ms: 1.7810075535264556\n",
      "    mean_raw_obs_processing_ms: 6.782477376317672\n",
      "  time_since_restore: 4089.2006409168243\n",
      "  time_this_iter_s: 57.00431036949158\n",
      "  time_total_s: 4089.2006409168243\n",
      "  timers:\n",
      "    learn_throughput: 158.094\n",
      "    learn_time_ms: 25301.335\n",
      "    sample_throughput: 125.77\n",
      "    sample_time_ms: 31804.158\n",
      "    update_time_ms: 3.169\n",
      "  timestamp: 1667279483\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 284000\n",
      "  training_iteration: 71\n",
      "  trial_id: 168ff_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 8.0/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/9.6 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_4_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_5_400b3145f046e80a4a7d53925cb6fe08, 0.0/6.0 CPU_group_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_2_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_0_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_3_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_1_400b3145f046e80a4a7d53925cb6fe08)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                                      | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000 | RUNNING  | 192.168.1.65:16335 |     71 |           4089.2 | 284000 |  147.997 |              155.423 |              141.505 |                400 |\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000:\n",
      "  agent_timesteps_total: 288000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-11-01_00-12-20\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 156.3797343095085\n",
      "  episode_reward_mean: 148.04405971497843\n",
      "  episode_reward_min: 141.1038134751134\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 720\n",
      "  experiment_id: cf8409b3cbbc42979d4540c7e6fbf708\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 3.388131789017201e-22\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.27985092904418707\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0031476671285190607\n",
      "          policy_loss: 0.0012733525509247556\n",
      "          total_loss: 355.88299614191055\n",
      "          vf_explained_var: -0.0015563052147626877\n",
      "          vf_loss: 355.88172084093094\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 288000\n",
      "    num_steps_sampled: 288000\n",
      "    num_steps_trained: 288000\n",
      "  iterations_since_restore: 72\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.463749999999997\n",
      "    ram_util_percent: 25.948749999999997\n",
      "  pid: 16335\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08569329562310855\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.5056568557339\n",
      "    mean_inference_ms: 1.7810282174110035\n",
      "    mean_raw_obs_processing_ms: 6.772373156404567\n",
      "  time_since_restore: 4145.785420179367\n",
      "  time_this_iter_s: 56.584779262542725\n",
      "  time_total_s: 4145.785420179367\n",
      "  timers:\n",
      "    learn_throughput: 158.418\n",
      "    learn_time_ms: 25249.599\n",
      "    sample_throughput: 125.746\n",
      "    sample_time_ms: 31810.164\n",
      "    update_time_ms: 3.166\n",
      "  timestamp: 1667279540\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 288000\n",
      "  training_iteration: 72\n",
      "  trial_id: 168ff_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 8.1/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/9.6 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_2_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_0_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_5_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_3_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_1_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_4_400b3145f046e80a4a7d53925cb6fe08, 0.0/6.0 CPU_group_400b3145f046e80a4a7d53925cb6fe08)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                                      | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000 | RUNNING  | 192.168.1.65:16335 |     72 |          4145.79 | 288000 |  148.044 |               156.38 |              141.104 |                400 |\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000:\n",
      "  agent_timesteps_total: 292000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-11-01_00-13-17\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 156.3797343095085\n",
      "  episode_reward_mean: 148.1320860759339\n",
      "  episode_reward_min: 141.1038134751134\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 730\n",
      "  experiment_id: cf8409b3cbbc42979d4540c7e6fbf708\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.6940658945086005e-22\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.2594162022229284\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.004208153607669374\n",
      "          policy_loss: 0.0021180934345466085\n",
      "          total_loss: 357.00376019477847\n",
      "          vf_explained_var: 7.450580707946131e-10\n",
      "          vf_loss: 357.00164341926575\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 292000\n",
      "    num_steps_sampled: 292000\n",
      "    num_steps_trained: 292000\n",
      "  iterations_since_restore: 73\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.273170731707314\n",
      "    ram_util_percent: 26.043902439024382\n",
      "  pid: 16335\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08569502951093096\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.505403730378546\n",
      "    mean_inference_ms: 1.7811169447967574\n",
      "    mean_raw_obs_processing_ms: 6.762708007261215\n",
      "  time_since_restore: 4202.521732091904\n",
      "  time_this_iter_s: 56.73631191253662\n",
      "  time_total_s: 4202.521732091904\n",
      "  timers:\n",
      "    learn_throughput: 158.661\n",
      "    learn_time_ms: 25211.013\n",
      "    sample_throughput: 125.767\n",
      "    sample_time_ms: 31804.742\n",
      "    update_time_ms: 3.22\n",
      "  timestamp: 1667279597\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 292000\n",
      "  training_iteration: 73\n",
      "  trial_id: 168ff_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 8.1/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/9.6 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_1_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_2_400b3145f046e80a4a7d53925cb6fe08, 0.0/6.0 CPU_group_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_3_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_4_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_5_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_0_400b3145f046e80a4a7d53925cb6fe08)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                                      | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000 | RUNNING  | 192.168.1.65:16335 |     73 |          4202.52 | 292000 |  148.132 |               156.38 |              141.104 |                400 |\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000:\n",
      "  agent_timesteps_total: 296000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-11-01_00-14-14\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 156.3797343095085\n",
      "  episode_reward_mean: 147.87078291502442\n",
      "  episode_reward_min: 141.1038134751134\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 740\n",
      "  experiment_id: cf8409b3cbbc42979d4540c7e6fbf708\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 8.470329472543003e-23\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.2806225365959108\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0037640858516738347\n",
      "          policy_loss: -0.0011504445705213585\n",
      "          total_loss: 354.44369955062865\n",
      "          vf_explained_var: -3.166496842510469e-09\n",
      "          vf_loss: 354.4448477983475\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 296000\n",
      "    num_steps_sampled: 296000\n",
      "    num_steps_trained: 296000\n",
      "  iterations_since_restore: 74\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.37037037037037\n",
      "    ram_util_percent: 25.953086419753085\n",
      "  pid: 16335\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08569655897286756\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.505271643668703\n",
      "    mean_inference_ms: 1.781143686644671\n",
      "    mean_raw_obs_processing_ms: 6.753307819568476\n",
      "  time_since_restore: 4259.6270434856415\n",
      "  time_this_iter_s: 57.10531139373779\n",
      "  time_total_s: 4259.6270434856415\n",
      "  timers:\n",
      "    learn_throughput: 158.135\n",
      "    learn_time_ms: 25294.852\n",
      "    sample_throughput: 125.765\n",
      "    sample_time_ms: 31805.29\n",
      "    update_time_ms: 3.174\n",
      "  timestamp: 1667279654\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 296000\n",
      "  training_iteration: 74\n",
      "  trial_id: 168ff_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 8.1/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/9.6 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_4_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_3_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_1_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_5_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_0_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_2_400b3145f046e80a4a7d53925cb6fe08, 0.0/6.0 CPU_group_400b3145f046e80a4a7d53925cb6fe08)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                                      | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000 | RUNNING  | 192.168.1.65:16335 |     74 |          4259.63 | 296000 |  147.871 |               156.38 |              141.104 |                400 |\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000:\n",
      "  agent_timesteps_total: 300000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-11-01_00-15-11\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 156.3797343095085\n",
      "  episode_reward_mean: 147.931747955192\n",
      "  episode_reward_min: 139.38905399578545\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 750\n",
      "  experiment_id: cf8409b3cbbc42979d4540c7e6fbf708\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 4.2351647362715013e-23\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.22860268563963473\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009296489486314386\n",
      "          policy_loss: -0.003707762248814106\n",
      "          total_loss: 351.1698782444\n",
      "          vf_explained_var: -0.0010378295555710793\n",
      "          vf_loss: 351.17358553409576\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 300000\n",
      "    num_steps_sampled: 300000\n",
      "    num_steps_trained: 300000\n",
      "  iterations_since_restore: 75\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.06219512195122\n",
      "    ram_util_percent: 26.037804878048785\n",
      "  pid: 16335\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08569956722065925\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.505124270646274\n",
      "    mean_inference_ms: 1.7812122317918258\n",
      "    mean_raw_obs_processing_ms: 6.744229862969703\n",
      "  time_since_restore: 4316.623652458191\n",
      "  time_this_iter_s: 56.99660897254944\n",
      "  time_total_s: 4316.623652458191\n",
      "  timers:\n",
      "    learn_throughput: 158.482\n",
      "    learn_time_ms: 25239.537\n",
      "    sample_throughput: 125.732\n",
      "    sample_time_ms: 31813.625\n",
      "    update_time_ms: 3.211\n",
      "  timestamp: 1667279711\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 300000\n",
      "  training_iteration: 75\n",
      "  trial_id: 168ff_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Memory usage on this node: 8.1/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/9.6 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_2_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_4_400b3145f046e80a4a7d53925cb6fe08, 0.0/6.0 CPU_group_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_0_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_1_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_3_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_5_400b3145f046e80a4a7d53925cb6fe08)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                                      | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000 | RUNNING  | 192.168.1.65:16335 |     75 |          4316.62 | 300000 |  147.932 |               156.38 |              139.389 |                400 |\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000:\n",
      "  agent_timesteps_total: 304000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-11-01_00-16-09\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 156.3797343095085\n",
      "  episode_reward_mean: 148.06445065259703\n",
      "  episode_reward_min: 139.38905399578545\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 760\n",
      "  experiment_id: cf8409b3cbbc42979d4540c7e6fbf708\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.1175823681357507e-23\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.2325426002498716\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.005122791419509584\n",
      "          policy_loss: 9.031706431414933e-05\n",
      "          total_loss: 352.0003077507019\n",
      "          vf_explained_var: -5.401671110405459e-09\n",
      "          vf_loss: 352.0002136230469\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 304000\n",
      "    num_steps_sampled: 304000\n",
      "    num_steps_trained: 304000\n",
      "  iterations_since_restore: 76\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.214634146341464\n",
      "    ram_util_percent: 25.907317073170727\n",
      "  pid: 16335\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08570197504268252\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.50343691002354\n",
      "    mean_inference_ms: 1.7813373570788644\n",
      "    mean_raw_obs_processing_ms: 6.735051020730525\n",
      "  time_since_restore: 4374.301378726959\n",
      "  time_this_iter_s: 57.67772626876831\n",
      "  time_total_s: 4374.301378726959\n",
      "  timers:\n",
      "    learn_throughput: 157.865\n",
      "    learn_time_ms: 25338.11\n",
      "    sample_throughput: 126.138\n",
      "    sample_time_ms: 31711.308\n",
      "    update_time_ms: 3.208\n",
      "  timestamp: 1667279769\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 304000\n",
      "  training_iteration: 76\n",
      "  trial_id: 168ff_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 8.1/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/9.6 GiB heap, 0.0/14.65 GiB objects (0.0/6.0 CPU_group_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_4_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_5_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_1_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_3_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_2_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_0_400b3145f046e80a4a7d53925cb6fe08)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                                      | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000 | RUNNING  | 192.168.1.65:16335 |     76 |           4374.3 | 304000 |  148.064 |               156.38 |              139.389 |                400 |\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000:\n",
      "  agent_timesteps_total: 308000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-11-01_00-17-06\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 156.3797343095085\n",
      "  episode_reward_mean: 147.94234699329655\n",
      "  episode_reward_min: 139.38905399578545\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 770\n",
      "  experiment_id: cf8409b3cbbc42979d4540c7e6fbf708\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0587911840678753e-23\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.19183337767608463\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0040015096500724216\n",
      "          policy_loss: -0.00034386825282126663\n",
      "          total_loss: 355.0097695112228\n",
      "          vf_explained_var: -0.0015738485381007195\n",
      "          vf_loss: 355.0101149082184\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 308000\n",
      "    num_steps_sampled: 308000\n",
      "    num_steps_trained: 308000\n",
      "  iterations_since_restore: 77\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.2\n",
      "    ram_util_percent: 25.87407407407407\n",
      "  pid: 16335\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08570295014067078\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.501892695457638\n",
      "    mean_inference_ms: 1.7813854111133531\n",
      "    mean_raw_obs_processing_ms: 6.7261366369525994\n",
      "  time_since_restore: 4431.105441331863\n",
      "  time_this_iter_s: 56.804062604904175\n",
      "  time_total_s: 4431.105441331863\n",
      "  timers:\n",
      "    learn_throughput: 157.952\n",
      "    learn_time_ms: 25324.16\n",
      "    sample_throughput: 126.155\n",
      "    sample_time_ms: 31706.991\n",
      "    update_time_ms: 3.207\n",
      "  timestamp: 1667279826\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 308000\n",
      "  training_iteration: 77\n",
      "  trial_id: 168ff_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 8.1/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/9.6 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_0_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_5_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_2_400b3145f046e80a4a7d53925cb6fe08, 0.0/6.0 CPU_group_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_4_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_1_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_3_400b3145f046e80a4a7d53925cb6fe08)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                                      | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000 | RUNNING  | 192.168.1.65:16335 |     77 |          4431.11 | 308000 |  147.942 |               156.38 |              139.389 |                400 |\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000:\n",
      "  agent_timesteps_total: 312000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-11-01_00-18-03\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 156.3797343095085\n",
      "  episode_reward_mean: 147.8873627700967\n",
      "  episode_reward_min: 139.38905399578545\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 780\n",
      "  experiment_id: cf8409b3cbbc42979d4540c7e6fbf708\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 5.293955920339377e-24\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.16747289304621518\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010347408483309579\n",
      "          policy_loss: -0.0035670102370204404\n",
      "          total_loss: 350.67543283700945\n",
      "          vf_explained_var: -4.842877210364804e-09\n",
      "          vf_loss: 350.6790003299713\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 312000\n",
      "    num_steps_sampled: 312000\n",
      "    num_steps_trained: 312000\n",
      "  iterations_since_restore: 78\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.210975609756098\n",
      "    ram_util_percent: 25.974390243902434\n",
      "  pid: 16335\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08570289989261487\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.500418344806494\n",
      "    mean_inference_ms: 1.7813560305384417\n",
      "    mean_raw_obs_processing_ms: 6.717557900213006\n",
      "  time_since_restore: 4488.484884262085\n",
      "  time_this_iter_s: 57.37944293022156\n",
      "  time_total_s: 4488.484884262085\n",
      "  timers:\n",
      "    learn_throughput: 157.872\n",
      "    learn_time_ms: 25337.021\n",
      "    sample_throughput: 126.195\n",
      "    sample_time_ms: 31696.927\n",
      "    update_time_ms: 3.239\n",
      "  timestamp: 1667279883\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 312000\n",
      "  training_iteration: 78\n",
      "  trial_id: 168ff_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 8.1/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/9.6 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_1_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_0_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_4_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_3_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_2_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_5_400b3145f046e80a4a7d53925cb6fe08, 0.0/6.0 CPU_group_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 accelerator_type:G)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                                      | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000 | RUNNING  | 192.168.1.65:16335 |     78 |          4488.48 | 312000 |  147.887 |               156.38 |              139.389 |                400 |\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000:\n",
      "  agent_timesteps_total: 316000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-11-01_00-19-00\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 156.3797343095085\n",
      "  episode_reward_mean: 147.98945153572288\n",
      "  episode_reward_min: 139.38905399578545\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 790\n",
      "  experiment_id: cf8409b3cbbc42979d4540c7e6fbf708\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 5.293955920339377e-24\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.17582392296753824\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.003948351520464443\n",
      "          policy_loss: -0.0013698235707124695\n",
      "          total_loss: 353.976076734066\n",
      "          vf_explained_var: -4.284083754413359e-09\n",
      "          vf_loss: 353.97744567394255\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 316000\n",
      "    num_steps_sampled: 316000\n",
      "    num_steps_trained: 316000\n",
      "  iterations_since_restore: 79\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.35061728395062\n",
      "    ram_util_percent: 26.11481481481481\n",
      "  pid: 16335\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08570353616445626\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.498960262132993\n",
      "    mean_inference_ms: 1.7813409251181864\n",
      "    mean_raw_obs_processing_ms: 6.709137771423941\n",
      "  time_since_restore: 4545.003717184067\n",
      "  time_this_iter_s: 56.51883292198181\n",
      "  time_total_s: 4545.003717184067\n",
      "  timers:\n",
      "    learn_throughput: 158.147\n",
      "    learn_time_ms: 25292.966\n",
      "    sample_throughput: 126.201\n",
      "    sample_time_ms: 31695.43\n",
      "    update_time_ms: 3.286\n",
      "  timestamp: 1667279940\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 316000\n",
      "  training_iteration: 79\n",
      "  trial_id: 168ff_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 8.1/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/9.6 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_5_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_1_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_2_400b3145f046e80a4a7d53925cb6fe08, 0.0/6.0 CPU_group_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_0_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_3_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_4_400b3145f046e80a4a7d53925cb6fe08)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                                      | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000 | RUNNING  | 192.168.1.65:16335 |     79 |             4545 | 316000 |  147.989 |               156.38 |              139.389 |                400 |\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=16334)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=16331)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=16333)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=16332)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=16330)\u001b[0m  Retrying in 1 seconds\n",
      "Result for PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000:\n",
      "  agent_timesteps_total: 320000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-11-01_00-19-58\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 156.3797343095085\n",
      "  episode_reward_mean: 148.1441204394637\n",
      "  episode_reward_min: 139.38905399578545\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 800\n",
      "  experiment_id: cf8409b3cbbc42979d4540c7e6fbf708\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.6469779601696883e-24\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.1387891518883407\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.006117666748059492\n",
      "          policy_loss: -0.002216873358702287\n",
      "          total_loss: 352.6712952494621\n",
      "          vf_explained_var: -0.0010053699370473623\n",
      "          vf_loss: 352.67351200580595\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 320000\n",
      "    num_steps_sampled: 320000\n",
      "    num_steps_trained: 320000\n",
      "  iterations_since_restore: 80\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.761445783132533\n",
      "    ram_util_percent: 26.05421686746988\n",
      "  pid: 16335\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08570305589789214\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.49762250902052\n",
      "    mean_inference_ms: 1.7812477331482268\n",
      "    mean_raw_obs_processing_ms: 6.702488674647243\n",
      "  time_since_restore: 4603.180857181549\n",
      "  time_this_iter_s: 58.1771399974823\n",
      "  time_total_s: 4603.180857181549\n",
      "  timers:\n",
      "    learn_throughput: 158.183\n",
      "    learn_time_ms: 25287.134\n",
      "    sample_throughput: 125.798\n",
      "    sample_time_ms: 31796.967\n",
      "    update_time_ms: 3.259\n",
      "  timestamp: 1667279998\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 320000\n",
      "  training_iteration: 80\n",
      "  trial_id: 168ff_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Memory usage on this node: 8.1/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/9.6 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_0_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_4_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_5_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_3_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_1_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_2_400b3145f046e80a4a7d53925cb6fe08, 0.0/6.0 CPU_group_400b3145f046e80a4a7d53925cb6fe08)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                                      | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000 | RUNNING  | 192.168.1.65:16335 |     80 |          4603.18 | 320000 |  148.144 |               156.38 |              139.389 |                400 |\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000:\n",
      "  agent_timesteps_total: 324000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-11-01_00-20-54\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 156.3797343095085\n",
      "  episode_reward_mean: 148.04459517550052\n",
      "  episode_reward_min: 138.9520556379916\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 810\n",
      "  experiment_id: cf8409b3cbbc42979d4540c7e6fbf708\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.3234889800848442e-24\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.11939603344071656\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008456213829870763\n",
      "          policy_loss: -0.0029227633320260795\n",
      "          total_loss: 354.6507480144501\n",
      "          vf_explained_var: 3.5390257391298974e-09\n",
      "          vf_loss: 354.65366876125336\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 324000\n",
      "    num_steps_sampled: 324000\n",
      "    num_steps_trained: 324000\n",
      "  iterations_since_restore: 81\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.4925\n",
      "    ram_util_percent: 25.941250000000004\n",
      "  pid: 16335\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0857032708294453\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.49630456742576\n",
      "    mean_inference_ms: 1.7811711189873285\n",
      "    mean_raw_obs_processing_ms: 6.695912051227855\n",
      "  time_since_restore: 4659.265609502792\n",
      "  time_this_iter_s: 56.084752321243286\n",
      "  time_total_s: 4659.265609502792\n",
      "  timers:\n",
      "    learn_throughput: 158.699\n",
      "    learn_time_ms: 25204.987\n",
      "    sample_throughput: 125.837\n",
      "    sample_time_ms: 31787.185\n",
      "    update_time_ms: 3.259\n",
      "  timestamp: 1667280054\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 324000\n",
      "  training_iteration: 81\n",
      "  trial_id: 168ff_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 8.1/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/9.6 GiB heap, 0.0/14.65 GiB objects (0.0/6.0 CPU_group_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_5_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_2_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_4_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_0_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_1_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_3_400b3145f046e80a4a7d53925cb6fe08)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                                      | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000 | RUNNING  | 192.168.1.65:16335 |     81 |          4659.27 | 324000 |  148.045 |               156.38 |              138.952 |                400 |\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=16331)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=16332)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=16334)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=16333)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=16330)\u001b[0m  Retrying in 1 seconds\n",
      "Result for PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000:\n",
      "  agent_timesteps_total: 328000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-11-01_00-21-52\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 156.06408454265937\n",
      "  episode_reward_mean: 147.9327992804725\n",
      "  episode_reward_min: 138.9520556379916\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 820\n",
      "  experiment_id: cf8409b3cbbc42979d4540c7e6fbf708\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 6.617444900424221e-25\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.12180836354382336\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.006270327648735645\n",
      "          policy_loss: -0.001844688321580179\n",
      "          total_loss: 357.3227332353592\n",
      "          vf_explained_var: -6.146728903644316e-09\n",
      "          vf_loss: 357.32457773685456\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 328000\n",
      "    num_steps_sampled: 328000\n",
      "    num_steps_trained: 328000\n",
      "  iterations_since_restore: 82\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.83132530120482\n",
      "    ram_util_percent: 25.97349397590361\n",
      "  pid: 16335\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08570192733078912\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.495000192005463\n",
      "    mean_inference_ms: 1.7810526772380058\n",
      "    mean_raw_obs_processing_ms: 6.691139111193562\n",
      "  time_since_restore: 4717.29772567749\n",
      "  time_this_iter_s: 58.032116174697876\n",
      "  time_total_s: 4717.29772567749\n",
      "  timers:\n",
      "    learn_throughput: 158.44\n",
      "    learn_time_ms: 25246.22\n",
      "    sample_throughput: 125.428\n",
      "    sample_time_ms: 31890.761\n",
      "    update_time_ms: 3.255\n",
      "  timestamp: 1667280112\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 328000\n",
      "  training_iteration: 82\n",
      "  trial_id: 168ff_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 8.1/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/9.6 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_5_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_4_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_3_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_0_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_2_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_1_400b3145f046e80a4a7d53925cb6fe08, 0.0/6.0 CPU_group_400b3145f046e80a4a7d53925cb6fe08)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                                      | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000 | RUNNING  | 192.168.1.65:16335 |     82 |           4717.3 | 328000 |  147.933 |              156.064 |              138.952 |                400 |\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000:\n",
      "  agent_timesteps_total: 332000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-11-01_00-22-49\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 156.59822286627073\n",
      "  episode_reward_mean: 148.00653308887078\n",
      "  episode_reward_min: 138.9520556379916\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 830\n",
      "  experiment_id: cf8409b3cbbc42979d4540c7e6fbf708\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 3.3087224502121104e-25\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.12311222143471241\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.005256098106372187\n",
      "          policy_loss: 0.0001980270229978487\n",
      "          total_loss: 359.86228559017184\n",
      "          vf_explained_var: -0.0003849632921628654\n",
      "          vf_loss: 359.8620887517929\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 332000\n",
      "    num_steps_sampled: 332000\n",
      "    num_steps_trained: 332000\n",
      "  iterations_since_restore: 83\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.22962962962963\n",
      "    ram_util_percent: 25.990123456790126\n",
      "  pid: 16335\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08569944292770632\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.493778666599802\n",
      "    mean_inference_ms: 1.780882188954177\n",
      "    mean_raw_obs_processing_ms: 6.68649133359779\n",
      "  time_since_restore: 4774.065125465393\n",
      "  time_this_iter_s: 56.76739978790283\n",
      "  time_total_s: 4774.065125465393\n",
      "  timers:\n",
      "    learn_throughput: 158.412\n",
      "    learn_time_ms: 25250.551\n",
      "    sample_throughput: 125.433\n",
      "    sample_time_ms: 31889.535\n",
      "    update_time_ms: 3.236\n",
      "  timestamp: 1667280169\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 332000\n",
      "  training_iteration: 83\n",
      "  trial_id: 168ff_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 8.1/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/9.6 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_5_400b3145f046e80a4a7d53925cb6fe08, 0.0/6.0 CPU_group_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_2_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_1_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_4_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_3_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_0_400b3145f046e80a4a7d53925cb6fe08)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                                      | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000 | RUNNING  | 192.168.1.65:16335 |     83 |          4774.07 | 332000 |  148.007 |              156.598 |              138.952 |                400 |\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000:\n",
      "  agent_timesteps_total: 336000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-11-01_00-23-47\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 156.59822286627073\n",
      "  episode_reward_mean: 147.994341944898\n",
      "  episode_reward_min: 138.9520556379916\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 840\n",
      "  experiment_id: cf8409b3cbbc42979d4540c7e6fbf708\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.6543612251060552e-25\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.1112577844876796\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009307143684780809\n",
      "          policy_loss: -9.146086813416332e-05\n",
      "          total_loss: 357.64606137275695\n",
      "          vf_explained_var: -4.0978194171259474e-09\n",
      "          vf_loss: 357.6461522340775\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 336000\n",
      "    num_steps_sampled: 336000\n",
      "    num_steps_trained: 336000\n",
      "  iterations_since_restore: 84\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.24578313253012\n",
      "    ram_util_percent: 26.24216867469879\n",
      "  pid: 16335\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08569700072001617\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.492516155203088\n",
      "    mean_inference_ms: 1.780774819455392\n",
      "    mean_raw_obs_processing_ms: 6.681975030979803\n",
      "  time_since_restore: 4831.755717992783\n",
      "  time_this_iter_s: 57.690592527389526\n",
      "  time_total_s: 4831.755717992783\n",
      "  timers:\n",
      "    learn_throughput: 157.998\n",
      "    learn_time_ms: 25316.741\n",
      "    sample_throughput: 125.463\n",
      "    sample_time_ms: 31881.858\n",
      "    update_time_ms: 3.244\n",
      "  timestamp: 1667280227\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 336000\n",
      "  training_iteration: 84\n",
      "  trial_id: 168ff_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 8.2/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/9.6 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_1_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_4_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 accelerator_type:G, 0.0/6.0 CPU_group_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_0_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_3_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_5_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_2_400b3145f046e80a4a7d53925cb6fe08)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                                      | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000 | RUNNING  | 192.168.1.65:16335 |     84 |          4831.76 | 336000 |  147.994 |              156.598 |              138.952 |                400 |\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000:\n",
      "  agent_timesteps_total: 340000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-11-01_00-24-44\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 156.59822286627073\n",
      "  episode_reward_mean: 147.94359403658672\n",
      "  episode_reward_min: 138.9520556379916\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 850\n",
      "  experiment_id: cf8409b3cbbc42979d4540c7e6fbf708\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 8.271806125530276e-26\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.10432182690128684\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.004948892600813951\n",
      "          policy_loss: -0.000933158140105661\n",
      "          total_loss: 355.4498815536499\n",
      "          vf_explained_var: -5.215406329028838e-09\n",
      "          vf_loss: 355.45081346035005\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 340000\n",
      "    num_steps_sampled: 340000\n",
      "    num_steps_trained: 340000\n",
      "  iterations_since_restore: 85\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.13086419753086\n",
      "    ram_util_percent: 26.12839506172839\n",
      "  pid: 16335\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08569378268773212\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.491235901407983\n",
      "    mean_inference_ms: 1.7807268000129808\n",
      "    mean_raw_obs_processing_ms: 6.677638031997976\n",
      "  time_since_restore: 4888.515733242035\n",
      "  time_this_iter_s: 56.76001524925232\n",
      "  time_total_s: 4888.515733242035\n",
      "  timers:\n",
      "    learn_throughput: 158.122\n",
      "    learn_time_ms: 25296.916\n",
      "    sample_throughput: 125.478\n",
      "    sample_time_ms: 31878.084\n",
      "    update_time_ms: 3.206\n",
      "  timestamp: 1667280284\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 340000\n",
      "  training_iteration: 85\n",
      "  trial_id: 168ff_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Memory usage on this node: 8.1/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/9.6 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_2_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_5_400b3145f046e80a4a7d53925cb6fe08, 0.0/6.0 CPU_group_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_0_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_4_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_3_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_1_400b3145f046e80a4a7d53925cb6fe08)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                                      | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000 | RUNNING  | 192.168.1.65:16335 |     85 |          4888.52 | 340000 |  147.944 |              156.598 |              138.952 |                400 |\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000:\n",
      "  agent_timesteps_total: 344000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-11-01_00-25-41\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 156.59822286627073\n",
      "  episode_reward_mean: 147.89852579213832\n",
      "  episode_reward_min: 138.9520556379916\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 860\n",
      "  experiment_id: cf8409b3cbbc42979d4540c7e6fbf708\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 4.135903062765138e-26\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.08061359750572591\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.007732155801295493\n",
      "          policy_loss: -0.0009496001715888269\n",
      "          total_loss: 356.73874661922457\n",
      "          vf_explained_var: -4.6566128730773926e-09\n",
      "          vf_loss: 356.7396940946579\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 344000\n",
      "    num_steps_sampled: 344000\n",
      "    num_steps_trained: 344000\n",
      "  iterations_since_restore: 86\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.27530864197531\n",
      "    ram_util_percent: 26.03456790123457\n",
      "  pid: 16335\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08569096947450333\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.490066628521408\n",
      "    mean_inference_ms: 1.7806497442384648\n",
      "    mean_raw_obs_processing_ms: 6.673470847453857\n",
      "  time_since_restore: 4945.365052938461\n",
      "  time_this_iter_s: 56.84931969642639\n",
      "  time_total_s: 4945.365052938461\n",
      "  timers:\n",
      "    learn_throughput: 158.687\n",
      "    learn_time_ms: 25206.84\n",
      "    sample_throughput: 125.449\n",
      "    sample_time_ms: 31885.424\n",
      "    update_time_ms: 3.211\n",
      "  timestamp: 1667280341\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 344000\n",
      "  training_iteration: 86\n",
      "  trial_id: 168ff_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 8.1/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/9.6 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_0_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_1_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_5_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_2_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_3_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_4_400b3145f046e80a4a7d53925cb6fe08, 0.0/6.0 CPU_group_400b3145f046e80a4a7d53925cb6fe08)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                                      | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000 | RUNNING  | 192.168.1.65:16335 |     86 |          4945.37 | 344000 |  147.899 |              156.598 |              138.952 |                400 |\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000:\n",
      "  agent_timesteps_total: 348000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-11-01_00-26-38\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 156.59822286627073\n",
      "  episode_reward_mean: 147.52589043575034\n",
      "  episode_reward_min: 138.9520556379916\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 870\n",
      "  experiment_id: cf8409b3cbbc42979d4540c7e6fbf708\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.067951531382569e-26\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.060614277608692646\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.005516143463501067\n",
      "          policy_loss: 0.0007472559926100075\n",
      "          total_loss: 354.309591460228\n",
      "          vf_explained_var: -6.332993685020938e-09\n",
      "          vf_loss: 354.30884356498717\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 348000\n",
      "    num_steps_sampled: 348000\n",
      "    num_steps_trained: 348000\n",
      "  iterations_since_restore: 87\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.319753086419755\n",
      "    ram_util_percent: 26.11111111111111\n",
      "  pid: 16335\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08568922870285967\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.488872178595372\n",
      "    mean_inference_ms: 1.7806142928458766\n",
      "    mean_raw_obs_processing_ms: 6.669433525741489\n",
      "  time_since_restore: 5002.134905576706\n",
      "  time_this_iter_s: 56.76985263824463\n",
      "  time_total_s: 5002.134905576706\n",
      "  timers:\n",
      "    learn_throughput: 158.714\n",
      "    learn_time_ms: 25202.529\n",
      "    sample_throughput: 125.446\n",
      "    sample_time_ms: 31886.271\n",
      "    update_time_ms: 3.258\n",
      "  timestamp: 1667280398\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 348000\n",
      "  training_iteration: 87\n",
      "  trial_id: 168ff_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 8.1/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/9.6 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_5_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_2_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_4_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_0_400b3145f046e80a4a7d53925cb6fe08, 0.0/6.0 CPU_group_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_3_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_1_400b3145f046e80a4a7d53925cb6fe08)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                                      | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000 | RUNNING  | 192.168.1.65:16335 |     87 |          5002.13 | 348000 |  147.526 |              156.598 |              138.952 |                400 |\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000:\n",
      "  agent_timesteps_total: 352000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-11-01_00-27-35\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 156.59822286627073\n",
      "  episode_reward_mean: 147.51272728909618\n",
      "  episode_reward_min: 138.92410403895212\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 880\n",
      "  experiment_id: cf8409b3cbbc42979d4540c7e6fbf708\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0339757656912845e-26\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.08478154840413481\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0019906900979322017\n",
      "          policy_loss: 0.0006194581859745085\n",
      "          total_loss: 354.5682263612747\n",
      "          vf_explained_var: -4.6566128730773926e-09\n",
      "          vf_loss: 354.56760742664335\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 352000\n",
      "    num_steps_sampled: 352000\n",
      "    num_steps_trained: 352000\n",
      "  iterations_since_restore: 88\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.123456790123456\n",
      "    ram_util_percent: 26.20740740740741\n",
      "  pid: 16335\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08568759350883823\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.487735465231808\n",
      "    mean_inference_ms: 1.78057003847166\n",
      "    mean_raw_obs_processing_ms: 6.665455825688016\n",
      "  time_since_restore: 5058.981796979904\n",
      "  time_this_iter_s: 56.84689140319824\n",
      "  time_total_s: 5058.981796979904\n",
      "  timers:\n",
      "    learn_throughput: 159.025\n",
      "    learn_time_ms: 25153.322\n",
      "    sample_throughput: 125.462\n",
      "    sample_time_ms: 31882.234\n",
      "    update_time_ms: 3.234\n",
      "  timestamp: 1667280455\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 352000\n",
      "  training_iteration: 88\n",
      "  trial_id: 168ff_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 8.2/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/9.6 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_4_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_3_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_2_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 accelerator_type:G, 0.0/6.0 CPU_group_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_5_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_1_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_0_400b3145f046e80a4a7d53925cb6fe08)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                                      | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000 | RUNNING  | 192.168.1.65:16335 |     88 |          5058.98 | 352000 |  147.513 |              156.598 |              138.924 |                400 |\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000:\n",
      "  agent_timesteps_total: 356000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-11-01_00-28-32\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 156.59822286627073\n",
      "  episode_reward_mean: 147.26462161030335\n",
      "  episode_reward_min: 138.92410403895212\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 890\n",
      "  experiment_id: cf8409b3cbbc42979d4540c7e6fbf708\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 5.1698788284564225e-27\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.03705874355218839\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.006466847021393829\n",
      "          policy_loss: -0.0005964966825558804\n",
      "          total_loss: 353.76019928455355\n",
      "          vf_explained_var: -6.332993685020938e-09\n",
      "          vf_loss: 353.76079407930376\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 356000\n",
      "    num_steps_sampled: 356000\n",
      "    num_steps_trained: 356000\n",
      "  iterations_since_restore: 89\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.273170731707314\n",
      "    ram_util_percent: 26.12073170731707\n",
      "  pid: 16335\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08568761519349483\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.486611110009434\n",
      "    mean_inference_ms: 1.7805686763754631\n",
      "    mean_raw_obs_processing_ms: 6.6616177513908745\n",
      "  time_since_restore: 5115.9185090065\n",
      "  time_this_iter_s: 56.93671202659607\n",
      "  time_total_s: 5115.9185090065\n",
      "  timers:\n",
      "    learn_throughput: 158.781\n",
      "    learn_time_ms: 25191.906\n",
      "    sample_throughput: 125.449\n",
      "    sample_time_ms: 31885.49\n",
      "    update_time_ms: 3.18\n",
      "  timestamp: 1667280512\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 356000\n",
      "  training_iteration: 89\n",
      "  trial_id: 168ff_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 8.1/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/9.6 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_1_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_0_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_2_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_3_400b3145f046e80a4a7d53925cb6fe08, 0.0/6.0 CPU_group_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_5_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_4_400b3145f046e80a4a7d53925cb6fe08)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                                      | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000 | RUNNING  | 192.168.1.65:16335 |     89 |          5115.92 | 356000 |  147.265 |              156.598 |              138.924 |                400 |\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000:\n",
      "  agent_timesteps_total: 360000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-11-01_00-29-29\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 156.59822286627073\n",
      "  episode_reward_mean: 147.08942369568797\n",
      "  episode_reward_min: 138.92410403895212\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 900\n",
      "  experiment_id: cf8409b3cbbc42979d4540c7e6fbf708\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.5849394142282113e-27\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -0.02667418872297276\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015979752080812672\n",
      "          policy_loss: -0.003868087352020666\n",
      "          total_loss: 352.7718230247498\n",
      "          vf_explained_var: -0.00038924068212509155\n",
      "          vf_loss: 352.7756884098053\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 360000\n",
      "    num_steps_sampled: 360000\n",
      "    num_steps_trained: 360000\n",
      "  iterations_since_restore: 90\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.349382716049384\n",
      "    ram_util_percent: 26.139506172839496\n",
      "  pid: 16335\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08568856710778747\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.485440732202164\n",
      "    mean_inference_ms: 1.7806406401160304\n",
      "    mean_raw_obs_processing_ms: 6.656403768136941\n",
      "  time_since_restore: 5173.011370420456\n",
      "  time_this_iter_s: 57.09286141395569\n",
      "  time_total_s: 5173.011370420456\n",
      "  timers:\n",
      "    learn_throughput: 158.828\n",
      "    learn_time_ms: 25184.449\n",
      "    sample_throughput: 125.847\n",
      "    sample_time_ms: 31784.567\n",
      "    update_time_ms: 3.149\n",
      "  timestamp: 1667280569\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 360000\n",
      "  training_iteration: 90\n",
      "  trial_id: 168ff_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Memory usage on this node: 8.2/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/9.6 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_4_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_0_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_5_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_2_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_1_400b3145f046e80a4a7d53925cb6fe08, 0.0/6.0 CPU_group_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_3_400b3145f046e80a4a7d53925cb6fe08)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                                      | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000 | RUNNING  | 192.168.1.65:16335 |     90 |          5173.01 | 360000 |  147.089 |              156.598 |              138.924 |                400 |\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000:\n",
      "  agent_timesteps_total: 364000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-11-01_00-30-26\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 156.59822286627073\n",
      "  episode_reward_mean: 147.17099322430815\n",
      "  episode_reward_min: 138.92410403895212\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 910\n",
      "  experiment_id: cf8409b3cbbc42979d4540c7e6fbf708\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.5849394142282113e-27\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -0.01699100516852923\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0011718081012930255\n",
      "          policy_loss: 1.5970038657542317e-05\n",
      "          total_loss: 353.7072398424149\n",
      "          vf_explained_var: -1.6763805898989403e-09\n",
      "          vf_loss: 353.70722489356996\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 364000\n",
      "    num_steps_sampled: 364000\n",
      "    num_steps_trained: 364000\n",
      "  iterations_since_restore: 91\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.103658536585368\n",
      "    ram_util_percent: 26.139024390243897\n",
      "  pid: 16335\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08568878169936181\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.48438462144898\n",
      "    mean_inference_ms: 1.780677978807185\n",
      "    mean_raw_obs_processing_ms: 6.651433183521189\n",
      "  time_since_restore: 5230.26127076149\n",
      "  time_this_iter_s: 57.249900341033936\n",
      "  time_total_s: 5230.26127076149\n",
      "  timers:\n",
      "    learn_throughput: 158.204\n",
      "    learn_time_ms: 25283.831\n",
      "    sample_throughput: 125.779\n",
      "    sample_time_ms: 31801.718\n",
      "    update_time_ms: 3.15\n",
      "  timestamp: 1667280626\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 364000\n",
      "  training_iteration: 91\n",
      "  trial_id: 168ff_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 8.2/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/9.6 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_2_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_5_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_1_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_4_400b3145f046e80a4a7d53925cb6fe08, 0.0/6.0 CPU_group_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_3_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_0_400b3145f046e80a4a7d53925cb6fe08)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                                      | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000 | RUNNING  | 192.168.1.65:16335 |     91 |          5230.26 | 364000 |  147.171 |              156.598 |              138.924 |                400 |\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000:\n",
      "  agent_timesteps_total: 368000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-11-01_00-31-24\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 156.59822286627073\n",
      "  episode_reward_mean: 147.16023252023123\n",
      "  episode_reward_min: 138.75791737761335\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 920\n",
      "  experiment_id: cf8409b3cbbc42979d4540c7e6fbf708\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.2924697071141056e-27\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -0.009942249607411214\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0036757662097272902\n",
      "          policy_loss: -6.15065495367162e-05\n",
      "          total_loss: 352.1340753555298\n",
      "          vf_explained_var: -2.9802322831784522e-09\n",
      "          vf_loss: 352.1341384649277\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 368000\n",
      "    num_steps_sampled: 368000\n",
      "    num_steps_trained: 368000\n",
      "  iterations_since_restore: 92\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.01829268292683\n",
      "    ram_util_percent: 26.14999999999999\n",
      "  pid: 16335\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08568939450439302\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.483378537867257\n",
      "    mean_inference_ms: 1.780674020771538\n",
      "    mean_raw_obs_processing_ms: 6.644934682221735\n",
      "  time_since_restore: 5287.46591258049\n",
      "  time_this_iter_s: 57.204641819000244\n",
      "  time_total_s: 5287.46591258049\n",
      "  timers:\n",
      "    learn_throughput: 158.037\n",
      "    learn_time_ms: 25310.5\n",
      "    sample_throughput: 126.214\n",
      "    sample_time_ms: 31692.231\n",
      "    update_time_ms: 3.15\n",
      "  timestamp: 1667280684\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 368000\n",
      "  training_iteration: 92\n",
      "  trial_id: 168ff_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 8.2/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/9.6 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_3_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_2_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_4_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_1_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_0_400b3145f046e80a4a7d53925cb6fe08, 0.0/6.0 CPU_group_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_5_400b3145f046e80a4a7d53925cb6fe08)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                                      | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000 | RUNNING  | 192.168.1.65:16335 |     92 |          5287.47 | 368000 |   147.16 |              156.598 |              138.758 |                400 |\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000:\n",
      "  agent_timesteps_total: 372000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-11-01_00-32-21\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 156.0819797557507\n",
      "  episode_reward_mean: 146.98979862845124\n",
      "  episode_reward_min: 138.75791737761335\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 930\n",
      "  experiment_id: cf8409b3cbbc42979d4540c7e6fbf708\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 6.462348535570528e-28\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.0141387005278375\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.006785332497995003\n",
      "          policy_loss: -0.0005294811213389039\n",
      "          total_loss: 351.3992146253586\n",
      "          vf_explained_var: -7.264316259636416e-09\n",
      "          vf_loss: 351.39974505901336\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 372000\n",
      "    num_steps_sampled: 372000\n",
      "    num_steps_trained: 372000\n",
      "  iterations_since_restore: 93\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.11829268292683\n",
      "    ram_util_percent: 26.154878048780493\n",
      "  pid: 16335\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08569061709336845\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.48244196511475\n",
      "    mean_inference_ms: 1.7806628650180665\n",
      "    mean_raw_obs_processing_ms: 6.638530186424901\n",
      "  time_since_restore: 5344.858641624451\n",
      "  time_this_iter_s: 57.39272904396057\n",
      "  time_total_s: 5344.858641624451\n",
      "  timers:\n",
      "    learn_throughput: 157.653\n",
      "    learn_time_ms: 25372.242\n",
      "    sample_throughput: 126.211\n",
      "    sample_time_ms: 31692.933\n",
      "    update_time_ms: 3.252\n",
      "  timestamp: 1667280741\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 372000\n",
      "  training_iteration: 93\n",
      "  trial_id: 168ff_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 8.2/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/9.6 GiB heap, 0.0/14.65 GiB objects (0.0/6.0 CPU_group_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_1_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_5_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_3_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_0_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_2_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_4_400b3145f046e80a4a7d53925cb6fe08)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                                      | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000 | RUNNING  | 192.168.1.65:16335 |     93 |          5344.86 | 372000 |   146.99 |              156.082 |              138.758 |                400 |\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000:\n",
      "  agent_timesteps_total: 376000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-11-01_00-33-19\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 156.0819797557507\n",
      "  episode_reward_mean: 147.03153410029373\n",
      "  episode_reward_min: 138.75791737761335\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 940\n",
      "  experiment_id: cf8409b3cbbc42979d4540c7e6fbf708\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 3.231174267785264e-28\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -0.03281352194899227\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0036291996881346483\n",
      "          policy_loss: -0.002230276602494996\n",
      "          total_loss: 351.5314989566803\n",
      "          vf_explained_var: -3.5390257391298974e-09\n",
      "          vf_loss: 351.53372902870177\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 376000\n",
      "    num_steps_sampled: 376000\n",
      "    num_steps_trained: 376000\n",
      "  iterations_since_restore: 94\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.203658536585365\n",
      "    ram_util_percent: 26.330487804878054\n",
      "  pid: 16335\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08569217225538349\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.481489735221132\n",
      "    mean_inference_ms: 1.7806769733222254\n",
      "    mean_raw_obs_processing_ms: 6.632283921241631\n",
      "  time_since_restore: 5402.106095314026\n",
      "  time_this_iter_s: 57.247453689575195\n",
      "  time_total_s: 5402.106095314026\n",
      "  timers:\n",
      "    learn_throughput: 157.947\n",
      "    learn_time_ms: 25325.013\n",
      "    sample_throughput: 126.199\n",
      "    sample_time_ms: 31695.947\n",
      "    update_time_ms: 3.232\n",
      "  timestamp: 1667280799\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 376000\n",
      "  training_iteration: 94\n",
      "  trial_id: 168ff_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 8.2/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/9.6 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_3_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_1_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_0_400b3145f046e80a4a7d53925cb6fe08, 0.0/6.0 CPU_group_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_5_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_2_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_4_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 accelerator_type:G)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                                      | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000 | RUNNING  | 192.168.1.65:16335 |     94 |          5402.11 | 376000 |  147.032 |              156.082 |              138.758 |                400 |\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=16330)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=16334)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=16331)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=16333)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=16332)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=16332)\u001b[0m Could not connect to TraCI server at localhost:41419 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=16332)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=16330)\u001b[0m Could not connect to TraCI server at localhost:44711 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=16330)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=16334)\u001b[0m Could not connect to TraCI server at localhost:47333 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=16334)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=16331)\u001b[0m Could not connect to TraCI server at localhost:45859 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=16331)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=16333)\u001b[0m Could not connect to TraCI server at localhost:60241 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=16333)\u001b[0m  Retrying in 2 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=16332)\u001b[0m Could not connect to TraCI server at localhost:41419 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=16332)\u001b[0m  Retrying in 3 seconds\n",
      "\u001b[2m\u001b[36m(pid=16330)\u001b[0m Could not connect to TraCI server at localhost:44711 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=16330)\u001b[0m  Retrying in 3 seconds\n",
      "\u001b[2m\u001b[36m(pid=16334)\u001b[0m Could not connect to TraCI server at localhost:47333 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=16334)\u001b[0m  Retrying in 3 seconds\n",
      "\u001b[2m\u001b[36m(pid=16331)\u001b[0m Could not connect to TraCI server at localhost:45859 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=16331)\u001b[0m  Retrying in 3 seconds\n",
      "\u001b[2m\u001b[36m(pid=16333)\u001b[0m Could not connect to TraCI server at localhost:60241 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=16333)\u001b[0m  Retrying in 3 seconds\n",
      "Result for PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000:\n",
      "  agent_timesteps_total: 380000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-11-01_00-34-22\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 156.0819797557507\n",
      "  episode_reward_mean: 147.246193322696\n",
      "  episode_reward_min: 138.75791737761335\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 950\n",
      "  experiment_id: cf8409b3cbbc42979d4540c7e6fbf708\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.615587133892632e-28\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -0.035353933612350376\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009129236792432582\n",
      "          policy_loss: -0.00040689737070351837\n",
      "          total_loss: 355.70526599884033\n",
      "          vf_explained_var: -6.146728903644316e-09\n",
      "          vf_loss: 355.7056748867035\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 380000\n",
      "    num_steps_sampled: 380000\n",
      "    num_steps_trained: 380000\n",
      "  iterations_since_restore: 95\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 18.67032967032967\n",
      "    ram_util_percent: 26.318681318681318\n",
      "  pid: 16335\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08569388872786594\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.480623212456766\n",
      "    mean_inference_ms: 1.7806182560855388\n",
      "    mean_raw_obs_processing_ms: 6.6339547098022456\n",
      "  time_since_restore: 5465.611485719681\n",
      "  time_this_iter_s: 63.50539040565491\n",
      "  time_total_s: 5465.611485719681\n",
      "  timers:\n",
      "    learn_throughput: 157.444\n",
      "    learn_time_ms: 25405.787\n",
      "    sample_throughput: 123.879\n",
      "    sample_time_ms: 32289.69\n",
      "    update_time_ms: 3.23\n",
      "  timestamp: 1667280862\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 380000\n",
      "  training_iteration: 95\n",
      "  trial_id: 168ff_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 8.2/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/9.6 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_4_400b3145f046e80a4a7d53925cb6fe08, 0.0/6.0 CPU_group_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_1_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_3_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_0_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_2_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_5_400b3145f046e80a4a7d53925cb6fe08)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                                      | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000 | RUNNING  | 192.168.1.65:16335 |     95 |          5465.61 | 380000 |  147.246 |              156.082 |              138.758 |                400 |\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000:\n",
      "  agent_timesteps_total: 384000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-11-01_00-35-20\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 156.0819797557507\n",
      "  episode_reward_mean: 147.32366681828327\n",
      "  episode_reward_min: 138.75791737761335\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 960\n",
      "  experiment_id: cf8409b3cbbc42979d4540c7e6fbf708\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 8.07793566946316e-29\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -0.06825399764929899\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.00768615140257225\n",
      "          policy_loss: 0.0002121897938195616\n",
      "          total_loss: 357.1705850601196\n",
      "          vf_explained_var: -5.9604645663569045e-09\n",
      "          vf_loss: 357.17037484645846\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 384000\n",
      "    num_steps_sampled: 384000\n",
      "    num_steps_trained: 384000\n",
      "  iterations_since_restore: 96\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.006097560975608\n",
      "    ram_util_percent: 26.332926829268295\n",
      "  pid: 16335\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08569580847017151\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.479838780190835\n",
      "    mean_inference_ms: 1.780492667877998\n",
      "    mean_raw_obs_processing_ms: 6.635585515676787\n",
      "  time_since_restore: 5522.990152359009\n",
      "  time_this_iter_s: 57.378666639328\n",
      "  time_total_s: 5522.990152359009\n",
      "  timers:\n",
      "    learn_throughput: 157.097\n",
      "    learn_time_ms: 25461.941\n",
      "    sample_throughput: 123.891\n",
      "    sample_time_ms: 32286.38\n",
      "    update_time_ms: 3.305\n",
      "  timestamp: 1667280920\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 384000\n",
      "  training_iteration: 96\n",
      "  trial_id: 168ff_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 8.2/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/9.6 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_0_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_1_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_5_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_4_400b3145f046e80a4a7d53925cb6fe08, 0.0/6.0 CPU_group_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_2_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_3_400b3145f046e80a4a7d53925cb6fe08)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                                      | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000 | RUNNING  | 192.168.1.65:16335 |     96 |          5522.99 | 384000 |  147.324 |              156.082 |              138.758 |                400 |\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000:\n",
      "  agent_timesteps_total: 388000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-11-01_00-36-17\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 156.0819797557507\n",
      "  episode_reward_mean: 147.51947284698142\n",
      "  episode_reward_min: 138.75791737761335\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 970\n",
      "  experiment_id: cf8409b3cbbc42979d4540c7e6fbf708\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 4.03896783473158e-29\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -0.07486701546004042\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.002604937985159995\n",
      "          policy_loss: 0.0009691134735476225\n",
      "          total_loss: 356.08878329992297\n",
      "          vf_explained_var: -2.9802322831784522e-09\n",
      "          vf_loss: 356.087814450264\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 388000\n",
      "    num_steps_sampled: 388000\n",
      "    num_steps_trained: 388000\n",
      "  iterations_since_restore: 97\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.24814814814815\n",
      "    ram_util_percent: 26.334567901234568\n",
      "  pid: 16335\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0856972363549274\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.479116547070713\n",
      "    mean_inference_ms: 1.780362682963263\n",
      "    mean_raw_obs_processing_ms: 6.63735415743194\n",
      "  time_since_restore: 5579.955278396606\n",
      "  time_this_iter_s: 56.965126037597656\n",
      "  time_total_s: 5579.955278396606\n",
      "  timers:\n",
      "    learn_throughput: 157.036\n",
      "    learn_time_ms: 25471.793\n",
      "    sample_throughput: 123.855\n",
      "    sample_time_ms: 32295.943\n",
      "    update_time_ms: 3.261\n",
      "  timestamp: 1667280977\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 388000\n",
      "  training_iteration: 97\n",
      "  trial_id: 168ff_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Memory usage on this node: 8.2/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/9.6 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_4_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_5_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_1_400b3145f046e80a4a7d53925cb6fe08, 0.0/6.0 CPU_group_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_3_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_2_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_0_400b3145f046e80a4a7d53925cb6fe08)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                                      | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000 | RUNNING  | 192.168.1.65:16335 |     97 |          5579.96 | 388000 |  147.519 |              156.082 |              138.758 |                400 |\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000:\n",
      "  agent_timesteps_total: 392000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-11-01_00-37-14\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 156.0819797557507\n",
      "  episode_reward_mean: 147.57366680365936\n",
      "  episode_reward_min: 138.75791737761335\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 980\n",
      "  experiment_id: cf8409b3cbbc42979d4540c7e6fbf708\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.01948391736579e-29\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -0.0989267386496067\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.005672024636214707\n",
      "          policy_loss: -0.0019466362515231594\n",
      "          total_loss: 352.2967935204506\n",
      "          vf_explained_var: -3.725290298461914e-09\n",
      "          vf_loss: 352.2987386703491\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 392000\n",
      "    num_steps_sampled: 392000\n",
      "    num_steps_trained: 392000\n",
      "  iterations_since_restore: 98\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.180487804878055\n",
      "    ram_util_percent: 26.38170731707317\n",
      "  pid: 16335\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08569912434656017\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.478364750674615\n",
      "    mean_inference_ms: 1.7802983632477984\n",
      "    mean_raw_obs_processing_ms: 6.639262298011297\n",
      "  time_since_restore: 5637.195268154144\n",
      "  time_this_iter_s: 57.23998975753784\n",
      "  time_total_s: 5637.195268154144\n",
      "  timers:\n",
      "    learn_throughput: 156.849\n",
      "    learn_time_ms: 25502.211\n",
      "    sample_throughput: 123.821\n",
      "    sample_time_ms: 32304.765\n",
      "    update_time_ms: 3.281\n",
      "  timestamp: 1667281034\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 392000\n",
      "  training_iteration: 98\n",
      "  trial_id: 168ff_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 8.2/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/9.6 GiB heap, 0.0/14.65 GiB objects (0.0/6.0 CPU_group_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_2_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_5_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_1_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_0_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_4_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_3_400b3145f046e80a4a7d53925cb6fe08)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                                      | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000 | RUNNING  | 192.168.1.65:16335 |     98 |           5637.2 | 392000 |  147.574 |              156.082 |              138.758 |                400 |\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000:\n",
      "  agent_timesteps_total: 396000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-11-01_00-38-12\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 155.81873951615546\n",
      "  episode_reward_mean: 147.3757051176675\n",
      "  episode_reward_min: 137.08918031704633\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 990\n",
      "  experiment_id: cf8409b3cbbc42979d4540c7e6fbf708\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.009741958682895e-29\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -0.12889792758505791\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0100769796728855\n",
      "          policy_loss: -0.0018644557785592042\n",
      "          total_loss: 353.0692337036133\n",
      "          vf_explained_var: -0.00017364509403705597\n",
      "          vf_loss: 353.0711010694504\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 396000\n",
      "    num_steps_sampled: 396000\n",
      "    num_steps_trained: 396000\n",
      "  iterations_since_restore: 99\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.19878048780488\n",
      "    ram_util_percent: 26.363414634146338\n",
      "  pid: 16335\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08569974106098865\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.477645369707897\n",
      "    mean_inference_ms: 1.780207410146067\n",
      "    mean_raw_obs_processing_ms: 6.641206381059114\n",
      "  time_since_restore: 5694.575968980789\n",
      "  time_this_iter_s: 57.3807008266449\n",
      "  time_total_s: 5694.575968980789\n",
      "  timers:\n",
      "    learn_throughput: 156.542\n",
      "    learn_time_ms: 25552.192\n",
      "    sample_throughput: 123.842\n",
      "    sample_time_ms: 32299.134\n",
      "    update_time_ms: 3.345\n",
      "  timestamp: 1667281092\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 396000\n",
      "  training_iteration: 99\n",
      "  trial_id: 168ff_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 8.3/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/9.6 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_0_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_3_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_1_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_2_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_5_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_4_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 accelerator_type:G, 0.0/6.0 CPU_group_400b3145f046e80a4a7d53925cb6fe08)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                                      | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000 | RUNNING  | 192.168.1.65:16335 |     99 |          5694.58 | 396000 |  147.376 |              155.819 |              137.089 |                400 |\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000:\n",
      "  agent_timesteps_total: 400000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-11-01_00-39-09\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 155.81873951615546\n",
      "  episode_reward_mean: 147.5328099476659\n",
      "  episode_reward_min: 137.08918031704633\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1000\n",
      "  experiment_id: cf8409b3cbbc42979d4540c7e6fbf708\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.009741958682895e-29\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -0.097313556750305\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.002776014869357002\n",
      "          policy_loss: 0.002374976755527314\n",
      "          total_loss: 356.8193503379822\n",
      "          vf_explained_var: -6.705522359595761e-09\n",
      "          vf_loss: 356.81697607040405\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 400000\n",
      "    num_steps_sampled: 400000\n",
      "    num_steps_trained: 400000\n",
      "  iterations_since_restore: 100\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.27560975609756\n",
      "    ram_util_percent: 26.378048780487806\n",
      "  pid: 16335\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08570021089331725\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.477006853486923\n",
      "    mean_inference_ms: 1.7800960847279683\n",
      "    mean_raw_obs_processing_ms: 6.643205632762002\n",
      "  time_since_restore: 5752.043920516968\n",
      "  time_this_iter_s: 57.46795153617859\n",
      "  time_total_s: 5752.043920516968\n",
      "  timers:\n",
      "    learn_throughput: 156.345\n",
      "    learn_time_ms: 25584.413\n",
      "    sample_throughput: 123.822\n",
      "    sample_time_ms: 32304.411\n",
      "    update_time_ms: 3.381\n",
      "  timestamp: 1667281149\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 400000\n",
      "  training_iteration: 100\n",
      "  trial_id: 168ff_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 8.2/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/9.6 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_3_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_0_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_5_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_4_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_2_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_1_400b3145f046e80a4a7d53925cb6fe08, 0.0/6.0 CPU_group_400b3145f046e80a4a7d53925cb6fe08)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                                      | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000 | RUNNING  | 192.168.1.65:16335 |    100 |          5752.04 | 400000 |  147.533 |              155.819 |              137.089 |                400 |\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000:\n",
      "  agent_timesteps_total: 404000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-11-01_00-40-07\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 155.81873951615546\n",
      "  episode_reward_mean: 147.54865785884692\n",
      "  episode_reward_min: 137.08918031704633\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1010\n",
      "  experiment_id: cf8409b3cbbc42979d4540c7e6fbf708\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 5.048709793414475e-30\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -0.06910505463602021\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.002479411471165357\n",
      "          policy_loss: 0.002516121510416269\n",
      "          total_loss: 356.25462582111356\n",
      "          vf_explained_var: -2.2351742678949904e-09\n",
      "          vf_loss: 356.25210840702056\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 404000\n",
      "    num_steps_sampled: 404000\n",
      "    num_steps_trained: 404000\n",
      "  iterations_since_restore: 101\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.125301204819277\n",
      "    ram_util_percent: 26.33373493975904\n",
      "  pid: 16335\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08570051600907272\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.476319469655163\n",
      "    mean_inference_ms: 1.7800185973857146\n",
      "    mean_raw_obs_processing_ms: 6.645249750200743\n",
      "  time_since_restore: 5809.902230978012\n",
      "  time_this_iter_s: 57.85831046104431\n",
      "  time_total_s: 5809.902230978012\n",
      "  timers:\n",
      "    learn_throughput: 155.969\n",
      "    learn_time_ms: 25646.155\n",
      "    sample_throughput: 123.826\n",
      "    sample_time_ms: 32303.452\n",
      "    update_time_ms: 3.381\n",
      "  timestamp: 1667281207\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 404000\n",
      "  training_iteration: 101\n",
      "  trial_id: 168ff_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 8.2/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/9.6 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_2_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_4_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_3_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_5_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_0_400b3145f046e80a4a7d53925cb6fe08, 0.0/6.0 CPU_group_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_1_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 accelerator_type:G)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                                      | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000 | RUNNING  | 192.168.1.65:16335 |    101 |           5809.9 | 404000 |  147.549 |              155.819 |              137.089 |                400 |\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000:\n",
      "  agent_timesteps_total: 408000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-11-01_00-41-05\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 155.81873951615546\n",
      "  episode_reward_mean: 147.74365412658034\n",
      "  episode_reward_min: 137.08918031704633\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1020\n",
      "  experiment_id: cf8409b3cbbc42979d4540c7e6fbf708\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.5243548967072376e-30\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -0.10596376517787576\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.005163172475113264\n",
      "          policy_loss: 0.0011561706036445686\n",
      "          total_loss: 357.02651925086974\n",
      "          vf_explained_var: 4.470348535789981e-09\n",
      "          vf_loss: 357.025360417366\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 408000\n",
      "    num_steps_sampled: 408000\n",
      "    num_steps_trained: 408000\n",
      "  iterations_since_restore: 102\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.973170731707317\n",
      "    ram_util_percent: 26.534146341463405\n",
      "  pid: 16335\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08570095063156355\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.475631747778134\n",
      "    mean_inference_ms: 1.7799838667235077\n",
      "    mean_raw_obs_processing_ms: 6.647324146340695\n",
      "  time_since_restore: 5867.206533908844\n",
      "  time_this_iter_s: 57.30430293083191\n",
      "  time_total_s: 5867.206533908844\n",
      "  timers:\n",
      "    learn_throughput: 155.938\n",
      "    learn_time_ms: 25651.294\n",
      "    sample_throughput: 123.807\n",
      "    sample_time_ms: 32308.276\n",
      "    update_time_ms: 3.381\n",
      "  timestamp: 1667281265\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 408000\n",
      "  training_iteration: 102\n",
      "  trial_id: 168ff_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Memory usage on this node: 8.3/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/9.6 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_5_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_3_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_2_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_1_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 accelerator_type:G, 0.0/6.0 CPU_group_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_4_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_0_400b3145f046e80a4a7d53925cb6fe08)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                                      | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000 | RUNNING  | 192.168.1.65:16335 |    102 |          5867.21 | 408000 |  147.744 |              155.819 |              137.089 |                400 |\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000:\n",
      "  agent_timesteps_total: 412000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-11-01_00-42-01\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 156.5496446206642\n",
      "  episode_reward_mean: 147.82098800465252\n",
      "  episode_reward_min: 137.08918031704633\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1030\n",
      "  experiment_id: cf8409b3cbbc42979d4540c7e6fbf708\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.2621774483536188e-30\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -0.15058295391499996\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.006989497939059702\n",
      "          policy_loss: -0.0006131056608865038\n",
      "          total_loss: 352.1960579872131\n",
      "          vf_explained_var: -1.0617076995345087e-08\n",
      "          vf_loss: 352.19667196273804\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 412000\n",
      "    num_steps_sampled: 412000\n",
      "    num_steps_trained: 412000\n",
      "  iterations_since_restore: 103\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.3725\n",
      "    ram_util_percent: 26.41625\n",
      "  pid: 16335\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08570126456003774\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.474915469827256\n",
      "    mean_inference_ms: 1.7799583833155834\n",
      "    mean_raw_obs_processing_ms: 6.649487951036884\n",
      "  time_since_restore: 5923.650617599487\n",
      "  time_this_iter_s: 56.44408369064331\n",
      "  time_total_s: 5923.650617599487\n",
      "  timers:\n",
      "    learn_throughput: 156.497\n",
      "    learn_time_ms: 25559.562\n",
      "    sample_throughput: 123.819\n",
      "    sample_time_ms: 32305.153\n",
      "    update_time_ms: 3.276\n",
      "  timestamp: 1667281321\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 412000\n",
      "  training_iteration: 103\n",
      "  trial_id: 168ff_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 8.3/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/9.6 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_1_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_2_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_4_400b3145f046e80a4a7d53925cb6fe08, 0.0/6.0 CPU_group_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_0_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_5_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_3_400b3145f046e80a4a7d53925cb6fe08)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                                      | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000 | RUNNING  | 192.168.1.65:16335 |    103 |          5923.65 | 412000 |  147.821 |               156.55 |              137.089 |                400 |\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000:\n",
      "  agent_timesteps_total: 416000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-11-01_00-42-59\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 156.5496446206642\n",
      "  episode_reward_mean: 147.85964035830614\n",
      "  episode_reward_min: 137.08918031704633\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1040\n",
      "  experiment_id: cf8409b3cbbc42979d4540c7e6fbf708\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 6.310887241768094e-31\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -0.16189896622672678\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008516971878998492\n",
      "          policy_loss: -0.0007940697105368599\n",
      "          total_loss: 354.2566484451294\n",
      "          vf_explained_var: -2.9802322831784522e-09\n",
      "          vf_loss: 354.25744289159775\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 416000\n",
      "    num_steps_sampled: 416000\n",
      "    num_steps_trained: 416000\n",
      "  iterations_since_restore: 104\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.112195121951217\n",
      "    ram_util_percent: 26.48048780487805\n",
      "  pid: 16335\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08570029462123842\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.474310609583455\n",
      "    mean_inference_ms: 1.7798512839587233\n",
      "    mean_raw_obs_processing_ms: 6.6517592149113405\n",
      "  time_since_restore: 5981.0313103199005\n",
      "  time_this_iter_s: 57.38069272041321\n",
      "  time_total_s: 5981.0313103199005\n",
      "  timers:\n",
      "    learn_throughput: 156.458\n",
      "    learn_time_ms: 25566.031\n",
      "    sample_throughput: 123.793\n",
      "    sample_time_ms: 32311.923\n",
      "    update_time_ms: 3.287\n",
      "  timestamp: 1667281379\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 416000\n",
      "  training_iteration: 104\n",
      "  trial_id: 168ff_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 8.2/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/9.6 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_0_400b3145f046e80a4a7d53925cb6fe08, 0.0/6.0 CPU_group_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_2_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_5_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_4_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_1_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_3_400b3145f046e80a4a7d53925cb6fe08)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                                      | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000 | RUNNING  | 192.168.1.65:16335 |    104 |          5981.03 | 416000 |   147.86 |               156.55 |              137.089 |                400 |\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000:\n",
      "  agent_timesteps_total: 420000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-11-01_00-43-56\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 156.5496446206642\n",
      "  episode_reward_mean: 147.53874377321935\n",
      "  episode_reward_min: 137.08918031704633\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1050\n",
      "  experiment_id: cf8409b3cbbc42979d4540c7e6fbf708\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 3.155443620884047e-31\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -0.18364025452174246\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01092195733327161\n",
      "          policy_loss: -0.0021749276274931617\n",
      "          total_loss: 354.14312732219696\n",
      "          vf_explained_var: -9.646881517255679e-05\n",
      "          vf_loss: 354.14529983997346\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 420000\n",
      "    num_steps_sampled: 420000\n",
      "    num_steps_trained: 420000\n",
      "  iterations_since_restore: 105\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.33012048192771\n",
      "    ram_util_percent: 26.650602409638548\n",
      "  pid: 16335\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08569923068220353\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.473715976579342\n",
      "    mean_inference_ms: 1.7799019034060712\n",
      "    mean_raw_obs_processing_ms: 6.646222515162503\n",
      "  time_since_restore: 6038.621712684631\n",
      "  time_this_iter_s: 57.590402364730835\n",
      "  time_total_s: 6038.621712684631\n",
      "  timers:\n",
      "    learn_throughput: 156.52\n",
      "    learn_time_ms: 25555.811\n",
      "    sample_throughput: 126.061\n",
      "    sample_time_ms: 31730.649\n",
      "    update_time_ms: 3.283\n",
      "  timestamp: 1667281436\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 420000\n",
      "  training_iteration: 105\n",
      "  trial_id: 168ff_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 8.3/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/9.6 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_2_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_5_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_0_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_3_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_1_400b3145f046e80a4a7d53925cb6fe08, 0.0/6.0 CPU_group_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_4_400b3145f046e80a4a7d53925cb6fe08)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                                      | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000 | RUNNING  | 192.168.1.65:16335 |    105 |          6038.62 | 420000 |  147.539 |               156.55 |              137.089 |                400 |\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000:\n",
      "  agent_timesteps_total: 424000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-11-01_00-44-54\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 156.5496446206642\n",
      "  episode_reward_mean: 147.68816761920073\n",
      "  episode_reward_min: 137.08918031704633\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1060\n",
      "  experiment_id: cf8409b3cbbc42979d4540c7e6fbf708\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 3.155443620884047e-31\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -0.19665143322199582\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.006840179024254667\n",
      "          policy_loss: 0.0006191630243847612\n",
      "          total_loss: 357.1071213722229\n",
      "          vf_explained_var: -3.166496842510469e-09\n",
      "          vf_loss: 357.10650262832644\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 424000\n",
      "    num_steps_sampled: 424000\n",
      "    num_steps_trained: 424000\n",
      "  iterations_since_restore: 106\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.252439024390245\n",
      "    ram_util_percent: 26.49756097560975\n",
      "  pid: 16335\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08569845809273384\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.472988278489282\n",
      "    mean_inference_ms: 1.7800651221121142\n",
      "    mean_raw_obs_processing_ms: 6.640880241114899\n",
      "  time_since_restore: 6096.010209321976\n",
      "  time_this_iter_s: 57.38849663734436\n",
      "  time_total_s: 6096.010209321976\n",
      "  timers:\n",
      "    learn_throughput: 156.487\n",
      "    learn_time_ms: 25561.21\n",
      "    sample_throughput: 126.078\n",
      "    sample_time_ms: 31726.28\n",
      "    update_time_ms: 3.209\n",
      "  timestamp: 1667281494\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 424000\n",
      "  training_iteration: 106\n",
      "  trial_id: 168ff_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 8.3/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/9.6 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_3_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_1_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_5_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_0_400b3145f046e80a4a7d53925cb6fe08, 0.0/6.0 CPU_group_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_4_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_2_400b3145f046e80a4a7d53925cb6fe08)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                                      | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000 | RUNNING  | 192.168.1.65:16335 |    106 |          6096.01 | 424000 |  147.688 |               156.55 |              137.089 |                400 |\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=16332)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=16330)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=16334)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=16331)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=16333)\u001b[0m  Retrying in 1 seconds\n",
      "Result for PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000:\n",
      "  agent_timesteps_total: 428000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-11-01_00-45-52\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 156.5496446206642\n",
      "  episode_reward_mean: 147.84737717215228\n",
      "  episode_reward_min: 137.08918031704633\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1070\n",
      "  experiment_id: cf8409b3cbbc42979d4540c7e6fbf708\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5777218104420235e-31\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -0.21651341035030783\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0023033270074678037\n",
      "          policy_loss: 0.000303343337145634\n",
      "          total_loss: 354.628346991539\n",
      "          vf_explained_var: -2.0489097085629737e-09\n",
      "          vf_loss: 354.6280447006226\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 428000\n",
      "    num_steps_sampled: 428000\n",
      "    num_steps_trained: 428000\n",
      "  iterations_since_restore: 107\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.806024096385542\n",
      "    ram_util_percent: 26.516867469879518\n",
      "  pid: 16335\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08569726250211121\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.472293284051176\n",
      "    mean_inference_ms: 1.7801853075582457\n",
      "    mean_raw_obs_processing_ms: 6.636710455868974\n",
      "  time_since_restore: 6154.106941699982\n",
      "  time_this_iter_s: 58.09673237800598\n",
      "  time_total_s: 6154.106941699982\n",
      "  timers:\n",
      "    learn_throughput: 156.357\n",
      "    learn_time_ms: 25582.475\n",
      "    sample_throughput: 125.714\n",
      "    sample_time_ms: 31818.295\n",
      "    update_time_ms: 3.206\n",
      "  timestamp: 1667281552\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 428000\n",
      "  training_iteration: 107\n",
      "  trial_id: 168ff_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Memory usage on this node: 8.3/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/9.6 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_4_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_1_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_5_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_2_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_0_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_3_400b3145f046e80a4a7d53925cb6fe08, 0.0/6.0 CPU_group_400b3145f046e80a4a7d53925cb6fe08)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                                      | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000 | RUNNING  | 192.168.1.65:16335 |    107 |          6154.11 | 428000 |  147.847 |               156.55 |              137.089 |                400 |\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000:\n",
      "  agent_timesteps_total: 432000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-11-01_00-46-50\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 156.5496446206642\n",
      "  episode_reward_mean: 148.04026085750792\n",
      "  episode_reward_min: 137.08918031704633\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1080\n",
      "  experiment_id: cf8409b3cbbc42979d4540c7e6fbf708\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 7.888609052210117e-32\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -0.21652247100137173\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.00605614223259181\n",
      "          policy_loss: -0.0031966365466360004\n",
      "          total_loss: 354.77820789813995\n",
      "          vf_explained_var: -7.483623630832881e-05\n",
      "          vf_loss: 354.78140338659284\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 432000\n",
      "    num_steps_sampled: 432000\n",
      "    num_steps_trained: 432000\n",
      "  iterations_since_restore: 108\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.19146341463415\n",
      "    ram_util_percent: 26.487804878048774\n",
      "  pid: 16335\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08569563485111635\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.4715999945167\n",
      "    mean_inference_ms: 1.7803181030970618\n",
      "    mean_raw_obs_processing_ms: 6.632596098032632\n",
      "  time_since_restore: 6211.781040906906\n",
      "  time_this_iter_s: 57.67409920692444\n",
      "  time_total_s: 6211.781040906906\n",
      "  timers:\n",
      "    learn_throughput: 156.104\n",
      "    learn_time_ms: 25623.971\n",
      "    sample_throughput: 125.706\n",
      "    sample_time_ms: 31820.221\n",
      "    update_time_ms: 3.252\n",
      "  timestamp: 1667281610\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 432000\n",
      "  training_iteration: 108\n",
      "  trial_id: 168ff_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 8.3/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/9.6 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_2_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_5_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_4_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 accelerator_type:G, 0.0/6.0 CPU_group_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_3_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_1_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_0_400b3145f046e80a4a7d53925cb6fe08)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                                      | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000 | RUNNING  | 192.168.1.65:16335 |    108 |          6211.78 | 432000 |   148.04 |               156.55 |              137.089 |                400 |\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000:\n",
      "  agent_timesteps_total: 436000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-11-01_00-47-47\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 156.58733734826907\n",
      "  episode_reward_mean: 148.43310798072406\n",
      "  episode_reward_min: 138.54927992639057\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1090\n",
      "  experiment_id: cf8409b3cbbc42979d4540c7e6fbf708\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 3.9443045261050587e-32\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -0.1852879394777119\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0033343789091987014\n",
      "          policy_loss: 0.0031082499132025986\n",
      "          total_loss: 360.98545837402344\n",
      "          vf_explained_var: -2.9802322831784522e-09\n",
      "          vf_loss: 360.98235216140745\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 436000\n",
      "    num_steps_sampled: 436000\n",
      "    num_steps_trained: 436000\n",
      "  iterations_since_restore: 109\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.112195121951224\n",
      "    ram_util_percent: 26.54634146341463\n",
      "  pid: 16335\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0856932269461139\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.47091686549797\n",
      "    mean_inference_ms: 1.78044511369613\n",
      "    mean_raw_obs_processing_ms: 6.628615963672343\n",
      "  time_since_restore: 6269.076303720474\n",
      "  time_this_iter_s: 57.295262813568115\n",
      "  time_total_s: 6269.076303720474\n",
      "  timers:\n",
      "    learn_throughput: 156.175\n",
      "    learn_time_ms: 25612.253\n",
      "    sample_throughput: 125.693\n",
      "    sample_time_ms: 31823.456\n",
      "    update_time_ms: 3.234\n",
      "  timestamp: 1667281667\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 436000\n",
      "  training_iteration: 109\n",
      "  trial_id: 168ff_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 8.3/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/9.6 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_1_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_3_400b3145f046e80a4a7d53925cb6fe08, 0.0/6.0 CPU_group_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_2_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_0_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_4_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_5_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 accelerator_type:G)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                                      | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000 | RUNNING  | 192.168.1.65:16335 |    109 |          6269.08 | 436000 |  148.433 |              156.587 |              138.549 |                400 |\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000:\n",
      "  agent_timesteps_total: 440000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-11-01_00-48-45\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 156.58733734826907\n",
      "  episode_reward_mean: 148.32654660903586\n",
      "  episode_reward_min: 138.54927992639057\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1100\n",
      "  experiment_id: cf8409b3cbbc42979d4540c7e6fbf708\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.9721522630525293e-32\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -0.18475290019996465\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.007320219554202716\n",
      "          policy_loss: 0.001250330577022396\n",
      "          total_loss: 359.1224870920181\n",
      "          vf_explained_var: -5.215406329028838e-09\n",
      "          vf_loss: 359.1212369918823\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 440000\n",
      "    num_steps_sampled: 440000\n",
      "    num_steps_trained: 440000\n",
      "  iterations_since_restore: 110\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.136585365853662\n",
      "    ram_util_percent: 26.60853658536585\n",
      "  pid: 16335\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08569184908277744\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.470245476979752\n",
      "    mean_inference_ms: 1.7805546253186428\n",
      "    mean_raw_obs_processing_ms: 6.624722941112971\n",
      "  time_since_restore: 6326.499560594559\n",
      "  time_this_iter_s: 57.42325687408447\n",
      "  time_total_s: 6326.499560594559\n",
      "  timers:\n",
      "    learn_throughput: 156.18\n",
      "    learn_time_ms: 25611.425\n",
      "    sample_throughput: 125.708\n",
      "    sample_time_ms: 31819.757\n",
      "    update_time_ms: 3.216\n",
      "  timestamp: 1667281725\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 440000\n",
      "  training_iteration: 110\n",
      "  trial_id: 168ff_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 8.3/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/9.6 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_2_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_4_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_3_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_0_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_5_400b3145f046e80a4a7d53925cb6fe08, 0.0/6.0 CPU_group_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_1_400b3145f046e80a4a7d53925cb6fe08)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                                      | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000 | RUNNING  | 192.168.1.65:16335 |    110 |           6326.5 | 440000 |  148.327 |              156.587 |              138.549 |                400 |\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000:\n",
      "  agent_timesteps_total: 444000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-11-01_00-49-42\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 156.58733734826907\n",
      "  episode_reward_mean: 147.90954567731396\n",
      "  episode_reward_min: 138.54927992639057\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1110\n",
      "  experiment_id: cf8409b3cbbc42979d4540c7e6fbf708\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 9.860761315262647e-33\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -0.23732430203817784\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.004765836281285418\n",
      "          policy_loss: -0.0014066228439332917\n",
      "          total_loss: 351.12561376094817\n",
      "          vf_explained_var: -5.401671110405459e-09\n",
      "          vf_loss: 351.12702264785764\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 444000\n",
      "    num_steps_sampled: 444000\n",
      "    num_steps_trained: 444000\n",
      "  iterations_since_restore: 111\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.897560975609753\n",
      "    ram_util_percent: 26.692682926829278\n",
      "  pid: 16335\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08569122825356443\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.469627875354035\n",
      "    mean_inference_ms: 1.7806105408716209\n",
      "    mean_raw_obs_processing_ms: 6.620886136975172\n",
      "  time_since_restore: 6383.4527995586395\n",
      "  time_this_iter_s: 56.95323896408081\n",
      "  time_total_s: 6383.4527995586395\n",
      "  timers:\n",
      "    learn_throughput: 156.692\n",
      "    learn_time_ms: 25527.854\n",
      "    sample_throughput: 125.736\n",
      "    sample_time_ms: 31812.746\n",
      "    update_time_ms: 3.236\n",
      "  timestamp: 1667281782\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 444000\n",
      "  training_iteration: 111\n",
      "  trial_id: 168ff_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 8.3/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/9.6 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_4_400b3145f046e80a4a7d53925cb6fe08, 0.0/6.0 CPU_group_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_3_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_5_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_0_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_2_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_1_400b3145f046e80a4a7d53925cb6fe08)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                                      | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000 | RUNNING  | 192.168.1.65:16335 |    111 |          6383.45 | 444000 |   147.91 |              156.587 |              138.549 |                400 |\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000:\n",
      "  agent_timesteps_total: 448000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-11-01_00-50-39\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 156.58733734826907\n",
      "  episode_reward_mean: 148.0142977560462\n",
      "  episode_reward_min: 138.54927992639057\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1120\n",
      "  experiment_id: cf8409b3cbbc42979d4540c7e6fbf708\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 4.9303806576313234e-33\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -0.2713899936527014\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011114256726523308\n",
      "          policy_loss: -0.000713481217098888\n",
      "          total_loss: 358.6805615901947\n",
      "          vf_explained_var: -4.6566128730773926e-09\n",
      "          vf_loss: 358.6812745332718\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 448000\n",
      "    num_steps_sampled: 448000\n",
      "    num_steps_trained: 448000\n",
      "  iterations_since_restore: 112\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.990243902439026\n",
      "    ram_util_percent: 26.687804878048787\n",
      "  pid: 16335\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0856912537718439\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.469051679105572\n",
      "    mean_inference_ms: 1.780652699277277\n",
      "    mean_raw_obs_processing_ms: 6.617066082949471\n",
      "  time_since_restore: 6440.7810707092285\n",
      "  time_this_iter_s: 57.32827115058899\n",
      "  time_total_s: 6440.7810707092285\n",
      "  timers:\n",
      "    learn_throughput: 156.624\n",
      "    learn_time_ms: 25538.866\n",
      "    sample_throughput: 125.77\n",
      "    sample_time_ms: 31804.186\n",
      "    update_time_ms: 3.235\n",
      "  timestamp: 1667281839\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 448000\n",
      "  training_iteration: 112\n",
      "  trial_id: 168ff_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Memory usage on this node: 8.3/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/9.6 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_3_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_5_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_1_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_0_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_4_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_2_400b3145f046e80a4a7d53925cb6fe08, 0.0/6.0 CPU_group_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 accelerator_type:G)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                                      | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000 | RUNNING  | 192.168.1.65:16335 |    112 |          6440.78 | 448000 |  148.014 |              156.587 |              138.549 |                400 |\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000:\n",
      "  agent_timesteps_total: 452000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-11-01_00-51-37\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 156.58733734826907\n",
      "  episode_reward_mean: 147.97262651391057\n",
      "  episode_reward_min: 138.54927992639057\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1130\n",
      "  experiment_id: cf8409b3cbbc42979d4540c7e6fbf708\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 4.9303806576313234e-33\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -0.3458274482749403\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.007617887093601894\n",
      "          policy_loss: -2.297469472978264e-05\n",
      "          total_loss: 355.45657033920287\n",
      "          vf_explained_var: -3.911554635749326e-09\n",
      "          vf_loss: 355.4565906524658\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 452000\n",
      "    num_steps_sampled: 452000\n",
      "    num_steps_trained: 452000\n",
      "  iterations_since_restore: 113\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.07439024390244\n",
      "    ram_util_percent: 26.625609756097557\n",
      "  pid: 16335\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08569131518148454\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.468494083050654\n",
      "    mean_inference_ms: 1.7806842678087669\n",
      "    mean_raw_obs_processing_ms: 6.613347462151764\n",
      "  time_since_restore: 6498.450393438339\n",
      "  time_this_iter_s: 57.66932272911072\n",
      "  time_total_s: 6498.450393438339\n",
      "  timers:\n",
      "    learn_throughput: 155.897\n",
      "    learn_time_ms: 25657.985\n",
      "    sample_throughput: 125.756\n",
      "    sample_time_ms: 31807.683\n",
      "    update_time_ms: 3.254\n",
      "  timestamp: 1667281897\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 452000\n",
      "  training_iteration: 113\n",
      "  trial_id: 168ff_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 8.3/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/9.6 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_4_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 accelerator_type:G, 0.0/6.0 CPU_group_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_2_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_1_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_3_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_0_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_5_400b3145f046e80a4a7d53925cb6fe08)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                                      | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000 | RUNNING  | 192.168.1.65:16335 |    113 |          6498.45 | 452000 |  147.973 |              156.587 |              138.549 |                400 |\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000:\n",
      "  agent_timesteps_total: 456000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-11-01_00-52-35\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 156.58733734826907\n",
      "  episode_reward_mean: 148.0593036775882\n",
      "  episode_reward_min: 138.54927992639057\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1140\n",
      "  experiment_id: cf8409b3cbbc42979d4540c7e6fbf708\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.4651903288156617e-33\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -0.368597106076777\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.005914117015231609\n",
      "          policy_loss: 0.0013569356538937426\n",
      "          total_loss: 359.29901802539825\n",
      "          vf_explained_var: -6.891787140972383e-09\n",
      "          vf_loss: 359.29766058921814\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 456000\n",
      "    num_steps_sampled: 456000\n",
      "    num_steps_trained: 456000\n",
      "  iterations_since_restore: 114\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.23292682926829\n",
      "    ram_util_percent: 26.660975609756107\n",
      "  pid: 16335\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08569196953377006\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.467918368593473\n",
      "    mean_inference_ms: 1.7807642582074177\n",
      "    mean_raw_obs_processing_ms: 6.609667947724128\n",
      "  time_since_restore: 6555.9659950733185\n",
      "  time_this_iter_s: 57.51560163497925\n",
      "  time_total_s: 6555.9659950733185\n",
      "  timers:\n",
      "    learn_throughput: 155.808\n",
      "    learn_time_ms: 25672.613\n",
      "    sample_throughput: 125.76\n",
      "    sample_time_ms: 31806.573\n",
      "    update_time_ms: 3.254\n",
      "  timestamp: 1667281955\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 456000\n",
      "  training_iteration: 114\n",
      "  trial_id: 168ff_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 8.3/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/9.6 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_0_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_5_400b3145f046e80a4a7d53925cb6fe08, 0.0/6.0 CPU_group_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_4_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_3_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_1_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_2_400b3145f046e80a4a7d53925cb6fe08)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                                      | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000 | RUNNING  | 192.168.1.65:16335 |    114 |          6555.97 | 456000 |  148.059 |              156.587 |              138.549 |                400 |\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000:\n",
      "  agent_timesteps_total: 460000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-11-01_00-53-32\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 156.58733734826907\n",
      "  episode_reward_mean: 148.22953729690838\n",
      "  episode_reward_min: 139.5175890323541\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1150\n",
      "  experiment_id: cf8409b3cbbc42979d4540c7e6fbf708\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.2325951644078308e-33\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -0.3639932959340513\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.006848747910044216\n",
      "          policy_loss: 0.001342917318106629\n",
      "          total_loss: 354.8346671581268\n",
      "          vf_explained_var: -4.183053897577338e-05\n",
      "          vf_loss: 354.83332307338713\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 460000\n",
      "    num_steps_sampled: 460000\n",
      "    num_steps_trained: 460000\n",
      "  iterations_since_restore: 115\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.21341463414634\n",
      "    ram_util_percent: 26.66463414634146\n",
      "  pid: 16335\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08569290113587778\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.46735094602004\n",
      "    mean_inference_ms: 1.7806999949063411\n",
      "    mean_raw_obs_processing_ms: 6.606107911004293\n",
      "  time_since_restore: 6613.020088434219\n",
      "  time_this_iter_s: 57.05409336090088\n",
      "  time_total_s: 6613.020088434219\n",
      "  timers:\n",
      "    learn_throughput: 156.075\n",
      "    learn_time_ms: 25628.732\n",
      "    sample_throughput: 125.799\n",
      "    sample_time_ms: 31796.744\n",
      "    update_time_ms: 3.251\n",
      "  timestamp: 1667282012\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 460000\n",
      "  training_iteration: 115\n",
      "  trial_id: 168ff_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 8.3/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/9.6 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_1_400b3145f046e80a4a7d53925cb6fe08, 0.0/6.0 CPU_group_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_4_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_0_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_2_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_5_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_3_400b3145f046e80a4a7d53925cb6fe08)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                                      | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000 | RUNNING  | 192.168.1.65:16335 |    115 |          6613.02 | 460000 |   148.23 |              156.587 |              139.518 |                400 |\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000:\n",
      "  agent_timesteps_total: 464000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-11-01_00-54-30\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 156.58733734826907\n",
      "  episode_reward_mean: 148.145833146719\n",
      "  episode_reward_min: 139.5175890323541\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1160\n",
      "  experiment_id: cf8409b3cbbc42979d4540c7e6fbf708\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 6.162975822039154e-34\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -0.3904296990483999\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010527691740955535\n",
      "          policy_loss: -0.003041535022202879\n",
      "          total_loss: 355.34555251598357\n",
      "          vf_explained_var: -5.587935669737476e-10\n",
      "          vf_loss: 355.34859293699265\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 464000\n",
      "    num_steps_sampled: 464000\n",
      "    num_steps_trained: 464000\n",
      "  iterations_since_restore: 116\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.25609756097561\n",
      "    ram_util_percent: 26.715853658536588\n",
      "  pid: 16335\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08569333960622628\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.46683119481617\n",
      "    mean_inference_ms: 1.780602914296203\n",
      "    mean_raw_obs_processing_ms: 6.602653241410448\n",
      "  time_since_restore: 6670.551797866821\n",
      "  time_this_iter_s: 57.53170943260193\n",
      "  time_total_s: 6670.551797866821\n",
      "  timers:\n",
      "    learn_throughput: 156.049\n",
      "    learn_time_ms: 25633.029\n",
      "    sample_throughput: 125.759\n",
      "    sample_time_ms: 31806.791\n",
      "    update_time_ms: 3.246\n",
      "  timestamp: 1667282070\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 464000\n",
      "  training_iteration: 116\n",
      "  trial_id: 168ff_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 8.4/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/9.6 GiB heap, 0.0/14.65 GiB objects (0.0/6.0 CPU_group_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_0_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_4_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_5_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_3_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_2_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_1_400b3145f046e80a4a7d53925cb6fe08)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                                      | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000 | RUNNING  | 192.168.1.65:16335 |    116 |          6670.55 | 464000 |  148.146 |              156.587 |              139.518 |                400 |\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000:\n",
      "  agent_timesteps_total: 468000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-11-01_00-55-27\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 156.58733734826907\n",
      "  episode_reward_mean: 147.95457698051115\n",
      "  episode_reward_min: 139.5175890323541\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1170\n",
      "  experiment_id: cf8409b3cbbc42979d4540c7e6fbf708\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 6.162975822039154e-34\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -0.40404461696743965\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0054130133253238845\n",
      "          policy_loss: 0.001720225453027524\n",
      "          total_loss: 358.06642327308657\n",
      "          vf_explained_var: -3.911554635749326e-09\n",
      "          vf_loss: 358.0647022485733\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 468000\n",
      "    num_steps_sampled: 468000\n",
      "    num_steps_trained: 468000\n",
      "  iterations_since_restore: 117\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.331707317073167\n",
      "    ram_util_percent: 26.917073170731705\n",
      "  pid: 16335\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08569535976576574\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.466280022524305\n",
      "    mean_inference_ms: 1.7805534995719308\n",
      "    mean_raw_obs_processing_ms: 6.598198860605376\n",
      "  time_since_restore: 6727.569625616074\n",
      "  time_this_iter_s: 57.01782774925232\n",
      "  time_total_s: 6727.569625616074\n",
      "  timers:\n",
      "    learn_throughput: 156.163\n",
      "    learn_time_ms: 25614.276\n",
      "    sample_throughput: 126.113\n",
      "    sample_time_ms: 31717.636\n",
      "    update_time_ms: 3.275\n",
      "  timestamp: 1667282127\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 468000\n",
      "  training_iteration: 117\n",
      "  trial_id: 168ff_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Memory usage on this node: 8.4/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/9.6 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_0_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_4_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_3_400b3145f046e80a4a7d53925cb6fe08, 0.0/6.0 CPU_group_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_1_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_5_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_2_400b3145f046e80a4a7d53925cb6fe08)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                                      | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000 | RUNNING  | 192.168.1.65:16335 |    117 |          6727.57 | 468000 |  147.955 |              156.587 |              139.518 |                400 |\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000:\n",
      "  agent_timesteps_total: 472000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-11-01_00-56-24\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 156.58733734826907\n",
      "  episode_reward_mean: 147.85918299245495\n",
      "  episode_reward_min: 139.5175890323541\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1180\n",
      "  experiment_id: cf8409b3cbbc42979d4540c7e6fbf708\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 3.081487911019577e-34\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -0.3776958707720041\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.002375313988295602\n",
      "          policy_loss: 0.0013261378247989342\n",
      "          total_loss: 360.32364609241483\n",
      "          vf_explained_var: -7.450580707946131e-10\n",
      "          vf_loss: 360.32232184410094\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 472000\n",
      "    num_steps_sampled: 472000\n",
      "    num_steps_trained: 472000\n",
      "  iterations_since_restore: 118\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.45555555555556\n",
      "    ram_util_percent: 26.745679012345686\n",
      "  pid: 16335\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08569721331699753\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.465754779163873\n",
      "    mean_inference_ms: 1.780493083807406\n",
      "    mean_raw_obs_processing_ms: 6.593845377075414\n",
      "  time_since_restore: 6784.4120626449585\n",
      "  time_this_iter_s: 56.84243702888489\n",
      "  time_total_s: 6784.4120626449585\n",
      "  timers:\n",
      "    learn_throughput: 156.677\n",
      "    learn_time_ms: 25530.181\n",
      "    sample_throughput: 126.109\n",
      "    sample_time_ms: 31718.669\n",
      "    update_time_ms: 3.199\n",
      "  timestamp: 1667282184\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 472000\n",
      "  training_iteration: 118\n",
      "  trial_id: 168ff_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 8.3/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/9.6 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_2_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_0_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_4_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_1_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_3_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_5_400b3145f046e80a4a7d53925cb6fe08, 0.0/6.0 CPU_group_400b3145f046e80a4a7d53925cb6fe08)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                                      | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000 | RUNNING  | 192.168.1.65:16335 |    118 |          6784.41 | 472000 |  147.859 |              156.587 |              139.518 |                400 |\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=16331)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=16330)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=16334)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=16333)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=16332)\u001b[0m  Retrying in 1 seconds\n",
      "Result for PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000:\n",
      "  agent_timesteps_total: 476000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-11-01_00-57-22\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 156.21777378803435\n",
      "  episode_reward_mean: 147.7146474731588\n",
      "  episode_reward_min: 139.5175890323541\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1190\n",
      "  experiment_id: cf8409b3cbbc42979d4540c7e6fbf708\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5407439555097886e-34\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -0.4318531963042915\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0068267105902123145\n",
      "          policy_loss: 0.000756086700130254\n",
      "          total_loss: 355.17736902236936\n",
      "          vf_explained_var: 7.450580707946131e-10\n",
      "          vf_loss: 355.176611661911\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 476000\n",
      "    num_steps_sampled: 476000\n",
      "    num_steps_trained: 476000\n",
      "  iterations_since_restore: 119\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.949397590361446\n",
      "    ram_util_percent: 26.72409638554218\n",
      "  pid: 16335\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08569994471032784\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.465224293914307\n",
      "    mean_inference_ms: 1.7804656696677026\n",
      "    mean_raw_obs_processing_ms: 6.590636342648329\n",
      "  time_since_restore: 6842.642774581909\n",
      "  time_this_iter_s: 58.230711936950684\n",
      "  time_total_s: 6842.642774581909\n",
      "  timers:\n",
      "    learn_throughput: 156.752\n",
      "    learn_time_ms: 25518.088\n",
      "    sample_throughput: 125.69\n",
      "    sample_time_ms: 31824.378\n",
      "    update_time_ms: 3.132\n",
      "  timestamp: 1667282242\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 476000\n",
      "  training_iteration: 119\n",
      "  trial_id: 168ff_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 8.3/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/9.6 GiB heap, 0.0/14.65 GiB objects (0.0/6.0 CPU_group_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_0_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_1_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_5_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_3_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_2_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_4_400b3145f046e80a4a7d53925cb6fe08)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                                      | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000 | RUNNING  | 192.168.1.65:16335 |    119 |          6842.64 | 476000 |  147.715 |              156.218 |              139.518 |                400 |\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000:\n",
      "  agent_timesteps_total: 480000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-11-01_00-58-19\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 156.21777378803435\n",
      "  episode_reward_mean: 147.86552042087285\n",
      "  episode_reward_min: 139.00701267355976\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1200\n",
      "  experiment_id: cf8409b3cbbc42979d4540c7e6fbf708\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 7.703719777548943e-35\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -0.45102497963234783\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0036297497101601016\n",
      "          policy_loss: 0.0013470711623085664\n",
      "          total_loss: 358.86028306484224\n",
      "          vf_explained_var: -3.3527611797978807e-09\n",
      "          vf_loss: 358.8589351892471\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 480000\n",
      "    num_steps_sampled: 480000\n",
      "    num_steps_trained: 480000\n",
      "  iterations_since_restore: 120\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.207317073170735\n",
      "    ram_util_percent: 26.931707317073165\n",
      "  pid: 16335\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0857023746670535\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.46468710564591\n",
      "    mean_inference_ms: 1.7804446005448598\n",
      "    mean_raw_obs_processing_ms: 6.587509294383988\n",
      "  time_since_restore: 6899.788294553757\n",
      "  time_this_iter_s: 57.145519971847534\n",
      "  time_total_s: 6899.788294553757\n",
      "  timers:\n",
      "    learn_throughput: 156.951\n",
      "    learn_time_ms: 25485.717\n",
      "    sample_throughput: 125.672\n",
      "    sample_time_ms: 31828.951\n",
      "    update_time_ms: 3.11\n",
      "  timestamp: 1667282299\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 480000\n",
      "  training_iteration: 120\n",
      "  trial_id: 168ff_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 8.4/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/9.6 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_1_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_4_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_0_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_3_400b3145f046e80a4a7d53925cb6fe08, 0.0/6.0 CPU_group_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_5_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_2_400b3145f046e80a4a7d53925cb6fe08)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                                      | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000 | RUNNING  | 192.168.1.65:16335 |    120 |          6899.79 | 480000 |  147.866 |              156.218 |              139.007 |                400 |\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000:\n",
      "  agent_timesteps_total: 484000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-11-01_00-59-16\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 156.21777378803435\n",
      "  episode_reward_mean: 148.30448818781818\n",
      "  episode_reward_min: 139.00701267355976\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1210\n",
      "  experiment_id: cf8409b3cbbc42979d4540c7e6fbf708\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 3.8518598887744714e-35\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -0.4528998830355704\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.006017029464283086\n",
      "          policy_loss: 0.0016508600558154285\n",
      "          total_loss: 359.37953593730924\n",
      "          vf_explained_var: -2.3167021936387755e-05\n",
      "          vf_loss: 359.37788565158843\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 484000\n",
      "    num_steps_sampled: 484000\n",
      "    num_steps_trained: 484000\n",
      "  iterations_since_restore: 121\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.471604938271607\n",
      "    ram_util_percent: 26.901234567901234\n",
      "  pid: 16335\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08570484448689573\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.46414539434157\n",
      "    mean_inference_ms: 1.780457664127853\n",
      "    mean_raw_obs_processing_ms: 6.584423769444588\n",
      "  time_since_restore: 6956.75677895546\n",
      "  time_this_iter_s: 56.96848440170288\n",
      "  time_total_s: 6956.75677895546\n",
      "  timers:\n",
      "    learn_throughput: 156.915\n",
      "    learn_time_ms: 25491.456\n",
      "    sample_throughput: 125.688\n",
      "    sample_time_ms: 31824.732\n",
      "    update_time_ms: 3.089\n",
      "  timestamp: 1667282356\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 484000\n",
      "  training_iteration: 121\n",
      "  trial_id: 168ff_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 8.4/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/9.6 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_4_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_5_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_1_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_0_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_2_400b3145f046e80a4a7d53925cb6fe08, 0.0/6.0 CPU_group_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_3_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 accelerator_type:G)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                                      | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000 | RUNNING  | 192.168.1.65:16335 |    121 |          6956.76 | 484000 |  148.304 |              156.218 |              139.007 |                400 |\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000:\n",
      "  agent_timesteps_total: 488000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-11-01_01-00-14\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 156.21777378803435\n",
      "  episode_reward_mean: 148.0796996818329\n",
      "  episode_reward_min: 139.00701267355976\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1220\n",
      "  experiment_id: cf8409b3cbbc42979d4540c7e6fbf708\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.9259299443872357e-35\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -0.47388016935437915\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.00574707047471752\n",
      "          policy_loss: 7.944302051328122e-05\n",
      "          total_loss: 354.6315140485764\n",
      "          vf_explained_var: -3.3527611797978807e-09\n",
      "          vf_loss: 354.6314368486404\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 488000\n",
      "    num_steps_sampled: 488000\n",
      "    num_steps_trained: 488000\n",
      "  iterations_since_restore: 122\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.965060240963858\n",
      "    ram_util_percent: 26.937349397590356\n",
      "  pid: 16335\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08570709100659535\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.463557547399258\n",
      "    mean_inference_ms: 1.7805240097462038\n",
      "    mean_raw_obs_processing_ms: 6.5814601043388645\n",
      "  time_since_restore: 7014.180199146271\n",
      "  time_this_iter_s: 57.42342019081116\n",
      "  time_total_s: 7014.180199146271\n",
      "  timers:\n",
      "    learn_throughput: 156.869\n",
      "    learn_time_ms: 25498.904\n",
      "    sample_throughput: 125.68\n",
      "    sample_time_ms: 31826.842\n",
      "    update_time_ms: 3.092\n",
      "  timestamp: 1667282414\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 488000\n",
      "  training_iteration: 122\n",
      "  trial_id: 168ff_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Memory usage on this node: 8.4/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/9.6 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_3_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_0_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_2_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_4_400b3145f046e80a4a7d53925cb6fe08, 0.0/6.0 CPU_group_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_1_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_5_400b3145f046e80a4a7d53925cb6fe08)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                                      | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000 | RUNNING  | 192.168.1.65:16335 |    122 |          7014.18 | 488000 |   148.08 |              156.218 |              139.007 |                400 |\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000:\n",
      "  agent_timesteps_total: 492000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-11-01_01-01-11\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 156.21777378803435\n",
      "  episode_reward_mean: 147.89147080231393\n",
      "  episode_reward_min: 139.00701267355976\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1230\n",
      "  experiment_id: cf8409b3cbbc42979d4540c7e6fbf708\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 9.629649721936178e-36\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -0.5136209807358683\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009297930825869294\n",
      "          policy_loss: -0.0007752373901894316\n",
      "          total_loss: 355.59724164009094\n",
      "          vf_explained_var: -1.67785219673533e-05\n",
      "          vf_loss: 355.59801721572876\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 492000\n",
      "    num_steps_sampled: 492000\n",
      "    num_steps_trained: 492000\n",
      "  iterations_since_restore: 123\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.482499999999998\n",
      "    ram_util_percent: 26.959999999999997\n",
      "  pid: 16335\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08570874563430461\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.463001044225138\n",
      "    mean_inference_ms: 1.7805852953777253\n",
      "    mean_raw_obs_processing_ms: 6.578528886569278\n",
      "  time_since_restore: 7070.718039035797\n",
      "  time_this_iter_s: 56.53783988952637\n",
      "  time_total_s: 7070.718039035797\n",
      "  timers:\n",
      "    learn_throughput: 157.563\n",
      "    learn_time_ms: 25386.673\n",
      "    sample_throughput: 125.684\n",
      "    sample_time_ms: 31825.796\n",
      "    update_time_ms: 3.089\n",
      "  timestamp: 1667282471\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 492000\n",
      "  training_iteration: 123\n",
      "  trial_id: 168ff_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 8.4/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/9.6 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_5_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_4_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_3_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 accelerator_type:G, 0.0/6.0 CPU_group_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_1_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_0_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_2_400b3145f046e80a4a7d53925cb6fe08)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                                      | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000 | RUNNING  | 192.168.1.65:16335 |    123 |          7070.72 | 492000 |  147.891 |              156.218 |              139.007 |                400 |\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000:\n",
      "  agent_timesteps_total: 496000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-11-01_01-02-08\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 156.21777378803435\n",
      "  episode_reward_mean: 147.58324508211552\n",
      "  episode_reward_min: 139.00701267355976\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1240\n",
      "  experiment_id: cf8409b3cbbc42979d4540c7e6fbf708\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 4.814824860968089e-36\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -0.5349042803049088\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.003273983756525922\n",
      "          policy_loss: 0.000460474009742029\n",
      "          total_loss: 354.3715278625488\n",
      "          vf_explained_var: 5.587935669737476e-10\n",
      "          vf_loss: 354.371066570282\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 496000\n",
      "    num_steps_sampled: 496000\n",
      "    num_steps_trained: 496000\n",
      "  iterations_since_restore: 124\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.242682926829268\n",
      "    ram_util_percent: 26.992682926829268\n",
      "  pid: 16335\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08571027911538205\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.46242754084536\n",
      "    mean_inference_ms: 1.7806499768507367\n",
      "    mean_raw_obs_processing_ms: 6.575686626669742\n",
      "  time_since_restore: 7127.9837391376495\n",
      "  time_this_iter_s: 57.26570010185242\n",
      "  time_total_s: 7127.9837391376495\n",
      "  timers:\n",
      "    learn_throughput: 157.719\n",
      "    learn_time_ms: 25361.593\n",
      "    sample_throughput: 125.684\n",
      "    sample_time_ms: 31825.86\n",
      "    update_time_ms: 3.095\n",
      "  timestamp: 1667282528\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 496000\n",
      "  training_iteration: 124\n",
      "  trial_id: 168ff_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 8.4/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/9.6 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_4_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_2_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_3_400b3145f046e80a4a7d53925cb6fe08, 0.0/6.0 CPU_group_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_0_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_1_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_5_400b3145f046e80a4a7d53925cb6fe08)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                                      | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000 | RUNNING  | 192.168.1.65:16335 |    124 |          7127.98 | 496000 |  147.583 |              156.218 |              139.007 |                400 |\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000:\n",
      "  agent_timesteps_total: 500000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-11-01_01-03-05\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 155.32504538728395\n",
      "  episode_reward_mean: 147.63525166193537\n",
      "  episode_reward_min: 139.00701267355976\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1250\n",
      "  experiment_id: cf8409b3cbbc42979d4540c7e6fbf708\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.4074124304840446e-36\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -0.5154397325590253\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.00630654671251705\n",
      "          policy_loss: 0.0009763042296981439\n",
      "          total_loss: 358.4089221000671\n",
      "          vf_explained_var: -6.705522359595761e-09\n",
      "          vf_loss: 358.4079479217529\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 500000\n",
      "    num_steps_sampled: 500000\n",
      "    num_steps_trained: 500000\n",
      "  iterations_since_restore: 125\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.47407407407407\n",
      "    ram_util_percent: 27.006172839506167\n",
      "  pid: 16335\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08571176479968522\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.46190008775594\n",
      "    mean_inference_ms: 1.7807205694196826\n",
      "    mean_raw_obs_processing_ms: 6.572870290599074\n",
      "  time_since_restore: 7184.426977396011\n",
      "  time_this_iter_s: 56.443238258361816\n",
      "  time_total_s: 7184.426977396011\n",
      "  timers:\n",
      "    learn_throughput: 158.094\n",
      "    learn_time_ms: 25301.369\n",
      "    sample_throughput: 125.687\n",
      "    sample_time_ms: 31825.078\n",
      "    update_time_ms: 3.095\n",
      "  timestamp: 1667282585\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 500000\n",
      "  training_iteration: 125\n",
      "  trial_id: 168ff_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 8.5/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/9.6 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_4_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_3_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_1_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_5_400b3145f046e80a4a7d53925cb6fe08, 0.0/6.0 CPU_group_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_0_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_2_400b3145f046e80a4a7d53925cb6fe08)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                                      | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000 | RUNNING  | 192.168.1.65:16335 |    125 |          7184.43 | 500000 |  147.635 |              155.325 |              139.007 |                400 |\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000:\n",
      "  agent_timesteps_total: 504000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-11-01_01-04-02\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 155.13447443775485\n",
      "  episode_reward_mean: 147.63944529151624\n",
      "  episode_reward_min: 139.00701267355976\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1260\n",
      "  experiment_id: cf8409b3cbbc42979d4540c7e6fbf708\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.2037062152420223e-36\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -0.5007417032495141\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.005630636263504135\n",
      "          policy_loss: 0.0015568453236483037\n",
      "          total_loss: 354.6761699914932\n",
      "          vf_explained_var: -2.9802322831784522e-09\n",
      "          vf_loss: 354.674613571167\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 504000\n",
      "    num_steps_sampled: 504000\n",
      "    num_steps_trained: 504000\n",
      "  iterations_since_restore: 126\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.26219512195122\n",
      "    ram_util_percent: 26.980487804878045\n",
      "  pid: 16335\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08571311846367287\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.461500075307395\n",
      "    mean_inference_ms: 1.7808216613822887\n",
      "    mean_raw_obs_processing_ms: 6.570098391473625\n",
      "  time_since_restore: 7242.063776731491\n",
      "  time_this_iter_s: 57.636799335479736\n",
      "  time_total_s: 7242.063776731491\n",
      "  timers:\n",
      "    learn_throughput: 158.094\n",
      "    learn_time_ms: 25301.419\n",
      "    sample_throughput: 125.646\n",
      "    sample_time_ms: 31835.534\n",
      "    update_time_ms: 3.081\n",
      "  timestamp: 1667282642\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 504000\n",
      "  training_iteration: 126\n",
      "  trial_id: 168ff_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 8.4/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/9.6 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_0_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_4_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_3_400b3145f046e80a4a7d53925cb6fe08, 0.0/6.0 CPU_group_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_5_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_1_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_2_400b3145f046e80a4a7d53925cb6fe08)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                                      | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000 | RUNNING  | 192.168.1.65:16335 |    126 |          7242.06 | 504000 |  147.639 |              155.134 |              139.007 |                400 |\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000:\n",
      "  agent_timesteps_total: 508000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-11-01_01-04-59\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 155.13447443775485\n",
      "  episode_reward_mean: 147.5974636677472\n",
      "  episode_reward_min: 139.00701267355976\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1270\n",
      "  experiment_id: cf8409b3cbbc42979d4540c7e6fbf708\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 6.0185310762101115e-37\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -0.5080139849334955\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.007063475394898067\n",
      "          policy_loss: -9.55550407525152e-05\n",
      "          total_loss: 355.29234635829926\n",
      "          vf_explained_var: 1.4901161415892261e-09\n",
      "          vf_loss: 355.292440533638\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 508000\n",
      "    num_steps_sampled: 508000\n",
      "    num_steps_trained: 508000\n",
      "  iterations_since_restore: 127\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.340740740740742\n",
      "    ram_util_percent: 26.954320987654317\n",
      "  pid: 16335\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08571328693019309\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.46114242755144\n",
      "    mean_inference_ms: 1.7809120947350447\n",
      "    mean_raw_obs_processing_ms: 6.567367277226343\n",
      "  time_since_restore: 7298.47248840332\n",
      "  time_this_iter_s: 56.408711671829224\n",
      "  time_total_s: 7298.47248840332\n",
      "  timers:\n",
      "    learn_throughput: 158.447\n",
      "    learn_time_ms: 25245.097\n",
      "    sample_throughput: 125.664\n",
      "    sample_time_ms: 31830.985\n",
      "    update_time_ms: 3.092\n",
      "  timestamp: 1667282699\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 508000\n",
      "  training_iteration: 127\n",
      "  trial_id: 168ff_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Memory usage on this node: 8.4/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/9.6 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_4_400b3145f046e80a4a7d53925cb6fe08, 0.0/6.0 CPU_group_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_2_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_5_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_3_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_1_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_0_400b3145f046e80a4a7d53925cb6fe08)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                                      | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000 | RUNNING  | 192.168.1.65:16335 |    127 |          7298.47 | 508000 |  147.597 |              155.134 |              139.007 |                400 |\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000:\n",
      "  agent_timesteps_total: 512000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-11-01_01-05-56\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 155.13447443775485\n",
      "  episode_reward_mean: 147.31868673640858\n",
      "  episode_reward_min: 139.00701267355976\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1280\n",
      "  experiment_id: cf8409b3cbbc42979d4540c7e6fbf708\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 3.0092655381050558e-37\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -0.5810668450780213\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010102864347481954\n",
      "          policy_loss: -0.0032696926064090803\n",
      "          total_loss: 353.65714433193205\n",
      "          vf_explained_var: -3.725290298461914e-09\n",
      "          vf_loss: 353.66041190624236\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 512000\n",
      "    num_steps_sampled: 512000\n",
      "    num_steps_trained: 512000\n",
      "  iterations_since_restore: 128\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.262962962962966\n",
      "    ram_util_percent: 27.050617283950615\n",
      "  pid: 16335\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08571368537461893\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.460820182555807\n",
      "    mean_inference_ms: 1.7809624816037075\n",
      "    mean_raw_obs_processing_ms: 6.564715429463531\n",
      "  time_since_restore: 7355.347236633301\n",
      "  time_this_iter_s: 56.87474822998047\n",
      "  time_total_s: 7355.347236633301\n",
      "  timers:\n",
      "    learn_throughput: 158.424\n",
      "    learn_time_ms: 25248.726\n",
      "    sample_throughput: 125.665\n",
      "    sample_time_ms: 31830.599\n",
      "    update_time_ms: 3.103\n",
      "  timestamp: 1667282756\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 512000\n",
      "  training_iteration: 128\n",
      "  trial_id: 168ff_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 8.4/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/9.6 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_3_400b3145f046e80a4a7d53925cb6fe08, 0.0/6.0 CPU_group_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_4_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_0_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_5_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_1_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_2_400b3145f046e80a4a7d53925cb6fe08)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                                      | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000 | RUNNING  | 192.168.1.65:16335 |    128 |          7355.35 | 512000 |  147.319 |              155.134 |              139.007 |                400 |\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000:\n",
      "  agent_timesteps_total: 516000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-11-01_01-06-53\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 155.13447443775485\n",
      "  episode_reward_mean: 147.39228255049247\n",
      "  episode_reward_min: 137.47522287168366\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1290\n",
      "  experiment_id: cf8409b3cbbc42979d4540c7e6fbf708\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 3.0092655381050558e-37\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -0.58403744045645\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0020882530896869866\n",
      "          policy_loss: 0.002666498959297314\n",
      "          total_loss: 355.0878219604492\n",
      "          vf_explained_var: 6.146728903644316e-09\n",
      "          vf_loss: 355.0851543426514\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 516000\n",
      "    num_steps_sampled: 516000\n",
      "    num_steps_trained: 516000\n",
      "  iterations_since_restore: 129\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.096341463414635\n",
      "    ram_util_percent: 27.067073170731707\n",
      "  pid: 16335\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08571351945420075\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.460505253110814\n",
      "    mean_inference_ms: 1.7809799979149596\n",
      "    mean_raw_obs_processing_ms: 6.561077882441603\n",
      "  time_since_restore: 7412.3226771354675\n",
      "  time_this_iter_s: 56.97544050216675\n",
      "  time_total_s: 7412.3226771354675\n",
      "  timers:\n",
      "    learn_throughput: 158.552\n",
      "    learn_time_ms: 25228.307\n",
      "    sample_throughput: 126.082\n",
      "    sample_time_ms: 31725.389\n",
      "    update_time_ms: 3.135\n",
      "  timestamp: 1667282813\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 516000\n",
      "  training_iteration: 129\n",
      "  trial_id: 168ff_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 8.4/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/9.6 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_5_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_0_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_3_400b3145f046e80a4a7d53925cb6fe08, 0.0/6.0 CPU_group_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_2_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_4_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_1_400b3145f046e80a4a7d53925cb6fe08)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                                      | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000 | RUNNING  | 192.168.1.65:16335 |    129 |          7412.32 | 516000 |  147.392 |              155.134 |              137.475 |                400 |\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000:\n",
      "  agent_timesteps_total: 520000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-11-01_01-07-50\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 155.13447443775485\n",
      "  episode_reward_mean: 147.17080897237193\n",
      "  episode_reward_min: 137.47522287168366\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1300\n",
      "  experiment_id: cf8409b3cbbc42979d4540c7e6fbf708\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5046327690525279e-37\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -0.6207627300173044\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.004598247366220848\n",
      "          policy_loss: -0.00011944400612264871\n",
      "          total_loss: 350.51326097249984\n",
      "          vf_explained_var: -4.0978194171259474e-09\n",
      "          vf_loss: 350.5133793592453\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 520000\n",
      "    num_steps_sampled: 520000\n",
      "    num_steps_trained: 520000\n",
      "  iterations_since_restore: 130\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.290123456790123\n",
      "    ram_util_percent: 26.96049382716049\n",
      "  pid: 16335\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08571350057389308\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.460196790961444\n",
      "    mean_inference_ms: 1.7809968881148783\n",
      "    mean_raw_obs_processing_ms: 6.557495303633123\n",
      "  time_since_restore: 7469.331331729889\n",
      "  time_this_iter_s: 57.00865459442139\n",
      "  time_total_s: 7469.331331729889\n",
      "  timers:\n",
      "    learn_throughput: 158.618\n",
      "    learn_time_ms: 25217.761\n",
      "    sample_throughput: 126.094\n",
      "    sample_time_ms: 31722.267\n",
      "    update_time_ms: 3.151\n",
      "  timestamp: 1667282870\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 520000\n",
      "  training_iteration: 130\n",
      "  trial_id: 168ff_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 8.4/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/9.6 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_3_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_1_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_5_400b3145f046e80a4a7d53925cb6fe08, 0.0/6.0 CPU_group_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_0_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_4_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_2_400b3145f046e80a4a7d53925cb6fe08)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                                      | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000 | RUNNING  | 192.168.1.65:16335 |    130 |          7469.33 | 520000 |  147.171 |              155.134 |              137.475 |                400 |\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000:\n",
      "  agent_timesteps_total: 524000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-11-01_01-08-48\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 156.02435613403588\n",
      "  episode_reward_mean: 147.35506613726258\n",
      "  episode_reward_min: 137.47522287168366\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1310\n",
      "  experiment_id: cf8409b3cbbc42979d4540c7e6fbf708\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 7.523163845262639e-38\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -0.5934506675228477\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.005898200563996347\n",
      "          policy_loss: 0.0023973325638507958\n",
      "          total_loss: 357.80838401317595\n",
      "          vf_explained_var: -2.2351742678949904e-09\n",
      "          vf_loss: 357.80598611831664\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 524000\n",
      "    num_steps_sampled: 524000\n",
      "    num_steps_trained: 524000\n",
      "  iterations_since_restore: 131\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.20731707317073\n",
      "    ram_util_percent: 27.10243902439024\n",
      "  pid: 16335\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08571296674231825\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.459893215071908\n",
      "    mean_inference_ms: 1.7810056517364905\n",
      "    mean_raw_obs_processing_ms: 6.553963621240205\n",
      "  time_since_restore: 7526.717995882034\n",
      "  time_this_iter_s: 57.386664152145386\n",
      "  time_total_s: 7526.717995882034\n",
      "  timers:\n",
      "    learn_throughput: 158.35\n",
      "    learn_time_ms: 25260.493\n",
      "    sample_throughput: 126.098\n",
      "    sample_time_ms: 31721.467\n",
      "    update_time_ms: 3.151\n",
      "  timestamp: 1667282928\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 524000\n",
      "  training_iteration: 131\n",
      "  trial_id: 168ff_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 8.4/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/9.6 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_2_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_5_400b3145f046e80a4a7d53925cb6fe08, 0.0/6.0 CPU_group_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_0_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_3_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_4_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_1_400b3145f046e80a4a7d53925cb6fe08)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                                      | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000 | RUNNING  | 192.168.1.65:16335 |    131 |          7526.72 | 524000 |  147.355 |              156.024 |              137.475 |                400 |\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000:\n",
      "  agent_timesteps_total: 528000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-11-01_01-09-45\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 156.57825106470727\n",
      "  episode_reward_mean: 147.44964793992537\n",
      "  episode_reward_min: 137.47522287168366\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1320\n",
      "  experiment_id: cf8409b3cbbc42979d4540c7e6fbf708\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 3.7615819226313197e-38\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -0.6287960559129715\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.003737157189532178\n",
      "          policy_loss: -0.0016076432744739576\n",
      "          total_loss: 354.6736453294754\n",
      "          vf_explained_var: -2.607703164514419e-09\n",
      "          vf_loss: 354.6752543449402\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 528000\n",
      "    num_steps_sampled: 528000\n",
      "    num_steps_trained: 528000\n",
      "  iterations_since_restore: 132\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.045121951219514\n",
      "    ram_util_percent: 27.15853658536586\n",
      "  pid: 16335\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08571248939757387\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.459592273364866\n",
      "    mean_inference_ms: 1.7810098730625348\n",
      "    mean_raw_obs_processing_ms: 6.550537549120793\n",
      "  time_since_restore: 7583.676146268845\n",
      "  time_this_iter_s: 56.9581503868103\n",
      "  time_total_s: 7583.676146268845\n",
      "  timers:\n",
      "    learn_throughput: 158.649\n",
      "    learn_time_ms: 25212.814\n",
      "    sample_throughput: 126.093\n",
      "    sample_time_ms: 31722.561\n",
      "    update_time_ms: 3.149\n",
      "  timestamp: 1667282985\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 528000\n",
      "  training_iteration: 132\n",
      "  trial_id: 168ff_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Memory usage on this node: 8.5/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/9.6 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_2_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_5_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 accelerator_type:G, 0.0/6.0 CPU_group_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_4_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_1_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_0_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_3_400b3145f046e80a4a7d53925cb6fe08)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                                      | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000 | RUNNING  | 192.168.1.65:16335 |    132 |          7583.68 | 528000 |   147.45 |              156.578 |              137.475 |                400 |\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000:\n",
      "  agent_timesteps_total: 532000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-11-01_01-10-42\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 156.57825106470727\n",
      "  episode_reward_mean: 147.4750375730192\n",
      "  episode_reward_min: 137.47522287168366\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1330\n",
      "  experiment_id: cf8409b3cbbc42979d4540c7e6fbf708\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.8807909613156599e-38\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -0.6431482991203665\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0037614233920068687\n",
      "          policy_loss: -0.003044591641810257\n",
      "          total_loss: 349.5452677130699\n",
      "          vf_explained_var: -9.685754420729609e-09\n",
      "          vf_loss: 349.5483115196228\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 532000\n",
      "    num_steps_sampled: 532000\n",
      "    num_steps_trained: 532000\n",
      "  iterations_since_restore: 133\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.36543209876543\n",
      "    ram_util_percent: 27.155555555555548\n",
      "  pid: 16335\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0857128611477798\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.45927148094245\n",
      "    mean_inference_ms: 1.7810477905561664\n",
      "    mean_raw_obs_processing_ms: 6.547172762668258\n",
      "  time_since_restore: 7640.766379356384\n",
      "  time_this_iter_s: 57.09023308753967\n",
      "  time_total_s: 7640.766379356384\n",
      "  timers:\n",
      "    learn_throughput: 158.321\n",
      "    learn_time_ms: 25265.048\n",
      "    sample_throughput: 126.081\n",
      "    sample_time_ms: 31725.61\n",
      "    update_time_ms: 3.188\n",
      "  timestamp: 1667283042\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 532000\n",
      "  training_iteration: 133\n",
      "  trial_id: 168ff_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 8.4/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/9.6 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_5_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_1_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_4_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_3_400b3145f046e80a4a7d53925cb6fe08, 0.0/6.0 CPU_group_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_2_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_0_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 accelerator_type:G)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                                      | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000 | RUNNING  | 192.168.1.65:16335 |    133 |          7640.77 | 532000 |  147.475 |              156.578 |              137.475 |                400 |\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000:\n",
      "  agent_timesteps_total: 536000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-11-01_01-11-39\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 156.57825106470727\n",
      "  episode_reward_mean: 147.65895867285025\n",
      "  episode_reward_min: 137.47522287168366\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1340\n",
      "  experiment_id: cf8409b3cbbc42979d4540c7e6fbf708\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 9.403954806578299e-39\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -0.6383392300456763\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0033788189015922397\n",
      "          policy_loss: -0.0005754183759563603\n",
      "          total_loss: 356.28826801776887\n",
      "          vf_explained_var: -2.0489097085629737e-09\n",
      "          vf_loss: 356.288845038414\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 536000\n",
      "    num_steps_sampled: 536000\n",
      "    num_steps_trained: 536000\n",
      "  iterations_since_restore: 134\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.15487804878049\n",
      "    ram_util_percent: 27.07560975609756\n",
      "  pid: 16335\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08571324033302041\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.458935441454013\n",
      "    mean_inference_ms: 1.781089072397631\n",
      "    mean_raw_obs_processing_ms: 6.543850536914172\n",
      "  time_since_restore: 7697.6453495025635\n",
      "  time_this_iter_s: 56.8789701461792\n",
      "  time_total_s: 7697.6453495025635\n",
      "  timers:\n",
      "    learn_throughput: 158.572\n",
      "    learn_time_ms: 25225.103\n",
      "    sample_throughput: 126.076\n",
      "    sample_time_ms: 31726.808\n",
      "    update_time_ms: 3.182\n",
      "  timestamp: 1667283099\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 536000\n",
      "  training_iteration: 134\n",
      "  trial_id: 168ff_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 8.5/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/9.6 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_5_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_0_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_4_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_1_400b3145f046e80a4a7d53925cb6fe08, 0.0/6.0 CPU_group_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_2_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_3_400b3145f046e80a4a7d53925cb6fe08)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                                      | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000 | RUNNING  | 192.168.1.65:16335 |    134 |          7697.65 | 536000 |  147.659 |              156.578 |              137.475 |                400 |\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000:\n",
      "  agent_timesteps_total: 540000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-11-01_01-12-36\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 156.57825106470727\n",
      "  episode_reward_mean: 147.62625902326835\n",
      "  episode_reward_min: 137.47522287168366\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1350\n",
      "  experiment_id: cf8409b3cbbc42979d4540c7e6fbf708\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 4.7019774032891496e-39\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -0.6134817754849792\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.00871245385842503\n",
      "          policy_loss: -0.0034283958753803744\n",
      "          total_loss: 358.2022851467133\n",
      "          vf_explained_var: -9.409524864167906e-06\n",
      "          vf_loss: 358.20571448802946\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 540000\n",
      "    num_steps_sampled: 540000\n",
      "    num_steps_trained: 540000\n",
      "  iterations_since_restore: 135\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.5975\n",
      "    ram_util_percent: 27.1125\n",
      "  pid: 16335\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08571424833999966\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.45851065164106\n",
      "    mean_inference_ms: 1.781191132821344\n",
      "    mean_raw_obs_processing_ms: 6.5405873689029885\n",
      "  time_since_restore: 7753.994204044342\n",
      "  time_this_iter_s: 56.348854541778564\n",
      "  time_total_s: 7753.994204044342\n",
      "  timers:\n",
      "    learn_throughput: 158.615\n",
      "    learn_time_ms: 25218.252\n",
      "    sample_throughput: 126.087\n",
      "    sample_time_ms: 31724.206\n",
      "    update_time_ms: 3.183\n",
      "  timestamp: 1667283156\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 540000\n",
      "  training_iteration: 135\n",
      "  trial_id: 168ff_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 8.5/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/9.6 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_2_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_3_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_5_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_1_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_0_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_4_400b3145f046e80a4a7d53925cb6fe08, 0.0/6.0 CPU_group_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 accelerator_type:G)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                                      | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000 | RUNNING  | 192.168.1.65:16335 |    135 |          7753.99 | 540000 |  147.626 |              156.578 |              137.475 |                400 |\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000:\n",
      "  agent_timesteps_total: 544000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-11-01_01-13-33\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 156.57825106470727\n",
      "  episode_reward_mean: 147.2646600278156\n",
      "  episode_reward_min: 137.47522287168366\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1360\n",
      "  experiment_id: cf8409b3cbbc42979d4540c7e6fbf708\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.3509887016445748e-39\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -0.6393951028585434\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008959780054738253\n",
      "          policy_loss: -0.0018152246688259766\n",
      "          total_loss: 352.4434413433075\n",
      "          vf_explained_var: -1.6763805898989403e-09\n",
      "          vf_loss: 352.4452585697174\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 544000\n",
      "    num_steps_sampled: 544000\n",
      "    num_steps_trained: 544000\n",
      "  iterations_since_restore: 136\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.207317073170735\n",
      "    ram_util_percent: 27.0609756097561\n",
      "  pid: 16335\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08571555646195966\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.4579881848127\n",
      "    mean_inference_ms: 1.781248169102084\n",
      "    mean_raw_obs_processing_ms: 6.537573642241667\n",
      "  time_since_restore: 7811.295825004578\n",
      "  time_this_iter_s: 57.301620960235596\n",
      "  time_total_s: 7811.295825004578\n",
      "  timers:\n",
      "    learn_throughput: 158.89\n",
      "    learn_time_ms: 25174.698\n",
      "    sample_throughput: 126.047\n",
      "    sample_time_ms: 31734.179\n",
      "    update_time_ms: 3.239\n",
      "  timestamp: 1667283213\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 544000\n",
      "  training_iteration: 136\n",
      "  trial_id: 168ff_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 8.4/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/9.6 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_3_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_4_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_5_400b3145f046e80a4a7d53925cb6fe08, 0.0/6.0 CPU_group_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_1_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_0_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_2_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 accelerator_type:G)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                                      | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000 | RUNNING  | 192.168.1.65:16335 |    136 |           7811.3 | 544000 |  147.265 |              156.578 |              137.475 |                400 |\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000:\n",
      "  agent_timesteps_total: 548000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-11-01_01-14-30\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 156.57825106470727\n",
      "  episode_reward_mean: 146.9952957529426\n",
      "  episode_reward_min: 137.47522287168366\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1370\n",
      "  experiment_id: cf8409b3cbbc42979d4540c7e6fbf708\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.1754943508222874e-39\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -0.6202569290995598\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.005449460715226451\n",
      "          policy_loss: 0.0013425889250356705\n",
      "          total_loss: 352.53264966011045\n",
      "          vf_explained_var: -7.450580707946131e-10\n",
      "          vf_loss: 352.53130807876585\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 548000\n",
      "    num_steps_sampled: 548000\n",
      "    num_steps_trained: 548000\n",
      "  iterations_since_restore: 137\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.124390243902436\n",
      "    ram_util_percent: 27.04512195121951\n",
      "  pid: 16335\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08571744860599495\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.457447279059387\n",
      "    mean_inference_ms: 1.7813078003616047\n",
      "    mean_raw_obs_processing_ms: 6.534590144162666\n",
      "  time_since_restore: 7868.626485109329\n",
      "  time_this_iter_s: 57.33066010475159\n",
      "  time_total_s: 7868.626485109329\n",
      "  timers:\n",
      "    learn_throughput: 158.321\n",
      "    learn_time_ms: 25265.146\n",
      "    sample_throughput: 126.04\n",
      "    sample_time_ms: 31735.914\n",
      "    update_time_ms: 3.189\n",
      "  timestamp: 1667283270\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 548000\n",
      "  training_iteration: 137\n",
      "  trial_id: 168ff_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Memory usage on this node: 8.4/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/9.6 GiB heap, 0.0/14.65 GiB objects (0.0/6.0 CPU_group_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_5_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_1_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_4_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_2_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_3_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_0_400b3145f046e80a4a7d53925cb6fe08)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                                      | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000 | RUNNING  | 192.168.1.65:16335 |    137 |          7868.63 | 548000 |  146.995 |              156.578 |              137.475 |                400 |\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000:\n",
      "  agent_timesteps_total: 552000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-11-01_01-15-28\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 156.57825106470727\n",
      "  episode_reward_mean: 147.3630799952111\n",
      "  episode_reward_min: 137.47522287168366\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1380\n",
      "  experiment_id: cf8409b3cbbc42979d4540c7e6fbf708\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 5.877471754111437e-40\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -0.7141938913613558\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0067363872565695026\n",
      "          policy_loss: -0.0008312511883559637\n",
      "          total_loss: 353.23049595355985\n",
      "          vf_explained_var: -7.450580596923828e-09\n",
      "          vf_loss: 353.231326341629\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 552000\n",
      "    num_steps_sampled: 552000\n",
      "    num_steps_trained: 552000\n",
      "  iterations_since_restore: 138\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.228048780487807\n",
      "    ram_util_percent: 27.20487804878049\n",
      "  pid: 16335\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08571929880092673\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.45688177795415\n",
      "    mean_inference_ms: 1.7814106056164798\n",
      "    mean_raw_obs_processing_ms: 6.531666732174253\n",
      "  time_since_restore: 7926.021887540817\n",
      "  time_this_iter_s: 57.39540243148804\n",
      "  time_total_s: 7926.021887540817\n",
      "  timers:\n",
      "    learn_throughput: 158.006\n",
      "    learn_time_ms: 25315.473\n",
      "    sample_throughput: 126.034\n",
      "    sample_time_ms: 31737.554\n",
      "    update_time_ms: 3.2\n",
      "  timestamp: 1667283328\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 552000\n",
      "  training_iteration: 138\n",
      "  trial_id: 168ff_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 8.5/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/9.6 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_5_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_4_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 accelerator_type:G, 0.0/6.0 CPU_group_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_2_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_3_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_0_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_1_400b3145f046e80a4a7d53925cb6fe08)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                                      | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000 | RUNNING  | 192.168.1.65:16335 |    138 |          7926.02 | 552000 |  147.363 |              156.578 |              137.475 |                400 |\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000:\n",
      "  agent_timesteps_total: 556000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-11-01_01-16-26\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 156.57825106470727\n",
      "  episode_reward_mean: 147.27925218077942\n",
      "  episode_reward_min: 137.49506789347976\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1390\n",
      "  experiment_id: cf8409b3cbbc42979d4540c7e6fbf708\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.9387358770557185e-40\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -0.7718324048444629\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008098477801377157\n",
      "          policy_loss: 0.001788659390877001\n",
      "          total_loss: 355.1142220020294\n",
      "          vf_explained_var: -3.725290298461914e-09\n",
      "          vf_loss: 355.11243364810946\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 556000\n",
      "    num_steps_sampled: 556000\n",
      "    num_steps_trained: 556000\n",
      "  iterations_since_restore: 139\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.079518072289158\n",
      "    ram_util_percent: 27.11204819277108\n",
      "  pid: 16335\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08572201552116214\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.456308494065087\n",
      "    mean_inference_ms: 1.781548680286068\n",
      "    mean_raw_obs_processing_ms: 6.528754090641299\n",
      "  time_since_restore: 7983.650118589401\n",
      "  time_this_iter_s: 57.628231048583984\n",
      "  time_total_s: 7983.650118589401\n",
      "  timers:\n",
      "    learn_throughput: 157.605\n",
      "    learn_time_ms: 25379.961\n",
      "    sample_throughput: 126.031\n",
      "    sample_time_ms: 31738.324\n",
      "    update_time_ms: 3.206\n",
      "  timestamp: 1667283386\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 556000\n",
      "  training_iteration: 139\n",
      "  trial_id: 168ff_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 8.4/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/9.6 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_0_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_1_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_4_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_5_400b3145f046e80a4a7d53925cb6fe08, 0.0/6.0 CPU_group_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_3_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_2_400b3145f046e80a4a7d53925cb6fe08)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                                      | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000 | RUNNING  | 192.168.1.65:16335 |    139 |          7983.65 | 556000 |  147.279 |              156.578 |              137.495 |                400 |\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000:\n",
      "  agent_timesteps_total: 560000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-11-01_01-17-23\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 156.57825106470727\n",
      "  episode_reward_mean: 147.1175083923907\n",
      "  episode_reward_min: 137.49506789347976\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1400\n",
      "  experiment_id: cf8409b3cbbc42979d4540c7e6fbf708\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.4693679385278593e-40\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -0.788808605261147\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.007628630712861373\n",
      "          policy_loss: 0.0001103060829336755\n",
      "          total_loss: 352.86250643730165\n",
      "          vf_explained_var: -5.401671110405459e-09\n",
      "          vf_loss: 352.8623935699463\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 560000\n",
      "    num_steps_sampled: 560000\n",
      "    num_steps_trained: 560000\n",
      "  iterations_since_restore: 140\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.34320987654321\n",
      "    ram_util_percent: 27.1641975308642\n",
      "  pid: 16335\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08572418097626136\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.455730910501305\n",
      "    mean_inference_ms: 1.7816920516430395\n",
      "    mean_raw_obs_processing_ms: 6.52593154574304\n",
      "  time_since_restore: 8040.744380474091\n",
      "  time_this_iter_s: 57.09426188468933\n",
      "  time_total_s: 8040.744380474091\n",
      "  timers:\n",
      "    learn_throughput: 157.574\n",
      "    learn_time_ms: 25384.957\n",
      "    sample_throughput: 126.016\n",
      "    sample_time_ms: 31741.935\n",
      "    update_time_ms: 3.194\n",
      "  timestamp: 1667283443\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 560000\n",
      "  training_iteration: 140\n",
      "  trial_id: 168ff_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 8.5/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/9.6 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_1_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_2_400b3145f046e80a4a7d53925cb6fe08, 0.0/6.0 CPU_group_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_4_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_3_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_0_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_5_400b3145f046e80a4a7d53925cb6fe08)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                                      | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000 | RUNNING  | 192.168.1.65:16335 |    140 |          8040.74 | 560000 |  147.118 |              156.578 |              137.495 |                400 |\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000:\n",
      "  agent_timesteps_total: 564000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-11-01_01-18-20\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 156.57825106470727\n",
      "  episode_reward_mean: 146.79703979841506\n",
      "  episode_reward_min: 137.49506789347976\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1410\n",
      "  experiment_id: cf8409b3cbbc42979d4540c7e6fbf708\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 7.346839692639296e-41\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -0.7622124876827001\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.003352863481132082\n",
      "          policy_loss: -0.0006323133202386089\n",
      "          total_loss: 352.1198576211929\n",
      "          vf_explained_var: 5.587935669737476e-10\n",
      "          vf_loss: 352.12049236297605\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 564000\n",
      "    num_steps_sampled: 564000\n",
      "    num_steps_trained: 564000\n",
      "  iterations_since_restore: 141\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.169512195121953\n",
      "    ram_util_percent: 27.341463414634145\n",
      "  pid: 16335\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08572629214040091\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.455197683994943\n",
      "    mean_inference_ms: 1.7818293280451303\n",
      "    mean_raw_obs_processing_ms: 6.52318764815225\n",
      "  time_since_restore: 8097.972901105881\n",
      "  time_this_iter_s: 57.22852063179016\n",
      "  time_total_s: 8097.972901105881\n",
      "  timers:\n",
      "    learn_throughput: 157.712\n",
      "    learn_time_ms: 25362.693\n",
      "    sample_throughput: 125.991\n",
      "    sample_time_ms: 31748.403\n",
      "    update_time_ms: 3.187\n",
      "  timestamp: 1667283500\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 564000\n",
      "  training_iteration: 141\n",
      "  trial_id: 168ff_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 8.5/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/9.6 GiB heap, 0.0/14.65 GiB objects (0.0/6.0 CPU_group_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_5_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_0_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_1_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_4_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_2_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_3_400b3145f046e80a4a7d53925cb6fe08)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                                      | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000 | RUNNING  | 192.168.1.65:16335 |    141 |          8097.97 | 564000 |  146.797 |              156.578 |              137.495 |                400 |\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000:\n",
      "  agent_timesteps_total: 568000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-11-01_01-19-18\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 154.56077250563936\n",
      "  episode_reward_mean: 146.6319932641604\n",
      "  episode_reward_min: 137.49506789347976\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1420\n",
      "  experiment_id: cf8409b3cbbc42979d4540c7e6fbf708\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 3.673419846319648e-41\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -0.7682183280587196\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008036832252037307\n",
      "          policy_loss: 6.831904756836593e-05\n",
      "          total_loss: 350.9051016807556\n",
      "          vf_explained_var: -4.284083754413359e-09\n",
      "          vf_loss: 350.9050340652466\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 568000\n",
      "    num_steps_sampled: 568000\n",
      "    num_steps_trained: 568000\n",
      "  iterations_since_restore: 142\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.26463414634146\n",
      "    ram_util_percent: 27.167073170731705\n",
      "  pid: 16335\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08572885127731826\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.454659204792186\n",
      "    mean_inference_ms: 1.7819797125076808\n",
      "    mean_raw_obs_processing_ms: 6.520491937737422\n",
      "  time_since_restore: 8155.091639757156\n",
      "  time_this_iter_s: 57.118738651275635\n",
      "  time_total_s: 8155.091639757156\n",
      "  timers:\n",
      "    learn_throughput: 157.645\n",
      "    learn_time_ms: 25373.387\n",
      "    sample_throughput: 125.969\n",
      "    sample_time_ms: 31753.784\n",
      "    update_time_ms: 3.209\n",
      "  timestamp: 1667283558\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 568000\n",
      "  training_iteration: 142\n",
      "  trial_id: 168ff_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Memory usage on this node: 8.5/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/9.6 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_3_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_4_400b3145f046e80a4a7d53925cb6fe08, 0.0/6.0 CPU_group_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_0_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_2_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_5_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_1_400b3145f046e80a4a7d53925cb6fe08)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                                      | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000 | RUNNING  | 192.168.1.65:16335 |    142 |          8155.09 | 568000 |  146.632 |              154.561 |              137.495 |                400 |\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000:\n",
      "  agent_timesteps_total: 572000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-11-01_01-20-15\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 154.56077250563936\n",
      "  episode_reward_mean: 146.5081549033553\n",
      "  episode_reward_min: 137.49506789347976\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1430\n",
      "  experiment_id: cf8409b3cbbc42979d4540c7e6fbf708\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.836709923159824e-41\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -0.7835182324051857\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008953059396662865\n",
      "          policy_loss: -0.0006061798078007997\n",
      "          total_loss: 350.0742069244385\n",
      "          vf_explained_var: -2.607703164514419e-09\n",
      "          vf_loss: 350.07481322288515\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 572000\n",
      "    num_steps_sampled: 572000\n",
      "    num_steps_trained: 572000\n",
      "  iterations_since_restore: 143\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.162195121951218\n",
      "    ram_util_percent: 27.281707317073174\n",
      "  pid: 16335\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08573120491104286\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.454083790211754\n",
      "    mean_inference_ms: 1.7821397349677264\n",
      "    mean_raw_obs_processing_ms: 6.517882999257012\n",
      "  time_since_restore: 8212.334783315659\n",
      "  time_this_iter_s: 57.2431435585022\n",
      "  time_total_s: 8212.334783315659\n",
      "  timers:\n",
      "    learn_throughput: 157.551\n",
      "    learn_time_ms: 25388.531\n",
      "    sample_throughput: 125.968\n",
      "    sample_time_ms: 31754.012\n",
      "    update_time_ms: 3.178\n",
      "  timestamp: 1667283615\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 572000\n",
      "  training_iteration: 143\n",
      "  trial_id: 168ff_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 8.5/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/9.6 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_1_400b3145f046e80a4a7d53925cb6fe08, 0.0/6.0 CPU_group_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_5_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_4_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_2_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_0_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_3_400b3145f046e80a4a7d53925cb6fe08)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                                      | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000 | RUNNING  | 192.168.1.65:16335 |    143 |          8212.33 | 572000 |  146.508 |              154.561 |              137.495 |                400 |\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000:\n",
      "  agent_timesteps_total: 576000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-11-01_01-21-12\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 154.56077250563936\n",
      "  episode_reward_mean: 146.30342029197408\n",
      "  episode_reward_min: 137.49506789347976\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1440\n",
      "  experiment_id: cf8409b3cbbc42979d4540c7e6fbf708\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 9.18354961579912e-42\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -0.7820441557094455\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.006306136974786369\n",
      "          policy_loss: -3.238680073991418e-06\n",
      "          total_loss: 352.11467559337615\n",
      "          vf_explained_var: -5.83995142733329e-06\n",
      "          vf_loss: 352.11467864513395\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 576000\n",
      "    num_steps_sampled: 576000\n",
      "    num_steps_trained: 576000\n",
      "  iterations_since_restore: 144\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.192682926829267\n",
      "    ram_util_percent: 27.229268292682928\n",
      "  pid: 16335\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08573290216148655\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.453562461210996\n",
      "    mean_inference_ms: 1.7822746838313455\n",
      "    mean_raw_obs_processing_ms: 6.515366507511508\n",
      "  time_since_restore: 8269.452726840973\n",
      "  time_this_iter_s: 57.11794352531433\n",
      "  time_total_s: 8269.452726840973\n",
      "  timers:\n",
      "    learn_throughput: 157.429\n",
      "    learn_time_ms: 25408.211\n",
      "    sample_throughput: 125.951\n",
      "    sample_time_ms: 31758.342\n",
      "    update_time_ms: 3.174\n",
      "  timestamp: 1667283672\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 576000\n",
      "  training_iteration: 144\n",
      "  trial_id: 168ff_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 8.5/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/9.6 GiB heap, 0.0/14.65 GiB objects (0.0/6.0 CPU_group_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_5_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_2_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_3_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_0_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_1_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_4_400b3145f046e80a4a7d53925cb6fe08)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                                      | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000 | RUNNING  | 192.168.1.65:16335 |    144 |          8269.45 | 576000 |  146.303 |              154.561 |              137.495 |                400 |\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000:\n",
      "  agent_timesteps_total: 580000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-11-01_01-22-10\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 154.56077250563936\n",
      "  episode_reward_mean: 146.16147748016962\n",
      "  episode_reward_min: 137.49506789347976\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1450\n",
      "  experiment_id: cf8409b3cbbc42979d4540c7e6fbf708\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 4.59177480789956e-42\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -0.8223776826635003\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.006654744711613603\n",
      "          policy_loss: 0.0006668144545983523\n",
      "          total_loss: 352.48079826831815\n",
      "          vf_explained_var: -1.6763805898989403e-09\n",
      "          vf_loss: 352.48013412952423\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 580000\n",
      "    num_steps_sampled: 580000\n",
      "    num_steps_trained: 580000\n",
      "  iterations_since_restore: 145\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.492592592592594\n",
      "    ram_util_percent: 27.227160493827157\n",
      "  pid: 16335\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08573378535699777\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.453100157877106\n",
      "    mean_inference_ms: 1.7823758483862777\n",
      "    mean_raw_obs_processing_ms: 6.512958864553977\n",
      "  time_since_restore: 8326.654331922531\n",
      "  time_this_iter_s: 57.20160508155823\n",
      "  time_total_s: 8326.654331922531\n",
      "  timers:\n",
      "    learn_throughput: 156.957\n",
      "    learn_time_ms: 25484.653\n",
      "    sample_throughput: 125.916\n",
      "    sample_time_ms: 31767.146\n",
      "    update_time_ms: 3.173\n",
      "  timestamp: 1667283730\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 580000\n",
      "  training_iteration: 145\n",
      "  trial_id: 168ff_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 8.4/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/9.6 GiB heap, 0.0/14.65 GiB objects (0.0/6.0 CPU_group_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_2_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_1_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_5_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_3_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_0_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_4_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 accelerator_type:G)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                                      | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000 | RUNNING  | 192.168.1.65:16335 |    145 |          8326.65 | 580000 |  146.161 |              154.561 |              137.495 |                400 |\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=16331)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=16333)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=16332)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=16330)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=16334)\u001b[0m  Retrying in 1 seconds\n",
      "Result for PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000:\n",
      "  agent_timesteps_total: 584000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-11-01_01-23-08\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 154.56077250563936\n",
      "  episode_reward_mean: 146.28353919344633\n",
      "  episode_reward_min: 137.49506789347976\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1460\n",
      "  experiment_id: cf8409b3cbbc42979d4540c7e6fbf708\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.29588740394978e-42\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -0.879410470277071\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.007006453165718796\n",
      "          policy_loss: -0.0018237173833767883\n",
      "          total_loss: 348.15795583724974\n",
      "          vf_explained_var: -5.587935447692871e-09\n",
      "          vf_loss: 348.15977907180786\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 584000\n",
      "    num_steps_sampled: 584000\n",
      "    num_steps_trained: 584000\n",
      "  iterations_since_restore: 146\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.939285714285717\n",
      "    ram_util_percent: 27.326190476190472\n",
      "  pid: 16335\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08573450134336039\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.4526336072291\n",
      "    mean_inference_ms: 1.782476808219258\n",
      "    mean_raw_obs_processing_ms: 6.5112605529112875\n",
      "  time_since_restore: 8385.138298273087\n",
      "  time_this_iter_s: 58.48396635055542\n",
      "  time_total_s: 8385.138298273087\n",
      "  timers:\n",
      "    learn_throughput: 156.708\n",
      "    learn_time_ms: 25525.19\n",
      "    sample_throughput: 125.609\n",
      "    sample_time_ms: 31844.901\n",
      "    update_time_ms: 3.138\n",
      "  timestamp: 1667283788\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 584000\n",
      "  training_iteration: 146\n",
      "  trial_id: 168ff_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 8.6/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/9.6 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_0_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_5_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_3_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_4_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_1_400b3145f046e80a4a7d53925cb6fe08, 0.0/6.0 CPU_group_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_2_400b3145f046e80a4a7d53925cb6fe08)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                                      | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000 | RUNNING  | 192.168.1.65:16335 |    146 |          8385.14 | 584000 |  146.284 |              154.561 |              137.495 |                400 |\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000:\n",
      "  agent_timesteps_total: 588000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-11-01_01-24-06\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 154.56077250563936\n",
      "  episode_reward_mean: 146.69024488974736\n",
      "  episode_reward_min: 137.71411764060164\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1470\n",
      "  experiment_id: cf8409b3cbbc42979d4540c7e6fbf708\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.14794370197489e-42\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -0.8730179050937295\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.005629890644650004\n",
      "          policy_loss: 0.0028987906989641488\n",
      "          total_loss: 355.959508562088\n",
      "          vf_explained_var: -1.4901161415892261e-09\n",
      "          vf_loss: 355.95661013126374\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 588000\n",
      "    num_steps_sampled: 588000\n",
      "    num_steps_trained: 588000\n",
      "  iterations_since_restore: 147\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.191463414634146\n",
      "    ram_util_percent: 27.354878048780492\n",
      "  pid: 16335\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08573479386714865\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.45217422373923\n",
      "    mean_inference_ms: 1.7825640377636416\n",
      "    mean_raw_obs_processing_ms: 6.509626852140661\n",
      "  time_since_restore: 8442.737370967865\n",
      "  time_this_iter_s: 57.59907269477844\n",
      "  time_total_s: 8442.737370967865\n",
      "  timers:\n",
      "    learn_throughput: 156.544\n",
      "    learn_time_ms: 25551.986\n",
      "    sample_throughput: 125.609\n",
      "    sample_time_ms: 31844.929\n",
      "    update_time_ms: 3.19\n",
      "  timestamp: 1667283846\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 588000\n",
      "  training_iteration: 147\n",
      "  trial_id: 168ff_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Memory usage on this node: 8.5/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/9.6 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_2_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_1_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_4_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_5_400b3145f046e80a4a7d53925cb6fe08, 0.0/6.0 CPU_group_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_3_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_0_400b3145f046e80a4a7d53925cb6fe08)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                                      | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000 | RUNNING  | 192.168.1.65:16335 |    147 |          8442.74 | 588000 |   146.69 |              154.561 |              137.714 |                400 |\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000:\n",
      "  agent_timesteps_total: 592000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-11-01_01-25-03\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 154.56077250563936\n",
      "  episode_reward_mean: 146.57109739579604\n",
      "  episode_reward_min: 137.71411764060164\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1480\n",
      "  experiment_id: cf8409b3cbbc42979d4540c7e6fbf708\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 5.73971850987445e-43\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -0.9114496236667037\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.00852295413060922\n",
      "          policy_loss: 0.00031895302527118474\n",
      "          total_loss: 352.88885469436644\n",
      "          vf_explained_var: -2.607703164514419e-09\n",
      "          vf_loss: 352.88853390216826\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 592000\n",
      "    num_steps_sampled: 592000\n",
      "    num_steps_trained: 592000\n",
      "  iterations_since_restore: 148\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.251219512195124\n",
      "    ram_util_percent: 27.34878048780487\n",
      "  pid: 16335\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0857345936705963\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.45172712621401\n",
      "    mean_inference_ms: 1.7826388756199547\n",
      "    mean_raw_obs_processing_ms: 6.507970397012286\n",
      "  time_since_restore: 8499.997365474701\n",
      "  time_this_iter_s: 57.25999450683594\n",
      "  time_total_s: 8499.997365474701\n",
      "  timers:\n",
      "    learn_throughput: 156.611\n",
      "    learn_time_ms: 25540.94\n",
      "    sample_throughput: 125.618\n",
      "    sample_time_ms: 31842.476\n",
      "    update_time_ms: 3.178\n",
      "  timestamp: 1667283903\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 592000\n",
      "  training_iteration: 148\n",
      "  trial_id: 168ff_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 8.5/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/9.6 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_5_400b3145f046e80a4a7d53925cb6fe08, 0.0/6.0 CPU_group_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_4_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_0_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_1_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_2_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_3_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 accelerator_type:G)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                                      | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000 | RUNNING  | 192.168.1.65:16335 |    148 |             8500 | 592000 |  146.571 |              154.561 |              137.714 |                400 |\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000:\n",
      "  agent_timesteps_total: 596000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-11-01_01-26-00\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 154.56077250563936\n",
      "  episode_reward_mean: 146.5033989551547\n",
      "  episode_reward_min: 138.38290088272865\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1490\n",
      "  experiment_id: cf8409b3cbbc42979d4540c7e6fbf708\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.869859254937225e-43\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -0.9546585066244007\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.007539616033136624\n",
      "          policy_loss: 0.0019326122928760014\n",
      "          total_loss: 353.032350730896\n",
      "          vf_explained_var: -1.1175871339474952e-09\n",
      "          vf_loss: 353.0304188489914\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 596000\n",
      "    num_steps_sampled: 596000\n",
      "    num_steps_trained: 596000\n",
      "  iterations_since_restore: 149\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.269512195121948\n",
      "    ram_util_percent: 27.31951219512195\n",
      "  pid: 16335\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08573322903500565\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.451320416809263\n",
      "    mean_inference_ms: 1.7826742119569001\n",
      "    mean_raw_obs_processing_ms: 6.506330221332832\n",
      "  time_since_restore: 8556.867118120193\n",
      "  time_this_iter_s: 56.869752645492554\n",
      "  time_total_s: 8556.867118120193\n",
      "  timers:\n",
      "    learn_throughput: 157.074\n",
      "    learn_time_ms: 25465.67\n",
      "    sample_throughput: 125.621\n",
      "    sample_time_ms: 31841.915\n",
      "    update_time_ms: 3.141\n",
      "  timestamp: 1667283960\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 596000\n",
      "  training_iteration: 149\n",
      "  trial_id: 168ff_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 8.5/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/9.6 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_5_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_4_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_2_400b3145f046e80a4a7d53925cb6fe08, 0.0/6.0 CPU_group_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_1_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_0_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_3_400b3145f046e80a4a7d53925cb6fe08)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                                      | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000 | RUNNING  | 192.168.1.65:16335 |    149 |          8556.87 | 596000 |  146.503 |              154.561 |              138.383 |                400 |\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000:\n",
      "  agent_timesteps_total: 600000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-11-01_01-26-57\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 154.56077250563936\n",
      "  episode_reward_mean: 146.7511932046965\n",
      "  episode_reward_min: 138.62968613054954\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1500\n",
      "  experiment_id: cf8409b3cbbc42979d4540c7e6fbf708\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.4349296274686126e-43\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -0.9902653971686959\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009660404380247258\n",
      "          policy_loss: -0.002240434451960027\n",
      "          total_loss: 351.51054762601854\n",
      "          vf_explained_var: -9.313225746154785e-10\n",
      "          vf_loss: 351.5127880811691\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 600000\n",
      "    num_steps_sampled: 600000\n",
      "    num_steps_trained: 600000\n",
      "  iterations_since_restore: 150\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.39625\n",
      "    ram_util_percent: 27.377499999999998\n",
      "  pid: 16335\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0857317878009215\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.450936196764605\n",
      "    mean_inference_ms: 1.7827039268482872\n",
      "    mean_raw_obs_processing_ms: 6.504743251115806\n",
      "  time_since_restore: 8613.352933645248\n",
      "  time_this_iter_s: 56.48581552505493\n",
      "  time_total_s: 8613.352933645248\n",
      "  timers:\n",
      "    learn_throughput: 157.468\n",
      "    learn_time_ms: 25401.922\n",
      "    sample_throughput: 125.609\n",
      "    sample_time_ms: 31844.854\n",
      "    update_time_ms: 3.121\n",
      "  timestamp: 1667284017\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 600000\n",
      "  training_iteration: 150\n",
      "  trial_id: 168ff_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 8.5/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/9.6 GiB heap, 0.0/14.65 GiB objects (0.0/6.0 CPU_group_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_3_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_1_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_2_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_0_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_5_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_4_400b3145f046e80a4a7d53925cb6fe08)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                                      | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000 | RUNNING  | 192.168.1.65:16335 |    150 |          8613.35 | 600000 |  146.751 |              154.561 |               138.63 |                400 |\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000:\n",
      "  agent_timesteps_total: 604000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-11-01_01-27-55\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 154.56077250563936\n",
      "  episode_reward_mean: 146.49958486358773\n",
      "  episode_reward_min: 138.62968613054954\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1510\n",
      "  experiment_id: cf8409b3cbbc42979d4540c7e6fbf708\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 7.174648137343063e-44\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -1.0094055980443954\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.005925245972446369\n",
      "          policy_loss: 0.0015808192241820508\n",
      "          total_loss: 354.0994774699211\n",
      "          vf_explained_var: -1.0244548320770264e-08\n",
      "          vf_loss: 354.0978954434395\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 604000\n",
      "    num_steps_sampled: 604000\n",
      "    num_steps_trained: 604000\n",
      "  iterations_since_restore: 151\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.103658536585364\n",
      "    ram_util_percent: 27.302439024390246\n",
      "  pid: 16335\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08573036866677913\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.45055026649447\n",
      "    mean_inference_ms: 1.7827343002772034\n",
      "    mean_raw_obs_processing_ms: 6.503193442548256\n",
      "  time_since_restore: 8670.581084489822\n",
      "  time_this_iter_s: 57.228150844573975\n",
      "  time_total_s: 8670.581084489822\n",
      "  timers:\n",
      "    learn_throughput: 157.494\n",
      "    learn_time_ms: 25397.784\n",
      "    sample_throughput: 125.593\n",
      "    sample_time_ms: 31848.871\n",
      "    update_time_ms: 3.137\n",
      "  timestamp: 1667284075\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 604000\n",
      "  training_iteration: 151\n",
      "  trial_id: 168ff_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 8.5/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/9.6 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_0_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_2_400b3145f046e80a4a7d53925cb6fe08, 0.0/6.0 CPU_group_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_3_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_1_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_5_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_4_400b3145f046e80a4a7d53925cb6fe08)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                                      | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000 | RUNNING  | 192.168.1.65:16335 |    151 |          8670.58 | 604000 |    146.5 |              154.561 |               138.63 |                400 |\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000:\n",
      "  agent_timesteps_total: 608000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-11-01_01-28-52\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 152.80832279222562\n",
      "  episode_reward_mean: 146.4826799995514\n",
      "  episode_reward_min: 138.62968613054954\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1520\n",
      "  experiment_id: cf8409b3cbbc42979d4540c7e6fbf708\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 3.5873240686715314e-44\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -1.0567847304046154\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008152295544641853\n",
      "          policy_loss: -0.00022408968943636863\n",
      "          total_loss: 350.2552865743637\n",
      "          vf_explained_var: -5.9604645663569045e-09\n",
      "          vf_loss: 350.2555111169815\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 608000\n",
      "    num_steps_sampled: 608000\n",
      "    num_steps_trained: 608000\n",
      "  iterations_since_restore: 152\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.31341463414634\n",
      "    ram_util_percent: 27.55853658536585\n",
      "  pid: 16335\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08572857328734403\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.450188784902345\n",
      "    mean_inference_ms: 1.7827534271949212\n",
      "    mean_raw_obs_processing_ms: 6.501672640334766\n",
      "  time_since_restore: 8727.492381572723\n",
      "  time_this_iter_s: 56.911297082901\n",
      "  time_total_s: 8727.492381572723\n",
      "  timers:\n",
      "    learn_throughput: 157.631\n",
      "    learn_time_ms: 25375.793\n",
      "    sample_throughput: 125.588\n",
      "    sample_time_ms: 31850.126\n",
      "    update_time_ms: 3.13\n",
      "  timestamp: 1667284132\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 608000\n",
      "  training_iteration: 152\n",
      "  trial_id: 168ff_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Memory usage on this node: 8.6/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/9.6 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_5_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_2_400b3145f046e80a4a7d53925cb6fe08, 0.0/6.0 CPU_group_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_0_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_1_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_4_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_3_400b3145f046e80a4a7d53925cb6fe08)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                                      | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000 | RUNNING  | 192.168.1.65:16335 |    152 |          8727.49 | 608000 |  146.483 |              152.808 |               138.63 |                400 |\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000:\n",
      "  agent_timesteps_total: 612000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-11-01_01-29-49\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 152.80832279222562\n",
      "  episode_reward_mean: 146.75511894504817\n",
      "  episode_reward_min: 138.62968613054954\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1530\n",
      "  experiment_id: cf8409b3cbbc42979d4540c7e6fbf708\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.7936620343357657e-44\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -1.078658238425851\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.006530869827656716\n",
      "          policy_loss: -8.606707269791514e-05\n",
      "          total_loss: 350.02207760810853\n",
      "          vf_explained_var: -6.146728903644316e-09\n",
      "          vf_loss: 350.0221625328064\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 612000\n",
      "    num_steps_sampled: 612000\n",
      "    num_steps_trained: 612000\n",
      "  iterations_since_restore: 153\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.096341463414632\n",
      "    ram_util_percent: 27.426829268292682\n",
      "  pid: 16335\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08572618852748441\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.449877866535854\n",
      "    mean_inference_ms: 1.782735441008832\n",
      "    mean_raw_obs_processing_ms: 6.50016776115729\n",
      "  time_since_restore: 8785.05576634407\n",
      "  time_this_iter_s: 57.563384771347046\n",
      "  time_total_s: 8785.05576634407\n",
      "  timers:\n",
      "    learn_throughput: 157.471\n",
      "    learn_time_ms: 25401.444\n",
      "    sample_throughput: 125.563\n",
      "    sample_time_ms: 31856.483\n",
      "    update_time_ms: 3.121\n",
      "  timestamp: 1667284189\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 612000\n",
      "  training_iteration: 153\n",
      "  trial_id: 168ff_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 8.6/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/9.6 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_3_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_0_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_4_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_2_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_1_400b3145f046e80a4a7d53925cb6fe08, 0.0/6.0 CPU_group_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_5_400b3145f046e80a4a7d53925cb6fe08)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                                      | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000 | RUNNING  | 192.168.1.65:16335 |    153 |          8785.06 | 612000 |  146.755 |              152.808 |               138.63 |                400 |\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000:\n",
      "  agent_timesteps_total: 616000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-11-01_01-30-47\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 152.80832279222562\n",
      "  episode_reward_mean: 146.79208424771147\n",
      "  episode_reward_min: 138.62968613054954\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1540\n",
      "  experiment_id: cf8409b3cbbc42979d4540c7e6fbf708\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 8.968310171678829e-45\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -1.1453794971108437\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009644678549739272\n",
      "          policy_loss: 6.628834235016257e-05\n",
      "          total_loss: 352.9573036193848\n",
      "          vf_explained_var: -2.8140843824076e-06\n",
      "          vf_loss: 352.957239484787\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 616000\n",
      "    num_steps_sampled: 616000\n",
      "    num_steps_trained: 616000\n",
      "  iterations_since_restore: 154\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.130487804878047\n",
      "    ram_util_percent: 27.331707317073167\n",
      "  pid: 16335\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08572446991658537\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.44958921425381\n",
      "    mean_inference_ms: 1.782704378070215\n",
      "    mean_raw_obs_processing_ms: 6.498595709732183\n",
      "  time_since_restore: 8842.403987646103\n",
      "  time_this_iter_s: 57.34822130203247\n",
      "  time_total_s: 8842.403987646103\n",
      "  timers:\n",
      "    learn_throughput: 157.277\n",
      "    learn_time_ms: 25432.766\n",
      "    sample_throughput: 125.596\n",
      "    sample_time_ms: 31848.202\n",
      "    update_time_ms: 3.125\n",
      "  timestamp: 1667284247\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 616000\n",
      "  training_iteration: 154\n",
      "  trial_id: 168ff_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 8.5/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/9.6 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_3_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_0_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_4_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_2_400b3145f046e80a4a7d53925cb6fe08, 0.0/6.0 CPU_group_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_5_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_1_400b3145f046e80a4a7d53925cb6fe08)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                                      | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000 | RUNNING  | 192.168.1.65:16335 |    154 |           8842.4 | 616000 |  146.792 |              152.808 |               138.63 |                400 |\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000:\n",
      "  agent_timesteps_total: 620000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-11-01_01-31-45\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 152.80832279222562\n",
      "  episode_reward_mean: 146.79899084957202\n",
      "  episode_reward_min: 138.62968613054954\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1550\n",
      "  experiment_id: cf8409b3cbbc42979d4540c7e6fbf708\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 4.484155085839414e-45\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -1.1769442826509475\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008076517906556724\n",
      "          policy_loss: 0.0018354496816755273\n",
      "          total_loss: 350.4935866832733\n",
      "          vf_explained_var: -5.587935447692871e-09\n",
      "          vf_loss: 350.49175066947936\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 620000\n",
      "    num_steps_sampled: 620000\n",
      "    num_steps_trained: 620000\n",
      "  iterations_since_restore: 155\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.954878048780486\n",
      "    ram_util_percent: 27.39512195121951\n",
      "  pid: 16335\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0857228774430515\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.449275648980585\n",
      "    mean_inference_ms: 1.7826979995272014\n",
      "    mean_raw_obs_processing_ms: 6.49697810655651\n",
      "  time_since_restore: 8899.96502494812\n",
      "  time_this_iter_s: 57.56103730201721\n",
      "  time_total_s: 8899.96502494812\n",
      "  timers:\n",
      "    learn_throughput: 157.016\n",
      "    learn_time_ms: 25475.111\n",
      "    sample_throughput: 125.621\n",
      "    sample_time_ms: 31841.819\n",
      "    update_time_ms: 3.125\n",
      "  timestamp: 1667284305\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 620000\n",
      "  training_iteration: 155\n",
      "  trial_id: 168ff_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 8.5/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/9.6 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_0_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_1_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_4_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_3_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_2_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_5_400b3145f046e80a4a7d53925cb6fe08, 0.0/6.0 CPU_group_400b3145f046e80a4a7d53925cb6fe08)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                                      | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000 | RUNNING  | 192.168.1.65:16335 |    155 |          8899.97 | 620000 |  146.799 |              152.808 |               138.63 |                400 |\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000:\n",
      "  agent_timesteps_total: 624000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-11-01_01-32-42\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 152.80832279222562\n",
      "  episode_reward_mean: 146.74881068697448\n",
      "  episode_reward_min: 139.3182114369092\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1560\n",
      "  experiment_id: cf8409b3cbbc42979d4540c7e6fbf708\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.242077542919707e-45\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -1.1666248731315136\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009415127069269147\n",
      "          policy_loss: -0.0007637008748133667\n",
      "          total_loss: 352.63546738624575\n",
      "          vf_explained_var: -1.4901161415892261e-09\n",
      "          vf_loss: 352.6362289905548\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 624000\n",
      "    num_steps_sampled: 624000\n",
      "    num_steps_trained: 624000\n",
      "  iterations_since_restore: 156\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.208536585365852\n",
      "    ram_util_percent: 27.570731707317066\n",
      "  pid: 16335\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0857212351121537\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.44897629391313\n",
      "    mean_inference_ms: 1.7827003892749445\n",
      "    mean_raw_obs_processing_ms: 6.494524708701459\n",
      "  time_since_restore: 8957.082077741623\n",
      "  time_this_iter_s: 57.11705279350281\n",
      "  time_total_s: 8957.082077741623\n",
      "  timers:\n",
      "    learn_throughput: 157.233\n",
      "    learn_time_ms: 25439.955\n",
      "    sample_throughput: 126.023\n",
      "    sample_time_ms: 31740.224\n",
      "    update_time_ms: 3.189\n",
      "  timestamp: 1667284362\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 624000\n",
      "  training_iteration: 156\n",
      "  trial_id: 168ff_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 8.6/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/9.6 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_5_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_3_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_0_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_4_400b3145f046e80a4a7d53925cb6fe08, 0.0/6.0 CPU_group_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_1_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_2_400b3145f046e80a4a7d53925cb6fe08)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                                      | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000 | RUNNING  | 192.168.1.65:16335 |    156 |          8957.08 | 624000 |  146.749 |              152.808 |              139.318 |                400 |\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000:\n",
      "  agent_timesteps_total: 628000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-11-01_01-33-40\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 153.5155159704313\n",
      "  episode_reward_mean: 146.66763813534183\n",
      "  episode_reward_min: 139.3182114369092\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1570\n",
      "  experiment_id: cf8409b3cbbc42979d4540c7e6fbf708\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.1210387714598536e-45\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -1.1665539182722569\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.006109481289976859\n",
      "          policy_loss: 0.0010287747340044008\n",
      "          total_loss: 349.83641185760496\n",
      "          vf_explained_var: 7.450580707946131e-10\n",
      "          vf_loss: 349.8353821277618\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 628000\n",
      "    num_steps_sampled: 628000\n",
      "    num_steps_trained: 628000\n",
      "  iterations_since_restore: 157\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.17710843373494\n",
      "    ram_util_percent: 27.592771084337347\n",
      "  pid: 16335\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0857200881175196\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.448804256175343\n",
      "    mean_inference_ms: 1.7827117208893788\n",
      "    mean_raw_obs_processing_ms: 6.49206745700239\n",
      "  time_since_restore: 9015.077963113785\n",
      "  time_this_iter_s: 57.995885372161865\n",
      "  time_total_s: 9015.077963113785\n",
      "  timers:\n",
      "    learn_throughput: 157.069\n",
      "    learn_time_ms: 25466.472\n",
      "    sample_throughput: 125.971\n",
      "    sample_time_ms: 31753.448\n",
      "    update_time_ms: 3.16\n",
      "  timestamp: 1667284420\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 628000\n",
      "  training_iteration: 157\n",
      "  trial_id: 168ff_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Memory usage on this node: 8.6/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/9.6 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_3_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_2_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_4_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_1_400b3145f046e80a4a7d53925cb6fe08, 0.0/6.0 CPU_group_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_5_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_0_400b3145f046e80a4a7d53925cb6fe08)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                                      | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000 | RUNNING  | 192.168.1.65:16335 |    157 |          9015.08 | 628000 |  146.668 |              153.516 |              139.318 |                400 |\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000:\n",
      "  agent_timesteps_total: 632000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-11-01_01-34-38\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 153.5155159704313\n",
      "  episode_reward_mean: 146.3542630346176\n",
      "  episode_reward_min: 139.3182114369092\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1580\n",
      "  experiment_id: cf8409b3cbbc42979d4540c7e6fbf708\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 5.605193857299268e-46\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -1.1364367589354516\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0055456758719117875\n",
      "          policy_loss: -0.0009560604317812249\n",
      "          total_loss: 348.20531420707704\n",
      "          vf_explained_var: -7.450580707946131e-10\n",
      "          vf_loss: 348.20626776218415\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 632000\n",
      "    num_steps_sampled: 632000\n",
      "    num_steps_trained: 632000\n",
      "  iterations_since_restore: 158\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.14268292682927\n",
      "    ram_util_percent: 27.657317073170727\n",
      "  pid: 16335\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08571940870145893\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.448641078540152\n",
      "    mean_inference_ms: 1.7827404814688712\n",
      "    mean_raw_obs_processing_ms: 6.489686579772995\n",
      "  time_since_restore: 9072.464584112167\n",
      "  time_this_iter_s: 57.38662099838257\n",
      "  time_total_s: 9072.464584112167\n",
      "  timers:\n",
      "    learn_throughput: 157.03\n",
      "    learn_time_ms: 25472.842\n",
      "    sample_throughput: 125.946\n",
      "    sample_time_ms: 31759.702\n",
      "    update_time_ms: 3.149\n",
      "  timestamp: 1667284478\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 632000\n",
      "  training_iteration: 158\n",
      "  trial_id: 168ff_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 8.6/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/9.6 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_5_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_2_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_4_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_3_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_0_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_1_400b3145f046e80a4a7d53925cb6fe08, 0.0/6.0 CPU_group_400b3145f046e80a4a7d53925cb6fe08)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                                      | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000 | RUNNING  | 192.168.1.65:16335 |    158 |          9072.46 | 632000 |  146.354 |              153.516 |              139.318 |                400 |\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000:\n",
      "  agent_timesteps_total: 636000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-11-01_01-35-35\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 153.5155159704313\n",
      "  episode_reward_mean: 146.5671728157807\n",
      "  episode_reward_min: 139.3182114369092\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1590\n",
      "  experiment_id: cf8409b3cbbc42979d4540c7e6fbf708\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.802596928649634e-46\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -1.1444617059081792\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.004670986298441626\n",
      "          policy_loss: 0.0027359349754988217\n",
      "          total_loss: 352.89217338562014\n",
      "          vf_explained_var: -7.078051478259795e-09\n",
      "          vf_loss: 352.8894361257553\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 636000\n",
      "    num_steps_sampled: 636000\n",
      "    num_steps_trained: 636000\n",
      "  iterations_since_restore: 159\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.135365853658534\n",
      "    ram_util_percent: 27.653658536585358\n",
      "  pid: 16335\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08571913223409226\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.448455138104713\n",
      "    mean_inference_ms: 1.782785778035846\n",
      "    mean_raw_obs_processing_ms: 6.487337248799827\n",
      "  time_since_restore: 9129.66084575653\n",
      "  time_this_iter_s: 57.1962616443634\n",
      "  time_total_s: 9129.66084575653\n",
      "  timers:\n",
      "    learn_throughput: 156.818\n",
      "    learn_time_ms: 25507.198\n",
      "    sample_throughput: 125.952\n",
      "    sample_time_ms: 31758.035\n",
      "    update_time_ms: 3.209\n",
      "  timestamp: 1667284535\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 636000\n",
      "  training_iteration: 159\n",
      "  trial_id: 168ff_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 8.6/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/9.6 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_2_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_4_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_1_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_3_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_0_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_5_400b3145f046e80a4a7d53925cb6fe08, 0.0/6.0 CPU_group_400b3145f046e80a4a7d53925cb6fe08)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                                      | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000 | RUNNING  | 192.168.1.65:16335 |    159 |          9129.66 | 636000 |  146.567 |              153.516 |              139.318 |                400 |\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000:\n",
      "  agent_timesteps_total: 640000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-11-01_01-36-33\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 153.5155159704313\n",
      "  episode_reward_mean: 146.43624534681396\n",
      "  episode_reward_min: 139.3182114369092\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1600\n",
      "  experiment_id: cf8409b3cbbc42979d4540c7e6fbf708\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.401298464324817e-46\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -1.1452666513621808\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0053401358060855665\n",
      "          policy_loss: -0.0009838124591624365\n",
      "          total_loss: 346.1788843870163\n",
      "          vf_explained_var: -3.7252903539730653e-10\n",
      "          vf_loss: 346.17986713647844\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 640000\n",
      "    num_steps_sampled: 640000\n",
      "    num_steps_trained: 640000\n",
      "  iterations_since_restore: 160\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.214634146341464\n",
      "    ram_util_percent: 27.60487804878049\n",
      "  pid: 16335\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08571963696392018\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.448281125591382\n",
      "    mean_inference_ms: 1.7828218536009817\n",
      "    mean_raw_obs_processing_ms: 6.484975906281586\n",
      "  time_since_restore: 9187.165895462036\n",
      "  time_this_iter_s: 57.50504970550537\n",
      "  time_total_s: 9187.165895462036\n",
      "  timers:\n",
      "    learn_throughput: 156.185\n",
      "    learn_time_ms: 25610.639\n",
      "    sample_throughput: 125.959\n",
      "    sample_time_ms: 31756.469\n",
      "    update_time_ms: 3.255\n",
      "  timestamp: 1667284593\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 640000\n",
      "  training_iteration: 160\n",
      "  trial_id: 168ff_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 8.6/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/9.6 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_3_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_4_400b3145f046e80a4a7d53925cb6fe08, 0.0/6.0 CPU_group_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_0_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_1_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_2_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_5_400b3145f046e80a4a7d53925cb6fe08)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                                      | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000 | RUNNING  | 192.168.1.65:16335 |    160 |          9187.17 | 640000 |  146.436 |              153.516 |              139.318 |                400 |\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000:\n",
      "  agent_timesteps_total: 644000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-11-01_01-37-30\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 153.5155159704313\n",
      "  episode_reward_mean: 146.59484594916378\n",
      "  episode_reward_min: 136.32325598957735\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1610\n",
      "  experiment_id: cf8409b3cbbc42979d4540c7e6fbf708\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 7.006492321624085e-47\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -1.1749516166746616\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.00749387134992503\n",
      "          policy_loss: 4.030655254609883e-05\n",
      "          total_loss: 348.7470389127731\n",
      "          vf_explained_var: -6.51925802230835e-09\n",
      "          vf_loss: 348.74699835777284\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 644000\n",
      "    num_steps_sampled: 644000\n",
      "    num_steps_trained: 644000\n",
      "  iterations_since_restore: 161\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.225609756097562\n",
      "    ram_util_percent: 27.679268292682934\n",
      "  pid: 16335\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0857201791710628\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.44808426093796\n",
      "    mean_inference_ms: 1.7828820577636544\n",
      "    mean_raw_obs_processing_ms: 6.482656865637112\n",
      "  time_since_restore: 9244.418012142181\n",
      "  time_this_iter_s: 57.252116680145264\n",
      "  time_total_s: 9244.418012142181\n",
      "  timers:\n",
      "    learn_throughput: 156.147\n",
      "    learn_time_ms: 25616.809\n",
      "    sample_throughput: 125.973\n",
      "    sample_time_ms: 31752.784\n",
      "    update_time_ms: 3.241\n",
      "  timestamp: 1667284650\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 644000\n",
      "  training_iteration: 161\n",
      "  trial_id: 168ff_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 8.6/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/9.6 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_2_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_3_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_1_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_5_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_0_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_4_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 accelerator_type:G, 0.0/6.0 CPU_group_400b3145f046e80a4a7d53925cb6fe08)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                                      | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000 | RUNNING  | 192.168.1.65:16335 |    161 |          9244.42 | 644000 |  146.595 |              153.516 |              136.323 |                400 |\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000:\n",
      "  agent_timesteps_total: 648000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-11-01_01-38-28\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 153.5155159704313\n",
      "  episode_reward_mean: 146.56250413216912\n",
      "  episode_reward_min: 136.32325598957735\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1620\n",
      "  experiment_id: cf8409b3cbbc42979d4540c7e6fbf708\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 3.5032461608120424e-47\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -1.2456343904137612\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008335846976116734\n",
      "          policy_loss: 0.001430561587039847\n",
      "          total_loss: 352.521013879776\n",
      "          vf_explained_var: -5.9604645663569045e-09\n",
      "          vf_loss: 352.51958374977113\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 648000\n",
      "    num_steps_sampled: 648000\n",
      "    num_steps_trained: 648000\n",
      "  iterations_since_restore: 162\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.12530120481928\n",
      "    ram_util_percent: 27.806024096385542\n",
      "  pid: 16335\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08572097327313241\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.44788929595999\n",
      "    mean_inference_ms: 1.7829447984142468\n",
      "    mean_raw_obs_processing_ms: 6.4804024716894\n",
      "  time_since_restore: 9302.179027795792\n",
      "  time_this_iter_s: 57.76101565361023\n",
      "  time_total_s: 9302.179027795792\n",
      "  timers:\n",
      "    learn_throughput: 155.673\n",
      "    learn_time_ms: 25694.861\n",
      "    sample_throughput: 125.946\n",
      "    sample_time_ms: 31759.703\n",
      "    update_time_ms: 3.233\n",
      "  timestamp: 1667284708\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 648000\n",
      "  training_iteration: 162\n",
      "  trial_id: 168ff_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Memory usage on this node: 8.7/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/9.6 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_3_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_5_400b3145f046e80a4a7d53925cb6fe08, 0.0/6.0 CPU_group_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_0_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_2_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_4_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_1_400b3145f046e80a4a7d53925cb6fe08)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                                      | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000 | RUNNING  | 192.168.1.65:16335 |    162 |          9302.18 | 648000 |  146.563 |              153.516 |              136.323 |                400 |\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000:\n",
      "  agent_timesteps_total: 652000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-11-01_01-39-25\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 153.5155159704313\n",
      "  episode_reward_mean: 146.31094937773426\n",
      "  episode_reward_min: 136.32325598957735\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1630\n",
      "  experiment_id: cf8409b3cbbc42979d4540c7e6fbf708\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.7516230804060212e-47\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -1.2807538352906704\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.00835992716220062\n",
      "          policy_loss: 0.0015448728678165934\n",
      "          total_loss: 349.2699098944664\n",
      "          vf_explained_var: -1.3738870165980188e-06\n",
      "          vf_loss: 349.268363237381\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 652000\n",
      "    num_steps_sampled: 652000\n",
      "    num_steps_trained: 652000\n",
      "  iterations_since_restore: 163\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.114634146341462\n",
      "    ram_util_percent: 27.701219512195117\n",
      "  pid: 16335\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08572240416589683\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.447707235115978\n",
      "    mean_inference_ms: 1.7830105782577699\n",
      "    mean_raw_obs_processing_ms: 6.478202071414107\n",
      "  time_since_restore: 9359.43774008751\n",
      "  time_this_iter_s: 57.25871229171753\n",
      "  time_total_s: 9359.43774008751\n",
      "  timers:\n",
      "    learn_throughput: 155.833\n",
      "    learn_time_ms: 25668.424\n",
      "    sample_throughput: 125.962\n",
      "    sample_time_ms: 31755.658\n",
      "    update_time_ms: 3.24\n",
      "  timestamp: 1667284765\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 652000\n",
      "  training_iteration: 163\n",
      "  trial_id: 168ff_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 8.6/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/9.6 GiB heap, 0.0/14.65 GiB objects (0.0/6.0 CPU_group_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_1_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_3_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_0_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_5_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_4_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_2_400b3145f046e80a4a7d53925cb6fe08)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                                      | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000 | RUNNING  | 192.168.1.65:16335 |    163 |          9359.44 | 652000 |  146.311 |              153.516 |              136.323 |                400 |\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000:\n",
      "  agent_timesteps_total: 656000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-11-01_01-40-23\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 153.5155159704313\n",
      "  episode_reward_mean: 146.04429068120132\n",
      "  episode_reward_min: 134.682137129095\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1640\n",
      "  experiment_id: cf8409b3cbbc42979d4540c7e6fbf708\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 8.758115402030106e-48\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -1.2836540389806033\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.007726733287472776\n",
      "          policy_loss: -3.8362189661711456e-05\n",
      "          total_loss: 348.6216629743576\n",
      "          vf_explained_var: 0.0\n",
      "          vf_loss: 348.6216989994049\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 656000\n",
      "    num_steps_sampled: 656000\n",
      "    num_steps_trained: 656000\n",
      "  iterations_since_restore: 164\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.065853658536586\n",
      "    ram_util_percent: 27.687804878048777\n",
      "  pid: 16335\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08572387345973016\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.447520398542625\n",
      "    mean_inference_ms: 1.7830850059020136\n",
      "    mean_raw_obs_processing_ms: 6.476093186534315\n",
      "  time_since_restore: 9416.720831394196\n",
      "  time_this_iter_s: 57.2830913066864\n",
      "  time_total_s: 9416.720831394196\n",
      "  timers:\n",
      "    learn_throughput: 155.935\n",
      "    learn_time_ms: 25651.77\n",
      "    sample_throughput: 125.922\n",
      "    sample_time_ms: 31765.725\n",
      "    update_time_ms: 3.267\n",
      "  timestamp: 1667284823\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 656000\n",
      "  training_iteration: 164\n",
      "  trial_id: 168ff_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 8.6/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/9.6 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_4_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_3_400b3145f046e80a4a7d53925cb6fe08, 0.0/6.0 CPU_group_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_0_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_1_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_2_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_5_400b3145f046e80a4a7d53925cb6fe08)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                                      | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000 | RUNNING  | 192.168.1.65:16335 |    164 |          9416.72 | 656000 |  146.044 |              153.516 |              134.682 |                400 |\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000:\n",
      "  agent_timesteps_total: 660000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-11-01_01-41-19\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 153.5155159704313\n",
      "  episode_reward_mean: 145.66057158254003\n",
      "  episode_reward_min: 134.682137129095\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1650\n",
      "  experiment_id: cf8409b3cbbc42979d4540c7e6fbf708\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 4.379057701015053e-48\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -1.2437248177826405\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.005555323024918834\n",
      "          policy_loss: 0.0012350516626611352\n",
      "          total_loss: 346.552631354332\n",
      "          vf_explained_var: -9.313225746154785e-09\n",
      "          vf_loss: 346.55139539241793\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 660000\n",
      "    num_steps_sampled: 660000\n",
      "    num_steps_trained: 660000\n",
      "  iterations_since_restore: 165\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.425\n",
      "    ram_util_percent: 27.736250000000005\n",
      "  pid: 16335\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08572555283933739\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.447322764561815\n",
      "    mean_inference_ms: 1.783142423346757\n",
      "    mean_raw_obs_processing_ms: 6.474062043815405\n",
      "  time_since_restore: 9472.734172582626\n",
      "  time_this_iter_s: 56.013341188430786\n",
      "  time_total_s: 9472.734172582626\n",
      "  timers:\n",
      "    learn_throughput: 156.88\n",
      "    learn_time_ms: 25497.161\n",
      "    sample_throughput: 125.923\n",
      "    sample_time_ms: 31765.506\n",
      "    update_time_ms: 3.259\n",
      "  timestamp: 1667284879\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 660000\n",
      "  training_iteration: 165\n",
      "  trial_id: 168ff_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 8.7/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/9.6 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_5_400b3145f046e80a4a7d53925cb6fe08, 0.0/6.0 CPU_group_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_4_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_1_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_2_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_3_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_0_400b3145f046e80a4a7d53925cb6fe08)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                                      | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000 | RUNNING  | 192.168.1.65:16335 |    165 |          9472.73 | 660000 |  145.661 |              153.516 |              134.682 |                400 |\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=16334)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=16332)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=16330)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=16331)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=16333)\u001b[0m  Retrying in 1 seconds\n",
      "Result for PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000:\n",
      "  agent_timesteps_total: 664000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-11-01_01-42-17\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 153.5155159704313\n",
      "  episode_reward_mean: 145.55515130374127\n",
      "  episode_reward_min: 134.682137129095\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1660\n",
      "  experiment_id: cf8409b3cbbc42979d4540c7e6fbf708\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.1895288505075265e-48\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -1.2543103139847518\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.007539073986210276\n",
      "          policy_loss: 0.001769823019276373\n",
      "          total_loss: 345.97117733955383\n",
      "          vf_explained_var: -3.166496842510469e-09\n",
      "          vf_loss: 345.9694042682648\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 664000\n",
      "    num_steps_sampled: 664000\n",
      "    num_steps_trained: 664000\n",
      "  iterations_since_restore: 166\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.980722891566263\n",
      "    ram_util_percent: 27.67349397590361\n",
      "  pid: 16335\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0857272160547441\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.447124909046092\n",
      "    mean_inference_ms: 1.7831924341662102\n",
      "    mean_raw_obs_processing_ms: 6.472844627046486\n",
      "  time_since_restore: 9530.927092313766\n",
      "  time_this_iter_s: 58.19291973114014\n",
      "  time_total_s: 9530.927092313766\n",
      "  timers:\n",
      "    learn_throughput: 156.859\n",
      "    learn_time_ms: 25500.663\n",
      "    sample_throughput: 125.511\n",
      "    sample_time_ms: 31869.641\n",
      "    update_time_ms: 3.191\n",
      "  timestamp: 1667284937\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 664000\n",
      "  training_iteration: 166\n",
      "  trial_id: 168ff_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 8.6/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/9.6 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_4_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_5_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_2_400b3145f046e80a4a7d53925cb6fe08, 0.0/6.0 CPU_group_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_1_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_0_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_3_400b3145f046e80a4a7d53925cb6fe08)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                                      | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000 | RUNNING  | 192.168.1.65:16335 |    166 |          9530.93 | 664000 |  145.555 |              153.516 |              134.682 |                400 |\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000:\n",
      "  agent_timesteps_total: 668000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-11-01_01-43-15\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 153.43333488748053\n",
      "  episode_reward_mean: 145.4194148626548\n",
      "  episode_reward_min: 134.682137129095\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1670\n",
      "  experiment_id: cf8409b3cbbc42979d4540c7e6fbf708\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0947644252537632e-48\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -1.2476749923080206\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008209089666957325\n",
      "          policy_loss: 0.0021664266707375644\n",
      "          total_loss: 346.52322285175325\n",
      "          vf_explained_var: -4.842877210364804e-09\n",
      "          vf_loss: 346.52105541229247\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 668000\n",
      "    num_steps_sampled: 668000\n",
      "    num_steps_trained: 668000\n",
      "  iterations_since_restore: 167\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.476829268292683\n",
      "    ram_util_percent: 27.664634146341463\n",
      "  pid: 16335\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08572859794402835\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.446793535753322\n",
      "    mean_inference_ms: 1.783240152052818\n",
      "    mean_raw_obs_processing_ms: 6.471677698086057\n",
      "  time_since_restore: 9588.59332561493\n",
      "  time_this_iter_s: 57.66623330116272\n",
      "  time_total_s: 9588.59332561493\n",
      "  timers:\n",
      "    learn_throughput: 156.979\n",
      "    learn_time_ms: 25481.169\n",
      "    sample_throughput: 125.565\n",
      "    sample_time_ms: 31856.115\n",
      "    update_time_ms: 3.174\n",
      "  timestamp: 1667284995\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 668000\n",
      "  training_iteration: 167\n",
      "  trial_id: 168ff_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Memory usage on this node: 8.7/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/9.6 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_0_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_1_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_4_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_3_400b3145f046e80a4a7d53925cb6fe08, 0.0/6.0 CPU_group_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_5_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_2_400b3145f046e80a4a7d53925cb6fe08)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                                      | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000 | RUNNING  | 192.168.1.65:16335 |    167 |          9588.59 | 668000 |  145.419 |              153.433 |              134.682 |                400 |\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000:\n",
      "  agent_timesteps_total: 672000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-11-01_01-44-12\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 153.43333488748053\n",
      "  episode_reward_mean: 145.48438457810553\n",
      "  episode_reward_min: 134.682137129095\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1680\n",
      "  experiment_id: cf8409b3cbbc42979d4540c7e6fbf708\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 5.473822126268816e-49\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -1.266105504706502\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.005035330739065547\n",
      "          policy_loss: 0.0017233946666237899\n",
      "          total_loss: 346.40985209941863\n",
      "          vf_explained_var: -1.2665987370041876e-08\n",
      "          vf_loss: 346.4081267595291\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 672000\n",
      "    num_steps_sampled: 672000\n",
      "    num_steps_trained: 672000\n",
      "  iterations_since_restore: 168\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.212195121951215\n",
      "    ram_util_percent: 27.68536585365854\n",
      "  pid: 16335\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08572949927190258\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.44647378903181\n",
      "    mean_inference_ms: 1.7832578938558392\n",
      "    mean_raw_obs_processing_ms: 6.470498834350272\n",
      "  time_since_restore: 9645.735395908356\n",
      "  time_this_iter_s: 57.142070293426514\n",
      "  time_total_s: 9645.735395908356\n",
      "  timers:\n",
      "    learn_throughput: 157.098\n",
      "    learn_time_ms: 25461.829\n",
      "    sample_throughput: 125.585\n",
      "    sample_time_ms: 31851.027\n",
      "    update_time_ms: 3.224\n",
      "  timestamp: 1667285052\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 672000\n",
      "  training_iteration: 168\n",
      "  trial_id: 168ff_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 8.6/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/9.6 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_4_400b3145f046e80a4a7d53925cb6fe08, 0.0/6.0 CPU_group_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_1_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_0_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_5_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_3_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_2_400b3145f046e80a4a7d53925cb6fe08)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                                      | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000 | RUNNING  | 192.168.1.65:16335 |    168 |          9645.74 | 672000 |  145.484 |              153.433 |              134.682 |                400 |\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000:\n",
      "  agent_timesteps_total: 676000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-11-01_01-45-10\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 152.99223407819557\n",
      "  episode_reward_mean: 145.2369836344373\n",
      "  episode_reward_min: 134.682137129095\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1690\n",
      "  experiment_id: cf8409b3cbbc42979d4540c7e6fbf708\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.736911063134408e-49\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -1.3032647524029017\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010101364831098626\n",
      "          policy_loss: 0.00016367720090784132\n",
      "          total_loss: 344.19004092216494\n",
      "          vf_explained_var: 0.0\n",
      "          vf_loss: 344.18987646102903\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 676000\n",
      "    num_steps_sampled: 676000\n",
      "    num_steps_trained: 676000\n",
      "  iterations_since_restore: 169\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.1\n",
      "    ram_util_percent: 27.821951219512197\n",
      "  pid: 16335\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08573064793295897\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.4462106625126\n",
      "    mean_inference_ms: 1.783239264700469\n",
      "    mean_raw_obs_processing_ms: 6.469374684951727\n",
      "  time_since_restore: 9702.975107192993\n",
      "  time_this_iter_s: 57.23971128463745\n",
      "  time_total_s: 9702.975107192993\n",
      "  timers:\n",
      "    learn_throughput: 157.099\n",
      "    learn_time_ms: 25461.697\n",
      "    sample_throughput: 125.567\n",
      "    sample_time_ms: 31855.492\n",
      "    update_time_ms: 3.18\n",
      "  timestamp: 1667285110\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 676000\n",
      "  training_iteration: 169\n",
      "  trial_id: 168ff_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 8.7/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/9.6 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_5_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_3_400b3145f046e80a4a7d53925cb6fe08, 0.0/6.0 CPU_group_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_2_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_1_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_4_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_0_400b3145f046e80a4a7d53925cb6fe08)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                                      | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000 | RUNNING  | 192.168.1.65:16335 |    169 |          9702.98 | 676000 |  145.237 |              152.992 |              134.682 |                400 |\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000:\n",
      "  agent_timesteps_total: 680000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-11-01_01-46-08\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 156.11131316828343\n",
      "  episode_reward_mean: 145.10062565535273\n",
      "  episode_reward_min: 134.682137129095\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1700\n",
      "  experiment_id: cf8409b3cbbc42979d4540c7e6fbf708\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.736911063134408e-49\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -1.3052080146968366\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.00797921515445541\n",
      "          policy_loss: 0.0012731728842481972\n",
      "          total_loss: 347.6995661735535\n",
      "          vf_explained_var: -7.51763593598298e-07\n",
      "          vf_loss: 347.6982924938202\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 680000\n",
      "    num_steps_sampled: 680000\n",
      "    num_steps_trained: 680000\n",
      "  iterations_since_restore: 170\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.22439024390244\n",
      "    ram_util_percent: 27.747560975609765\n",
      "  pid: 16335\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08573113374398156\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.445947219500876\n",
      "    mean_inference_ms: 1.7832220287377958\n",
      "    mean_raw_obs_processing_ms: 6.46829916184616\n",
      "  time_since_restore: 9760.550207138062\n",
      "  time_this_iter_s: 57.57509994506836\n",
      "  time_total_s: 9760.550207138062\n",
      "  timers:\n",
      "    learn_throughput: 157.074\n",
      "    learn_time_ms: 25465.693\n",
      "    sample_throughput: 125.555\n",
      "    sample_time_ms: 31858.509\n",
      "    update_time_ms: 3.142\n",
      "  timestamp: 1667285168\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 680000\n",
      "  training_iteration: 170\n",
      "  trial_id: 168ff_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 8.6/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/9.6 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_2_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_3_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_4_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_1_400b3145f046e80a4a7d53925cb6fe08, 0.0/6.0 CPU_group_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_5_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_0_400b3145f046e80a4a7d53925cb6fe08)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                                      | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000 | RUNNING  | 192.168.1.65:16335 |    170 |          9760.55 | 680000 |  145.101 |              156.111 |              134.682 |                400 |\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000:\n",
      "  agent_timesteps_total: 684000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-11-01_01-47-05\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 156.11131316828343\n",
      "  episode_reward_mean: 145.12489389942934\n",
      "  episode_reward_min: 134.682137129095\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1710\n",
      "  experiment_id: cf8409b3cbbc42979d4540c7e6fbf708\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.368455531567204e-49\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -1.3070564039051533\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.007442140041302991\n",
      "          policy_loss: -0.002978107566013932\n",
      "          total_loss: 346.540436565876\n",
      "          vf_explained_var: -2.7939677238464355e-09\n",
      "          vf_loss: 346.54341676235197\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 684000\n",
      "    num_steps_sampled: 684000\n",
      "    num_steps_trained: 684000\n",
      "  iterations_since_restore: 171\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.072289156626507\n",
      "    ram_util_percent: 27.76024096385542\n",
      "  pid: 16335\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08573158771788872\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.445715049885845\n",
      "    mean_inference_ms: 1.7831823973264962\n",
      "    mean_raw_obs_processing_ms: 6.467227159144966\n",
      "  time_since_restore: 9818.190050125122\n",
      "  time_this_iter_s: 57.63984298706055\n",
      "  time_total_s: 9818.190050125122\n",
      "  timers:\n",
      "    learn_throughput: 156.835\n",
      "    learn_time_ms: 25504.59\n",
      "    sample_throughput: 125.556\n",
      "    sample_time_ms: 31858.319\n",
      "    update_time_ms: 3.173\n",
      "  timestamp: 1667285225\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 684000\n",
      "  training_iteration: 171\n",
      "  trial_id: 168ff_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 8.7/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/9.6 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_0_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_5_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_1_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_3_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_2_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_4_400b3145f046e80a4a7d53925cb6fe08, 0.0/6.0 CPU_group_400b3145f046e80a4a7d53925cb6fe08)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                                      | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000 | RUNNING  | 192.168.1.65:16335 |    171 |          9818.19 | 684000 |  145.125 |              156.111 |              134.682 |                400 |\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000:\n",
      "  agent_timesteps_total: 688000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-11-01_01-48-03\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 156.11131316828343\n",
      "  episode_reward_mean: 144.94416037822032\n",
      "  episode_reward_min: 134.682137129095\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1720\n",
      "  experiment_id: cf8409b3cbbc42979d4540c7e6fbf708\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 6.84227765783602e-50\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -1.298617308959365\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.007081624624129468\n",
      "          policy_loss: 0.00039445438160328193\n",
      "          total_loss: 346.69066617488863\n",
      "          vf_explained_var: -5.587935447692871e-09\n",
      "          vf_loss: 346.6902713775635\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 688000\n",
      "    num_steps_sampled: 688000\n",
      "    num_steps_trained: 688000\n",
      "  iterations_since_restore: 172\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.201219512195124\n",
      "    ram_util_percent: 27.73658536585366\n",
      "  pid: 16335\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08573190763503546\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.445479470698913\n",
      "    mean_inference_ms: 1.7831507216592302\n",
      "    mean_raw_obs_processing_ms: 6.466120719539878\n",
      "  time_since_restore: 9875.55725312233\n",
      "  time_this_iter_s: 57.36720299720764\n",
      "  time_total_s: 9875.55725312233\n",
      "  timers:\n",
      "    learn_throughput: 157.03\n",
      "    learn_time_ms: 25472.823\n",
      "    sample_throughput: 125.586\n",
      "    sample_time_ms: 31850.66\n",
      "    update_time_ms: 3.184\n",
      "  timestamp: 1667285283\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 688000\n",
      "  training_iteration: 172\n",
      "  trial_id: 168ff_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Memory usage on this node: 8.7/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/9.6 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_1_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_2_400b3145f046e80a4a7d53925cb6fe08, 0.0/6.0 CPU_group_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_3_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_4_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_5_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_0_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 accelerator_type:G)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                                      | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000 | RUNNING  | 192.168.1.65:16335 |    172 |          9875.56 | 688000 |  144.944 |              156.111 |              134.682 |                400 |\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000:\n",
      "  agent_timesteps_total: 692000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-11-01_01-49-01\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 156.11131316828343\n",
      "  episode_reward_mean: 144.98911416030842\n",
      "  episode_reward_min: 134.682137129095\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1730\n",
      "  experiment_id: cf8409b3cbbc42979d4540c7e6fbf708\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 3.42113882891801e-50\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -1.3477028969675302\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010917623538444587\n",
      "          policy_loss: 0.0007559656805824488\n",
      "          total_loss: 346.1998490333557\n",
      "          vf_explained_var: -3.3527611797978807e-09\n",
      "          vf_loss: 346.19908967018125\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 692000\n",
      "    num_steps_sampled: 692000\n",
      "    num_steps_trained: 692000\n",
      "  iterations_since_restore: 173\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.17560975609756\n",
      "    ram_util_percent: 27.804878048780488\n",
      "  pid: 16335\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08573168059078227\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.445241755567984\n",
      "    mean_inference_ms: 1.7831163807029569\n",
      "    mean_raw_obs_processing_ms: 6.464990075549087\n",
      "  time_since_restore: 9933.063559532166\n",
      "  time_this_iter_s: 57.506306409835815\n",
      "  time_total_s: 9933.063559532166\n",
      "  timers:\n",
      "    learn_throughput: 156.849\n",
      "    learn_time_ms: 25502.213\n",
      "    sample_throughput: 125.605\n",
      "    sample_time_ms: 31845.896\n",
      "    update_time_ms: 3.197\n",
      "  timestamp: 1667285341\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 692000\n",
      "  training_iteration: 173\n",
      "  trial_id: 168ff_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 8.7/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/9.6 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 accelerator_type:G, 0.0/6.0 CPU_group_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_1_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_5_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_4_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_2_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_0_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_3_400b3145f046e80a4a7d53925cb6fe08)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                                      | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000 | RUNNING  | 192.168.1.65:16335 |    173 |          9933.06 | 692000 |  144.989 |              156.111 |              134.682 |                400 |\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=16331)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=16333)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=16332)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=16330)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=16334)\u001b[0m  Retrying in 1 seconds\n",
      "Result for PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000:\n",
      "  agent_timesteps_total: 696000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-11-01_01-49-59\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 156.11131316828343\n",
      "  episode_reward_mean: 145.15682114630135\n",
      "  episode_reward_min: 135.2133877216859\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1740\n",
      "  experiment_id: cf8409b3cbbc42979d4540c7e6fbf708\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 3.42113882891801e-50\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -1.3506610129028558\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.00846888098017189\n",
      "          policy_loss: 0.003065982343105134\n",
      "          total_loss: 345.2694343805313\n",
      "          vf_explained_var: -5.029141991741426e-09\n",
      "          vf_loss: 345.2663668394089\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 696000\n",
      "    num_steps_sampled: 696000\n",
      "    num_steps_trained: 696000\n",
      "  iterations_since_restore: 174\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.842857142857145\n",
      "    ram_util_percent: 27.796428571428578\n",
      "  pid: 16335\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08573130639703659\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.444980497050256\n",
      "    mean_inference_ms: 1.783093713978177\n",
      "    mean_raw_obs_processing_ms: 6.464563430975879\n",
      "  time_since_restore: 9991.313678264618\n",
      "  time_this_iter_s: 58.25011873245239\n",
      "  time_total_s: 9991.313678264618\n",
      "  timers:\n",
      "    learn_throughput: 156.827\n",
      "    learn_time_ms: 25505.861\n",
      "    sample_throughput: 125.239\n",
      "    sample_time_ms: 31938.959\n",
      "    update_time_ms: 3.22\n",
      "  timestamp: 1667285399\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 696000\n",
      "  training_iteration: 174\n",
      "  trial_id: 168ff_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 8.7/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/9.6 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_4_400b3145f046e80a4a7d53925cb6fe08, 0.0/6.0 CPU_group_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_1_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_5_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_2_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_3_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_0_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 accelerator_type:G)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                                      | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000 | RUNNING  | 192.168.1.65:16335 |    174 |          9991.31 | 696000 |  145.157 |              156.111 |              135.213 |                400 |\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000:\n",
      "  agent_timesteps_total: 700000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-11-01_01-50-57\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 156.11131316828343\n",
      "  episode_reward_mean: 145.3203140440982\n",
      "  episode_reward_min: 132.46280178320555\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1750\n",
      "  experiment_id: cf8409b3cbbc42979d4540c7e6fbf708\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.710569414459005e-50\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -1.391441909968853\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012476036733778706\n",
      "          policy_loss: -0.0022396879488951527\n",
      "          total_loss: 346.0929931163788\n",
      "          vf_explained_var: -4.252418932537694e-07\n",
      "          vf_loss: 346.0952347040176\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 700000\n",
      "    num_steps_sampled: 700000\n",
      "    num_steps_trained: 700000\n",
      "  iterations_since_restore: 175\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.196341463414633\n",
      "    ram_util_percent: 27.775609756097566\n",
      "  pid: 16335\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08573050109599777\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.444734170291554\n",
      "    mean_inference_ms: 1.7830734243816506\n",
      "    mean_raw_obs_processing_ms: 6.464157600761451\n",
      "  time_since_restore: 10048.757952451706\n",
      "  time_this_iter_s: 57.44427418708801\n",
      "  time_total_s: 10048.757952451706\n",
      "  timers:\n",
      "    learn_throughput: 155.974\n",
      "    learn_time_ms: 25645.25\n",
      "    sample_throughput: 125.224\n",
      "    sample_time_ms: 31942.696\n",
      "    update_time_ms: 3.227\n",
      "  timestamp: 1667285457\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 700000\n",
      "  training_iteration: 175\n",
      "  trial_id: 168ff_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 8.6/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/9.6 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_3_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_1_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_5_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_0_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_2_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_4_400b3145f046e80a4a7d53925cb6fe08, 0.0/6.0 CPU_group_400b3145f046e80a4a7d53925cb6fe08)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                                      | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000 | RUNNING  | 192.168.1.65:16335 |    175 |          10048.8 | 700000 |   145.32 |              156.111 |              132.463 |                400 |\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000:\n",
      "  agent_timesteps_total: 704000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-11-01_01-51-54\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 156.11131316828343\n",
      "  episode_reward_mean: 145.35270027264002\n",
      "  episode_reward_min: 132.46280178320555\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1760\n",
      "  experiment_id: cf8409b3cbbc42979d4540c7e6fbf708\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.710569414459005e-50\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -1.400783222168684\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.005646743231727672\n",
      "          policy_loss: 0.0014755773299839348\n",
      "          total_loss: 345.11979315280917\n",
      "          vf_explained_var: -2.4624168304399063e-07\n",
      "          vf_loss: 345.1183172702789\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 704000\n",
      "    num_steps_sampled: 704000\n",
      "    num_steps_trained: 704000\n",
      "  iterations_since_restore: 176\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.21829268292683\n",
      "    ram_util_percent: 27.985365853658532\n",
      "  pid: 16335\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08572939089227356\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.444503920252135\n",
      "    mean_inference_ms: 1.7830399028492372\n",
      "    mean_raw_obs_processing_ms: 6.463001460004664\n",
      "  time_since_restore: 10106.162661075592\n",
      "  time_this_iter_s: 57.40470862388611\n",
      "  time_total_s: 10106.162661075592\n",
      "  timers:\n",
      "    learn_throughput: 155.827\n",
      "    learn_time_ms: 25669.428\n",
      "    sample_throughput: 125.63\n",
      "    sample_time_ms: 31839.651\n",
      "    update_time_ms: 3.265\n",
      "  timestamp: 1667285514\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 704000\n",
      "  training_iteration: 176\n",
      "  trial_id: 168ff_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 8.7/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/9.6 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_0_400b3145f046e80a4a7d53925cb6fe08, 0.0/6.0 CPU_group_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_4_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_2_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_1_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_3_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_5_400b3145f046e80a4a7d53925cb6fe08)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                                      | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000 | RUNNING  | 192.168.1.65:16335 |    176 |          10106.2 | 704000 |  145.353 |              156.111 |              132.463 |                400 |\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000:\n",
      "  agent_timesteps_total: 708000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-11-01_01-52-52\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 156.11131316828343\n",
      "  episode_reward_mean: 145.46470404302067\n",
      "  episode_reward_min: 132.46280178320555\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1770\n",
      "  experiment_id: cf8409b3cbbc42979d4540c7e6fbf708\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 8.552847072295025e-51\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -1.4238641753792762\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.006410712403703655\n",
      "          policy_loss: 0.0014042163820704445\n",
      "          total_loss: 347.32289400100706\n",
      "          vf_explained_var: -6.51925802230835e-09\n",
      "          vf_loss: 347.3214904785156\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 708000\n",
      "    num_steps_sampled: 708000\n",
      "    num_steps_trained: 708000\n",
      "  iterations_since_restore: 177\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.948192771084337\n",
      "    ram_util_percent: 27.987951807228917\n",
      "  pid: 16335\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08572828129634398\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.444294223838924\n",
      "    mean_inference_ms: 1.7829919098000737\n",
      "    mean_raw_obs_processing_ms: 6.46187128680868\n",
      "  time_since_restore: 10163.990484952927\n",
      "  time_this_iter_s: 57.827823877334595\n",
      "  time_total_s: 10163.990484952927\n",
      "  timers:\n",
      "    learn_throughput: 155.741\n",
      "    learn_time_ms: 25683.699\n",
      "    sample_throughput: 125.622\n",
      "    sample_time_ms: 31841.485\n",
      "    update_time_ms: 3.27\n",
      "  timestamp: 1667285572\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 708000\n",
      "  training_iteration: 177\n",
      "  trial_id: 168ff_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Memory usage on this node: 8.7/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/9.6 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_4_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_3_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_2_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_1_400b3145f046e80a4a7d53925cb6fe08, 0.0/6.0 CPU_group_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_5_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_0_400b3145f046e80a4a7d53925cb6fe08)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                                      | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000 | RUNNING  | 192.168.1.65:16335 |    177 |            10164 | 708000 |  145.465 |              156.111 |              132.463 |                400 |\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000:\n",
      "  agent_timesteps_total: 712000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-11-01_01-53-50\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 156.11131316828343\n",
      "  episode_reward_mean: 145.29731545579654\n",
      "  episode_reward_min: 132.46280178320555\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1780\n",
      "  experiment_id: cf8409b3cbbc42979d4540c7e6fbf708\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 4.2764235361475127e-51\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -1.4443282343447208\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008934323650385245\n",
      "          policy_loss: 0.0018714417703449727\n",
      "          total_loss: 349.89760093688966\n",
      "          vf_explained_var: -5.029141991741426e-09\n",
      "          vf_loss: 349.8957324743271\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 712000\n",
      "    num_steps_sampled: 712000\n",
      "    num_steps_trained: 712000\n",
      "  iterations_since_restore: 178\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.24457831325301\n",
      "    ram_util_percent: 28.053012048192766\n",
      "  pid: 16335\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08572766079590516\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.444129070521566\n",
      "    mean_inference_ms: 1.7830036448160107\n",
      "    mean_raw_obs_processing_ms: 6.460787792322996\n",
      "  time_since_restore: 10221.980948925018\n",
      "  time_this_iter_s: 57.990463972091675\n",
      "  time_total_s: 10221.980948925018\n",
      "  timers:\n",
      "    learn_throughput: 155.306\n",
      "    learn_time_ms: 25755.66\n",
      "    sample_throughput: 125.571\n",
      "    sample_time_ms: 31854.404\n",
      "    update_time_ms: 3.203\n",
      "  timestamp: 1667285630\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 712000\n",
      "  training_iteration: 178\n",
      "  trial_id: 168ff_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 8.7/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/9.6 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_4_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_1_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_2_400b3145f046e80a4a7d53925cb6fe08, 0.0/6.0 CPU_group_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_5_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_0_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_3_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 accelerator_type:G)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                                      | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000 | RUNNING  | 192.168.1.65:16335 |    178 |            10222 | 712000 |  145.297 |              156.111 |              132.463 |                400 |\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000:\n",
      "  agent_timesteps_total: 716000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-11-01_01-54-48\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 156.11131316828343\n",
      "  episode_reward_mean: 145.28379847809225\n",
      "  episode_reward_min: 132.46280178320555\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1790\n",
      "  experiment_id: cf8409b3cbbc42979d4540c7e6fbf708\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.1382117680737563e-51\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -1.4038330022245646\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0068986512784420025\n",
      "          policy_loss: -0.0013824859059241134\n",
      "          total_loss: 343.69541707038877\n",
      "          vf_explained_var: -7.078051478259795e-09\n",
      "          vf_loss: 343.69679832458496\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 716000\n",
      "    num_steps_sampled: 716000\n",
      "    num_steps_trained: 716000\n",
      "  iterations_since_restore: 179\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.43048780487805\n",
      "    ram_util_percent: 27.965853658536584\n",
      "  pid: 16335\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08572721968009361\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.443933667728352\n",
      "    mean_inference_ms: 1.7830531915457448\n",
      "    mean_raw_obs_processing_ms: 6.459688081505774\n",
      "  time_since_restore: 10279.805463075638\n",
      "  time_this_iter_s: 57.82451415061951\n",
      "  time_total_s: 10279.805463075638\n",
      "  timers:\n",
      "    learn_throughput: 154.953\n",
      "    learn_time_ms: 25814.216\n",
      "    sample_throughput: 125.571\n",
      "    sample_time_ms: 31854.436\n",
      "    update_time_ms: 3.193\n",
      "  timestamp: 1667285688\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 716000\n",
      "  training_iteration: 179\n",
      "  trial_id: 168ff_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 8.7/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/9.6 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_3_400b3145f046e80a4a7d53925cb6fe08, 0.0/6.0 CPU_group_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_2_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_0_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_1_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_5_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_4_400b3145f046e80a4a7d53925cb6fe08)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                                      | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000 | RUNNING  | 192.168.1.65:16335 |    179 |          10279.8 | 716000 |  145.284 |              156.111 |              132.463 |                400 |\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000:\n",
      "  agent_timesteps_total: 720000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-11-01_01-55-46\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 155.62956955178853\n",
      "  episode_reward_mean: 145.14944321659786\n",
      "  episode_reward_min: 132.46280178320555\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1800\n",
      "  experiment_id: cf8409b3cbbc42979d4540c7e6fbf708\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0691058840368782e-51\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -1.4264548435807227\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.00992677395929622\n",
      "          policy_loss: -0.0007963411262608133\n",
      "          total_loss: 341.685144174099\n",
      "          vf_explained_var: -4.842877210364804e-09\n",
      "          vf_loss: 341.68594148159025\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 720000\n",
      "    num_steps_sampled: 720000\n",
      "    num_steps_trained: 720000\n",
      "  iterations_since_restore: 180\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.06144578313253\n",
      "    ram_util_percent: 27.989156626506016\n",
      "  pid: 16335\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08572667698331404\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.443729209231105\n",
      "    mean_inference_ms: 1.7831255009825093\n",
      "    mean_raw_obs_processing_ms: 6.458587226459409\n",
      "  time_since_restore: 10337.346945762634\n",
      "  time_this_iter_s: 57.54148268699646\n",
      "  time_total_s: 10337.346945762634\n",
      "  timers:\n",
      "    learn_throughput: 154.979\n",
      "    learn_time_ms: 25809.945\n",
      "    sample_throughput: 125.568\n",
      "    sample_time_ms: 31855.363\n",
      "    update_time_ms: 3.184\n",
      "  timestamp: 1667285746\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 720000\n",
      "  training_iteration: 180\n",
      "  trial_id: 168ff_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 8.8/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/9.6 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_4_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_0_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_1_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 accelerator_type:G, 0.0/6.0 CPU_group_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_2_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_5_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_3_400b3145f046e80a4a7d53925cb6fe08)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                                      | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000 | RUNNING  | 192.168.1.65:16335 |    180 |          10337.3 | 720000 |  145.149 |               155.63 |              132.463 |                400 |\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000:\n",
      "  agent_timesteps_total: 724000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-11-01_01-56-44\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 154.94484890473413\n",
      "  episode_reward_mean: 145.12882016983022\n",
      "  episode_reward_min: 132.46280178320555\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1810\n",
      "  experiment_id: cf8409b3cbbc42979d4540c7e6fbf708\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 5.345529420184391e-52\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -1.4713068030774594\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.005687445621606546\n",
      "          policy_loss: 0.0021304520952980964\n",
      "          total_loss: 346.39654607772826\n",
      "          vf_explained_var: -9.126961408867373e-09\n",
      "          vf_loss: 346.39441599845884\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 724000\n",
      "    num_steps_sampled: 724000\n",
      "    num_steps_trained: 724000\n",
      "  iterations_since_restore: 181\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.971951219512196\n",
      "    ram_util_percent: 28.02682926829268\n",
      "  pid: 16335\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.085726330143603\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.443503063824366\n",
      "    mean_inference_ms: 1.783208981421567\n",
      "    mean_raw_obs_processing_ms: 6.45752795157286\n",
      "  time_since_restore: 10395.034230947495\n",
      "  time_this_iter_s: 57.68728518486023\n",
      "  time_total_s: 10395.034230947495\n",
      "  timers:\n",
      "    learn_throughput: 154.969\n",
      "    learn_time_ms: 25811.544\n",
      "    sample_throughput: 125.555\n",
      "    sample_time_ms: 31858.51\n",
      "    update_time_ms: 3.144\n",
      "  timestamp: 1667285804\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 724000\n",
      "  training_iteration: 181\n",
      "  trial_id: 168ff_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 8.7/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/9.6 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_0_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_5_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_4_400b3145f046e80a4a7d53925cb6fe08, 0.0/6.0 CPU_group_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_2_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_3_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_1_400b3145f046e80a4a7d53925cb6fe08)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                                      | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000 | RUNNING  | 192.168.1.65:16335 |    181 |            10395 | 724000 |  145.129 |              154.945 |              132.463 |                400 |\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000:\n",
      "  agent_timesteps_total: 728000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-11-01_01-57-42\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 157.17291655567135\n",
      "  episode_reward_mean: 145.29923459178565\n",
      "  episode_reward_min: 132.26372909186884\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1820\n",
      "  experiment_id: cf8409b3cbbc42979d4540c7e6fbf708\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.6727647100921954e-52\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -1.4794798593968153\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008117437248029092\n",
      "          policy_loss: 0.0007724420487647876\n",
      "          total_loss: 345.9326283931732\n",
      "          vf_explained_var: -6.146728903644316e-09\n",
      "          vf_loss: 345.93185687065125\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 728000\n",
      "    num_steps_sampled: 728000\n",
      "    num_steps_trained: 728000\n",
      "  iterations_since_restore: 182\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.103614457831327\n",
      "    ram_util_percent: 28.057831325301198\n",
      "  pid: 16335\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.085725673027343\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.4432985692477\n",
      "    mean_inference_ms: 1.7832801664831197\n",
      "    mean_raw_obs_processing_ms: 6.4565300577402684\n",
      "  time_since_restore: 10452.76453924179\n",
      "  time_this_iter_s: 57.730308294296265\n",
      "  time_total_s: 10452.76453924179\n",
      "  timers:\n",
      "    learn_throughput: 154.798\n",
      "    learn_time_ms: 25840.07\n",
      "    sample_throughput: 125.525\n",
      "    sample_time_ms: 31866.15\n",
      "    update_time_ms: 3.306\n",
      "  timestamp: 1667285862\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 728000\n",
      "  training_iteration: 182\n",
      "  trial_id: 168ff_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Memory usage on this node: 8.8/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/9.6 GiB heap, 0.0/14.65 GiB objects (0.0/6.0 CPU_group_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_5_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_1_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_2_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_4_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_0_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_3_400b3145f046e80a4a7d53925cb6fe08)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                                      | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000 | RUNNING  | 192.168.1.65:16335 |    182 |          10452.8 | 728000 |  145.299 |              157.173 |              132.264 |                400 |\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000:\n",
      "  agent_timesteps_total: 732000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-11-01_01-58-40\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 157.17291655567135\n",
      "  episode_reward_mean: 145.3479151610726\n",
      "  episode_reward_min: 132.26372909186884\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1830\n",
      "  experiment_id: cf8409b3cbbc42979d4540c7e6fbf708\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.3363823550460977e-52\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -1.507164641469717\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.007120618917997668\n",
      "          policy_loss: -0.0003415908664464951\n",
      "          total_loss: 344.97848827838897\n",
      "          vf_explained_var: -2.0489097085629737e-09\n",
      "          vf_loss: 344.9788299322128\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 732000\n",
      "    num_steps_sampled: 732000\n",
      "    num_steps_trained: 732000\n",
      "  iterations_since_restore: 183\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.128915662650602\n",
      "    ram_util_percent: 28.113253012048187\n",
      "  pid: 16335\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08572600143706811\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.443051826635465\n",
      "    mean_inference_ms: 1.783390376457899\n",
      "    mean_raw_obs_processing_ms: 6.45553079541939\n",
      "  time_since_restore: 10510.568464279175\n",
      "  time_this_iter_s: 57.80392503738403\n",
      "  time_total_s: 10510.568464279175\n",
      "  timers:\n",
      "    learn_throughput: 154.627\n",
      "    learn_time_ms: 25868.719\n",
      "    sample_throughput: 125.52\n",
      "    sample_time_ms: 31867.332\n",
      "    update_time_ms: 3.337\n",
      "  timestamp: 1667285920\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 732000\n",
      "  training_iteration: 183\n",
      "  trial_id: 168ff_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 8.7/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/9.6 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_0_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_1_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 accelerator_type:G, 0.0/6.0 CPU_group_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_4_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_3_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_2_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_5_400b3145f046e80a4a7d53925cb6fe08)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                                      | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000 | RUNNING  | 192.168.1.65:16335 |    183 |          10510.6 | 732000 |  145.348 |              157.173 |              132.264 |                400 |\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000:\n",
      "  agent_timesteps_total: 736000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-11-01_01-59-38\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 157.17291655567135\n",
      "  episode_reward_mean: 144.90534038231678\n",
      "  episode_reward_min: 131.4114953326591\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1840\n",
      "  experiment_id: cf8409b3cbbc42979d4540c7e6fbf708\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 6.681911775230489e-53\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -1.5243577215820552\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0063049000830687875\n",
      "          policy_loss: 0.0004991484602214769\n",
      "          total_loss: 340.74307070970536\n",
      "          vf_explained_var: -7.450580707946131e-10\n",
      "          vf_loss: 340.74257184267043\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 736000\n",
      "    num_steps_sampled: 736000\n",
      "    num_steps_trained: 736000\n",
      "  iterations_since_restore: 184\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.119512195121953\n",
      "    ram_util_percent: 28.193902439024384\n",
      "  pid: 16335\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0857262516815002\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.442842310756486\n",
      "    mean_inference_ms: 1.7834855069498814\n",
      "    mean_raw_obs_processing_ms: 6.453814406757703\n",
      "  time_since_restore: 10568.362842321396\n",
      "  time_this_iter_s: 57.79437804222107\n",
      "  time_total_s: 10568.362842321396\n",
      "  timers:\n",
      "    learn_throughput: 154.309\n",
      "    learn_time_ms: 25921.96\n",
      "    sample_throughput: 125.911\n",
      "    sample_time_ms: 31768.475\n",
      "    update_time_ms: 3.317\n",
      "  timestamp: 1667285978\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 736000\n",
      "  training_iteration: 184\n",
      "  trial_id: 168ff_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 8.8/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/9.6 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_5_400b3145f046e80a4a7d53925cb6fe08, 0.0/6.0 CPU_group_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_1_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_0_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_4_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_3_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_2_400b3145f046e80a4a7d53925cb6fe08)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                                      | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000 | RUNNING  | 192.168.1.65:16335 |    184 |          10568.4 | 736000 |  144.905 |              157.173 |              131.411 |                400 |\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000:\n",
      "  agent_timesteps_total: 740000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-11-01_02-00-36\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 157.17291655567135\n",
      "  episode_reward_mean: 145.37225182498022\n",
      "  episode_reward_min: 131.4114953326591\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1850\n",
      "  experiment_id: cf8409b3cbbc42979d4540c7e6fbf708\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 3.3409558876152443e-53\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -1.5524518985301257\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010516785610161606\n",
      "          policy_loss: -0.0007246068184031173\n",
      "          total_loss: 346.71695573329924\n",
      "          vf_explained_var: -1.467764434437413e-07\n",
      "          vf_loss: 346.71767830848694\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 740000\n",
      "    num_steps_sampled: 740000\n",
      "    num_steps_trained: 740000\n",
      "  iterations_since_restore: 185\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.91325301204819\n",
      "    ram_util_percent: 28.12168674698795\n",
      "  pid: 16335\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08572693413913292\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.442636517732016\n",
      "    mean_inference_ms: 1.7835886904593823\n",
      "    mean_raw_obs_processing_ms: 6.452089603106476\n",
      "  time_since_restore: 10626.32265663147\n",
      "  time_this_iter_s: 57.95981431007385\n",
      "  time_total_s: 10626.32265663147\n",
      "  timers:\n",
      "    learn_throughput: 154.007\n",
      "    learn_time_ms: 25972.87\n",
      "    sample_throughput: 125.908\n",
      "    sample_time_ms: 31769.127\n",
      "    update_time_ms: 3.32\n",
      "  timestamp: 1667286036\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 740000\n",
      "  training_iteration: 185\n",
      "  trial_id: 168ff_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 8.8/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/9.6 GiB heap, 0.0/14.65 GiB objects (0.0/6.0 CPU_group_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_0_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_4_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_5_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_2_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_1_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_3_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 accelerator_type:G)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                                      | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000 | RUNNING  | 192.168.1.65:16335 |    185 |          10626.3 | 740000 |  145.372 |              157.173 |              131.411 |                400 |\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000:\n",
      "  agent_timesteps_total: 744000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-11-01_02-01-34\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 157.17291655567135\n",
      "  episode_reward_mean: 145.4608128049024\n",
      "  episode_reward_min: 131.4114953326591\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1860\n",
      "  experiment_id: cf8409b3cbbc42979d4540c7e6fbf708\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 3.3409558876152443e-53\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -1.5951129581779242\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015511004887321355\n",
      "          policy_loss: 0.0023883814574219286\n",
      "          total_loss: 347.9990239858627\n",
      "          vf_explained_var: -1.7974525690078735e-07\n",
      "          vf_loss: 347.9966343164444\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 744000\n",
      "    num_steps_sampled: 744000\n",
      "    num_steps_trained: 744000\n",
      "  iterations_since_restore: 186\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.099999999999998\n",
      "    ram_util_percent: 28.22168674698796\n",
      "  pid: 16335\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08572817273440149\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.442434718077894\n",
      "    mean_inference_ms: 1.7837022683433588\n",
      "    mean_raw_obs_processing_ms: 6.45034945162163\n",
      "  time_since_restore: 10684.072343111038\n",
      "  time_this_iter_s: 57.74968647956848\n",
      "  time_total_s: 10684.072343111038\n",
      "  timers:\n",
      "    learn_throughput: 153.791\n",
      "    learn_time_ms: 26009.323\n",
      "    sample_throughput: 125.916\n",
      "    sample_time_ms: 31767.164\n",
      "    update_time_ms: 3.339\n",
      "  timestamp: 1667286094\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 744000\n",
      "  training_iteration: 186\n",
      "  trial_id: 168ff_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 8.8/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/9.6 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_1_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_5_400b3145f046e80a4a7d53925cb6fe08, 0.0/6.0 CPU_group_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_3_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_4_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_0_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_2_400b3145f046e80a4a7d53925cb6fe08)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                                      | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000 | RUNNING  | 192.168.1.65:16335 |    186 |          10684.1 | 744000 |  145.461 |              157.173 |              131.411 |                400 |\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000:\n",
      "  agent_timesteps_total: 748000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-11-01_02-02-32\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 157.8844547287064\n",
      "  episode_reward_mean: 145.51059581853627\n",
      "  episode_reward_min: 131.4114953326591\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1870\n",
      "  experiment_id: cf8409b3cbbc42979d4540c7e6fbf708\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 3.3409558876152443e-53\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -1.60498076826334\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012623388543215874\n",
      "          policy_loss: -0.002007313694048207\n",
      "          total_loss: 348.2947946190834\n",
      "          vf_explained_var: -6.51925802230835e-09\n",
      "          vf_loss: 348.2968040585518\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 748000\n",
      "    num_steps_sampled: 748000\n",
      "    num_steps_trained: 748000\n",
      "  iterations_since_restore: 187\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.90843373493976\n",
      "    ram_util_percent: 28.126506024096376\n",
      "  pid: 16335\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08572934754886191\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.442244614167063\n",
      "    mean_inference_ms: 1.7838009779810777\n",
      "    mean_raw_obs_processing_ms: 6.448636070858948\n",
      "  time_since_restore: 10742.2706656456\n",
      "  time_this_iter_s: 58.19832253456116\n",
      "  time_total_s: 10742.2706656456\n",
      "  timers:\n",
      "    learn_throughput: 153.57\n",
      "    learn_time_ms: 26046.823\n",
      "    sample_throughput: 125.918\n",
      "    sample_time_ms: 31766.691\n",
      "    update_time_ms: 3.34\n",
      "  timestamp: 1667286152\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 748000\n",
      "  training_iteration: 187\n",
      "  trial_id: 168ff_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Memory usage on this node: 8.8/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/9.6 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_1_400b3145f046e80a4a7d53925cb6fe08, 0.0/6.0 CPU_group_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_3_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_4_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_0_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_5_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_2_400b3145f046e80a4a7d53925cb6fe08)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                                      | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000 | RUNNING  | 192.168.1.65:16335 |    187 |          10742.3 | 748000 |  145.511 |              157.884 |              131.411 |                400 |\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000:\n",
      "  agent_timesteps_total: 752000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-11-01_02-03-31\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 157.8844547287064\n",
      "  episode_reward_mean: 145.2880991538522\n",
      "  episode_reward_min: 131.4114953326591\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1880\n",
      "  experiment_id: cf8409b3cbbc42979d4540c7e6fbf708\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 3.3409558876152443e-53\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -1.6489773340523244\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01093666204974415\n",
      "          policy_loss: 0.002717105635383632\n",
      "          total_loss: 345.7580029964447\n",
      "          vf_explained_var: -7.264316259636416e-09\n",
      "          vf_loss: 345.75528671741483\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 752000\n",
      "    num_steps_sampled: 752000\n",
      "    num_steps_trained: 752000\n",
      "  iterations_since_restore: 188\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.179761904761907\n",
      "    ram_util_percent: 28.265476190476193\n",
      "  pid: 16335\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08573058156657595\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.442000439996573\n",
      "    mean_inference_ms: 1.783863481861093\n",
      "    mean_raw_obs_processing_ms: 6.446946163231368\n",
      "  time_since_restore: 10801.071296453476\n",
      "  time_this_iter_s: 58.80063080787659\n",
      "  time_total_s: 10801.071296453476\n",
      "  timers:\n",
      "    learn_throughput: 153.046\n",
      "    learn_time_ms: 26135.969\n",
      "    sample_throughput: 125.95\n",
      "    sample_time_ms: 31758.552\n",
      "    update_time_ms: 3.362\n",
      "  timestamp: 1667286211\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 752000\n",
      "  training_iteration: 188\n",
      "  trial_id: 168ff_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 8.8/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/9.6 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_1_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_2_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_4_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_5_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_0_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_3_400b3145f046e80a4a7d53925cb6fe08, 0.0/6.0 CPU_group_400b3145f046e80a4a7d53925cb6fe08)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                                      | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000 | RUNNING  | 192.168.1.65:16335 |    188 |          10801.1 | 752000 |  145.288 |              157.884 |              131.411 |                400 |\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=16334)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=16331)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=16333)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=16332)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=16330)\u001b[0m  Retrying in 1 seconds\n",
      "Result for PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000:\n",
      "  agent_timesteps_total: 756000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-11-01_02-04-31\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 157.8844547287064\n",
      "  episode_reward_mean: 145.05267461977658\n",
      "  episode_reward_min: 131.4114953326591\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1890\n",
      "  experiment_id: cf8409b3cbbc42979d4540c7e6fbf708\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 3.3409558876152443e-53\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -1.6794314477592707\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.007459526063997401\n",
      "          policy_loss: 0.003798992169322446\n",
      "          total_loss: 346.477029633522\n",
      "          vf_explained_var: -5.6438146600612527e-08\n",
      "          vf_loss: 346.4732308149338\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 756000\n",
      "    num_steps_sampled: 756000\n",
      "    num_steps_trained: 756000\n",
      "  iterations_since_restore: 189\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.565882352941177\n",
      "    ram_util_percent: 28.348235294117647\n",
      "  pid: 16335\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08573125887056868\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.44175313770014\n",
      "    mean_inference_ms: 1.783909916547655\n",
      "    mean_raw_obs_processing_ms: 6.446001449047626\n",
      "  time_since_restore: 10860.556951522827\n",
      "  time_this_iter_s: 59.485655069351196\n",
      "  time_total_s: 10860.556951522827\n",
      "  timers:\n",
      "    learn_throughput: 152.681\n",
      "    learn_time_ms: 26198.39\n",
      "    sample_throughput: 125.541\n",
      "    sample_time_ms: 31862.221\n",
      "    update_time_ms: 3.36\n",
      "  timestamp: 1667286271\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 756000\n",
      "  training_iteration: 189\n",
      "  trial_id: 168ff_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 8.9/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/9.6 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_2_400b3145f046e80a4a7d53925cb6fe08, 0.0/6.0 CPU_group_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_5_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_0_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_4_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_3_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_1_400b3145f046e80a4a7d53925cb6fe08)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                                      | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000 | RUNNING  | 192.168.1.65:16335 |    189 |          10860.6 | 756000 |  145.053 |              157.884 |              131.411 |                400 |\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000:\n",
      "  agent_timesteps_total: 760000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-11-01_02-05-30\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 157.8844547287064\n",
      "  episode_reward_mean: 145.1519345704505\n",
      "  episode_reward_min: 131.4114953326591\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1900\n",
      "  experiment_id: cf8409b3cbbc42979d4540c7e6fbf708\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.6704779438076221e-53\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -1.7178648874163627\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009147375621614672\n",
      "          policy_loss: 0.0020253868628060446\n",
      "          total_loss: 341.1950031399727\n",
      "          vf_explained_var: -2.0489097085629737e-09\n",
      "          vf_loss: 341.19297914505006\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 760000\n",
      "    num_steps_sampled: 760000\n",
      "    num_steps_trained: 760000\n",
      "  iterations_since_restore: 190\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.760714285714286\n",
      "    ram_util_percent: 28.341666666666676\n",
      "  pid: 16335\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08573181420923585\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.44151127727946\n",
      "    mean_inference_ms: 1.7839293166317665\n",
      "    mean_raw_obs_processing_ms: 6.445086911893892\n",
      "  time_since_restore: 10919.1328394413\n",
      "  time_this_iter_s: 58.57588791847229\n",
      "  time_total_s: 10919.1328394413\n",
      "  timers:\n",
      "    learn_throughput: 152.066\n",
      "    learn_time_ms: 26304.385\n",
      "    sample_throughput: 125.551\n",
      "    sample_time_ms: 31859.662\n",
      "    update_time_ms: 3.429\n",
      "  timestamp: 1667286330\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 760000\n",
      "  training_iteration: 190\n",
      "  trial_id: 168ff_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 8.8/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/9.6 GiB heap, 0.0/14.65 GiB objects (0.0/6.0 CPU_group_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_2_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_5_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_3_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_1_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_4_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_0_400b3145f046e80a4a7d53925cb6fe08)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                                      | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000 | RUNNING  | 192.168.1.65:16335 |    190 |          10919.1 | 760000 |  145.152 |              157.884 |              131.411 |                400 |\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000:\n",
      "  agent_timesteps_total: 764000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-11-01_02-06-29\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 157.8844547287064\n",
      "  episode_reward_mean: 144.68443745386529\n",
      "  episode_reward_min: 131.4114953326591\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1910\n",
      "  experiment_id: cf8409b3cbbc42979d4540c7e6fbf708\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 8.352389719038111e-54\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -1.7209002405405045\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009169376553756117\n",
      "          policy_loss: 0.0005506083078216761\n",
      "          total_loss: 339.11621899604796\n",
      "          vf_explained_var: -3.911554635749326e-09\n",
      "          vf_loss: 339.1156695365906\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 764000\n",
      "    num_steps_sampled: 764000\n",
      "    num_steps_trained: 764000\n",
      "  iterations_since_restore: 191\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.69294117647059\n",
      "    ram_util_percent: 28.40941176470588\n",
      "  pid: 16335\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08573205594752663\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.44129077187279\n",
      "    mean_inference_ms: 1.7839351713645073\n",
      "    mean_raw_obs_processing_ms: 6.444203964982863\n",
      "  time_since_restore: 10978.256470441818\n",
      "  time_this_iter_s: 59.1236310005188\n",
      "  time_total_s: 10978.256470441818\n",
      "  timers:\n",
      "    learn_throughput: 151.25\n",
      "    learn_time_ms: 26446.208\n",
      "    sample_throughput: 125.543\n",
      "    sample_time_ms: 31861.476\n",
      "    update_time_ms: 3.501\n",
      "  timestamp: 1667286389\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 764000\n",
      "  training_iteration: 191\n",
      "  trial_id: 168ff_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 8.9/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/9.6 GiB heap, 0.0/14.65 GiB objects (0.0/6.0 CPU_group_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_2_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_1_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_4_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_0_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_3_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_5_400b3145f046e80a4a7d53925cb6fe08)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                                      | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000 | RUNNING  | 192.168.1.65:16335 |    191 |          10978.3 | 764000 |  144.684 |              157.884 |              131.411 |                400 |\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000:\n",
      "  agent_timesteps_total: 768000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-11-01_02-07-28\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 157.8844547287064\n",
      "  episode_reward_mean: 144.74074478615316\n",
      "  episode_reward_min: 131.4114953326591\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1920\n",
      "  experiment_id: cf8409b3cbbc42979d4540c7e6fbf708\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 4.1761948595190553e-54\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -1.7394919484853744\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011910410758082671\n",
      "          policy_loss: 0.0014426452755287756\n",
      "          total_loss: 339.140501499176\n",
      "          vf_explained_var: -8.75443184611413e-09\n",
      "          vf_loss: 339.1390599489212\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 768000\n",
      "    num_steps_sampled: 768000\n",
      "    num_steps_trained: 768000\n",
      "  iterations_since_restore: 192\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.032142857142855\n",
      "    ram_util_percent: 28.354761904761908\n",
      "  pid: 16335\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08573275904747661\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.441035818503494\n",
      "    mean_inference_ms: 1.7839617211441603\n",
      "    mean_raw_obs_processing_ms: 6.4432948574361095\n",
      "  time_since_restore: 11036.82731628418\n",
      "  time_this_iter_s: 58.57084584236145\n",
      "  time_total_s: 11036.82731628418\n",
      "  timers:\n",
      "    learn_throughput: 150.716\n",
      "    learn_time_ms: 26539.926\n",
      "    sample_throughput: 125.581\n",
      "    sample_time_ms: 31852.021\n",
      "    update_time_ms: 3.333\n",
      "  timestamp: 1667286448\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 768000\n",
      "  training_iteration: 192\n",
      "  trial_id: 168ff_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Memory usage on this node: 8.8/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/9.6 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_1_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 accelerator_type:G, 0.0/6.0 CPU_group_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_0_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_4_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_3_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_5_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_2_400b3145f046e80a4a7d53925cb6fe08)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                                      | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000 | RUNNING  | 192.168.1.65:16335 |    192 |          11036.8 | 768000 |  144.741 |              157.884 |              131.411 |                400 |\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=16334)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=16331)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=16333)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=16332)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=16330)\u001b[0m  Retrying in 1 seconds\n",
      "Result for PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000:\n",
      "  agent_timesteps_total: 772000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-11-01_02-08-28\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 157.8844547287064\n",
      "  episode_reward_mean: 144.6857990444681\n",
      "  episode_reward_min: 131.4114953326591\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1930\n",
      "  experiment_id: cf8409b3cbbc42979d4540c7e6fbf708\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 4.1761948595190553e-54\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -1.746466600522399\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010952839568744821\n",
      "          policy_loss: -0.00038457734626717865\n",
      "          total_loss: 343.3672527313232\n",
      "          vf_explained_var: -5.587935669737476e-10\n",
      "          vf_loss: 343.3676374197006\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 772000\n",
      "    num_steps_sampled: 772000\n",
      "    num_steps_trained: 772000\n",
      "  iterations_since_restore: 193\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.70232558139535\n",
      "    ram_util_percent: 28.32209302325581\n",
      "  pid: 16335\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08573298165315144\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.440821498317117\n",
      "    mean_inference_ms: 1.7839751671468413\n",
      "    mean_raw_obs_processing_ms: 6.443068474086825\n",
      "  time_since_restore: 11096.98253250122\n",
      "  time_this_iter_s: 60.155216217041016\n",
      "  time_total_s: 11096.98253250122\n",
      "  timers:\n",
      "    learn_throughput: 150.002\n",
      "    learn_time_ms: 26666.304\n",
      "    sample_throughput: 125.153\n",
      "    sample_time_ms: 31960.856\n",
      "    update_time_ms: 3.269\n",
      "  timestamp: 1667286508\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 772000\n",
      "  training_iteration: 193\n",
      "  trial_id: 168ff_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 8.9/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/9.6 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_3_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_1_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_4_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_2_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_5_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_0_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 accelerator_type:G, 0.0/6.0 CPU_group_400b3145f046e80a4a7d53925cb6fe08)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                                      | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000 | RUNNING  | 192.168.1.65:16335 |    193 |            11097 | 772000 |  144.686 |              157.884 |              131.411 |                400 |\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=16334)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=16331)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=16333)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=16332)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=16330)\u001b[0m  Retrying in 1 seconds\n",
      "Result for PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000:\n",
      "  agent_timesteps_total: 776000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-11-01_02-09-27\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 157.8844547287064\n",
      "  episode_reward_mean: 145.48283434510358\n",
      "  episode_reward_min: 131.4114953326591\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1940\n",
      "  experiment_id: cf8409b3cbbc42979d4540c7e6fbf708\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 4.1761948595190553e-54\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -1.7538575127720832\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010618356709849407\n",
      "          policy_loss: -0.0008830579667119309\n",
      "          total_loss: 340.99596722126006\n",
      "          vf_explained_var: -1.4528632519272833e-08\n",
      "          vf_loss: 340.9968537330627\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 776000\n",
      "    num_steps_sampled: 776000\n",
      "    num_steps_trained: 776000\n",
      "  iterations_since_restore: 194\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.86265060240964\n",
      "    ram_util_percent: 28.375903614457833\n",
      "  pid: 16335\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08573375980450257\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.440573347006456\n",
      "    mean_inference_ms: 1.7840120970288667\n",
      "    mean_raw_obs_processing_ms: 6.4435763285181\n",
      "  time_since_restore: 11155.450963020325\n",
      "  time_this_iter_s: 58.468430519104004\n",
      "  time_total_s: 11155.450963020325\n",
      "  timers:\n",
      "    learn_throughput: 150.215\n",
      "    learn_time_ms: 26628.53\n",
      "    sample_throughput: 124.742\n",
      "    sample_time_ms: 32066.124\n",
      "    update_time_ms: 3.238\n",
      "  timestamp: 1667286567\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 776000\n",
      "  training_iteration: 194\n",
      "  trial_id: 168ff_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 8.8/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/9.6 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_0_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_4_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_5_400b3145f046e80a4a7d53925cb6fe08, 0.0/6.0 CPU_group_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_3_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_2_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_1_400b3145f046e80a4a7d53925cb6fe08)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                                      | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000 | RUNNING  | 192.168.1.65:16335 |    194 |          11155.5 | 776000 |  145.483 |              157.884 |              131.411 |                400 |\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000:\n",
      "  agent_timesteps_total: 780000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-11-01_02-10-27\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 157.8844547287064\n",
      "  episode_reward_mean: 145.2256445799493\n",
      "  episode_reward_min: 131.4114953326591\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1950\n",
      "  experiment_id: cf8409b3cbbc42979d4540c7e6fbf708\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 4.1761948595190553e-54\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -1.7294587649405002\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.006799010570830433\n",
      "          policy_loss: 0.0024996265594381837\n",
      "          total_loss: 346.62203652858733\n",
      "          vf_explained_var: -3.054737973684496e-08\n",
      "          vf_loss: 346.61953446865084\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 780000\n",
      "    num_steps_sampled: 780000\n",
      "    num_steps_trained: 780000\n",
      "  iterations_since_restore: 195\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.47931034482759\n",
      "    ram_util_percent: 28.356321839080465\n",
      "  pid: 16335\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08573448365048485\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.440347466850437\n",
      "    mean_inference_ms: 1.7840271164070336\n",
      "    mean_raw_obs_processing_ms: 6.44408748517827\n",
      "  time_since_restore: 11215.652883052826\n",
      "  time_this_iter_s: 60.20192003250122\n",
      "  time_total_s: 11215.652883052826\n",
      "  timers:\n",
      "    learn_throughput: 148.975\n",
      "    learn_time_ms: 26850.116\n",
      "    sample_throughput: 124.732\n",
      "    sample_time_ms: 32068.726\n",
      "    update_time_ms: 3.228\n",
      "  timestamp: 1667286627\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 780000\n",
      "  training_iteration: 195\n",
      "  trial_id: 168ff_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 8.8/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/9.6 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_4_400b3145f046e80a4a7d53925cb6fe08, 0.0/6.0 CPU_group_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_2_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_5_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_0_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_1_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_3_400b3145f046e80a4a7d53925cb6fe08)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                                      | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000 | RUNNING  | 192.168.1.65:16335 |    195 |          11215.7 | 780000 |  145.226 |              157.884 |              131.411 |                400 |\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=16331)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=16333)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=16330)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=16333)\u001b[0m Could not connect to TraCI server at localhost:60241 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=16333)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=16334)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=16331)\u001b[0m Could not connect to TraCI server at localhost:45859 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=16331)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=16332)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=16330)\u001b[0m Could not connect to TraCI server at localhost:44711 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=16330)\u001b[0m  Retrying in 2 seconds\n",
      "Result for PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000:\n",
      "  agent_timesteps_total: 784000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-11-01_02-11-30\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 157.8844547287064\n",
      "  episode_reward_mean: 145.17265586789304\n",
      "  episode_reward_min: 131.4114953326591\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1960\n",
      "  experiment_id: cf8409b3cbbc42979d4540c7e6fbf708\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.0880974297595277e-54\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -1.7600052498281002\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010195160790044788\n",
      "          policy_loss: 0.003993765151244588\n",
      "          total_loss: 348.8322259426117\n",
      "          vf_explained_var: -2.9802322831784522e-09\n",
      "          vf_loss: 348.8282353639603\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 784000\n",
      "    num_steps_sampled: 784000\n",
      "    num_steps_trained: 784000\n",
      "  iterations_since_restore: 196\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.004444444444445\n",
      "    ram_util_percent: 28.39888888888889\n",
      "  pid: 16335\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08573470249475529\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.44039458449727\n",
      "    mean_inference_ms: 1.7840334370511857\n",
      "    mean_raw_obs_processing_ms: 6.446029490308008\n",
      "  time_since_restore: 11278.718616962433\n",
      "  time_this_iter_s: 63.065733909606934\n",
      "  time_total_s: 11278.718616962433\n",
      "  timers:\n",
      "    learn_throughput: 147.712\n",
      "    learn_time_ms: 27079.66\n",
      "    sample_throughput: 123.568\n",
      "    sample_time_ms: 32370.814\n",
      "    update_time_ms: 3.203\n",
      "  timestamp: 1667286690\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 784000\n",
      "  training_iteration: 196\n",
      "  trial_id: 168ff_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 8.8/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/9.6 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_3_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 accelerator_type:G, 0.0/6.0 CPU_group_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_4_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_0_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_1_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_5_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_2_400b3145f046e80a4a7d53925cb6fe08)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                                      | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000 | RUNNING  | 192.168.1.65:16335 |    196 |          11278.7 | 784000 |  145.173 |              157.884 |              131.411 |                400 |\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000:\n",
      "  agent_timesteps_total: 788000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-11-01_02-12-31\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 157.48904774856496\n",
      "  episode_reward_mean: 144.52104320915717\n",
      "  episode_reward_min: 131.4114953326591\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1970\n",
      "  experiment_id: cf8409b3cbbc42979d4540c7e6fbf708\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.0880974297595277e-54\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -1.7810354597866536\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010603800072567716\n",
      "          policy_loss: 0.0018401944849756545\n",
      "          total_loss: 345.89280195236205\n",
      "          vf_explained_var: -3.3527611797978807e-09\n",
      "          vf_loss: 345.8909613609314\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 788000\n",
      "    num_steps_sampled: 788000\n",
      "    num_steps_trained: 788000\n",
      "  iterations_since_restore: 197\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.590697674418603\n",
      "    ram_util_percent: 28.525581395348823\n",
      "  pid: 16335\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08573493311036827\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.44044557484871\n",
      "    mean_inference_ms: 1.7840525975160972\n",
      "    mean_raw_obs_processing_ms: 6.447910923096179\n",
      "  time_since_restore: 11338.85474729538\n",
      "  time_this_iter_s: 60.13613033294678\n",
      "  time_total_s: 11338.85474729538\n",
      "  timers:\n",
      "    learn_throughput: 146.629\n",
      "    learn_time_ms: 27279.812\n",
      "    sample_throughput: 123.592\n",
      "    sample_time_ms: 32364.513\n",
      "    update_time_ms: 3.195\n",
      "  timestamp: 1667286751\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 788000\n",
      "  training_iteration: 197\n",
      "  trial_id: 168ff_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Memory usage on this node: 8.9/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/9.6 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_2_400b3145f046e80a4a7d53925cb6fe08, 0.0/6.0 CPU_group_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_4_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_0_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_3_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_5_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_1_400b3145f046e80a4a7d53925cb6fe08)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                                      | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000 | RUNNING  | 192.168.1.65:16335 |    197 |          11338.9 | 788000 |  144.521 |              157.489 |              131.411 |                400 |\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000:\n",
      "  agent_timesteps_total: 792000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-11-01_02-13-32\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 157.48904774856496\n",
      "  episode_reward_mean: 143.98527174915174\n",
      "  episode_reward_min: 131.4114953326591\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1980\n",
      "  experiment_id: cf8409b3cbbc42979d4540c7e6fbf708\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.0880974297595277e-54\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -1.784045783802867\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009250914729091165\n",
      "          policy_loss: 0.004631255657295696\n",
      "          total_loss: 342.0152273416519\n",
      "          vf_explained_var: -2.3655593039961786e-08\n",
      "          vf_loss: 342.01059567928314\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 792000\n",
      "    num_steps_sampled: 792000\n",
      "    num_steps_trained: 792000\n",
      "  iterations_since_restore: 198\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.8551724137931\n",
      "    ram_util_percent: 28.548275862068962\n",
      "  pid: 16335\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0857364620886448\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.440455055630363\n",
      "    mean_inference_ms: 1.7840956855398802\n",
      "    mean_raw_obs_processing_ms: 6.449807616381927\n",
      "  time_since_restore: 11399.541046857834\n",
      "  time_this_iter_s: 60.686299562454224\n",
      "  time_total_s: 11399.541046857834\n",
      "  timers:\n",
      "    learn_throughput: 145.607\n",
      "    learn_time_ms: 27471.187\n",
      "    sample_throughput: 123.603\n",
      "    sample_time_ms: 32361.63\n",
      "    update_time_ms: 3.181\n",
      "  timestamp: 1667286812\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 792000\n",
      "  training_iteration: 198\n",
      "  trial_id: 168ff_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 8.9/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/9.6 GiB heap, 0.0/14.65 GiB objects (0.0/6.0 CPU_group_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_3_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_0_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_4_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_2_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_1_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_5_400b3145f046e80a4a7d53925cb6fe08)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                                      | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000 | RUNNING  | 192.168.1.65:16335 |    198 |          11399.5 | 792000 |  143.985 |              157.489 |              131.411 |                400 |\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000:\n",
      "  agent_timesteps_total: 796000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-11-01_02-14-32\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 157.48904774856496\n",
      "  episode_reward_mean: 143.72060051753908\n",
      "  episode_reward_min: 131.4114953326591\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1990\n",
      "  experiment_id: cf8409b3cbbc42979d4540c7e6fbf708\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0440487148797638e-54\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -1.7348809234797955\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010155368797859587\n",
      "          policy_loss: 0.0025541480135871097\n",
      "          total_loss: 338.60812764167787\n",
      "          vf_explained_var: -6.891787140972383e-09\n",
      "          vf_loss: 338.6055736064911\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 796000\n",
      "    num_steps_sampled: 796000\n",
      "    num_steps_trained: 796000\n",
      "  iterations_since_restore: 199\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.687209302325584\n",
      "    ram_util_percent: 28.40232558139535\n",
      "  pid: 16335\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08573831675204396\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.440476995402115\n",
      "    mean_inference_ms: 1.7841478327130966\n",
      "    mean_raw_obs_processing_ms: 6.451069295842309\n",
      "  time_since_restore: 11459.928552865982\n",
      "  time_this_iter_s: 60.38750600814819\n",
      "  time_total_s: 11459.928552865982\n",
      "  timers:\n",
      "    learn_throughput: 144.633\n",
      "    learn_time_ms: 27656.246\n",
      "    sample_throughput: 123.967\n",
      "    sample_time_ms: 32266.653\n",
      "    update_time_ms: 3.186\n",
      "  timestamp: 1667286872\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 796000\n",
      "  training_iteration: 199\n",
      "  trial_id: 168ff_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 8.8/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/9.6 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_2_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_0_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_1_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_5_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_3_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_4_400b3145f046e80a4a7d53925cb6fe08, 0.0/6.0 CPU_group_400b3145f046e80a4a7d53925cb6fe08)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                                      | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000 | RUNNING  | 192.168.1.65:16335 |    199 |          11459.9 | 796000 |  143.721 |              157.489 |              131.411 |                400 |\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000:\n",
      "  agent_timesteps_total: 800000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-11-01_02-15-34\n",
      "  done: true\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 157.48904774856496\n",
      "  episode_reward_mean: 144.10495255927526\n",
      "  episode_reward_min: 131.4114953326591\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 2000\n",
      "  experiment_id: cf8409b3cbbc42979d4540c7e6fbf708\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.0440487148797638e-54\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -1.7033811867237092\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.006593064460003066\n",
      "          policy_loss: -5.005165585316718e-05\n",
      "          total_loss: 334.82631220817564\n",
      "          vf_explained_var: -1.6763806343078613e-08\n",
      "          vf_loss: 334.8263610243797\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 800000\n",
      "    num_steps_sampled: 800000\n",
      "    num_steps_trained: 800000\n",
      "  iterations_since_restore: 200\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.30568181818182\n",
      "    ram_util_percent: 28.410227272727273\n",
      "  pid: 16335\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08574043767418814\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.440490625711345\n",
      "    mean_inference_ms: 1.7842178429969555\n",
      "    mean_raw_obs_processing_ms: 6.452349513354672\n",
      "  time_since_restore: 11521.30799293518\n",
      "  time_this_iter_s: 61.37944006919861\n",
      "  time_total_s: 11521.30799293518\n",
      "  timers:\n",
      "    learn_throughput: 143.2\n",
      "    learn_time_ms: 27932.999\n",
      "    sample_throughput: 123.953\n",
      "    sample_time_ms: 32270.272\n",
      "    update_time_ms: 3.135\n",
      "  timestamp: 1667286934\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 800000\n",
      "  training_iteration: 200\n",
      "  trial_id: 168ff_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 8.9/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/9.6 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_2_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_0_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_5_400b3145f046e80a4a7d53925cb6fe08, 0.0/6.0 CPU_group_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_4_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_1_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_3_400b3145f046e80a4a7d53925cb6fe08)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                                      | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000 | RUNNING  | 192.168.1.65:16335 |    200 |          11521.3 | 800000 |  144.105 |              157.489 |              131.411 |                400 |\n",
      "+-------------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Memory usage on this node: 8.9/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 0/6 CPUs, 0/6 GPUs, 0.0/9.6 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_4_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_2_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_0_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_3_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_5_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 CPU_group_1_400b3145f046e80a4a7d53925cb6fe08, 0.0/1.0 accelerator_type:G, 0.0/6.0 CPU_group_400b3145f046e80a4a7d53925cb6fe08)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 TERMINATED)\n",
      "+-------------------------------------------------+------------+-------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                                      | status     | loc   |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-------------------------------------------------+------------+-------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridBenchmarkEnv-v0_168ff_00000 | TERMINATED |       |    200 |          11521.3 | 800000 |  144.105 |              157.489 |              131.411 |                400 |\n",
      "+-------------------------------------------------+------------+-------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=16331)\u001b[0m 2022-11-01 02:15:34,735\tERROR worker.py:421 -- SystemExit was raised from the worker\n",
      "\u001b[2m\u001b[36m(pid=16331)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(pid=16331)\u001b[0m   File \"python/ray/_raylet.pyx\", line 530, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=16331)\u001b[0m   File \"python/ray/_raylet.pyx\", line 534, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=16331)\u001b[0m   File \"python/ray/_raylet.pyx\", line 484, in ray._raylet.execute_task.function_executor\n",
      "\u001b[2m\u001b[36m(pid=16331)\u001b[0m   File \"/home/michael/anaconda3/envs/flow/lib/python3.7/site-packages/ray/_private/function_manager.py\", line 563, in actor_method_executor\n",
      "\u001b[2m\u001b[36m(pid=16331)\u001b[0m     return method(__ray_actor, *args, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=16331)\u001b[0m   File \"/home/michael/anaconda3/envs/flow/lib/python3.7/site-packages/ray/actor.py\", line 1027, in __ray_terminate__\n",
      "\u001b[2m\u001b[36m(pid=16331)\u001b[0m     ray.actor.exit_actor()\n",
      "\u001b[2m\u001b[36m(pid=16331)\u001b[0m   File \"/home/michael/anaconda3/envs/flow/lib/python3.7/site-packages/ray/actor.py\", line 1103, in exit_actor\n",
      "\u001b[2m\u001b[36m(pid=16331)\u001b[0m     raise exit\n",
      "\u001b[2m\u001b[36m(pid=16331)\u001b[0m SystemExit: 0\n",
      "\u001b[2m\u001b[36m(pid=16331)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=16331)\u001b[0m During handling of the above exception, another exception occurred:\n",
      "\u001b[2m\u001b[36m(pid=16331)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=16331)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(pid=16331)\u001b[0m   File \"python/ray/_raylet.pyx\", line 632, in ray._raylet.task_execution_handler\n",
      "\u001b[2m\u001b[36m(pid=16331)\u001b[0m   File \"python/ray/_raylet.pyx\", line 486, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=16331)\u001b[0m   File \"python/ray/_raylet.pyx\", line 523, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=16331)\u001b[0m   File \"python/ray/includes/libcoreworker.pxi\", line 42, in ray._raylet.ProfileEvent.__exit__\n",
      "\u001b[2m\u001b[36m(pid=16331)\u001b[0m   File \"/home/michael/anaconda3/envs/flow/lib/python3.7/json/__init__.py\", line 231, in dumps\n",
      "\u001b[2m\u001b[36m(pid=16331)\u001b[0m     return _default_encoder.encode(obj)\n",
      "\u001b[2m\u001b[36m(pid=16331)\u001b[0m   File \"/home/michael/anaconda3/envs/flow/lib/python3.7/json/encoder.py\", line 199, in encode\n",
      "\u001b[2m\u001b[36m(pid=16331)\u001b[0m     chunks = self.iterencode(o, _one_shot=True)\n",
      "\u001b[2m\u001b[36m(pid=16331)\u001b[0m   File \"/home/michael/anaconda3/envs/flow/lib/python3.7/json/encoder.py\", line 257, in iterencode\n",
      "\u001b[2m\u001b[36m(pid=16331)\u001b[0m     return _iterencode(o, 0)\n",
      "\u001b[2m\u001b[36m(pid=16331)\u001b[0m   File \"/home/michael/anaconda3/envs/flow/lib/python3.7/site-packages/ray/worker.py\", line 418, in sigterm_handler\n",
      "\u001b[2m\u001b[36m(pid=16331)\u001b[0m     sys.exit(1)\n",
      "\u001b[2m\u001b[36m(pid=16331)\u001b[0m SystemExit: 1\n",
      "\u001b[2m\u001b[36m(pid=16332)\u001b[0m 2022-11-01 02:15:34,735\tERROR worker.py:421 -- SystemExit was raised from the worker\n",
      "\u001b[2m\u001b[36m(pid=16332)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(pid=16332)\u001b[0m   File \"python/ray/_raylet.pyx\", line 523, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=16332)\u001b[0m   File \"python/ray/_raylet.pyx\", line 530, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=16332)\u001b[0m   File \"python/ray/_raylet.pyx\", line 534, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=16332)\u001b[0m   File \"python/ray/_raylet.pyx\", line 484, in ray._raylet.execute_task.function_executor\n",
      "\u001b[2m\u001b[36m(pid=16332)\u001b[0m   File \"/home/michael/anaconda3/envs/flow/lib/python3.7/site-packages/ray/_private/function_manager.py\", line 563, in actor_method_executor\n",
      "\u001b[2m\u001b[36m(pid=16332)\u001b[0m     return method(__ray_actor, *args, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=16332)\u001b[0m   File \"/home/michael/anaconda3/envs/flow/lib/python3.7/site-packages/ray/actor.py\", line 1027, in __ray_terminate__\n",
      "\u001b[2m\u001b[36m(pid=16332)\u001b[0m     ray.actor.exit_actor()\n",
      "\u001b[2m\u001b[36m(pid=16332)\u001b[0m   File \"/home/michael/anaconda3/envs/flow/lib/python3.7/site-packages/ray/actor.py\", line 1103, in exit_actor\n",
      "\u001b[2m\u001b[36m(pid=16332)\u001b[0m     raise exit\n",
      "\u001b[2m\u001b[36m(pid=16332)\u001b[0m SystemExit: 0\n",
      "\u001b[2m\u001b[36m(pid=16332)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=16332)\u001b[0m During handling of the above exception, another exception occurred:\n",
      "\u001b[2m\u001b[36m(pid=16332)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=16332)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(pid=16332)\u001b[0m   File \"python/ray/_raylet.pyx\", line 632, in ray._raylet.task_execution_handler\n",
      "\u001b[2m\u001b[36m(pid=16332)\u001b[0m   File \"python/ray/_raylet.pyx\", line 486, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=16332)\u001b[0m   File \"python/ray/includes/libcoreworker.pxi\", line 42, in ray._raylet.ProfileEvent.__exit__\n",
      "\u001b[2m\u001b[36m(pid=16332)\u001b[0m   File \"/home/michael/anaconda3/envs/flow/lib/python3.7/json/__init__.py\", line 231, in dumps\n",
      "\u001b[2m\u001b[36m(pid=16332)\u001b[0m     return _default_encoder.encode(obj)\n",
      "\u001b[2m\u001b[36m(pid=16332)\u001b[0m   File \"/home/michael/anaconda3/envs/flow/lib/python3.7/json/encoder.py\", line 199, in encode\n",
      "\u001b[2m\u001b[36m(pid=16332)\u001b[0m     chunks = self.iterencode(o, _one_shot=True)\n",
      "\u001b[2m\u001b[36m(pid=16332)\u001b[0m   File \"/home/michael/anaconda3/envs/flow/lib/python3.7/json/encoder.py\", line 251, in iterencode\n",
      "\u001b[2m\u001b[36m(pid=16332)\u001b[0m     self.skipkeys, self.allow_nan)\n",
      "\u001b[2m\u001b[36m(pid=16332)\u001b[0m   File \"/home/michael/anaconda3/envs/flow/lib/python3.7/site-packages/ray/worker.py\", line 418, in sigterm_handler\n",
      "\u001b[2m\u001b[36m(pid=16332)\u001b[0m     sys.exit(1)\n",
      "\u001b[2m\u001b[36m(pid=16332)\u001b[0m SystemExit: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-11-01 02:15:34,841\tINFO tune.py:550 -- Total run time: 11549.18 seconds (11548.75 seconds for the tuning loop).\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!python3 examples/train.py grid0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
