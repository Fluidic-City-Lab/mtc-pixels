{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53dddb6d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/michael/Desktop/flow/flow/utils/flow_warnings.py:26: PendingDeprecationWarning: The attribute departLane in InFlows is deprecated, use depart_lane instead.\n",
      "  PendingDeprecationWarning\n",
      "/home/michael/Desktop/flow/flow/utils/flow_warnings.py:26: PendingDeprecationWarning: The attribute departSpeed in InFlows is deprecated, use depart_speed instead.\n",
      "  PendingDeprecationWarning\n",
      "/home/michael/Desktop/flow/flow/utils/flow_warnings.py:26: PendingDeprecationWarning: The attribute departLane in InFlows is deprecated, use depart_lane instead.\n",
      "  PendingDeprecationWarning\n",
      "/home/michael/Desktop/flow/flow/utils/flow_warnings.py:26: PendingDeprecationWarning: The attribute departSpeed in InFlows is deprecated, use depart_speed instead.\n",
      "  PendingDeprecationWarning\n",
      "/home/michael/Desktop/flow/flow/utils/flow_warnings.py:26: PendingDeprecationWarning: The attribute departLane in InFlows is deprecated, use depart_lane instead.\n",
      "  PendingDeprecationWarning\n",
      "/home/michael/Desktop/flow/flow/utils/flow_warnings.py:26: PendingDeprecationWarning: The attribute departSpeed in InFlows is deprecated, use depart_speed instead.\n",
      "  PendingDeprecationWarning\n",
      "/home/michael/Desktop/flow/flow/utils/flow_warnings.py:26: PendingDeprecationWarning: The attribute departLane in InFlows is deprecated, use depart_lane instead.\n",
      "  PendingDeprecationWarning\n",
      "/home/michael/Desktop/flow/flow/utils/flow_warnings.py:26: PendingDeprecationWarning: The attribute departSpeed in InFlows is deprecated, use depart_speed instead.\n",
      "  PendingDeprecationWarning\n",
      "{'num_workers': 5, 'num_envs_per_worker': 1, 'create_env_on_driver': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'gamma': 0.999, 'lr': 5e-05, 'train_batch_size': 4000, 'model': {'_use_default_native_models': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': [[16, [8, 8], 4], [32, [4, 4], 2], [256, [11, 11], 1]], 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'num_framestacks': 'auto', 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1, 'framestack': True}, 'optimizer': {}, 'horizon': 400, 'soft_horizon': False, 'no_done_at_end': False, 'env': None, 'observation_space': None, 'action_space': None, 'env_config': {}, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'torch', 'eager_tracing': False, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_num_episodes': 10, 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'collect_metrics_timeout': 180, 'metrics_smoothing_episodes': 100, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'min_iter_time_s': 0, 'timesteps_per_iteration': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {}, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, 'simple_optimizer': -1, 'monitor': -1, 'use_critic': True, 'use_gae': True, 'lambda': 0.97, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 10, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.02, 'vf_share_layers': -1}\n",
      "2023-01-01 15:19:28,109\tINFO services.py:1247 -- View the Ray dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265\u001b[39m\u001b[22m\n",
      "== Status ==\n",
      "Memory usage on this node: 7.3/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 0/6 CPUs, 0/6 GPUs, 0.0/7.87 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 PENDING)\n",
      "+------------------------------------------+----------+-------+\n",
      "| Trial name                               | status   | loc   |\n",
      "|------------------------------------------+----------+-------|\n",
      "| PPO_TrafficLightGridPOEnv-v0_f9c59_00000 | PENDING  |       |\n",
      "+------------------------------------------+----------+-------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=17432)\u001b[0m 2023-01-01 15:19:30,321\tINFO trainer.py:720 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=17432)\u001b[0m 2023-01-01 15:19:33,220\tWARNING util.py:55 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(pid=17433)\u001b[0m  Retrying in 1 seconds\n",
      "Result for PPO_TrafficLightGridPOEnv-v0_f9c59_00000:\n",
      "  agent_timesteps_total: 4000\n",
      "  custom_metrics: {}\n",
      "  date: 2023-01-01_15-20-34\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -194.62792209287323\n",
      "  episode_reward_mean: -224.40306009836598\n",
      "  episode_reward_min: -296.84366388498574\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 10\n",
      "  experiment_id: 610b8b5226964181899719d25df7617e\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.19999999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 4.2344758927822115\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0035806799800411704\n",
      "          policy_loss: -0.0021463456767378377\n",
      "          total_loss: 356.09146733283995\n",
      "          vf_explained_var: 5.7129560445901006e-05\n",
      "          vf_loss: 356.0928984642029\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 4000\n",
      "    num_steps_sampled: 4000\n",
      "    num_steps_trained: 4000\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.177272727272726\n",
      "    ram_util_percent: 26.170454545454547\n",
      "  pid: 17432\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09109143460734506\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 34.97503556859925\n",
      "    mean_inference_ms: 1.8748018476698132\n",
      "    mean_raw_obs_processing_ms: 6.567467226368954\n",
      "  time_since_restore: 61.45532250404358\n",
      "  time_this_iter_s: 61.45532250404358\n",
      "  time_total_s: 61.45532250404358\n",
      "  timers:\n",
      "    learn_throughput: 157.142\n",
      "    learn_time_ms: 25454.655\n",
      "    sample_throughput: 111.155\n",
      "    sample_time_ms: 35985.65\n",
      "    update_time_ms: 3.485\n",
      "  timestamp: 1672608034\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 4000\n",
      "  training_iteration: 1\n",
      "  trial_id: f9c59_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Memory usage on this node: 7.7/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.87 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                               | status   | loc                |   iter |   total time (s) |   ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|------------------------------------------+----------+--------------------+--------+------------------+------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridPOEnv-v0_f9c59_00000 | RUNNING  | 192.168.1.65:17432 |      1 |          61.4553 | 4000 | -224.403 |             -194.628 |             -296.844 |                400 |\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=17431)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=17433)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=17430)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=17429)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=17428)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=17433)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=17431)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=17429)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=17428)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=17430)\u001b[0m  Retrying in 1 seconds\n",
      "Result for PPO_TrafficLightGridPOEnv-v0_f9c59_00000:\n",
      "  agent_timesteps_total: 8000\n",
      "  custom_metrics: {}\n",
      "  date: 2023-01-01_15-21-34\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -194.62792209287323\n",
      "  episode_reward_mean: -249.7922433475806\n",
      "  episode_reward_min: -448.02581399313004\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 20\n",
      "  experiment_id: 610b8b5226964181899719d25df7617e\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.09999999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 4.252721385657788\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0017115406593461596\n",
      "          policy_loss: 2.6134918152820318e-05\n",
      "          total_loss: 559.163765335083\n",
      "          vf_explained_var: 3.5762788286319847e-08\n",
      "          vf_loss: 559.1635698318481\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 8000\n",
      "    num_steps_sampled: 8000\n",
      "    num_steps_trained: 8000\n",
      "  iterations_since_restore: 2\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.0\n",
      "    ram_util_percent: 24.841176470588234\n",
      "  pid: 17432\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09034001140959784\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 34.13679286053257\n",
      "    mean_inference_ms: 1.860335222496167\n",
      "    mean_raw_obs_processing_ms: 7.05052406812761\n",
      "  time_since_restore: 120.84320425987244\n",
      "  time_this_iter_s: 59.38788175582886\n",
      "  time_total_s: 120.84320425987244\n",
      "  timers:\n",
      "    learn_throughput: 157.287\n",
      "    learn_time_ms: 25431.247\n",
      "    sample_throughput: 114.366\n",
      "    sample_time_ms: 34975.497\n",
      "    update_time_ms: 3.289\n",
      "  timestamp: 1672608094\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 8000\n",
      "  training_iteration: 2\n",
      "  trial_id: f9c59_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 7.8/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.87 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_3_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_5_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_1_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_0_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_4_6417799359d4e2284884fd3ae2ec9735, 0.0/6.0 CPU_group_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_2_6417799359d4e2284884fd3ae2ec9735)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                               | status   | loc                |   iter |   total time (s) |   ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|------------------------------------------+----------+--------------------+--------+------------------+------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridPOEnv-v0_f9c59_00000 | RUNNING  | 192.168.1.65:17432 |      2 |          120.843 | 8000 | -249.792 |             -194.628 |             -448.026 |                400 |\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=17431)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=17430)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=17429)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=17428)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=17433)\u001b[0m  Retrying in 1 seconds\n",
      "Result for PPO_TrafficLightGridPOEnv-v0_f9c59_00000:\n",
      "  agent_timesteps_total: 12000\n",
      "  custom_metrics: {}\n",
      "  date: 2023-01-01_15-22-32\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -194.62792209287323\n",
      "  episode_reward_mean: -262.2107141138266\n",
      "  episode_reward_min: -448.02581399313004\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 30\n",
      "  experiment_id: 610b8b5226964181899719d25df7617e\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.049999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 4.25866859704256\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0062268983505994425\n",
      "          policy_loss: -0.0023902305256342515\n",
      "          total_loss: 581.7572187423706\n",
      "          vf_explained_var: 9.685754420729609e-09\n",
      "          vf_loss: 581.7592967033386\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 12000\n",
      "    num_steps_sampled: 12000\n",
      "    num_steps_trained: 12000\n",
      "  iterations_since_restore: 3\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.020481927710843\n",
      "    ram_util_percent: 24.94578313253012\n",
      "  pid: 17432\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0899196147314951\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 33.65725288671071\n",
      "    mean_inference_ms: 1.8528990278571575\n",
      "    mean_raw_obs_processing_ms: 7.180418244888855\n",
      "  time_since_restore: 179.10902428627014\n",
      "  time_this_iter_s: 58.265820026397705\n",
      "  time_total_s: 179.10902428627014\n",
      "  timers:\n",
      "    learn_throughput: 157.21\n",
      "    learn_time_ms: 25443.709\n",
      "    sample_throughput: 116.807\n",
      "    sample_time_ms: 34244.585\n",
      "    update_time_ms: 3.413\n",
      "  timestamp: 1672608152\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 12000\n",
      "  training_iteration: 3\n",
      "  trial_id: f9c59_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 7.8/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.87 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_3_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_4_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_5_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_0_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_2_6417799359d4e2284884fd3ae2ec9735, 0.0/6.0 CPU_group_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_1_6417799359d4e2284884fd3ae2ec9735)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                               | status   | loc                |   iter |   total time (s) |    ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|------------------------------------------+----------+--------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridPOEnv-v0_f9c59_00000 | RUNNING  | 192.168.1.65:17432 |      3 |          179.109 | 12000 | -262.211 |             -194.628 |             -448.026 |                400 |\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=17428)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=17433)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=17430)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=17429)\u001b[0m  Retrying in 1 seconds\n",
      "Result for PPO_TrafficLightGridPOEnv-v0_f9c59_00000:\n",
      "  agent_timesteps_total: 16000\n",
      "  custom_metrics: {}\n",
      "  date: 2023-01-01_15-23-32\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -193.2686889020825\n",
      "  episode_reward_mean: -261.33082955149774\n",
      "  episode_reward_min: -448.02581399313004\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 40\n",
      "  experiment_id: 610b8b5226964181899719d25df7617e\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.024999999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 4.215958182513714\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.00754770623123307\n",
      "          policy_loss: -0.006152498323353938\n",
      "          total_loss: 442.3540611743927\n",
      "          vf_explained_var: 0.0005588604253716767\n",
      "          vf_loss: 442.36002345085143\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 16000\n",
      "    num_steps_sampled: 16000\n",
      "    num_steps_trained: 16000\n",
      "  iterations_since_restore: 4\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.803529411764707\n",
      "    ram_util_percent: 25.064705882352936\n",
      "  pid: 17432\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08980388769780612\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 33.40501877271621\n",
      "    mean_inference_ms: 1.8527907899365903\n",
      "    mean_raw_obs_processing_ms: 7.220983502391084\n",
      "  time_since_restore: 238.60232257843018\n",
      "  time_this_iter_s: 59.493298292160034\n",
      "  time_total_s: 238.60232257843018\n",
      "  timers:\n",
      "    learn_throughput: 156.518\n",
      "    learn_time_ms: 25556.215\n",
      "    sample_throughput: 117.372\n",
      "    sample_time_ms: 34079.675\n",
      "    update_time_ms: 3.344\n",
      "  timestamp: 1672608212\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 16000\n",
      "  training_iteration: 4\n",
      "  trial_id: f9c59_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 7.8/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.87 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_2_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 accelerator_type:G, 0.0/6.0 CPU_group_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_5_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_0_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_4_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_1_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_3_6417799359d4e2284884fd3ae2ec9735)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                               | status   | loc                |   iter |   total time (s) |    ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|------------------------------------------+----------+--------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridPOEnv-v0_f9c59_00000 | RUNNING  | 192.168.1.65:17432 |      4 |          238.602 | 16000 | -261.331 |             -193.269 |             -448.026 |                400 |\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridPOEnv-v0_f9c59_00000:\n",
      "  agent_timesteps_total: 20000\n",
      "  custom_metrics: {}\n",
      "  date: 2023-01-01_15-24-29\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -193.2686889020825\n",
      "  episode_reward_mean: -269.2825156137557\n",
      "  episode_reward_min: -448.02581399313004\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 50\n",
      "  experiment_id: 610b8b5226964181899719d25df7617e\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.012499999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 4.298589374125004\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.004164966457424415\n",
      "          policy_loss: -0.0008358875013072975\n",
      "          total_loss: 647.7503384590149\n",
      "          vf_explained_var: 0.0005449140444397926\n",
      "          vf_loss: 647.7511185646057\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 20000\n",
      "    num_steps_sampled: 20000\n",
      "    num_steps_trained: 20000\n",
      "  iterations_since_restore: 5\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.134146341463413\n",
      "    ram_util_percent: 25.036585365853657\n",
      "  pid: 17432\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0895988478824442\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 33.21229142805533\n",
      "    mean_inference_ms: 1.8510254907070862\n",
      "    mean_raw_obs_processing_ms: 7.190768745632106\n",
      "  time_since_restore: 295.8373210430145\n",
      "  time_this_iter_s: 57.23499846458435\n",
      "  time_total_s: 295.8373210430145\n",
      "  timers:\n",
      "    learn_throughput: 156.79\n",
      "    learn_time_ms: 25511.863\n",
      "    sample_throughput: 118.903\n",
      "    sample_time_ms: 33640.885\n",
      "    update_time_ms: 3.364\n",
      "  timestamp: 1672608269\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 20000\n",
      "  training_iteration: 5\n",
      "  trial_id: f9c59_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 7.8/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.87 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_0_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_2_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_3_6417799359d4e2284884fd3ae2ec9735, 0.0/6.0 CPU_group_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_5_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_1_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_4_6417799359d4e2284884fd3ae2ec9735)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                               | status   | loc                |   iter |   total time (s) |    ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|------------------------------------------+----------+--------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridPOEnv-v0_f9c59_00000 | RUNNING  | 192.168.1.65:17432 |      5 |          295.837 | 20000 | -269.283 |             -193.269 |             -448.026 |                400 |\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridPOEnv-v0_f9c59_00000:\n",
      "  agent_timesteps_total: 24000\n",
      "  custom_metrics: {}\n",
      "  date: 2023-01-01_15-25-27\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -193.2686889020825\n",
      "  episode_reward_mean: -274.0537404853284\n",
      "  episode_reward_min: -452.65121251785365\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 60\n",
      "  experiment_id: 610b8b5226964181899719d25df7617e\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.0062499999999999995\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 4.312536938488483\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.00462977369037532\n",
      "          policy_loss: -0.0029371111289947295\n",
      "          total_loss: 606.7986507415771\n",
      "          vf_explained_var: 0.0008243031916208565\n",
      "          vf_loss: 606.801556301117\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 24000\n",
      "    num_steps_sampled: 24000\n",
      "    num_steps_trained: 24000\n",
      "  iterations_since_restore: 6\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.149397590361446\n",
      "    ram_util_percent: 25.042168674698797\n",
      "  pid: 17432\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08941343597973964\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 33.07610504375886\n",
      "    mean_inference_ms: 1.8478314768188886\n",
      "    mean_raw_obs_processing_ms: 7.142302646639594\n",
      "  time_since_restore: 354.0126163959503\n",
      "  time_this_iter_s: 58.17529535293579\n",
      "  time_total_s: 354.0126163959503\n",
      "  timers:\n",
      "    learn_throughput: 156.538\n",
      "    learn_time_ms: 25552.926\n",
      "    sample_throughput: 119.637\n",
      "    sample_time_ms: 33434.59\n",
      "    update_time_ms: 3.317\n",
      "  timestamp: 1672608327\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 24000\n",
      "  training_iteration: 6\n",
      "  trial_id: f9c59_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 7.8/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.87 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_1_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_4_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_5_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_0_6417799359d4e2284884fd3ae2ec9735, 0.0/6.0 CPU_group_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_3_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_2_6417799359d4e2284884fd3ae2ec9735)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                               | status   | loc                |   iter |   total time (s) |    ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|------------------------------------------+----------+--------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridPOEnv-v0_f9c59_00000 | RUNNING  | 192.168.1.65:17432 |      6 |          354.013 | 24000 | -274.054 |             -193.269 |             -452.651 |                400 |\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_TrafficLightGridPOEnv-v0_f9c59_00000:\n",
      "  agent_timesteps_total: 28000\n",
      "  custom_metrics: {}\n",
      "  date: 2023-01-01_15-26-25\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -193.2686889020825\n",
      "  episode_reward_mean: -269.33135623256294\n",
      "  episode_reward_min: -452.65121251785365\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 70\n",
      "  experiment_id: 610b8b5226964181899719d25df7617e\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.0031249999999999997\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 4.332142344117164\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.00849543792967609\n",
      "          policy_loss: -0.002125947625609115\n",
      "          total_loss: 415.35692138671874\n",
      "          vf_explained_var: 0.00024867485626600683\n",
      "          vf_loss: 415.35901937484743\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 28000\n",
      "    num_steps_sampled: 28000\n",
      "    num_steps_trained: 28000\n",
      "  iterations_since_restore: 7\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.193975903614458\n",
      "    ram_util_percent: 25.086746987951805\n",
      "  pid: 17432\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08926309875174376\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 32.971421701825484\n",
      "    mean_inference_ms: 1.8454957560793295\n",
      "    mean_raw_obs_processing_ms: 7.090102155846613\n",
      "  time_since_restore: 412.3408885002136\n",
      "  time_this_iter_s: 58.328272104263306\n",
      "  time_total_s: 412.3408885002136\n",
      "  timers:\n",
      "    learn_throughput: 156.154\n",
      "    learn_time_ms: 25615.692\n",
      "    sample_throughput: 120.208\n",
      "    sample_time_ms: 33275.597\n",
      "    update_time_ms: 3.274\n",
      "  timestamp: 1672608385\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 28000\n",
      "  training_iteration: 7\n",
      "  trial_id: f9c59_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 7.8/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.87 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_2_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_4_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_1_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_3_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_0_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_5_6417799359d4e2284884fd3ae2ec9735, 0.0/6.0 CPU_group_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 accelerator_type:G)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                               | status   | loc                |   iter |   total time (s) |    ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|------------------------------------------+----------+--------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridPOEnv-v0_f9c59_00000 | RUNNING  | 192.168.1.65:17432 |      7 |          412.341 | 28000 | -269.331 |             -193.269 |             -452.651 |                400 |\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridPOEnv-v0_f9c59_00000:\n",
      "  agent_timesteps_total: 32000\n",
      "  custom_metrics: {}\n",
      "  date: 2023-01-01_15-27-23\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -191.3118367705028\n",
      "  episode_reward_mean: -270.13451756085635\n",
      "  episode_reward_min: -452.65121251785365\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 80\n",
      "  experiment_id: 610b8b5226964181899719d25df7617e\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.0015624999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 4.272038295865059\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009706531388437158\n",
      "          policy_loss: -0.0041546900931280105\n",
      "          total_loss: 567.8095763206481\n",
      "          vf_explained_var: 0.00044764409540221095\n",
      "          vf_loss: 567.8137166976928\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 32000\n",
      "    num_steps_sampled: 32000\n",
      "    num_steps_trained: 32000\n",
      "  iterations_since_restore: 8\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.231325301204823\n",
      "    ram_util_percent: 25.222891566265066\n",
      "  pid: 17432\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08914403631667789\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 32.88510772259483\n",
      "    mean_inference_ms: 1.8439837840552649\n",
      "    mean_raw_obs_processing_ms: 7.039300727067248\n",
      "  time_since_restore: 470.11502289772034\n",
      "  time_this_iter_s: 57.774134397506714\n",
      "  time_total_s: 470.11502289772034\n",
      "  timers:\n",
      "    learn_throughput: 156.166\n",
      "    learn_time_ms: 25613.691\n",
      "    sample_throughput: 120.714\n",
      "    sample_time_ms: 33136.222\n",
      "    update_time_ms: 3.25\n",
      "  timestamp: 1672608443\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 32000\n",
      "  training_iteration: 8\n",
      "  trial_id: f9c59_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 7.8/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.87 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_2_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_0_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_1_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_4_6417799359d4e2284884fd3ae2ec9735, 0.0/6.0 CPU_group_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_3_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_5_6417799359d4e2284884fd3ae2ec9735)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                               | status   | loc                |   iter |   total time (s) |    ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|------------------------------------------+----------+--------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridPOEnv-v0_f9c59_00000 | RUNNING  | 192.168.1.65:17432 |      8 |          470.115 | 32000 | -270.135 |             -191.312 |             -452.651 |                400 |\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridPOEnv-v0_f9c59_00000:\n",
      "  agent_timesteps_total: 36000\n",
      "  custom_metrics: {}\n",
      "  date: 2023-01-01_15-28-21\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -186.8984014108179\n",
      "  episode_reward_mean: -268.99673013387996\n",
      "  episode_reward_min: -452.65121251785365\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 90\n",
      "  experiment_id: 610b8b5226964181899719d25df7617e\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.0007812499999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 4.268131832778454\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.004419349649085371\n",
      "          policy_loss: 0.001246097733383067\n",
      "          total_loss: 505.00790548324585\n",
      "          vf_explained_var: 0.00024000051780603826\n",
      "          vf_loss: 505.00665559768674\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 36000\n",
      "    num_steps_sampled: 36000\n",
      "    num_steps_trained: 36000\n",
      "  iterations_since_restore: 9\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.078048780487805\n",
      "    ram_util_percent: 25.10487804878049\n",
      "  pid: 17432\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08903043441438864\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 32.81420960097803\n",
      "    mean_inference_ms: 1.842141624023826\n",
      "    mean_raw_obs_processing_ms: 6.991616619147966\n",
      "  time_since_restore: 527.9591283798218\n",
      "  time_this_iter_s: 57.84410548210144\n",
      "  time_total_s: 527.9591283798218\n",
      "  timers:\n",
      "    learn_throughput: 156.151\n",
      "    learn_time_ms: 25616.298\n",
      "    sample_throughput: 121.097\n",
      "    sample_time_ms: 33031.328\n",
      "    update_time_ms: 3.302\n",
      "  timestamp: 1672608501\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 36000\n",
      "  training_iteration: 9\n",
      "  trial_id: f9c59_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 7.8/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.87 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_3_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_2_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_0_6417799359d4e2284884fd3ae2ec9735, 0.0/6.0 CPU_group_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_1_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_5_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_4_6417799359d4e2284884fd3ae2ec9735)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                               | status   | loc                |   iter |   total time (s) |    ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|------------------------------------------+----------+--------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridPOEnv-v0_f9c59_00000 | RUNNING  | 192.168.1.65:17432 |      9 |          527.959 | 36000 | -268.997 |             -186.898 |             -452.651 |                400 |\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_TrafficLightGridPOEnv-v0_f9c59_00000:\n",
      "  agent_timesteps_total: 40000\n",
      "  custom_metrics: {}\n",
      "  date: 2023-01-01_15-29-18\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -146.00593813450914\n",
      "  episode_reward_mean: -268.93251488281743\n",
      "  episode_reward_min: -452.65121251785365\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 100\n",
      "  experiment_id: 610b8b5226964181899719d25df7617e\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.00039062499999999997\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 4.261991389095783\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0077914913261594165\n",
      "          policy_loss: -0.0023620118241524323\n",
      "          total_loss: 517.0921808242798\n",
      "          vf_explained_var: 1.3038515822572094e-09\n",
      "          vf_loss: 517.0945375442504\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 40000\n",
      "    num_steps_sampled: 40000\n",
      "    num_steps_trained: 40000\n",
      "  iterations_since_restore: 10\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.249382716049382\n",
      "    ram_util_percent: 25.07901234567901\n",
      "  pid: 17432\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0889268906105378\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 32.74877557915177\n",
      "    mean_inference_ms: 1.8402998628204308\n",
      "    mean_raw_obs_processing_ms: 6.948518126224558\n",
      "  time_since_restore: 584.6967332363129\n",
      "  time_this_iter_s: 56.73760485649109\n",
      "  time_total_s: 584.6967332363129\n",
      "  timers:\n",
      "    learn_throughput: 156.518\n",
      "    learn_time_ms: 25556.114\n",
      "    sample_throughput: 121.584\n",
      "    sample_time_ms: 32899.114\n",
      "    update_time_ms: 3.284\n",
      "  timestamp: 1672608558\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 40000\n",
      "  training_iteration: 10\n",
      "  trial_id: f9c59_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 7.8/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.87 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_1_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_0_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_5_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_2_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_3_6417799359d4e2284884fd3ae2ec9735, 0.0/6.0 CPU_group_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_4_6417799359d4e2284884fd3ae2ec9735)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                               | status   | loc                |   iter |   total time (s) |    ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|------------------------------------------+----------+--------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridPOEnv-v0_f9c59_00000 | RUNNING  | 192.168.1.65:17432 |     10 |          584.697 | 40000 | -268.933 |             -146.006 |             -452.651 |                400 |\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=17433)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=17430)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=17431)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=17429)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=17428)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=17433)\u001b[0m Could not connect to TraCI server at localhost:33079 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=17433)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=17430)\u001b[0m Could not connect to TraCI server at localhost:33057 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=17430)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=17431)\u001b[0m Could not connect to TraCI server at localhost:42343 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=17431)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=17429)\u001b[0m Could not connect to TraCI server at localhost:41647 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=17429)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=17428)\u001b[0m Could not connect to TraCI server at localhost:47505 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=17428)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=17433)\u001b[0m Could not connect to TraCI server at localhost:33079 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=17433)\u001b[0m  Retrying in 3 seconds\n",
      "\u001b[2m\u001b[36m(pid=17430)\u001b[0m Could not connect to TraCI server at localhost:33057 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=17430)\u001b[0m  Retrying in 3 seconds\n",
      "\u001b[2m\u001b[36m(pid=17431)\u001b[0m Could not connect to TraCI server at localhost:42343 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=17431)\u001b[0m  Retrying in 3 seconds\n",
      "\u001b[2m\u001b[36m(pid=17429)\u001b[0m Could not connect to TraCI server at localhost:41647 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=17429)\u001b[0m  Retrying in 3 seconds\n",
      "\u001b[2m\u001b[36m(pid=17428)\u001b[0m Could not connect to TraCI server at localhost:47505 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=17428)\u001b[0m  Retrying in 3 seconds\n",
      "Result for PPO_TrafficLightGridPOEnv-v0_f9c59_00000:\n",
      "  agent_timesteps_total: 44000\n",
      "  custom_metrics: {}\n",
      "  date: 2023-01-01_15-30-21\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -146.00593813450914\n",
      "  episode_reward_mean: -271.1504281267796\n",
      "  episode_reward_min: -452.65121251785365\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 110\n",
      "  experiment_id: 610b8b5226964181899719d25df7617e\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.00019531249999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 4.215181885659694\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0058724430286531525\n",
      "          policy_loss: -0.003921520190488081\n",
      "          total_loss: 394.8679044723511\n",
      "          vf_explained_var: -3.166496842510469e-09\n",
      "          vf_loss: 394.8718240737915\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 44000\n",
      "    num_steps_sampled: 44000\n",
      "    num_steps_trained: 44000\n",
      "  iterations_since_restore: 11\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 18.891208791208793\n",
      "    ram_util_percent: 25.059340659340656\n",
      "  pid: 17432\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0886132737433916\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 32.461503733730744\n",
      "    mean_inference_ms: 1.8353090949753643\n",
      "    mean_raw_obs_processing_ms: 7.01108101146286\n",
      "  time_since_restore: 648.2443561553955\n",
      "  time_this_iter_s: 63.54762291908264\n",
      "  time_total_s: 648.2443561553955\n",
      "  timers:\n",
      "    learn_throughput: 156.339\n",
      "    learn_time_ms: 25585.507\n",
      "    sample_throughput: 120.922\n",
      "    sample_time_ms: 33079.053\n",
      "    update_time_ms: 3.246\n",
      "  timestamp: 1672608621\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 44000\n",
      "  training_iteration: 11\n",
      "  trial_id: f9c59_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 7.8/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.87 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_2_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_0_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_3_6417799359d4e2284884fd3ae2ec9735, 0.0/6.0 CPU_group_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_5_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_1_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_4_6417799359d4e2284884fd3ae2ec9735)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                               | status   | loc                |   iter |   total time (s) |    ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|------------------------------------------+----------+--------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridPOEnv-v0_f9c59_00000 | RUNNING  | 192.168.1.65:17432 |     11 |          648.244 | 44000 |  -271.15 |             -146.006 |             -452.651 |                400 |\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_TrafficLightGridPOEnv-v0_f9c59_00000:\n",
      "  agent_timesteps_total: 48000\n",
      "  custom_metrics: {}\n",
      "  date: 2023-01-01_15-31-19\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -146.00593813450914\n",
      "  episode_reward_mean: -268.60175490921387\n",
      "  episode_reward_min: -452.65121251785365\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 120\n",
      "  experiment_id: 610b8b5226964181899719d25df7617e\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 9.765624999999999e-05\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 4.176164807379246\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0036174131021908876\n",
      "          policy_loss: -0.0007347169201239012\n",
      "          total_loss: 432.79970064163206\n",
      "          vf_explained_var: -1.9354187315911986e-05\n",
      "          vf_loss: 432.8004369735718\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 48000\n",
      "    num_steps_sampled: 48000\n",
      "    num_steps_trained: 48000\n",
      "  iterations_since_restore: 12\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.165853658536584\n",
      "    ram_util_percent: 25.067073170731707\n",
      "  pid: 17432\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0884409275030964\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 32.33770150409871\n",
      "    mean_inference_ms: 1.8328725665350873\n",
      "    mean_raw_obs_processing_ms: 6.9675706097203145\n",
      "  time_since_restore: 705.3291523456573\n",
      "  time_this_iter_s: 57.08479619026184\n",
      "  time_total_s: 705.3291523456573\n",
      "  timers:\n",
      "    learn_throughput: 156.43\n",
      "    learn_time_ms: 25570.554\n",
      "    sample_throughput: 121.715\n",
      "    sample_time_ms: 32863.769\n",
      "    update_time_ms: 3.239\n",
      "  timestamp: 1672608679\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 48000\n",
      "  training_iteration: 12\n",
      "  trial_id: f9c59_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 7.8/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.87 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_5_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_4_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_2_6417799359d4e2284884fd3ae2ec9735, 0.0/6.0 CPU_group_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_3_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_0_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_1_6417799359d4e2284884fd3ae2ec9735)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                               | status   | loc                |   iter |   total time (s) |    ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|------------------------------------------+----------+--------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridPOEnv-v0_f9c59_00000 | RUNNING  | 192.168.1.65:17432 |     12 |          705.329 | 48000 | -268.602 |             -146.006 |             -452.651 |                400 |\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridPOEnv-v0_f9c59_00000:\n",
      "  agent_timesteps_total: 52000\n",
      "  custom_metrics: {}\n",
      "  date: 2023-01-01_15-32-16\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -146.00593813450914\n",
      "  episode_reward_mean: -267.5169021217625\n",
      "  episode_reward_min: -452.65121251785365\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 130\n",
      "  experiment_id: 610b8b5226964181899719d25df7617e\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 4.8828124999999996e-05\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 4.124636845290661\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.006218582125686117\n",
      "          policy_loss: 0.0009266352513805032\n",
      "          total_loss: 558.8517518997193\n",
      "          vf_explained_var: 5.106311436975375e-05\n",
      "          vf_loss: 558.8508281707764\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 52000\n",
      "    num_steps_sampled: 52000\n",
      "    num_steps_trained: 52000\n",
      "  iterations_since_restore: 13\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.350617283950616\n",
      "    ram_util_percent: 25.082716049382718\n",
      "  pid: 17432\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08831232484135215\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 32.269027686584515\n",
      "    mean_inference_ms: 1.8311445284367769\n",
      "    mean_raw_obs_processing_ms: 6.925312956066949\n",
      "  time_since_restore: 762.5793259143829\n",
      "  time_this_iter_s: 57.250173568725586\n",
      "  time_total_s: 762.5793259143829\n",
      "  timers:\n",
      "    learn_throughput: 156.396\n",
      "    learn_time_ms: 25576.073\n",
      "    sample_throughput: 122.113\n",
      "    sample_time_ms: 32756.672\n",
      "    update_time_ms: 3.186\n",
      "  timestamp: 1672608736\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 52000\n",
      "  training_iteration: 13\n",
      "  trial_id: f9c59_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 7.8/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.87 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_1_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_5_6417799359d4e2284884fd3ae2ec9735, 0.0/6.0 CPU_group_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_2_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_3_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_0_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_4_6417799359d4e2284884fd3ae2ec9735)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                               | status   | loc                |   iter |   total time (s) |    ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|------------------------------------------+----------+--------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridPOEnv-v0_f9c59_00000 | RUNNING  | 192.168.1.65:17432 |     13 |          762.579 | 52000 | -267.517 |             -146.006 |             -452.651 |                400 |\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridPOEnv-v0_f9c59_00000:\n",
      "  agent_timesteps_total: 56000\n",
      "  custom_metrics: {}\n",
      "  date: 2023-01-01_15-33-13\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -146.00593813450914\n",
      "  episode_reward_mean: -265.36832681423357\n",
      "  episode_reward_min: -452.65121251785365\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 140\n",
      "  experiment_id: 610b8b5226964181899719d25df7617e\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.4414062499999998e-05\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 4.047843954712152\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009012904600939469\n",
      "          policy_loss: -0.0018420529173454269\n",
      "          total_loss: 404.44017148017883\n",
      "          vf_explained_var: -3.4074670111294836e-05\n",
      "          vf_loss: 404.4420128822327\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 56000\n",
      "    num_steps_sampled: 56000\n",
      "    num_steps_trained: 56000\n",
      "  iterations_since_restore: 14\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.23658536585366\n",
      "    ram_util_percent: 25.090243902439024\n",
      "  pid: 17432\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08814106665713341\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 32.20618560530686\n",
      "    mean_inference_ms: 1.827584164929404\n",
      "    mean_raw_obs_processing_ms: 6.885703535580392\n",
      "  time_since_restore: 819.9958789348602\n",
      "  time_this_iter_s: 57.416553020477295\n",
      "  time_total_s: 819.9958789348602\n",
      "  timers:\n",
      "    learn_throughput: 156.86\n",
      "    learn_time_ms: 25500.464\n",
      "    sample_throughput: 122.607\n",
      "    sample_time_ms: 32624.623\n",
      "    update_time_ms: 3.187\n",
      "  timestamp: 1672608793\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 56000\n",
      "  training_iteration: 14\n",
      "  trial_id: f9c59_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Memory usage on this node: 7.8/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.87 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_3_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_0_6417799359d4e2284884fd3ae2ec9735, 0.0/6.0 CPU_group_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_1_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_4_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_2_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_5_6417799359d4e2284884fd3ae2ec9735)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                               | status   | loc                |   iter |   total time (s) |    ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|------------------------------------------+----------+--------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridPOEnv-v0_f9c59_00000 | RUNNING  | 192.168.1.65:17432 |     14 |          819.996 | 56000 | -265.368 |             -146.006 |             -452.651 |                400 |\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridPOEnv-v0_f9c59_00000:\n",
      "  agent_timesteps_total: 60000\n",
      "  custom_metrics: {}\n",
      "  date: 2023-01-01_15-34-11\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -146.00593813450914\n",
      "  episode_reward_mean: -257.9686798418919\n",
      "  episode_reward_min: -452.65121251785365\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 150\n",
      "  experiment_id: 610b8b5226964181899719d25df7617e\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.2207031249999999e-05\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 4.024245060235262\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010388485435882451\n",
      "          policy_loss: -0.0034430591185810044\n",
      "          total_loss: 407.95037932395934\n",
      "          vf_explained_var: -4.482455551624298e-06\n",
      "          vf_loss: 407.95381903648376\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 60000\n",
      "    num_steps_sampled: 60000\n",
      "    num_steps_trained: 60000\n",
      "  iterations_since_restore: 15\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.17228915662651\n",
      "    ram_util_percent: 25.126506024096386\n",
      "  pid: 17432\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08803490081598259\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 32.161476534924965\n",
      "    mean_inference_ms: 1.824742015397598\n",
      "    mean_raw_obs_processing_ms: 6.867740953680661\n",
      "  time_since_restore: 877.5810732841492\n",
      "  time_this_iter_s: 57.58519434928894\n",
      "  time_total_s: 877.5810732841492\n",
      "  timers:\n",
      "    learn_throughput: 156.703\n",
      "    learn_time_ms: 25526.07\n",
      "    sample_throughput: 122.571\n",
      "    sample_time_ms: 32634.124\n",
      "    update_time_ms: 3.179\n",
      "  timestamp: 1672608851\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 60000\n",
      "  training_iteration: 15\n",
      "  trial_id: f9c59_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 7.8/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.87 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_5_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_1_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_0_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_3_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_4_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_2_6417799359d4e2284884fd3ae2ec9735, 0.0/6.0 CPU_group_6417799359d4e2284884fd3ae2ec9735)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                               | status   | loc                |   iter |   total time (s) |    ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|------------------------------------------+----------+--------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridPOEnv-v0_f9c59_00000 | RUNNING  | 192.168.1.65:17432 |     15 |          877.581 | 60000 | -257.969 |             -146.006 |             -452.651 |                400 |\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridPOEnv-v0_f9c59_00000:\n",
      "  agent_timesteps_total: 64000\n",
      "  custom_metrics: {}\n",
      "  date: 2023-01-01_15-35-10\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -146.00593813450914\n",
      "  episode_reward_mean: -251.36117418703594\n",
      "  episode_reward_min: -435.4967444705686\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 160\n",
      "  experiment_id: 610b8b5226964181899719d25df7617e\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.2207031249999999e-05\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 4.0476192578673365\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.004808128484106086\n",
      "          policy_loss: -0.0009343948971945792\n",
      "          total_loss: 416.365075302124\n",
      "          vf_explained_var: 0.000125865830341354\n",
      "          vf_loss: 416.3660081863403\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 64000\n",
      "    num_steps_sampled: 64000\n",
      "    num_steps_trained: 64000\n",
      "  iterations_since_restore: 16\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.130120481927708\n",
      "    ram_util_percent: 25.17590361445783\n",
      "  pid: 17432\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08795865585057003\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 32.12300003270356\n",
      "    mean_inference_ms: 1.8232398456884635\n",
      "    mean_raw_obs_processing_ms: 6.8611265846410605\n",
      "  time_since_restore: 936.1736187934875\n",
      "  time_this_iter_s: 58.59254550933838\n",
      "  time_total_s: 936.1736187934875\n",
      "  timers:\n",
      "    learn_throughput: 156.465\n",
      "    learn_time_ms: 25564.866\n",
      "    sample_throughput: 122.56\n",
      "    sample_time_ms: 32636.991\n",
      "    update_time_ms: 3.181\n",
      "  timestamp: 1672608910\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 64000\n",
      "  training_iteration: 16\n",
      "  trial_id: f9c59_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 7.8/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.87 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_5_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_3_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_0_6417799359d4e2284884fd3ae2ec9735, 0.0/6.0 CPU_group_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_1_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_4_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_2_6417799359d4e2284884fd3ae2ec9735)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                               | status   | loc                |   iter |   total time (s) |    ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|------------------------------------------+----------+--------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridPOEnv-v0_f9c59_00000 | RUNNING  | 192.168.1.65:17432 |     16 |          936.174 | 64000 | -251.361 |             -146.006 |             -435.497 |                400 |\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_TrafficLightGridPOEnv-v0_f9c59_00000:\n",
      "  agent_timesteps_total: 68000\n",
      "  custom_metrics: {}\n",
      "  date: 2023-01-01_15-36-07\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -146.00593813450914\n",
      "  episode_reward_mean: -252.07338527742218\n",
      "  episode_reward_min: -435.4967444705686\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 170\n",
      "  experiment_id: 610b8b5226964181899719d25df7617e\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 6.1035156249999995e-06\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 4.031709042936564\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009602709446852495\n",
      "          policy_loss: -0.003445043784449808\n",
      "          total_loss: 467.62781744003297\n",
      "          vf_explained_var: 0.00011664982594083995\n",
      "          vf_loss: 467.6312602996826\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 68000\n",
      "    num_steps_sampled: 68000\n",
      "    num_steps_trained: 68000\n",
      "  iterations_since_restore: 17\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.317073170731707\n",
      "    ram_util_percent: 25.130487804878047\n",
      "  pid: 17432\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08789332012272326\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 32.08703215718683\n",
      "    mean_inference_ms: 1.8218144949702617\n",
      "    mean_raw_obs_processing_ms: 6.862262043001924\n",
      "  time_since_restore: 993.4778113365173\n",
      "  time_this_iter_s: 57.304192543029785\n",
      "  time_total_s: 993.4778113365173\n",
      "  timers:\n",
      "    learn_throughput: 156.793\n",
      "    learn_time_ms: 25511.352\n",
      "    sample_throughput: 122.744\n",
      "    sample_time_ms: 32588.102\n",
      "    update_time_ms: 3.225\n",
      "  timestamp: 1672608967\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 68000\n",
      "  training_iteration: 17\n",
      "  trial_id: f9c59_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 7.8/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.87 GiB heap, 0.0/14.65 GiB objects (0.0/6.0 CPU_group_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_2_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_3_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_0_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_1_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_5_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_4_6417799359d4e2284884fd3ae2ec9735)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                               | status   | loc                |   iter |   total time (s) |    ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|------------------------------------------+----------+--------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridPOEnv-v0_f9c59_00000 | RUNNING  | 192.168.1.65:17432 |     17 |          993.478 | 68000 | -252.073 |             -146.006 |             -435.497 |                400 |\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridPOEnv-v0_f9c59_00000:\n",
      "  agent_timesteps_total: 72000\n",
      "  custom_metrics: {}\n",
      "  date: 2023-01-01_15-37-04\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -146.00593813450914\n",
      "  episode_reward_mean: -249.9863795717985\n",
      "  episode_reward_min: -440.68199000122263\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 180\n",
      "  experiment_id: 610b8b5226964181899719d25df7617e\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 3.0517578124999997e-06\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 3.976345792412758\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010079567285055203\n",
      "          policy_loss: -0.0034833403697120957\n",
      "          total_loss: 516.5364399909973\n",
      "          vf_explained_var: 4.939734935760498e-05\n",
      "          vf_loss: 516.5399251937866\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 72000\n",
      "    num_steps_sampled: 72000\n",
      "    num_steps_trained: 72000\n",
      "  iterations_since_restore: 18\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.276829268292687\n",
      "    ram_util_percent: 25.146341463414632\n",
      "  pid: 17432\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08783482885161224\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 32.05421741983803\n",
      "    mean_inference_ms: 1.820272531619351\n",
      "    mean_raw_obs_processing_ms: 6.868846549010669\n",
      "  time_since_restore: 1050.5767269134521\n",
      "  time_this_iter_s: 57.098915576934814\n",
      "  time_total_s: 1050.5767269134521\n",
      "  timers:\n",
      "    learn_throughput: 157.003\n",
      "    learn_time_ms: 25477.266\n",
      "    sample_throughput: 122.87\n",
      "    sample_time_ms: 32554.64\n",
      "    update_time_ms: 3.24\n",
      "  timestamp: 1672609024\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 72000\n",
      "  training_iteration: 18\n",
      "  trial_id: f9c59_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 7.8/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.87 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_2_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_0_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_4_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_5_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_1_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_3_6417799359d4e2284884fd3ae2ec9735, 0.0/6.0 CPU_group_6417799359d4e2284884fd3ae2ec9735)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                               | status   | loc                |   iter |   total time (s) |    ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|------------------------------------------+----------+--------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridPOEnv-v0_f9c59_00000 | RUNNING  | 192.168.1.65:17432 |     18 |          1050.58 | 72000 | -249.986 |             -146.006 |             -440.682 |                400 |\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=17433)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=17431)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=17430)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=17429)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=17428)\u001b[0m  Retrying in 1 seconds\n",
      "Result for PPO_TrafficLightGridPOEnv-v0_f9c59_00000:\n",
      "  agent_timesteps_total: 76000\n",
      "  custom_metrics: {}\n",
      "  date: 2023-01-01_15-38-02\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -146.00593813450914\n",
      "  episode_reward_mean: -249.9782951316422\n",
      "  episode_reward_min: -440.68199000122263\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 190\n",
      "  experiment_id: 610b8b5226964181899719d25df7617e\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 3.0517578124999997e-06\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 3.8844110399484633\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008259930070789786\n",
      "          policy_loss: -0.002685883153753821\n",
      "          total_loss: 493.35472841262816\n",
      "          vf_explained_var: 0.00025874245329760015\n",
      "          vf_loss: 493.3574147224426\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 76000\n",
      "    num_steps_sampled: 76000\n",
      "    num_steps_trained: 76000\n",
      "  iterations_since_restore: 19\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.904878048780485\n",
      "    ram_util_percent: 25.17560975609756\n",
      "  pid: 17432\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08779055040895326\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 32.02279488991539\n",
      "    mean_inference_ms: 1.8189446513524508\n",
      "    mean_raw_obs_processing_ms: 6.88531699888378\n",
      "  time_since_restore: 1108.5366995334625\n",
      "  time_this_iter_s: 57.959972620010376\n",
      "  time_total_s: 1108.5366995334625\n",
      "  timers:\n",
      "    learn_throughput: 157.306\n",
      "    learn_time_ms: 25428.069\n",
      "    sample_throughput: 122.641\n",
      "    sample_time_ms: 32615.462\n",
      "    update_time_ms: 3.176\n",
      "  timestamp: 1672609082\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 76000\n",
      "  training_iteration: 19\n",
      "  trial_id: f9c59_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Memory usage on this node: 7.8/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.87 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_4_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_1_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_3_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_0_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_2_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_5_6417799359d4e2284884fd3ae2ec9735, 0.0/6.0 CPU_group_6417799359d4e2284884fd3ae2ec9735)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                               | status   | loc                |   iter |   total time (s) |    ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|------------------------------------------+----------+--------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridPOEnv-v0_f9c59_00000 | RUNNING  | 192.168.1.65:17432 |     19 |          1108.54 | 76000 | -249.978 |             -146.006 |             -440.682 |                400 |\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridPOEnv-v0_f9c59_00000:\n",
      "  agent_timesteps_total: 80000\n",
      "  custom_metrics: {}\n",
      "  date: 2023-01-01_15-38-59\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -192.49721062552686\n",
      "  episode_reward_mean: -248.80948081254672\n",
      "  episode_reward_min: -440.68199000122263\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 200\n",
      "  experiment_id: 610b8b5226964181899719d25df7617e\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.5258789062499999e-06\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 3.929415311664343\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.004944584550218433\n",
      "          policy_loss: 0.000758402535575442\n",
      "          total_loss: 448.27771906852723\n",
      "          vf_explained_var: 3.8018450140953064e-05\n",
      "          vf_loss: 448.2769584655762\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 80000\n",
      "    num_steps_sampled: 80000\n",
      "    num_steps_trained: 80000\n",
      "  iterations_since_restore: 20\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.335365853658537\n",
      "    ram_util_percent: 25.146341463414632\n",
      "  pid: 17432\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08775690580564457\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.99772922586086\n",
      "    mean_inference_ms: 1.8181243469869601\n",
      "    mean_raw_obs_processing_ms: 6.902898297638326\n",
      "  time_since_restore: 1165.4794418811798\n",
      "  time_this_iter_s: 56.942742347717285\n",
      "  time_total_s: 1165.4794418811798\n",
      "  timers:\n",
      "    learn_throughput: 157.202\n",
      "    learn_time_ms: 25444.89\n",
      "    sample_throughput: 122.628\n",
      "    sample_time_ms: 32619.103\n",
      "    update_time_ms: 3.237\n",
      "  timestamp: 1672609139\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 80000\n",
      "  training_iteration: 20\n",
      "  trial_id: f9c59_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 7.8/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.87 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_4_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_1_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_0_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_5_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_2_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_3_6417799359d4e2284884fd3ae2ec9735, 0.0/6.0 CPU_group_6417799359d4e2284884fd3ae2ec9735)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                               | status   | loc                |   iter |   total time (s) |    ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|------------------------------------------+----------+--------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridPOEnv-v0_f9c59_00000 | RUNNING  | 192.168.1.65:17432 |     20 |          1165.48 | 80000 | -248.809 |             -192.497 |             -440.682 |                400 |\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=17433)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=17431)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=17429)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=17428)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=17430)\u001b[0m  Retrying in 1 seconds\n",
      "Result for PPO_TrafficLightGridPOEnv-v0_f9c59_00000:\n",
      "  agent_timesteps_total: 84000\n",
      "  custom_metrics: {}\n",
      "  date: 2023-01-01_15-39-58\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -192.49721062552686\n",
      "  episode_reward_mean: -249.54494576972607\n",
      "  episode_reward_min: -440.68199000122263\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 210\n",
      "  experiment_id: 610b8b5226964181899719d25df7617e\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 7.629394531249999e-07\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 3.8922872841358185\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008716534224112147\n",
      "          policy_loss: -0.003188693055562908\n",
      "          total_loss: 469.33230924606323\n",
      "          vf_explained_var: 0.0001541731908218935\n",
      "          vf_loss: 469.3354963302612\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 84000\n",
      "    num_steps_sampled: 84000\n",
      "    num_steps_trained: 84000\n",
      "  iterations_since_restore: 21\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.11204819277108\n",
      "    ram_util_percent: 25.140963855421685\n",
      "  pid: 17432\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08772617265618728\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.977786589140532\n",
      "    mean_inference_ms: 1.8171654455007102\n",
      "    mean_raw_obs_processing_ms: 6.85979323708553\n",
      "  time_since_restore: 1224.0811593532562\n",
      "  time_this_iter_s: 58.601717472076416\n",
      "  time_total_s: 1224.0811593532562\n",
      "  timers:\n",
      "    learn_throughput: 157.323\n",
      "    learn_time_ms: 25425.465\n",
      "    sample_throughput: 124.44\n",
      "    sample_time_ms: 32143.909\n",
      "    update_time_ms: 3.232\n",
      "  timestamp: 1672609198\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 84000\n",
      "  training_iteration: 21\n",
      "  trial_id: f9c59_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 7.8/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.87 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_2_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_1_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_4_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_3_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_5_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 accelerator_type:G, 0.0/6.0 CPU_group_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_0_6417799359d4e2284884fd3ae2ec9735)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                               | status   | loc                |   iter |   total time (s) |    ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|------------------------------------------+----------+--------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridPOEnv-v0_f9c59_00000 | RUNNING  | 192.168.1.65:17432 |     21 |          1224.08 | 84000 | -249.545 |             -192.497 |             -440.682 |                400 |\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_TrafficLightGridPOEnv-v0_f9c59_00000:\n",
      "  agent_timesteps_total: 88000\n",
      "  custom_metrics: {}\n",
      "  date: 2023-01-01_15-40-56\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -192.49721062552686\n",
      "  episode_reward_mean: -250.47473826170565\n",
      "  episode_reward_min: -440.68199000122263\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 220\n",
      "  experiment_id: 610b8b5226964181899719d25df7617e\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 3.8146972656249997e-07\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 3.912882287800312\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.007682194566842798\n",
      "          policy_loss: -0.00457593474566238\n",
      "          total_loss: 467.4608045578003\n",
      "          vf_explained_var: -6.332993685020938e-09\n",
      "          vf_loss: 467.46538219451907\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 88000\n",
      "    num_steps_sampled: 88000\n",
      "    num_steps_trained: 88000\n",
      "  iterations_since_restore: 22\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.87228915662651\n",
      "    ram_util_percent: 25.08192771084337\n",
      "  pid: 17432\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08770211624847656\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.95989902043083\n",
      "    mean_inference_ms: 1.816310801314911\n",
      "    mean_raw_obs_processing_ms: 6.82312666596124\n",
      "  time_since_restore: 1281.832421541214\n",
      "  time_this_iter_s: 57.751262187957764\n",
      "  time_total_s: 1281.832421541214\n",
      "  timers:\n",
      "    learn_throughput: 156.898\n",
      "    learn_time_ms: 25494.265\n",
      "    sample_throughput: 124.449\n",
      "    sample_time_ms: 32141.721\n",
      "    update_time_ms: 3.265\n",
      "  timestamp: 1672609256\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 88000\n",
      "  training_iteration: 22\n",
      "  trial_id: f9c59_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 7.8/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.87 GiB heap, 0.0/14.65 GiB objects (0.0/6.0 CPU_group_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_1_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_3_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_4_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_0_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_2_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_5_6417799359d4e2284884fd3ae2ec9735)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                               | status   | loc                |   iter |   total time (s) |    ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|------------------------------------------+----------+--------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridPOEnv-v0_f9c59_00000 | RUNNING  | 192.168.1.65:17432 |     22 |          1281.83 | 88000 | -250.475 |             -192.497 |             -440.682 |                400 |\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridPOEnv-v0_f9c59_00000:\n",
      "  agent_timesteps_total: 92000\n",
      "  custom_metrics: {}\n",
      "  date: 2023-01-01_15-41-53\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -192.49721062552686\n",
      "  episode_reward_mean: -249.03292260848812\n",
      "  episode_reward_min: -440.68199000122263\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 230\n",
      "  experiment_id: 610b8b5226964181899719d25df7617e\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.9073486328124998e-07\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 3.861367493122816\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008987013402965273\n",
      "          policy_loss: -0.002823548475862481\n",
      "          total_loss: 465.1676613807678\n",
      "          vf_explained_var: 5.166455957805738e-05\n",
      "          vf_loss: 465.17048625946046\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 92000\n",
      "    num_steps_sampled: 92000\n",
      "    num_steps_trained: 92000\n",
      "  iterations_since_restore: 23\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.297560975609755\n",
      "    ram_util_percent: 25.131707317073168\n",
      "  pid: 17432\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08768513274789677\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.94658277410177\n",
      "    mean_inference_ms: 1.81563116196559\n",
      "    mean_raw_obs_processing_ms: 6.791283186270781\n",
      "  time_since_restore: 1339.3250720500946\n",
      "  time_this_iter_s: 57.492650508880615\n",
      "  time_total_s: 1339.3250720500946\n",
      "  timers:\n",
      "    learn_throughput: 156.946\n",
      "    learn_time_ms: 25486.445\n",
      "    sample_throughput: 124.324\n",
      "    sample_time_ms: 32173.877\n",
      "    update_time_ms: 3.259\n",
      "  timestamp: 1672609313\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 92000\n",
      "  training_iteration: 23\n",
      "  trial_id: f9c59_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 7.8/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.87 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_0_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_2_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_1_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_4_6417799359d4e2284884fd3ae2ec9735, 0.0/6.0 CPU_group_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_5_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_3_6417799359d4e2284884fd3ae2ec9735)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                               | status   | loc                |   iter |   total time (s) |    ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|------------------------------------------+----------+--------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridPOEnv-v0_f9c59_00000 | RUNNING  | 192.168.1.65:17432 |     23 |          1339.33 | 92000 | -249.033 |             -192.497 |             -440.682 |                400 |\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridPOEnv-v0_f9c59_00000:\n",
      "  agent_timesteps_total: 96000\n",
      "  custom_metrics: {}\n",
      "  date: 2023-01-01_15-42-51\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -192.49721062552686\n",
      "  episode_reward_mean: -252.40712922737006\n",
      "  episode_reward_min: -440.68199000122263\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 240\n",
      "  experiment_id: 610b8b5226964181899719d25df7617e\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 9.536743164062499e-08\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 3.9346211954951285\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.006348502804191027\n",
      "          policy_loss: 1.7907426808960735e-06\n",
      "          total_loss: 471.9778522491455\n",
      "          vf_explained_var: 0.00011498537060106173\n",
      "          vf_loss: 471.97784729003905\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 96000\n",
      "    num_steps_sampled: 96000\n",
      "    num_steps_trained: 96000\n",
      "  iterations_since_restore: 24\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.228048780487807\n",
      "    ram_util_percent: 25.195121951219512\n",
      "  pid: 17432\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08767475206246976\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.932267602142293\n",
      "    mean_inference_ms: 1.8154692210618901\n",
      "    mean_raw_obs_processing_ms: 6.763800555636058\n",
      "  time_since_restore: 1396.8437643051147\n",
      "  time_this_iter_s: 57.51869225502014\n",
      "  time_total_s: 1396.8437643051147\n",
      "  timers:\n",
      "    learn_throughput: 156.767\n",
      "    learn_time_ms: 25515.625\n",
      "    sample_throughput: 124.398\n",
      "    sample_time_ms: 32154.94\n",
      "    update_time_ms: 3.255\n",
      "  timestamp: 1672609371\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 96000\n",
      "  training_iteration: 24\n",
      "  trial_id: f9c59_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Memory usage on this node: 7.8/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.87 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_3_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_2_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_0_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_5_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_4_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_1_6417799359d4e2284884fd3ae2ec9735, 0.0/6.0 CPU_group_6417799359d4e2284884fd3ae2ec9735)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                               | status   | loc                |   iter |   total time (s) |    ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|------------------------------------------+----------+--------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridPOEnv-v0_f9c59_00000 | RUNNING  | 192.168.1.65:17432 |     24 |          1396.84 | 96000 | -252.407 |             -192.497 |             -440.682 |                400 |\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=17433)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=17430)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=17428)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=17429)\u001b[0m  Retrying in 1 seconds\n",
      "Result for PPO_TrafficLightGridPOEnv-v0_f9c59_00000:\n",
      "  agent_timesteps_total: 100000\n",
      "  custom_metrics: {}\n",
      "  date: 2023-01-01_15-43-50\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -194.45193242205082\n",
      "  episode_reward_mean: -255.51226176049187\n",
      "  episode_reward_min: -440.68199000122263\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 250\n",
      "  experiment_id: 610b8b5226964181899719d25df7617e\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 4.7683715820312496e-08\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 3.8199284330010412\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0077385310380805195\n",
      "          policy_loss: -0.0006959471851587295\n",
      "          total_loss: 527.4229202270508\n",
      "          vf_explained_var: 0.00015923846513032913\n",
      "          vf_loss: 527.4236183166504\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 100000\n",
      "    num_steps_sampled: 100000\n",
      "    num_steps_trained: 100000\n",
      "  iterations_since_restore: 25\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.85952380952381\n",
      "    ram_util_percent: 25.15238095238095\n",
      "  pid: 17432\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08766482203723697\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.921035487070483\n",
      "    mean_inference_ms: 1.8154400477464028\n",
      "    mean_raw_obs_processing_ms: 6.743274927471597\n",
      "  time_since_restore: 1455.7718350887299\n",
      "  time_this_iter_s: 58.92807078361511\n",
      "  time_total_s: 1455.7718350887299\n",
      "  timers:\n",
      "    learn_throughput: 156.696\n",
      "    learn_time_ms: 25527.118\n",
      "    sample_throughput: 123.924\n",
      "    sample_time_ms: 32277.749\n",
      "    update_time_ms: 3.225\n",
      "  timestamp: 1672609430\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 100000\n",
      "  training_iteration: 25\n",
      "  trial_id: f9c59_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 7.8/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.87 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_0_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_3_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_4_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_5_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_1_6417799359d4e2284884fd3ae2ec9735, 0.0/6.0 CPU_group_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_2_6417799359d4e2284884fd3ae2ec9735)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                               | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridPOEnv-v0_f9c59_00000 | RUNNING  | 192.168.1.65:17432 |     25 |          1455.77 | 100000 | -255.512 |             -194.452 |             -440.682 |                400 |\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridPOEnv-v0_f9c59_00000:\n",
      "  agent_timesteps_total: 104000\n",
      "  custom_metrics: {}\n",
      "  date: 2023-01-01_15-44-49\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -177.12121227875517\n",
      "  episode_reward_mean: -259.08295186777355\n",
      "  episode_reward_min: -440.68199000122263\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 260\n",
      "  experiment_id: 610b8b5226964181899719d25df7617e\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.3841857910156248e-08\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 3.74410108178854\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01050319478776772\n",
      "          policy_loss: -0.003543085910496302\n",
      "          total_loss: 500.6195202827454\n",
      "          vf_explained_var: 0.00011594816896831617\n",
      "          vf_loss: 500.62306509017947\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 104000\n",
      "    num_steps_sampled: 104000\n",
      "    num_steps_trained: 104000\n",
      "  iterations_since_restore: 26\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 22.409411764705883\n",
      "    ram_util_percent: 26.27764705882353\n",
      "  pid: 17432\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08765531654261036\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.908220664973022\n",
      "    mean_inference_ms: 1.815335108826386\n",
      "    mean_raw_obs_processing_ms: 6.7255491441940265\n",
      "  time_since_restore: 1514.8808121681213\n",
      "  time_this_iter_s: 59.10897707939148\n",
      "  time_total_s: 1514.8808121681213\n",
      "  timers:\n",
      "    learn_throughput: 156.186\n",
      "    learn_time_ms: 25610.422\n",
      "    sample_throughput: 124.046\n",
      "    sample_time_ms: 32246.158\n",
      "    update_time_ms: 3.217\n",
      "  timestamp: 1672609489\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 104000\n",
      "  training_iteration: 26\n",
      "  trial_id: f9c59_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 8.5/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.87 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_5_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_2_6417799359d4e2284884fd3ae2ec9735, 0.0/6.0 CPU_group_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_0_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_4_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_1_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_3_6417799359d4e2284884fd3ae2ec9735)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                               | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridPOEnv-v0_f9c59_00000 | RUNNING  | 192.168.1.65:17432 |     26 |          1514.88 | 104000 | -259.083 |             -177.121 |             -440.682 |                400 |\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_TrafficLightGridPOEnv-v0_f9c59_00000:\n",
      "  agent_timesteps_total: 108000\n",
      "  custom_metrics: {}\n",
      "  date: 2023-01-01_15-45-47\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -177.12121227875517\n",
      "  episode_reward_mean: -260.829341392639\n",
      "  episode_reward_min: -440.68199000122263\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 270\n",
      "  experiment_id: 610b8b5226964181899719d25df7617e\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.3841857910156248e-08\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 3.7003276862204073\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01198714954085177\n",
      "          policy_loss: -0.004410425703099463\n",
      "          total_loss: 509.27320818901063\n",
      "          vf_explained_var: 0.0002016151265706867\n",
      "          vf_loss: 509.2776176929474\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 108000\n",
      "    num_steps_sampled: 108000\n",
      "    num_steps_trained: 108000\n",
      "  iterations_since_restore: 27\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.50843373493976\n",
      "    ram_util_percent: 27.595180722891563\n",
      "  pid: 17432\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08764549871801477\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.898817732418593\n",
      "    mean_inference_ms: 1.8149794341117482\n",
      "    mean_raw_obs_processing_ms: 6.710149556325061\n",
      "  time_since_restore: 1573.0205359458923\n",
      "  time_this_iter_s: 58.139723777770996\n",
      "  time_total_s: 1573.0205359458923\n",
      "  timers:\n",
      "    learn_throughput: 155.943\n",
      "    learn_time_ms: 25650.376\n",
      "    sample_throughput: 123.878\n",
      "    sample_time_ms: 32289.831\n",
      "    update_time_ms: 3.177\n",
      "  timestamp: 1672609547\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 108000\n",
      "  training_iteration: 27\n",
      "  trial_id: f9c59_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 8.6/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.87 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_3_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_0_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_2_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_4_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 accelerator_type:G, 0.0/6.0 CPU_group_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_1_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_5_6417799359d4e2284884fd3ae2ec9735)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                               | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridPOEnv-v0_f9c59_00000 | RUNNING  | 192.168.1.65:17432 |     27 |          1573.02 | 108000 | -260.829 |             -177.121 |             -440.682 |                400 |\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridPOEnv-v0_f9c59_00000:\n",
      "  agent_timesteps_total: 112000\n",
      "  custom_metrics: {}\n",
      "  date: 2023-01-01_15-46-44\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -177.12121227875517\n",
      "  episode_reward_mean: -261.2323837562001\n",
      "  episode_reward_min: -376.16927459523515\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 280\n",
      "  experiment_id: 610b8b5226964181899719d25df7617e\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.3841857910156248e-08\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 3.701001160591841\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008092212020136469\n",
      "          policy_loss: -0.0007442811926011928\n",
      "          total_loss: 604.8318091869354\n",
      "          vf_explained_var: 4.4980086386203766e-05\n",
      "          vf_loss: 604.832555770874\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 112000\n",
      "    num_steps_sampled: 112000\n",
      "    num_steps_trained: 112000\n",
      "  iterations_since_restore: 28\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.048148148148147\n",
      "    ram_util_percent: 27.618518518518513\n",
      "  pid: 17432\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08764093960864255\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.89161674360839\n",
      "    mean_inference_ms: 1.8147074375737204\n",
      "    mean_raw_obs_processing_ms: 6.696319592476981\n",
      "  time_since_restore: 1630.3304686546326\n",
      "  time_this_iter_s: 57.309932708740234\n",
      "  time_total_s: 1630.3304686546326\n",
      "  timers:\n",
      "    learn_throughput: 155.873\n",
      "    learn_time_ms: 25661.846\n",
      "    sample_throughput: 123.841\n",
      "    sample_time_ms: 32299.412\n",
      "    update_time_ms: 3.162\n",
      "  timestamp: 1672609604\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 112000\n",
      "  training_iteration: 28\n",
      "  trial_id: f9c59_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 8.6/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.87 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_2_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_4_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_5_6417799359d4e2284884fd3ae2ec9735, 0.0/6.0 CPU_group_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_3_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_1_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_0_6417799359d4e2284884fd3ae2ec9735)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                               | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridPOEnv-v0_f9c59_00000 | RUNNING  | 192.168.1.65:17432 |     28 |          1630.33 | 112000 | -261.232 |             -177.121 |             -376.169 |                400 |\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=17429)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=17428)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=17433)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=17431)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=17430)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=17429)\u001b[0m Could not connect to TraCI server at localhost:41647 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=17429)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=17428)\u001b[0m Could not connect to TraCI server at localhost:47505 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=17428)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=17433)\u001b[0m Could not connect to TraCI server at localhost:33079 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=17433)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=17431)\u001b[0m Could not connect to TraCI server at localhost:42343 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=17431)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=17430)\u001b[0m Could not connect to TraCI server at localhost:33057 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=17430)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=17431)\u001b[0m Could not connect to TraCI server at localhost:42343 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=17431)\u001b[0m  Retrying in 3 seconds\n",
      "\u001b[2m\u001b[36m(pid=17429)\u001b[0m Could not connect to TraCI server at localhost:41647 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=17429)\u001b[0m  Retrying in 3 seconds\n",
      "\u001b[2m\u001b[36m(pid=17428)\u001b[0m Could not connect to TraCI server at localhost:47505 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=17428)\u001b[0m  Retrying in 3 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=17433)\u001b[0m Could not connect to TraCI server at localhost:33079 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=17433)\u001b[0m  Retrying in 3 seconds\n",
      "\u001b[2m\u001b[36m(pid=17430)\u001b[0m Could not connect to TraCI server at localhost:33057 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=17430)\u001b[0m  Retrying in 3 seconds\n",
      "Result for PPO_TrafficLightGridPOEnv-v0_f9c59_00000:\n",
      "  agent_timesteps_total: 116000\n",
      "  custom_metrics: {}\n",
      "  date: 2023-01-01_15-47-47\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -177.12121227875517\n",
      "  episode_reward_mean: -258.10672225426833\n",
      "  episode_reward_min: -343.20837122953543\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 290\n",
      "  experiment_id: 610b8b5226964181899719d25df7617e\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.1920928955078124e-08\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 3.6815273009240626\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.005113747005248128\n",
      "          policy_loss: -0.0014584290533093736\n",
      "          total_loss: 580.7264873504639\n",
      "          vf_explained_var: -2.2351742678949904e-09\n",
      "          vf_loss: 580.7279449462891\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 116000\n",
      "    num_steps_sampled: 116000\n",
      "    num_steps_trained: 116000\n",
      "  iterations_since_restore: 29\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.37333333333333\n",
      "    ram_util_percent: 27.640000000000004\n",
      "  pid: 17432\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08764153100601338\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.884698035546226\n",
      "    mean_inference_ms: 1.8148323463544966\n",
      "    mean_raw_obs_processing_ms: 6.703990658244663\n",
      "  time_since_restore: 1693.340524673462\n",
      "  time_this_iter_s: 63.010056018829346\n",
      "  time_total_s: 1693.340524673462\n",
      "  timers:\n",
      "    learn_throughput: 155.807\n",
      "    learn_time_ms: 25672.842\n",
      "    sample_throughput: 121.975\n",
      "    sample_time_ms: 32793.523\n",
      "    update_time_ms: 3.165\n",
      "  timestamp: 1672609667\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 116000\n",
      "  training_iteration: 29\n",
      "  trial_id: f9c59_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 8.6/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.87 GiB heap, 0.0/14.65 GiB objects (0.0/6.0 CPU_group_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_3_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_0_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_2_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_5_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_1_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_4_6417799359d4e2284884fd3ae2ec9735)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                               | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridPOEnv-v0_f9c59_00000 | RUNNING  | 192.168.1.65:17432 |     29 |          1693.34 | 116000 | -258.107 |             -177.121 |             -343.208 |                400 |\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridPOEnv-v0_f9c59_00000:\n",
      "  agent_timesteps_total: 120000\n",
      "  custom_metrics: {}\n",
      "  date: 2023-01-01_15-48-45\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -177.12121227875517\n",
      "  episode_reward_mean: -256.37514172475835\n",
      "  episode_reward_min: -343.20837122953543\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 300\n",
      "  experiment_id: 610b8b5226964181899719d25df7617e\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 5.960464477539062e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 3.558542263507843\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009022592458632784\n",
      "          policy_loss: -0.0002639031474245712\n",
      "          total_loss: 560.1797323226929\n",
      "          vf_explained_var: 1.1175871339474952e-09\n",
      "          vf_loss: 560.1799975395203\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 120000\n",
      "    num_steps_sampled: 120000\n",
      "    num_steps_trained: 120000\n",
      "  iterations_since_restore: 30\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.406024096385547\n",
      "    ram_util_percent: 27.7421686746988\n",
      "  pid: 17432\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08764663924233539\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.87866914924703\n",
      "    mean_inference_ms: 1.8148579118308508\n",
      "    mean_raw_obs_processing_ms: 6.712800364656979\n",
      "  time_since_restore: 1750.884458065033\n",
      "  time_this_iter_s: 57.543933391571045\n",
      "  time_total_s: 1750.884458065033\n",
      "  timers:\n",
      "    learn_throughput: 155.437\n",
      "    learn_time_ms: 25733.864\n",
      "    sample_throughput: 121.979\n",
      "    sample_time_ms: 32792.657\n",
      "    update_time_ms: 3.126\n",
      "  timestamp: 1672609725\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 120000\n",
      "  training_iteration: 30\n",
      "  trial_id: f9c59_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 8.7/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.87 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_0_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_2_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_5_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_4_6417799359d4e2284884fd3ae2ec9735, 0.0/6.0 CPU_group_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_3_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_1_6417799359d4e2284884fd3ae2ec9735)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                               | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridPOEnv-v0_f9c59_00000 | RUNNING  | 192.168.1.65:17432 |     30 |          1750.88 | 120000 | -256.375 |             -177.121 |             -343.208 |                400 |\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridPOEnv-v0_f9c59_00000:\n",
      "  agent_timesteps_total: 124000\n",
      "  custom_metrics: {}\n",
      "  date: 2023-01-01_15-49-42\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -177.12121227875517\n",
      "  episode_reward_mean: -256.7714178276186\n",
      "  episode_reward_min: -333.13279976400264\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 310\n",
      "  experiment_id: 610b8b5226964181899719d25df7617e\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.980232238769531e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 3.6645510576665403\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0057550770969953795\n",
      "          policy_loss: -0.0004097793047549203\n",
      "          total_loss: 568.2739014148713\n",
      "          vf_explained_var: 7.450580707946131e-10\n",
      "          vf_loss: 568.274311208725\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 124000\n",
      "    num_steps_sampled: 124000\n",
      "    num_steps_trained: 124000\n",
      "  iterations_since_restore: 31\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.808641975308642\n",
      "    ram_util_percent: 27.89012345679012\n",
      "  pid: 17432\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08765609364197335\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.87168707776027\n",
      "    mean_inference_ms: 1.8149300719174415\n",
      "    mean_raw_obs_processing_ms: 6.716325186601806\n",
      "  time_since_restore: 1808.2044956684113\n",
      "  time_this_iter_s: 57.320037603378296\n",
      "  time_total_s: 1808.2044956684113\n",
      "  timers:\n",
      "    learn_throughput: 155.384\n",
      "    learn_time_ms: 25742.718\n",
      "    sample_throughput: 122.491\n",
      "    sample_time_ms: 32655.58\n",
      "    update_time_ms: 3.17\n",
      "  timestamp: 1672609782\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 124000\n",
      "  training_iteration: 31\n",
      "  trial_id: f9c59_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Memory usage on this node: 8.7/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.87 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_5_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_4_6417799359d4e2284884fd3ae2ec9735, 0.0/6.0 CPU_group_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_0_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_3_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_2_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_1_6417799359d4e2284884fd3ae2ec9735)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                               | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridPOEnv-v0_f9c59_00000 | RUNNING  | 192.168.1.65:17432 |     31 |           1808.2 | 124000 | -256.771 |             -177.121 |             -333.133 |                400 |\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=17433)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=17429)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=17428)\u001b[0m  Retrying in 1 seconds\n",
      "Result for PPO_TrafficLightGridPOEnv-v0_f9c59_00000:\n",
      "  agent_timesteps_total: 128000\n",
      "  custom_metrics: {}\n",
      "  date: 2023-01-01_15-50-41\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -177.12121227875517\n",
      "  episode_reward_mean: -256.0653207159201\n",
      "  episode_reward_min: -366.35812344741777\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 320\n",
      "  experiment_id: 610b8b5226964181899719d25df7617e\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.4901161193847655e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 3.633818097412586\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012527112230239385\n",
      "          policy_loss: -0.003478152325260453\n",
      "          total_loss: 650.6071121215821\n",
      "          vf_explained_var: 1.8393993741483428e-05\n",
      "          vf_loss: 650.6105854034424\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 128000\n",
      "    num_steps_sampled: 128000\n",
      "    num_steps_trained: 128000\n",
      "  iterations_since_restore: 32\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.353571428571428\n",
      "    ram_util_percent: 27.772619047619052\n",
      "  pid: 17432\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08767120916368172\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.86589419175759\n",
      "    mean_inference_ms: 1.8151910750724793\n",
      "    mean_raw_obs_processing_ms: 6.72265088018321\n",
      "  time_since_restore: 1866.701010465622\n",
      "  time_this_iter_s: 58.49651479721069\n",
      "  time_total_s: 1866.701010465622\n",
      "  timers:\n",
      "    learn_throughput: 155.557\n",
      "    learn_time_ms: 25714.045\n",
      "    sample_throughput: 122.105\n",
      "    sample_time_ms: 32758.751\n",
      "    update_time_ms: 3.146\n",
      "  timestamp: 1672609841\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 128000\n",
      "  training_iteration: 32\n",
      "  trial_id: f9c59_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 8.6/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.87 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_2_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_5_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_3_6417799359d4e2284884fd3ae2ec9735, 0.0/6.0 CPU_group_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_1_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_4_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_0_6417799359d4e2284884fd3ae2ec9735)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                               | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridPOEnv-v0_f9c59_00000 | RUNNING  | 192.168.1.65:17432 |     32 |           1866.7 | 128000 | -256.065 |             -177.121 |             -366.358 |                400 |\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridPOEnv-v0_f9c59_00000:\n",
      "  agent_timesteps_total: 132000\n",
      "  custom_metrics: {}\n",
      "  date: 2023-01-01_15-51-38\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -177.12121227875517\n",
      "  episode_reward_mean: -255.25299570031177\n",
      "  episode_reward_min: -366.35812344741777\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 330\n",
      "  experiment_id: 610b8b5226964181899719d25df7617e\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.4901161193847655e-09\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 3.5860275022685526\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.00971994180805213\n",
      "          policy_loss: -0.000787435463280417\n",
      "          total_loss: 596.529242324829\n",
      "          vf_explained_var: 2.9037521017016843e-05\n",
      "          vf_loss: 596.5300307273865\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 132000\n",
      "    num_steps_sampled: 132000\n",
      "    num_steps_trained: 132000\n",
      "  iterations_since_restore: 33\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.822222222222223\n",
      "    ram_util_percent: 27.750617283950618\n",
      "  pid: 17432\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08768999344781385\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.859499241601217\n",
      "    mean_inference_ms: 1.815430076882208\n",
      "    mean_raw_obs_processing_ms: 6.729688727187401\n",
      "  time_since_restore: 1923.679363489151\n",
      "  time_this_iter_s: 56.97835302352905\n",
      "  time_total_s: 1923.679363489151\n",
      "  timers:\n",
      "    learn_throughput: 155.719\n",
      "    learn_time_ms: 25687.234\n",
      "    sample_throughput: 122.197\n",
      "    sample_time_ms: 32734.061\n",
      "    update_time_ms: 3.134\n",
      "  timestamp: 1672609898\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 132000\n",
      "  training_iteration: 33\n",
      "  trial_id: f9c59_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 8.6/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.87 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_2_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_5_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_1_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_0_6417799359d4e2284884fd3ae2ec9735, 0.0/6.0 CPU_group_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_4_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_3_6417799359d4e2284884fd3ae2ec9735)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                               | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridPOEnv-v0_f9c59_00000 | RUNNING  | 192.168.1.65:17432 |     33 |          1923.68 | 132000 | -255.253 |             -177.121 |             -366.358 |                400 |\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_TrafficLightGridPOEnv-v0_f9c59_00000:\n",
      "  agent_timesteps_total: 136000\n",
      "  custom_metrics: {}\n",
      "  date: 2023-01-01_15-52-36\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -177.12121227875517\n",
      "  episode_reward_mean: -255.32211047808156\n",
      "  episode_reward_min: -368.1926754510366\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 340\n",
      "  experiment_id: 610b8b5226964181899719d25df7617e\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 7.450580596923828e-10\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 3.5185629077255727\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01598627472393447\n",
      "          policy_loss: -0.01040285951748956\n",
      "          total_loss: 556.346047115326\n",
      "          vf_explained_var: 9.489618241786957e-05\n",
      "          vf_loss: 556.3564477920532\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 136000\n",
      "    num_steps_sampled: 136000\n",
      "    num_steps_trained: 136000\n",
      "  iterations_since_restore: 34\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.50722891566265\n",
      "    ram_util_percent: 27.914457831325297\n",
      "  pid: 17432\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08770957873739213\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.853316259323066\n",
      "    mean_inference_ms: 1.8156672204220712\n",
      "    mean_raw_obs_processing_ms: 6.737412516967247\n",
      "  time_since_restore: 1981.5893559455872\n",
      "  time_this_iter_s: 57.90999245643616\n",
      "  time_total_s: 1981.5893559455872\n",
      "  timers:\n",
      "    learn_throughput: 155.56\n",
      "    learn_time_ms: 25713.615\n",
      "    sample_throughput: 122.149\n",
      "    sample_time_ms: 32746.776\n",
      "    update_time_ms: 3.167\n",
      "  timestamp: 1672609956\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 136000\n",
      "  training_iteration: 34\n",
      "  trial_id: f9c59_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 8.7/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.87 GiB heap, 0.0/14.65 GiB objects (0.0/6.0 CPU_group_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_5_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_2_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_1_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_0_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_4_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_3_6417799359d4e2284884fd3ae2ec9735)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                               | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridPOEnv-v0_f9c59_00000 | RUNNING  | 192.168.1.65:17432 |     34 |          1981.59 | 136000 | -255.322 |             -177.121 |             -368.193 |                400 |\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridPOEnv-v0_f9c59_00000:\n",
      "  agent_timesteps_total: 140000\n",
      "  custom_metrics: {}\n",
      "  date: 2023-01-01_15-53-33\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -177.12121227875517\n",
      "  episode_reward_mean: -257.6132227115998\n",
      "  episode_reward_min: -396.43695900677665\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 350\n",
      "  experiment_id: 610b8b5226964181899719d25df7617e\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 7.450580596923828e-10\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 3.523770260065794\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.006518924221933298\n",
      "          policy_loss: 2.576643746579066e-05\n",
      "          total_loss: 622.0202500343323\n",
      "          vf_explained_var: 5.593337118625641e-05\n",
      "          vf_loss: 622.0202270030975\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 140000\n",
      "    num_steps_sampled: 140000\n",
      "    num_steps_trained: 140000\n",
      "  iterations_since_restore: 35\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.614634146341462\n",
      "    ram_util_percent: 27.82682926829269\n",
      "  pid: 17432\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0877359707640177\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.845444051012517\n",
      "    mean_inference_ms: 1.8160755335602388\n",
      "    mean_raw_obs_processing_ms: 6.741759347519165\n",
      "  time_since_restore: 2039.005544424057\n",
      "  time_this_iter_s: 57.41618847846985\n",
      "  time_total_s: 2039.005544424057\n",
      "  timers:\n",
      "    learn_throughput: 155.611\n",
      "    learn_time_ms: 25705.053\n",
      "    sample_throughput: 122.684\n",
      "    sample_time_ms: 32604.028\n",
      "    update_time_ms: 3.15\n",
      "  timestamp: 1672610013\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 140000\n",
      "  training_iteration: 35\n",
      "  trial_id: f9c59_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 8.7/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.87 GiB heap, 0.0/14.65 GiB objects (0.0/6.0 CPU_group_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_0_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_1_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_5_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_4_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_3_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_2_6417799359d4e2284884fd3ae2ec9735)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                               | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridPOEnv-v0_f9c59_00000 | RUNNING  | 192.168.1.65:17432 |     35 |          2039.01 | 140000 | -257.613 |             -177.121 |             -396.437 |                400 |\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridPOEnv-v0_f9c59_00000:\n",
      "  agent_timesteps_total: 144000\n",
      "  custom_metrics: {}\n",
      "  date: 2023-01-01_15-54-31\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -183.70846913146787\n",
      "  episode_reward_mean: -258.19697955605943\n",
      "  episode_reward_min: -396.43695900677665\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 360\n",
      "  experiment_id: 610b8b5226964181899719d25df7617e\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 3.725290298461914e-10\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 3.380682700872421\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014614287420926094\n",
      "          policy_loss: -0.0041381287010153756\n",
      "          total_loss: 553.7711462020874\n",
      "          vf_explained_var: 0.00013264865265227854\n",
      "          vf_loss: 553.7752837657929\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 144000\n",
      "    num_steps_sampled: 144000\n",
      "    num_steps_trained: 144000\n",
      "  iterations_since_restore: 36\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.50602409638554\n",
      "    ram_util_percent: 27.722891566265073\n",
      "  pid: 17432\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08776304091076481\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.838361450904568\n",
      "    mean_inference_ms: 1.8164570433073224\n",
      "    mean_raw_obs_processing_ms: 6.747244104383972\n",
      "  time_since_restore: 2096.935721874237\n",
      "  time_this_iter_s: 57.930177450180054\n",
      "  time_total_s: 2096.935721874237\n",
      "  timers:\n",
      "    learn_throughput: 156.52\n",
      "    learn_time_ms: 25555.902\n",
      "    sample_throughput: 122.567\n",
      "    sample_time_ms: 32635.257\n",
      "    update_time_ms: 3.185\n",
      "  timestamp: 1672610071\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 144000\n",
      "  training_iteration: 36\n",
      "  trial_id: f9c59_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Memory usage on this node: 8.7/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.87 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_4_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_0_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_1_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_3_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_5_6417799359d4e2284884fd3ae2ec9735, 0.0/6.0 CPU_group_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_2_6417799359d4e2284884fd3ae2ec9735)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                               | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridPOEnv-v0_f9c59_00000 | RUNNING  | 192.168.1.65:17432 |     36 |          2096.94 | 144000 | -258.197 |             -183.708 |             -396.437 |                400 |\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridPOEnv-v0_f9c59_00000:\n",
      "  agent_timesteps_total: 148000\n",
      "  custom_metrics: {}\n",
      "  date: 2023-01-01_15-55-29\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -183.70846913146787\n",
      "  episode_reward_mean: -261.51393812691407\n",
      "  episode_reward_min: -401.72443621338334\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 370\n",
      "  experiment_id: 610b8b5226964181899719d25df7617e\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 3.725290298461914e-10\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 3.320097980648279\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01297721614616878\n",
      "          policy_loss: -0.0022940995084354656\n",
      "          total_loss: 638.0353530406952\n",
      "          vf_explained_var: 4.417113814270124e-05\n",
      "          vf_loss: 638.0376451969147\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 148000\n",
      "    num_steps_sampled: 148000\n",
      "    num_steps_trained: 148000\n",
      "  iterations_since_restore: 37\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.781707317073167\n",
      "    ram_util_percent: 27.835365853658537\n",
      "  pid: 17432\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08779478547529063\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.82992719112097\n",
      "    mean_inference_ms: 1.8170803532957152\n",
      "    mean_raw_obs_processing_ms: 6.7531319961233995\n",
      "  time_since_restore: 2154.685559988022\n",
      "  time_this_iter_s: 57.74983811378479\n",
      "  time_total_s: 2154.685559988022\n",
      "  timers:\n",
      "    learn_throughput: 156.54\n",
      "    learn_time_ms: 25552.634\n",
      "    sample_throughput: 122.701\n",
      "    sample_time_ms: 32599.486\n",
      "    update_time_ms: 3.188\n",
      "  timestamp: 1672610129\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 148000\n",
      "  training_iteration: 37\n",
      "  trial_id: f9c59_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 8.7/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.87 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_2_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_3_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_1_6417799359d4e2284884fd3ae2ec9735, 0.0/6.0 CPU_group_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_0_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_4_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_5_6417799359d4e2284884fd3ae2ec9735)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                               | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridPOEnv-v0_f9c59_00000 | RUNNING  | 192.168.1.65:17432 |     37 |          2154.69 | 148000 | -261.514 |             -183.708 |             -401.724 |                400 |\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridPOEnv-v0_f9c59_00000:\n",
      "  agent_timesteps_total: 152000\n",
      "  custom_metrics: {}\n",
      "  date: 2023-01-01_15-56-27\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -183.70846913146787\n",
      "  episode_reward_mean: -263.0334361930578\n",
      "  episode_reward_min: -401.72443621338334\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 380\n",
      "  experiment_id: 610b8b5226964181899719d25df7617e\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 3.725290298461914e-10\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 3.220615720003843\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011915268855493402\n",
      "          policy_loss: -0.001343984284903854\n",
      "          total_loss: 542.2225708961487\n",
      "          vf_explained_var: 0.00010911617573583499\n",
      "          vf_loss: 542.2239134788513\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 152000\n",
      "    num_steps_sampled: 152000\n",
      "    num_steps_trained: 152000\n",
      "  iterations_since_restore: 38\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.574698795180723\n",
      "    ram_util_percent: 27.87590361445783\n",
      "  pid: 17432\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0878208160056556\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.822709412852056\n",
      "    mean_inference_ms: 1.8175881995216798\n",
      "    mean_raw_obs_processing_ms: 6.759517272350276\n",
      "  time_since_restore: 2212.5411636829376\n",
      "  time_this_iter_s: 57.85560369491577\n",
      "  time_total_s: 2212.5411636829376\n",
      "  timers:\n",
      "    learn_throughput: 156.386\n",
      "    learn_time_ms: 25577.666\n",
      "    sample_throughput: 122.59\n",
      "    sample_time_ms: 32629.037\n",
      "    update_time_ms: 3.236\n",
      "  timestamp: 1672610187\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 152000\n",
      "  training_iteration: 38\n",
      "  trial_id: f9c59_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 8.7/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.87 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_0_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_1_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_2_6417799359d4e2284884fd3ae2ec9735, 0.0/6.0 CPU_group_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_5_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_3_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_4_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 accelerator_type:G)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                               | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridPOEnv-v0_f9c59_00000 | RUNNING  | 192.168.1.65:17432 |     38 |          2212.54 | 152000 | -263.033 |             -183.708 |             -401.724 |                400 |\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_TrafficLightGridPOEnv-v0_f9c59_00000:\n",
      "  agent_timesteps_total: 156000\n",
      "  custom_metrics: {}\n",
      "  date: 2023-01-01_15-57-26\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -183.70846913146787\n",
      "  episode_reward_mean: -265.37664690597995\n",
      "  episode_reward_min: -401.72443621338334\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 390\n",
      "  experiment_id: 610b8b5226964181899719d25df7617e\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 3.725290298461914e-10\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 3.2057579167187216\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.006832275844359986\n",
      "          policy_loss: -0.003102834481978789\n",
      "          total_loss: 708.8968565940856\n",
      "          vf_explained_var: 3.7994981539668515e-05\n",
      "          vf_loss: 708.8999628067016\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 156000\n",
      "    num_steps_sampled: 156000\n",
      "    num_steps_trained: 156000\n",
      "  iterations_since_restore: 39\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.46867469879518\n",
      "    ram_util_percent: 27.95542168674699\n",
      "  pid: 17432\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.087850337439352\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.81776071437649\n",
      "    mean_inference_ms: 1.8181629456788533\n",
      "    mean_raw_obs_processing_ms: 6.740303893656499\n",
      "  time_since_restore: 2270.8199355602264\n",
      "  time_this_iter_s: 58.27877187728882\n",
      "  time_total_s: 2270.8199355602264\n",
      "  timers:\n",
      "    learn_throughput: 155.978\n",
      "    learn_time_ms: 25644.62\n",
      "    sample_throughput: 124.654\n",
      "    sample_time_ms: 32088.907\n",
      "    update_time_ms: 3.258\n",
      "  timestamp: 1672610246\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 156000\n",
      "  training_iteration: 39\n",
      "  trial_id: f9c59_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 8.7/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.87 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_4_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_2_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_3_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_0_6417799359d4e2284884fd3ae2ec9735, 0.0/6.0 CPU_group_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_5_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_1_6417799359d4e2284884fd3ae2ec9735)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                               | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridPOEnv-v0_f9c59_00000 | RUNNING  | 192.168.1.65:17432 |     39 |          2270.82 | 156000 | -265.377 |             -183.708 |             -401.724 |                400 |\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=17433)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=17430)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=17429)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=17428)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=17431)\u001b[0m  Retrying in 1 seconds\n",
      "Result for PPO_TrafficLightGridPOEnv-v0_f9c59_00000:\n",
      "  agent_timesteps_total: 160000\n",
      "  custom_metrics: {}\n",
      "  date: 2023-01-01_15-58-24\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -183.70846913146787\n",
      "  episode_reward_mean: -269.42982224076803\n",
      "  episode_reward_min: -401.72443621338334\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 400\n",
      "  experiment_id: 610b8b5226964181899719d25df7617e\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.862645149230957e-10\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 3.184425416588783\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.007915813352623525\n",
      "          policy_loss: -0.0015419202551129273\n",
      "          total_loss: 635.5793821811676\n",
      "          vf_explained_var: 1.4177709999785293e-05\n",
      "          vf_loss: 635.5809178829193\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 160000\n",
      "    num_steps_sampled: 160000\n",
      "    num_steps_trained: 160000\n",
      "  iterations_since_restore: 40\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.312048192771087\n",
      "    ram_util_percent: 27.945783132530114\n",
      "  pid: 17432\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08787929730891268\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.813280672462444\n",
      "    mean_inference_ms: 1.818816913792246\n",
      "    mean_raw_obs_processing_ms: 6.725338799390008\n",
      "  time_since_restore: 2329.161231994629\n",
      "  time_this_iter_s: 58.341296434402466\n",
      "  time_total_s: 2329.161231994629\n",
      "  timers:\n",
      "    learn_throughput: 156.126\n",
      "    learn_time_ms: 25620.29\n",
      "    sample_throughput: 124.251\n",
      "    sample_time_ms: 32192.976\n",
      "    update_time_ms: 3.234\n",
      "  timestamp: 1672610304\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 160000\n",
      "  training_iteration: 40\n",
      "  trial_id: f9c59_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 8.7/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.87 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_5_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_1_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_3_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_4_6417799359d4e2284884fd3ae2ec9735, 0.0/6.0 CPU_group_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_2_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_0_6417799359d4e2284884fd3ae2ec9735)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                               | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridPOEnv-v0_f9c59_00000 | RUNNING  | 192.168.1.65:17432 |     40 |          2329.16 | 160000 |  -269.43 |             -183.708 |             -401.724 |                400 |\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=17433)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=17429)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=17428)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=17431)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=17430)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=17433)\u001b[0m Could not connect to TraCI server at localhost:33079 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=17433)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=17429)\u001b[0m Could not connect to TraCI server at localhost:41647 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=17429)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=17428)\u001b[0m Could not connect to TraCI server at localhost:47505 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=17428)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=17431)\u001b[0m Could not connect to TraCI server at localhost:42343 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=17431)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=17430)\u001b[0m Could not connect to TraCI server at localhost:33057 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=17430)\u001b[0m  Retrying in 2 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=17433)\u001b[0m Could not connect to TraCI server at localhost:33079 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=17433)\u001b[0m  Retrying in 3 seconds\n",
      "\u001b[2m\u001b[36m(pid=17431)\u001b[0m Could not connect to TraCI server at localhost:42343 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=17431)\u001b[0m  Retrying in 3 seconds\n",
      "\u001b[2m\u001b[36m(pid=17430)\u001b[0m Could not connect to TraCI server at localhost:33057 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=17430)\u001b[0m  Retrying in 3 seconds\n",
      "\u001b[2m\u001b[36m(pid=17429)\u001b[0m Could not connect to TraCI server at localhost:41647 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=17429)\u001b[0m  Retrying in 3 seconds\n",
      "\u001b[2m\u001b[36m(pid=17428)\u001b[0m Could not connect to TraCI server at localhost:47505 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=17428)\u001b[0m  Retrying in 3 seconds\n",
      "Result for PPO_TrafficLightGridPOEnv-v0_f9c59_00000:\n",
      "  agent_timesteps_total: 164000\n",
      "  custom_metrics: {}\n",
      "  date: 2023-01-01_15-59-28\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -175.97858059569566\n",
      "  episode_reward_mean: -271.9878984450289\n",
      "  episode_reward_min: -403.78791890242206\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 410\n",
      "  experiment_id: 610b8b5226964181899719d25df7617e\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 9.313225746154784e-11\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 3.1621433712542055\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008460254574151834\n",
      "          policy_loss: -0.002349111024523154\n",
      "          total_loss: 645.7953674316407\n",
      "          vf_explained_var: 1.8859655028791167e-05\n",
      "          vf_loss: 645.7977172374725\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 164000\n",
      "    num_steps_sampled: 164000\n",
      "    num_steps_trained: 164000\n",
      "  iterations_since_restore: 41\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.920652173913048\n",
      "    ram_util_percent: 28.011956521739126\n",
      "  pid: 17432\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08790909027202173\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.809229529507174\n",
      "    mean_inference_ms: 1.8196158024231477\n",
      "    mean_raw_obs_processing_ms: 6.729938512418394\n",
      "  time_since_restore: 2393.5116443634033\n",
      "  time_this_iter_s: 64.35041236877441\n",
      "  time_total_s: 2393.5116443634033\n",
      "  timers:\n",
      "    learn_throughput: 155.646\n",
      "    learn_time_ms: 25699.415\n",
      "    sample_throughput: 121.888\n",
      "    sample_time_ms: 32816.993\n",
      "    update_time_ms: 3.194\n",
      "  timestamp: 1672610368\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 164000\n",
      "  training_iteration: 41\n",
      "  trial_id: f9c59_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 8.7/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.87 GiB heap, 0.0/14.65 GiB objects (0.0/6.0 CPU_group_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_5_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_1_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_4_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_0_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_3_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_2_6417799359d4e2284884fd3ae2ec9735)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                               | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridPOEnv-v0_f9c59_00000 | RUNNING  | 192.168.1.65:17432 |     41 |          2393.51 | 164000 | -271.988 |             -175.979 |             -403.788 |                400 |\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridPOEnv-v0_f9c59_00000:\n",
      "  agent_timesteps_total: 168000\n",
      "  custom_metrics: {}\n",
      "  date: 2023-01-01_16-00-27\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -175.97858059569566\n",
      "  episode_reward_mean: -275.3160076563491\n",
      "  episode_reward_min: -403.78791890242206\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 420\n",
      "  experiment_id: 610b8b5226964181899719d25df7617e\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 4.656612873077392e-11\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 3.0668085128068925\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0075390143916195225\n",
      "          policy_loss: 0.0025940121937310322\n",
      "          total_loss: 622.0812945365906\n",
      "          vf_explained_var: 4.73117470392026e-05\n",
      "          vf_loss: 622.0786996364593\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 168000\n",
      "    num_steps_sampled: 168000\n",
      "    num_steps_trained: 168000\n",
      "  iterations_since_restore: 42\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.307228915662648\n",
      "    ram_util_percent: 27.971084337349396\n",
      "  pid: 17432\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08793398800499848\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.805460573925984\n",
      "    mean_inference_ms: 1.8204135679932716\n",
      "    mean_raw_obs_processing_ms: 6.732665447308165\n",
      "  time_since_restore: 2451.64080286026\n",
      "  time_this_iter_s: 58.12915849685669\n",
      "  time_total_s: 2451.64080286026\n",
      "  timers:\n",
      "    learn_throughput: 155.216\n",
      "    learn_time_ms: 25770.599\n",
      "    sample_throughput: 122.29\n",
      "    sample_time_ms: 32709.163\n",
      "    update_time_ms: 3.198\n",
      "  timestamp: 1672610427\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 168000\n",
      "  training_iteration: 42\n",
      "  trial_id: f9c59_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 8.7/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.87 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_4_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_5_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_3_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_0_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_1_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_2_6417799359d4e2284884fd3ae2ec9735, 0.0/6.0 CPU_group_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 accelerator_type:G)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                               | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridPOEnv-v0_f9c59_00000 | RUNNING  | 192.168.1.65:17432 |     42 |          2451.64 | 168000 | -275.316 |             -175.979 |             -403.788 |                400 |\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridPOEnv-v0_f9c59_00000:\n",
      "  agent_timesteps_total: 172000\n",
      "  custom_metrics: {}\n",
      "  date: 2023-01-01_16-01-25\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -175.97858059569566\n",
      "  episode_reward_mean: -279.4745948804251\n",
      "  episode_reward_min: -403.78791890242206\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 430\n",
      "  experiment_id: 610b8b5226964181899719d25df7617e\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.328306436538696e-11\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 3.046734544634819\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014024058198441533\n",
      "          policy_loss: -0.002808372466824949\n",
      "          total_loss: 582.2401630878448\n",
      "          vf_explained_var: 0.00019527450785972178\n",
      "          vf_loss: 582.2429725170135\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 172000\n",
      "    num_steps_sampled: 172000\n",
      "    num_steps_trained: 172000\n",
      "  iterations_since_restore: 43\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.618072289156622\n",
      "    ram_util_percent: 28.145783132530113\n",
      "  pid: 17432\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08795620921503616\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.8036451880211\n",
      "    mean_inference_ms: 1.8212069014204386\n",
      "    mean_raw_obs_processing_ms: 6.736191146297738\n",
      "  time_since_restore: 2509.5943562984467\n",
      "  time_this_iter_s: 57.953553438186646\n",
      "  time_total_s: 2509.5943562984467\n",
      "  timers:\n",
      "    learn_throughput: 155.036\n",
      "    learn_time_ms: 25800.473\n",
      "    sample_throughput: 122.038\n",
      "    sample_time_ms: 32776.8\n",
      "    update_time_ms: 3.25\n",
      "  timestamp: 1672610485\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 172000\n",
      "  training_iteration: 43\n",
      "  trial_id: f9c59_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Memory usage on this node: 8.8/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.87 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_4_6417799359d4e2284884fd3ae2ec9735, 0.0/6.0 CPU_group_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_1_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_5_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_2_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_0_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_3_6417799359d4e2284884fd3ae2ec9735)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                               | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridPOEnv-v0_f9c59_00000 | RUNNING  | 192.168.1.65:17432 |     43 |          2509.59 | 172000 | -279.475 |             -175.979 |             -403.788 |                400 |\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridPOEnv-v0_f9c59_00000:\n",
      "  agent_timesteps_total: 176000\n",
      "  custom_metrics: {}\n",
      "  date: 2023-01-01_16-02-23\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -175.97858059569566\n",
      "  episode_reward_mean: -275.422532649939\n",
      "  episode_reward_min: -403.78791890242206\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 440\n",
      "  experiment_id: 610b8b5226964181899719d25df7617e\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.328306436538696e-11\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 3.0369591720402243\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009644531428801914\n",
      "          policy_loss: -0.004751642161863856\n",
      "          total_loss: 893.7342066764832\n",
      "          vf_explained_var: 8.383765930375375e-07\n",
      "          vf_loss: 893.7389605522155\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 176000\n",
      "    num_steps_sampled: 176000\n",
      "    num_steps_trained: 176000\n",
      "  iterations_since_restore: 44\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.18658536585366\n",
      "    ram_util_percent: 28.10243902439024\n",
      "  pid: 17432\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08798143005488099\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.802920272314992\n",
      "    mean_inference_ms: 1.8220095652581785\n",
      "    mean_raw_obs_processing_ms: 6.7401112749899585\n",
      "  time_since_restore: 2567.487813949585\n",
      "  time_this_iter_s: 57.893457651138306\n",
      "  time_total_s: 2567.487813949585\n",
      "  timers:\n",
      "    learn_throughput: 155.248\n",
      "    learn_time_ms: 25765.237\n",
      "    sample_throughput: 121.913\n",
      "    sample_time_ms: 32810.403\n",
      "    update_time_ms: 3.219\n",
      "  timestamp: 1672610543\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 176000\n",
      "  training_iteration: 44\n",
      "  trial_id: f9c59_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 8.7/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.87 GiB heap, 0.0/14.65 GiB objects (0.0/6.0 CPU_group_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_3_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_5_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_0_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_1_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_4_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_2_6417799359d4e2284884fd3ae2ec9735)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                               | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridPOEnv-v0_f9c59_00000 | RUNNING  | 192.168.1.65:17432 |     44 |          2567.49 | 176000 | -275.423 |             -175.979 |             -403.788 |                400 |\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridPOEnv-v0_f9c59_00000:\n",
      "  agent_timesteps_total: 180000\n",
      "  custom_metrics: {}\n",
      "  date: 2023-01-01_16-03-21\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -175.97858059569566\n",
      "  episode_reward_mean: -274.4802664981509\n",
      "  episode_reward_min: -403.78791890242206\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 450\n",
      "  experiment_id: 610b8b5226964181899719d25df7617e\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.164153218269348e-11\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 3.0131812311708925\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.007579476568810179\n",
      "          policy_loss: -0.002934188769722823\n",
      "          total_loss: 688.2016789436341\n",
      "          vf_explained_var: 4.999023076379672e-05\n",
      "          vf_loss: 688.2046140670776\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 180000\n",
      "    num_steps_sampled: 180000\n",
      "    num_steps_trained: 180000\n",
      "  iterations_since_restore: 45\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.39404761904762\n",
      "    ram_util_percent: 28.076190476190472\n",
      "  pid: 17432\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08800834063462797\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.803461162192697\n",
      "    mean_inference_ms: 1.822766864951721\n",
      "    mean_raw_obs_processing_ms: 6.7444926658692985\n",
      "  time_since_restore: 2625.8658316135406\n",
      "  time_this_iter_s: 58.37801766395569\n",
      "  time_total_s: 2625.8658316135406\n",
      "  timers:\n",
      "    learn_throughput: 154.957\n",
      "    learn_time_ms: 25813.547\n",
      "    sample_throughput: 121.735\n",
      "    sample_time_ms: 32858.283\n",
      "    update_time_ms: 3.25\n",
      "  timestamp: 1672610601\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 180000\n",
      "  training_iteration: 45\n",
      "  trial_id: f9c59_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 8.7/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.87 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_3_6417799359d4e2284884fd3ae2ec9735, 0.0/6.0 CPU_group_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_4_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_0_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_5_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_1_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_2_6417799359d4e2284884fd3ae2ec9735)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                               | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridPOEnv-v0_f9c59_00000 | RUNNING  | 192.168.1.65:17432 |     45 |          2625.87 | 180000 |  -274.48 |             -175.979 |             -403.788 |                400 |\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_TrafficLightGridPOEnv-v0_f9c59_00000:\n",
      "  agent_timesteps_total: 184000\n",
      "  custom_metrics: {}\n",
      "  date: 2023-01-01_16-04-19\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -154.56079734639783\n",
      "  episode_reward_mean: -270.37901575653837\n",
      "  episode_reward_min: -403.78791890242206\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 460\n",
      "  experiment_id: 610b8b5226964181899719d25df7617e\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 5.82076609134674e-12\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.956467352807522\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.007891022916346956\n",
      "          policy_loss: -0.00044869345147162675\n",
      "          total_loss: 874.0609501838684\n",
      "          vf_explained_var: 2.7414412215875927e-06\n",
      "          vf_loss: 874.0614025115967\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 184000\n",
      "    num_steps_sampled: 184000\n",
      "    num_steps_trained: 184000\n",
      "  iterations_since_restore: 46\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.866265060240966\n",
      "    ram_util_percent: 28.186746987951803\n",
      "  pid: 17432\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08803668067049386\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.803596492751616\n",
      "    mean_inference_ms: 1.8236685981575829\n",
      "    mean_raw_obs_processing_ms: 6.748984575090584\n",
      "  time_since_restore: 2683.738356590271\n",
      "  time_this_iter_s: 57.87252497673035\n",
      "  time_total_s: 2683.738356590271\n",
      "  timers:\n",
      "    learn_throughput: 154.905\n",
      "    learn_time_ms: 25822.224\n",
      "    sample_throughput: 121.789\n",
      "    sample_time_ms: 32843.8\n",
      "    update_time_ms: 3.261\n",
      "  timestamp: 1672610659\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 184000\n",
      "  training_iteration: 46\n",
      "  trial_id: f9c59_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 8.8/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.87 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_1_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_2_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_3_6417799359d4e2284884fd3ae2ec9735, 0.0/6.0 CPU_group_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_4_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_5_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_0_6417799359d4e2284884fd3ae2ec9735)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                               | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridPOEnv-v0_f9c59_00000 | RUNNING  | 192.168.1.65:17432 |     46 |          2683.74 | 184000 | -270.379 |             -154.561 |             -403.788 |                400 |\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridPOEnv-v0_f9c59_00000:\n",
      "  agent_timesteps_total: 188000\n",
      "  custom_metrics: {}\n",
      "  date: 2023-01-01_16-05-17\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -154.56079734639783\n",
      "  episode_reward_mean: -262.2713991826061\n",
      "  episode_reward_min: -403.78791890242206\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 470\n",
      "  experiment_id: 610b8b5226964181899719d25df7617e\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.91038304567337e-12\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.8501496709883214\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011911635175260964\n",
      "          policy_loss: -0.002473237065714784\n",
      "          total_loss: 971.8195693492889\n",
      "          vf_explained_var: 6.582960395462578e-06\n",
      "          vf_loss: 971.822041463852\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 188000\n",
      "    num_steps_sampled: 188000\n",
      "    num_steps_trained: 188000\n",
      "  iterations_since_restore: 47\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.257317073170732\n",
      "    ram_util_percent: 28.148780487804874\n",
      "  pid: 17432\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08806648076976406\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.80498685848353\n",
      "    mean_inference_ms: 1.8246314188071373\n",
      "    mean_raw_obs_processing_ms: 6.753653520308704\n",
      "  time_since_restore: 2741.746638059616\n",
      "  time_this_iter_s: 58.00828146934509\n",
      "  time_total_s: 2741.746638059616\n",
      "  timers:\n",
      "    learn_throughput: 155.008\n",
      "    learn_time_ms: 25805.094\n",
      "    sample_throughput: 121.629\n",
      "    sample_time_ms: 32886.77\n",
      "    update_time_ms: 3.255\n",
      "  timestamp: 1672610717\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 188000\n",
      "  training_iteration: 47\n",
      "  trial_id: f9c59_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 8.8/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.87 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_2_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_1_6417799359d4e2284884fd3ae2ec9735, 0.0/6.0 CPU_group_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_5_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_3_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_4_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_0_6417799359d4e2284884fd3ae2ec9735)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                               | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridPOEnv-v0_f9c59_00000 | RUNNING  | 192.168.1.65:17432 |     47 |          2741.75 | 188000 | -262.271 |             -154.561 |             -403.788 |                400 |\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridPOEnv-v0_f9c59_00000:\n",
      "  agent_timesteps_total: 192000\n",
      "  custom_metrics: {}\n",
      "  date: 2023-01-01_16-06-16\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -154.56079734639783\n",
      "  episode_reward_mean: -257.75903681814043\n",
      "  episode_reward_min: -403.78791890242206\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 480\n",
      "  experiment_id: 610b8b5226964181899719d25df7617e\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.91038304567337e-12\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.713385137915611\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014034040728043401\n",
      "          policy_loss: -0.0026613814130541868\n",
      "          total_loss: 956.6782363891601\n",
      "          vf_explained_var: 4.50965035270201e-06\n",
      "          vf_loss: 956.6808912277222\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 192000\n",
      "    num_steps_sampled: 192000\n",
      "    num_steps_trained: 192000\n",
      "  iterations_since_restore: 48\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.465476190476195\n",
      "    ram_util_percent: 28.124999999999993\n",
      "  pid: 17432\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08809435378493817\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.80644413116386\n",
      "    mean_inference_ms: 1.825598538753605\n",
      "    mean_raw_obs_processing_ms: 6.75868072781509\n",
      "  time_since_restore: 2800.3935253620148\n",
      "  time_this_iter_s: 58.64688730239868\n",
      "  time_total_s: 2800.3935253620148\n",
      "  timers:\n",
      "    learn_throughput: 154.651\n",
      "    learn_time_ms: 25864.655\n",
      "    sample_throughput: 121.557\n",
      "    sample_time_ms: 32906.353\n",
      "    update_time_ms: 3.216\n",
      "  timestamp: 1672610776\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 192000\n",
      "  training_iteration: 48\n",
      "  trial_id: f9c59_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Memory usage on this node: 8.8/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.87 GiB heap, 0.0/14.65 GiB objects (0.0/6.0 CPU_group_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_1_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_3_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_0_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_4_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_5_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_2_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 accelerator_type:G)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                               | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridPOEnv-v0_f9c59_00000 | RUNNING  | 192.168.1.65:17432 |     48 |          2800.39 | 192000 | -257.759 |             -154.561 |             -403.788 |                400 |\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridPOEnv-v0_f9c59_00000:\n",
      "  agent_timesteps_total: 196000\n",
      "  custom_metrics: {}\n",
      "  date: 2023-01-01_16-07-14\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -154.56079734639783\n",
      "  episode_reward_mean: -253.93048072002443\n",
      "  episode_reward_min: -403.78791890242206\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 490\n",
      "  experiment_id: 610b8b5226964181899719d25df7617e\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.91038304567337e-12\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.7052537128329277\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008111131541352314\n",
      "          policy_loss: -0.000938692592899315\n",
      "          total_loss: 938.582137298584\n",
      "          vf_explained_var: -1.1119991540908813e-06\n",
      "          vf_loss: 938.5830766677857\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 196000\n",
      "    num_steps_sampled: 196000\n",
      "    num_steps_trained: 196000\n",
      "  iterations_since_restore: 49\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.657831325301203\n",
      "    ram_util_percent: 28.24578313253013\n",
      "  pid: 17432\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08812042596224007\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.807884678593336\n",
      "    mean_inference_ms: 1.8265254010614251\n",
      "    mean_raw_obs_processing_ms: 6.763936264484494\n",
      "  time_since_restore: 2858.2733924388885\n",
      "  time_this_iter_s: 57.87986707687378\n",
      "  time_total_s: 2858.2733924388885\n",
      "  timers:\n",
      "    learn_throughput: 154.902\n",
      "    learn_time_ms: 25822.734\n",
      "    sample_throughput: 121.55\n",
      "    sample_time_ms: 32908.382\n",
      "    update_time_ms: 3.185\n",
      "  timestamp: 1672610834\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 196000\n",
      "  training_iteration: 49\n",
      "  trial_id: f9c59_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 8.8/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.87 GiB heap, 0.0/14.65 GiB objects (0.0/6.0 CPU_group_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_0_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_3_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_4_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_1_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_5_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_2_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 accelerator_type:G)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                               | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridPOEnv-v0_f9c59_00000 | RUNNING  | 192.168.1.65:17432 |     49 |          2858.27 | 196000 |  -253.93 |             -154.561 |             -403.788 |                400 |\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridPOEnv-v0_f9c59_00000:\n",
      "  agent_timesteps_total: 200000\n",
      "  custom_metrics: {}\n",
      "  date: 2023-01-01_16-08-12\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -154.56079734639783\n",
      "  episode_reward_mean: -248.79665155144303\n",
      "  episode_reward_min: -403.78791890242206\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 500\n",
      "  experiment_id: 610b8b5226964181899719d25df7617e\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.455191522836685e-12\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.594199974834919\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0103612318219462\n",
      "          policy_loss: -0.0024425547628197817\n",
      "          total_loss: 928.4619448184967\n",
      "          vf_explained_var: 8.749402695684694e-06\n",
      "          vf_loss: 928.4643889427185\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 200000\n",
      "    num_steps_sampled: 200000\n",
      "    num_steps_trained: 200000\n",
      "  iterations_since_restore: 50\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.721686746987952\n",
      "    ram_util_percent: 28.168674698795176\n",
      "  pid: 17432\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0881476819924904\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.810936901930642\n",
      "    mean_inference_ms: 1.8274341083065344\n",
      "    mean_raw_obs_processing_ms: 6.766275562412725\n",
      "  time_since_restore: 2916.756172657013\n",
      "  time_this_iter_s: 58.48278021812439\n",
      "  time_total_s: 2916.756172657013\n",
      "  timers:\n",
      "    learn_throughput: 154.561\n",
      "    learn_time_ms: 25879.807\n",
      "    sample_throughput: 121.708\n",
      "    sample_time_ms: 32865.464\n",
      "    update_time_ms: 3.195\n",
      "  timestamp: 1672610892\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 200000\n",
      "  training_iteration: 50\n",
      "  trial_id: f9c59_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 8.8/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.87 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 accelerator_type:G, 0.0/6.0 CPU_group_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_1_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_0_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_2_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_3_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_4_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_5_6417799359d4e2284884fd3ae2ec9735)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                               | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridPOEnv-v0_f9c59_00000 | RUNNING  | 192.168.1.65:17432 |     50 |          2916.76 | 200000 | -248.797 |             -154.561 |             -403.788 |                400 |\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_TrafficLightGridPOEnv-v0_f9c59_00000:\n",
      "  agent_timesteps_total: 204000\n",
      "  custom_metrics: {}\n",
      "  date: 2023-01-01_16-09-09\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -154.56079734639783\n",
      "  episode_reward_mean: -242.60952915337634\n",
      "  episode_reward_min: -368.9112194193273\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 510\n",
      "  experiment_id: 610b8b5226964181899719d25df7617e\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.455191522836685e-12\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.5475881710648536\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015387329951053685\n",
      "          policy_loss: -0.0039431741141015665\n",
      "          total_loss: 782.1445920467377\n",
      "          vf_explained_var: 7.450580707946131e-10\n",
      "          vf_loss: 782.1485316276551\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 204000\n",
      "    num_steps_sampled: 204000\n",
      "    num_steps_trained: 204000\n",
      "  iterations_since_restore: 51\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.804878048780488\n",
      "    ram_util_percent: 28.09878048780487\n",
      "  pid: 17432\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08817089013661268\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.814077432992605\n",
      "    mean_inference_ms: 1.8282504947465006\n",
      "    mean_raw_obs_processing_ms: 6.750492581710689\n",
      "  time_since_restore: 2973.73583483696\n",
      "  time_this_iter_s: 56.9796621799469\n",
      "  time_total_s: 2973.73583483696\n",
      "  timers:\n",
      "    learn_throughput: 155.229\n",
      "    learn_time_ms: 25768.442\n",
      "    sample_throughput: 124.071\n",
      "    sample_time_ms: 32239.675\n",
      "    update_time_ms: 3.199\n",
      "  timestamp: 1672610949\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 204000\n",
      "  training_iteration: 51\n",
      "  trial_id: f9c59_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 8.7/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.87 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_4_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_2_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_3_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_0_6417799359d4e2284884fd3ae2ec9735, 0.0/6.0 CPU_group_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_5_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_1_6417799359d4e2284884fd3ae2ec9735)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                               | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridPOEnv-v0_f9c59_00000 | RUNNING  | 192.168.1.65:17432 |     51 |          2973.74 | 204000 |  -242.61 |             -154.561 |             -368.911 |                400 |\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridPOEnv-v0_f9c59_00000:\n",
      "  agent_timesteps_total: 208000\n",
      "  custom_metrics: {}\n",
      "  date: 2023-01-01_16-10-07\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -154.56079734639783\n",
      "  episode_reward_mean: -237.34676672457928\n",
      "  episode_reward_min: -369.1011661096841\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 520\n",
      "  experiment_id: 610b8b5226964181899719d25df7617e\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.455191522836685e-12\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.4670823134481905\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.007986882096602732\n",
      "          policy_loss: 0.0003525092761265114\n",
      "          total_loss: 794.5040884494781\n",
      "          vf_explained_var: -6.591901069441519e-07\n",
      "          vf_loss: 794.5037341117859\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 208000\n",
      "    num_steps_sampled: 208000\n",
      "    num_steps_trained: 208000\n",
      "  iterations_since_restore: 52\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.604819277108433\n",
      "    ram_util_percent: 28.192771084337355\n",
      "  pid: 17432\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08819809085416377\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.817373104615925\n",
      "    mean_inference_ms: 1.8292659665437248\n",
      "    mean_raw_obs_processing_ms: 6.735584424944247\n",
      "  time_since_restore: 3031.893748998642\n",
      "  time_this_iter_s: 58.15791416168213\n",
      "  time_total_s: 3031.893748998642\n",
      "  timers:\n",
      "    learn_throughput: 155.32\n",
      "    learn_time_ms: 25753.363\n",
      "    sample_throughput: 124.002\n",
      "    sample_time_ms: 32257.592\n",
      "    update_time_ms: 3.2\n",
      "  timestamp: 1672611007\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 208000\n",
      "  training_iteration: 52\n",
      "  trial_id: f9c59_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 8.8/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.87 GiB heap, 0.0/14.65 GiB objects (0.0/6.0 CPU_group_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_0_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_5_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_1_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_2_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_3_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_4_6417799359d4e2284884fd3ae2ec9735)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                               | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridPOEnv-v0_f9c59_00000 | RUNNING  | 192.168.1.65:17432 |     52 |          3031.89 | 208000 | -237.347 |             -154.561 |             -369.101 |                400 |\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridPOEnv-v0_f9c59_00000:\n",
      "  agent_timesteps_total: 212000\n",
      "  custom_metrics: {}\n",
      "  date: 2023-01-01_16-11-05\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -154.56079734639783\n",
      "  episode_reward_mean: -231.08707387756226\n",
      "  episode_reward_min: -430.74791281871165\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 530\n",
      "  experiment_id: 610b8b5226964181899719d25df7617e\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 7.275957614183425e-13\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.4078438013792036\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01108758711900468\n",
      "          policy_loss: -0.002115897584008053\n",
      "          total_loss: 898.4203727722168\n",
      "          vf_explained_var: 1.019984438244137e-06\n",
      "          vf_loss: 898.4224907875061\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 212000\n",
      "    num_steps_sampled: 212000\n",
      "    num_steps_trained: 212000\n",
      "  iterations_since_restore: 53\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.691358024691354\n",
      "    ram_util_percent: 28.17037037037037\n",
      "  pid: 17432\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08822447495592918\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.81904046071765\n",
      "    mean_inference_ms: 1.8301846328462181\n",
      "    mean_raw_obs_processing_ms: 6.721141219147605\n",
      "  time_since_restore: 3089.1097707748413\n",
      "  time_this_iter_s: 57.21602177619934\n",
      "  time_total_s: 3089.1097707748413\n",
      "  timers:\n",
      "    learn_throughput: 155.333\n",
      "    learn_time_ms: 25751.119\n",
      "    sample_throughput: 124.277\n",
      "    sample_time_ms: 32186.107\n",
      "    update_time_ms: 3.169\n",
      "  timestamp: 1672611065\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 212000\n",
      "  training_iteration: 53\n",
      "  trial_id: f9c59_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Memory usage on this node: 8.8/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.87 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_4_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 accelerator_type:G, 0.0/6.0 CPU_group_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_2_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_5_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_0_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_1_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_3_6417799359d4e2284884fd3ae2ec9735)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                               | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridPOEnv-v0_f9c59_00000 | RUNNING  | 192.168.1.65:17432 |     53 |          3089.11 | 212000 | -231.087 |             -154.561 |             -430.748 |                400 |\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridPOEnv-v0_f9c59_00000:\n",
      "  agent_timesteps_total: 216000\n",
      "  custom_metrics: {}\n",
      "  date: 2023-01-01_16-12-02\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -154.56079734639783\n",
      "  episode_reward_mean: -230.1370759942964\n",
      "  episode_reward_min: -430.74791281871165\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 540\n",
      "  experiment_id: 610b8b5226964181899719d25df7617e\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 7.275957614183425e-13\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.4439184881746767\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.007497924923467335\n",
      "          policy_loss: -0.0018084555180394092\n",
      "          total_loss: 767.843169593811\n",
      "          vf_explained_var: 5.630776286125183e-06\n",
      "          vf_loss: 767.8449688911438\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 216000\n",
      "    num_steps_sampled: 216000\n",
      "    num_steps_trained: 216000\n",
      "  iterations_since_restore: 54\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.426506024096387\n",
      "    ram_util_percent: 28.292771084337357\n",
      "  pid: 17432\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08824644431674092\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.818822053279582\n",
      "    mean_inference_ms: 1.8309625784465455\n",
      "    mean_raw_obs_processing_ms: 6.707190303392383\n",
      "  time_since_restore: 3146.7253551483154\n",
      "  time_this_iter_s: 57.61558437347412\n",
      "  time_total_s: 3146.7253551483154\n",
      "  timers:\n",
      "    learn_throughput: 155.011\n",
      "    learn_time_ms: 25804.645\n",
      "    sample_throughput: 124.592\n",
      "    sample_time_ms: 32104.819\n",
      "    update_time_ms: 3.207\n",
      "  timestamp: 1672611122\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 216000\n",
      "  training_iteration: 54\n",
      "  trial_id: f9c59_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 8.8/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.87 GiB heap, 0.0/14.65 GiB objects (0.0/6.0 CPU_group_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_0_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_5_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_2_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_4_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_3_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_1_6417799359d4e2284884fd3ae2ec9735)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                               | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridPOEnv-v0_f9c59_00000 | RUNNING  | 192.168.1.65:17432 |     54 |          3146.73 | 216000 | -230.137 |             -154.561 |             -430.748 |                400 |\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridPOEnv-v0_f9c59_00000:\n",
      "  agent_timesteps_total: 220000\n",
      "  custom_metrics: {}\n",
      "  date: 2023-01-01_16-13-00\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -154.56079734639783\n",
      "  episode_reward_mean: -223.63574982708883\n",
      "  episode_reward_min: -430.74791281871165\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 550\n",
      "  experiment_id: 610b8b5226964181899719d25df7617e\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 3.6379788070917126e-13\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.4077712185680866\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009625426962374234\n",
      "          policy_loss: -0.0022042744720238263\n",
      "          total_loss: 864.5595198631287\n",
      "          vf_explained_var: 2.777017698463169e-06\n",
      "          vf_loss: 864.5617285728455\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 220000\n",
      "    num_steps_sampled: 220000\n",
      "    num_steps_trained: 220000\n",
      "  iterations_since_restore: 55\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.0641975308642\n",
      "    ram_util_percent: 28.265432098765434\n",
      "  pid: 17432\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08826426507810274\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.817919286744658\n",
      "    mean_inference_ms: 1.831733950140183\n",
      "    mean_raw_obs_processing_ms: 6.693657603966094\n",
      "  time_since_restore: 3203.797084093094\n",
      "  time_this_iter_s: 57.07172894477844\n",
      "  time_total_s: 3203.797084093094\n",
      "  timers:\n",
      "    learn_throughput: 155.523\n",
      "    learn_time_ms: 25719.742\n",
      "    sample_throughput: 124.769\n",
      "    sample_time_ms: 32059.183\n",
      "    update_time_ms: 3.245\n",
      "  timestamp: 1672611180\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 220000\n",
      "  training_iteration: 55\n",
      "  trial_id: f9c59_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 8.8/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.87 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_2_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_1_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_0_6417799359d4e2284884fd3ae2ec9735, 0.0/6.0 CPU_group_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_5_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_4_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_3_6417799359d4e2284884fd3ae2ec9735)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                               | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridPOEnv-v0_f9c59_00000 | RUNNING  | 192.168.1.65:17432 |     55 |           3203.8 | 220000 | -223.636 |             -154.561 |             -430.748 |                400 |\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_TrafficLightGridPOEnv-v0_f9c59_00000:\n",
      "  agent_timesteps_total: 224000\n",
      "  custom_metrics: {}\n",
      "  date: 2023-01-01_16-13-58\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -189.09394423661467\n",
      "  episode_reward_mean: -223.74050637198204\n",
      "  episode_reward_min: -430.74791281871165\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 560\n",
      "  experiment_id: 610b8b5226964181899719d25df7617e\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.8189894035458563e-13\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.436829335987568\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010720638210000329\n",
      "          policy_loss: -0.001469000126235187\n",
      "          total_loss: 847.7179599761963\n",
      "          vf_explained_var: 8.169375178113114e-06\n",
      "          vf_loss: 847.7194261550903\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 224000\n",
      "    num_steps_sampled: 224000\n",
      "    num_steps_trained: 224000\n",
      "  iterations_since_restore: 56\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.580722891566264\n",
      "    ram_util_percent: 28.274698795180715\n",
      "  pid: 17432\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08828113995020397\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.816666906862174\n",
      "    mean_inference_ms: 1.8323959749348477\n",
      "    mean_raw_obs_processing_ms: 6.680618508986979\n",
      "  time_since_restore: 3261.657800912857\n",
      "  time_this_iter_s: 57.860716819763184\n",
      "  time_total_s: 3261.657800912857\n",
      "  timers:\n",
      "    learn_throughput: 155.441\n",
      "    learn_time_ms: 25733.257\n",
      "    sample_throughput: 124.826\n",
      "    sample_time_ms: 32044.5\n",
      "    update_time_ms: 3.203\n",
      "  timestamp: 1672611238\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 224000\n",
      "  training_iteration: 56\n",
      "  trial_id: f9c59_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 8.8/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.87 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_0_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_4_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_2_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_3_6417799359d4e2284884fd3ae2ec9735, 0.0/6.0 CPU_group_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_1_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_5_6417799359d4e2284884fd3ae2ec9735)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                               | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridPOEnv-v0_f9c59_00000 | RUNNING  | 192.168.1.65:17432 |     56 |          3261.66 | 224000 | -223.741 |             -189.094 |             -430.748 |                400 |\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridPOEnv-v0_f9c59_00000:\n",
      "  agent_timesteps_total: 228000\n",
      "  custom_metrics: {}\n",
      "  date: 2023-01-01_16-14-55\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -189.09394423661467\n",
      "  episode_reward_mean: -223.16458200061265\n",
      "  episode_reward_min: -430.74791281871165\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 570\n",
      "  experiment_id: 610b8b5226964181899719d25df7617e\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.8189894035458563e-13\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.343772379308939\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013600570146131561\n",
      "          policy_loss: -0.004161700842087157\n",
      "          total_loss: 797.1978288650513\n",
      "          vf_explained_var: -4.2486936990826507e-07\n",
      "          vf_loss: 797.2019928455353\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 228000\n",
      "    num_steps_sampled: 228000\n",
      "    num_steps_trained: 228000\n",
      "  iterations_since_restore: 57\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.706097560975607\n",
      "    ram_util_percent: 28.380487804878047\n",
      "  pid: 17432\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08829434799225769\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.81499265235614\n",
      "    mean_inference_ms: 1.832949240137577\n",
      "    mean_raw_obs_processing_ms: 6.668179520687006\n",
      "  time_since_restore: 3319.3942742347717\n",
      "  time_this_iter_s: 57.73647332191467\n",
      "  time_total_s: 3319.3942742347717\n",
      "  timers:\n",
      "    learn_throughput: 155.526\n",
      "    learn_time_ms: 25719.201\n",
      "    sample_throughput: 124.878\n",
      "    sample_time_ms: 32031.338\n",
      "    update_time_ms: 3.221\n",
      "  timestamp: 1672611295\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 228000\n",
      "  training_iteration: 57\n",
      "  trial_id: f9c59_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 8.9/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.87 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_1_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_2_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_3_6417799359d4e2284884fd3ae2ec9735, 0.0/6.0 CPU_group_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_5_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_4_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_0_6417799359d4e2284884fd3ae2ec9735)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                               | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridPOEnv-v0_f9c59_00000 | RUNNING  | 192.168.1.65:17432 |     57 |          3319.39 | 228000 | -223.165 |             -189.094 |             -430.748 |                400 |\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridPOEnv-v0_f9c59_00000:\n",
      "  agent_timesteps_total: 232000\n",
      "  custom_metrics: {}\n",
      "  date: 2023-01-01_16-15-54\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -189.09394423661467\n",
      "  episode_reward_mean: -221.58469804491145\n",
      "  episode_reward_min: -430.74791281871165\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 580\n",
      "  experiment_id: 610b8b5226964181899719d25df7617e\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.8189894035458563e-13\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.2326681800186634\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013280879282461956\n",
      "          policy_loss: -0.0034496460895752533\n",
      "          total_loss: 801.6224238872528\n",
      "          vf_explained_var: 2.975761844936642e-06\n",
      "          vf_loss: 801.6258758544922\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 232000\n",
      "    num_steps_sampled: 232000\n",
      "    num_steps_trained: 232000\n",
      "  iterations_since_restore: 58\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.476190476190474\n",
      "    ram_util_percent: 28.417857142857148\n",
      "  pid: 17432\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0883093742924462\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.8133177357053\n",
      "    mean_inference_ms: 1.8334841914260571\n",
      "    mean_raw_obs_processing_ms: 6.656227371167331\n",
      "  time_since_restore: 3377.5184490680695\n",
      "  time_this_iter_s: 58.12417483329773\n",
      "  time_total_s: 3377.5184490680695\n",
      "  timers:\n",
      "    learn_throughput: 155.831\n",
      "    learn_time_ms: 25668.899\n",
      "    sample_throughput: 124.886\n",
      "    sample_time_ms: 32029.336\n",
      "    update_time_ms: 3.28\n",
      "  timestamp: 1672611354\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 232000\n",
      "  training_iteration: 58\n",
      "  trial_id: f9c59_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Memory usage on this node: 8.8/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.87 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_5_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_3_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 accelerator_type:G, 0.0/6.0 CPU_group_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_0_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_1_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_4_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_2_6417799359d4e2284884fd3ae2ec9735)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                               | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridPOEnv-v0_f9c59_00000 | RUNNING  | 192.168.1.65:17432 |     58 |          3377.52 | 232000 | -221.585 |             -189.094 |             -430.748 |                400 |\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridPOEnv-v0_f9c59_00000:\n",
      "  agent_timesteps_total: 236000\n",
      "  custom_metrics: {}\n",
      "  date: 2023-01-01_16-16-51\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -189.09394423661467\n",
      "  episode_reward_mean: -221.8620546698719\n",
      "  episode_reward_min: -430.74791281871165\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 590\n",
      "  experiment_id: 610b8b5226964181899719d25df7617e\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.8189894035458563e-13\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.294401877373457\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01132809252771386\n",
      "          policy_loss: -0.003556671900150832\n",
      "          total_loss: 733.3882724285126\n",
      "          vf_explained_var: -5.029141991741426e-09\n",
      "          vf_loss: 733.3918300628662\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 236000\n",
      "    num_steps_sampled: 236000\n",
      "    num_steps_trained: 236000\n",
      "  iterations_since_restore: 59\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.608536585365858\n",
      "    ram_util_percent: 28.458536585365852\n",
      "  pid: 17432\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08832236790066904\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.810374490345612\n",
      "    mean_inference_ms: 1.833941495580297\n",
      "    mean_raw_obs_processing_ms: 6.644670350255537\n",
      "  time_since_restore: 3435.0670914649963\n",
      "  time_this_iter_s: 57.54864239692688\n",
      "  time_total_s: 3435.0670914649963\n",
      "  timers:\n",
      "    learn_throughput: 155.615\n",
      "    learn_time_ms: 25704.43\n",
      "    sample_throughput: 125.154\n",
      "    sample_time_ms: 31960.709\n",
      "    update_time_ms: 3.288\n",
      "  timestamp: 1672611411\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 236000\n",
      "  training_iteration: 59\n",
      "  trial_id: f9c59_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 8.9/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.87 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_5_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_3_6417799359d4e2284884fd3ae2ec9735, 0.0/6.0 CPU_group_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_2_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_1_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_4_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_0_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 accelerator_type:G)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                               | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridPOEnv-v0_f9c59_00000 | RUNNING  | 192.168.1.65:17432 |     59 |          3435.07 | 236000 | -221.862 |             -189.094 |             -430.748 |                400 |\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=17433)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=17431)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=17430)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=17429)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=17428)\u001b[0m  Retrying in 1 seconds\n",
      "Result for PPO_TrafficLightGridPOEnv-v0_f9c59_00000:\n",
      "  agent_timesteps_total: 240000\n",
      "  custom_metrics: {}\n",
      "  date: 2023-01-01_16-17-50\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -189.09394423661467\n",
      "  episode_reward_mean: -220.61354951432676\n",
      "  episode_reward_min: -430.74791281871165\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 600\n",
      "  experiment_id: 610b8b5226964181899719d25df7617e\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.8189894035458563e-13\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.258348963409662\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010072746071074395\n",
      "          policy_loss: -0.004362055189267267\n",
      "          total_loss: 754.1962095737457\n",
      "          vf_explained_var: -5.319714659890451e-07\n",
      "          vf_loss: 754.2005705356598\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 240000\n",
      "    num_steps_sampled: 240000\n",
      "    num_steps_trained: 240000\n",
      "  iterations_since_restore: 60\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.490361445783133\n",
      "    ram_util_percent: 28.486746987951797\n",
      "  pid: 17432\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0883331213319466\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.806297486071045\n",
      "    mean_inference_ms: 1.8343272556256238\n",
      "    mean_raw_obs_processing_ms: 6.635703288891633\n",
      "  time_since_restore: 3493.523649454117\n",
      "  time_this_iter_s: 58.45655798912048\n",
      "  time_total_s: 3493.523649454117\n",
      "  timers:\n",
      "    learn_throughput: 155.929\n",
      "    learn_time_ms: 25652.747\n",
      "    sample_throughput: 124.962\n",
      "    sample_time_ms: 32009.772\n",
      "    update_time_ms: 3.322\n",
      "  timestamp: 1672611470\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 240000\n",
      "  training_iteration: 60\n",
      "  trial_id: f9c59_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 8.9/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.87 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_4_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_3_6417799359d4e2284884fd3ae2ec9735, 0.0/6.0 CPU_group_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_1_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_0_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_5_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_2_6417799359d4e2284884fd3ae2ec9735)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                               | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridPOEnv-v0_f9c59_00000 | RUNNING  | 192.168.1.65:17432 |     60 |          3493.52 | 240000 | -220.614 |             -189.094 |             -430.748 |                400 |\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_TrafficLightGridPOEnv-v0_f9c59_00000:\n",
      "  agent_timesteps_total: 244000\n",
      "  custom_metrics: {}\n",
      "  date: 2023-01-01_16-18-47\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -188.1095505975143\n",
      "  episode_reward_mean: -221.09193207851249\n",
      "  episode_reward_min: -430.74791281871165\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 610\n",
      "  experiment_id: 610b8b5226964181899719d25df7617e\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.8189894035458563e-13\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.2180660769343374\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010600842818968236\n",
      "          policy_loss: -0.00216866280825343\n",
      "          total_loss: 786.7080481052399\n",
      "          vf_explained_var: 8.27200710773468e-06\n",
      "          vf_loss: 786.7102133750916\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 244000\n",
      "    num_steps_sampled: 244000\n",
      "    num_steps_trained: 244000\n",
      "  iterations_since_restore: 61\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.857317073170734\n",
      "    ram_util_percent: 28.565853658536582\n",
      "  pid: 17432\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0883444316938219\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.802750725392652\n",
      "    mean_inference_ms: 1.8346497738321668\n",
      "    mean_raw_obs_processing_ms: 6.62705187394751\n",
      "  time_since_restore: 3550.553661584854\n",
      "  time_this_iter_s: 57.030012130737305\n",
      "  time_total_s: 3550.553661584854\n",
      "  timers:\n",
      "    learn_throughput: 155.967\n",
      "    learn_time_ms: 25646.399\n",
      "    sample_throughput: 124.917\n",
      "    sample_time_ms: 32021.201\n",
      "    update_time_ms: 3.355\n",
      "  timestamp: 1672611527\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 244000\n",
      "  training_iteration: 61\n",
      "  trial_id: f9c59_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 8.9/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.87 GiB heap, 0.0/14.65 GiB objects (0.0/6.0 CPU_group_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_1_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_3_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_0_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_5_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_4_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_2_6417799359d4e2284884fd3ae2ec9735)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                               | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridPOEnv-v0_f9c59_00000 | RUNNING  | 192.168.1.65:17432 |     61 |          3550.55 | 244000 | -221.092 |              -188.11 |             -430.748 |                400 |\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridPOEnv-v0_f9c59_00000:\n",
      "  agent_timesteps_total: 248000\n",
      "  custom_metrics: {}\n",
      "  date: 2023-01-01_16-19-45\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -188.1095505975143\n",
      "  episode_reward_mean: -219.409791400002\n",
      "  episode_reward_min: -430.74791281871165\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 620\n",
      "  experiment_id: 610b8b5226964181899719d25df7617e\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.8189894035458563e-13\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.156973071396351\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010831203273437513\n",
      "          policy_loss: -0.0006613770383410156\n",
      "          total_loss: 718.6780117034912\n",
      "          vf_explained_var: -3.7252903539730653e-10\n",
      "          vf_loss: 718.678671503067\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 248000\n",
      "    num_steps_sampled: 248000\n",
      "    num_steps_trained: 248000\n",
      "  iterations_since_restore: 62\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.56341463414634\n",
      "    ram_util_percent: 28.56463414634146\n",
      "  pid: 17432\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0883528417085103\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.799140440627124\n",
      "    mean_inference_ms: 1.8347448730124671\n",
      "    mean_raw_obs_processing_ms: 6.618755724319113\n",
      "  time_since_restore: 3608.2653393745422\n",
      "  time_this_iter_s: 57.71167778968811\n",
      "  time_total_s: 3608.2653393745422\n",
      "  timers:\n",
      "    learn_throughput: 156.139\n",
      "    learn_time_ms: 25618.223\n",
      "    sample_throughput: 124.982\n",
      "    sample_time_ms: 32004.648\n",
      "    update_time_ms: 3.348\n",
      "  timestamp: 1672611585\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 248000\n",
      "  training_iteration: 62\n",
      "  trial_id: f9c59_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 8.9/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.87 GiB heap, 0.0/14.65 GiB objects (0.0/6.0 CPU_group_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_0_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_2_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_5_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_1_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_4_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_3_6417799359d4e2284884fd3ae2ec9735)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                               | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridPOEnv-v0_f9c59_00000 | RUNNING  | 192.168.1.65:17432 |     62 |          3608.27 | 248000 |  -219.41 |              -188.11 |             -430.748 |                400 |\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridPOEnv-v0_f9c59_00000:\n",
      "  agent_timesteps_total: 252000\n",
      "  custom_metrics: {}\n",
      "  date: 2023-01-01_16-20-43\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -188.1095505975143\n",
      "  episode_reward_mean: -217.82181118080823\n",
      "  episode_reward_min: -396.5733705425011\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 630\n",
      "  experiment_id: 610b8b5226964181899719d25df7617e\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.8189894035458563e-13\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1413586802780626\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01305449561214124\n",
      "          policy_loss: -0.003517899753933307\n",
      "          total_loss: 691.2085606098175\n",
      "          vf_explained_var: 2.131722794729285e-05\n",
      "          vf_loss: 691.2120743274688\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 252000\n",
      "    num_steps_sampled: 252000\n",
      "    num_steps_trained: 252000\n",
      "  iterations_since_restore: 63\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.487951807228917\n",
      "    ram_util_percent: 28.645783132530113\n",
      "  pid: 17432\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08836028559399121\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.79682455566015\n",
      "    mean_inference_ms: 1.8348417963540293\n",
      "    mean_raw_obs_processing_ms: 6.610793365565591\n",
      "  time_since_restore: 3666.1139175891876\n",
      "  time_this_iter_s: 57.848578214645386\n",
      "  time_total_s: 3666.1139175891876\n",
      "  timers:\n",
      "    learn_throughput: 156.13\n",
      "    learn_time_ms: 25619.613\n",
      "    sample_throughput: 124.741\n",
      "    sample_time_ms: 32066.524\n",
      "    update_time_ms: 3.347\n",
      "  timestamp: 1672611643\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 252000\n",
      "  training_iteration: 63\n",
      "  trial_id: f9c59_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Memory usage on this node: 8.9/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.87 GiB heap, 0.0/14.65 GiB objects (0.0/6.0 CPU_group_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_4_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_2_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_1_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_3_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_5_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_0_6417799359d4e2284884fd3ae2ec9735)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                               | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridPOEnv-v0_f9c59_00000 | RUNNING  | 192.168.1.65:17432 |     63 |          3666.11 | 252000 | -217.822 |              -188.11 |             -396.573 |                400 |\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridPOEnv-v0_f9c59_00000:\n",
      "  agent_timesteps_total: 256000\n",
      "  custom_metrics: {}\n",
      "  date: 2023-01-01_16-21-41\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -188.1095505975143\n",
      "  episode_reward_mean: -216.5773344534121\n",
      "  episode_reward_min: -396.5733705425011\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 640\n",
      "  experiment_id: 610b8b5226964181899719d25df7617e\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.8189894035458563e-13\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.147442053258419\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008072147543646295\n",
      "          policy_loss: -0.0008328372292453423\n",
      "          total_loss: 712.2648305892944\n",
      "          vf_explained_var: 5.134008915774757e-06\n",
      "          vf_loss: 712.2656633377076\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 256000\n",
      "    num_steps_sampled: 256000\n",
      "    num_steps_trained: 256000\n",
      "  iterations_since_restore: 64\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.83170731707317\n",
      "    ram_util_percent: 28.700000000000006\n",
      "  pid: 17432\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08836763702028344\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.795237980102613\n",
      "    mean_inference_ms: 1.8349460710742727\n",
      "    mean_raw_obs_processing_ms: 6.6031518553594655\n",
      "  time_since_restore: 3723.9367311000824\n",
      "  time_this_iter_s: 57.822813510894775\n",
      "  time_total_s: 3723.9367311000824\n",
      "  timers:\n",
      "    learn_throughput: 156.235\n",
      "    learn_time_ms: 25602.413\n",
      "    sample_throughput: 124.593\n",
      "    sample_time_ms: 32104.417\n",
      "    update_time_ms: 3.324\n",
      "  timestamp: 1672611701\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 256000\n",
      "  training_iteration: 64\n",
      "  trial_id: f9c59_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 8.9/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.87 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_1_6417799359d4e2284884fd3ae2ec9735, 0.0/6.0 CPU_group_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_4_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_2_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_5_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_0_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_3_6417799359d4e2284884fd3ae2ec9735)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                               | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridPOEnv-v0_f9c59_00000 | RUNNING  | 192.168.1.65:17432 |     64 |          3723.94 | 256000 | -216.577 |              -188.11 |             -396.573 |                400 |\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridPOEnv-v0_f9c59_00000:\n",
      "  agent_timesteps_total: 260000\n",
      "  custom_metrics: {}\n",
      "  date: 2023-01-01_16-22-39\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -188.1095505975143\n",
      "  episode_reward_mean: -217.82937796619655\n",
      "  episode_reward_min: -396.5733705425011\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 650\n",
      "  experiment_id: 610b8b5226964181899719d25df7617e\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 9.094947017729282e-14\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.105901087075472\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010553102399893582\n",
      "          policy_loss: -0.0006062039770768024\n",
      "          total_loss: 688.0787146091461\n",
      "          vf_explained_var: 8.547492143406998e-06\n",
      "          vf_loss: 688.0793219566345\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 260000\n",
      "    num_steps_sampled: 260000\n",
      "    num_steps_trained: 260000\n",
      "  iterations_since_restore: 65\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.5297619047619\n",
      "    ram_util_percent: 28.663095238095245\n",
      "  pid: 17432\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08837422797336343\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.79442189748766\n",
      "    mean_inference_ms: 1.8349862361345126\n",
      "    mean_raw_obs_processing_ms: 6.5959245533088975\n",
      "  time_since_restore: 3782.297868013382\n",
      "  time_this_iter_s: 58.36113691329956\n",
      "  time_total_s: 3782.297868013382\n",
      "  timers:\n",
      "    learn_throughput: 155.708\n",
      "    learn_time_ms: 25689.155\n",
      "    sample_throughput: 124.43\n",
      "    sample_time_ms: 32146.648\n",
      "    update_time_ms: 3.283\n",
      "  timestamp: 1672611759\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 260000\n",
      "  training_iteration: 65\n",
      "  trial_id: f9c59_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 8.9/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.87 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_4_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_2_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_1_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_3_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_0_6417799359d4e2284884fd3ae2ec9735, 0.0/6.0 CPU_group_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_5_6417799359d4e2284884fd3ae2ec9735)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                               | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridPOEnv-v0_f9c59_00000 | RUNNING  | 192.168.1.65:17432 |     65 |           3782.3 | 260000 | -217.829 |              -188.11 |             -396.573 |                400 |\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_TrafficLightGridPOEnv-v0_f9c59_00000:\n",
      "  agent_timesteps_total: 264000\n",
      "  custom_metrics: {}\n",
      "  date: 2023-01-01_16-23-37\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -188.1095505975143\n",
      "  episode_reward_mean: -216.50067951044946\n",
      "  episode_reward_min: -377.55376643338803\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 660\n",
      "  experiment_id: 610b8b5226964181899719d25df7617e\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 9.094947017729282e-14\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0493955317884684\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008075294579458655\n",
      "          policy_loss: -0.001790689115296118\n",
      "          total_loss: 683.7121622085572\n",
      "          vf_explained_var: 4.499591796047753e-06\n",
      "          vf_loss: 683.713951921463\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 264000\n",
      "    num_steps_sampled: 264000\n",
      "    num_steps_trained: 264000\n",
      "  iterations_since_restore: 66\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.897560975609757\n",
      "    ram_util_percent: 28.698780487804882\n",
      "  pid: 17432\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0883798437706716\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.793045497459424\n",
      "    mean_inference_ms: 1.835025574545897\n",
      "    mean_raw_obs_processing_ms: 6.588964202063641\n",
      "  time_since_restore: 3840.095154285431\n",
      "  time_this_iter_s: 57.79728627204895\n",
      "  time_total_s: 3840.095154285431\n",
      "  timers:\n",
      "    learn_throughput: 155.548\n",
      "    learn_time_ms: 25715.561\n",
      "    sample_throughput: 124.557\n",
      "    sample_time_ms: 32113.931\n",
      "    update_time_ms: 3.29\n",
      "  timestamp: 1672611817\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 264000\n",
      "  training_iteration: 66\n",
      "  trial_id: f9c59_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 9.0/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.87 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_3_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_1_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_4_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_5_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_0_6417799359d4e2284884fd3ae2ec9735, 0.0/6.0 CPU_group_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_2_6417799359d4e2284884fd3ae2ec9735)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                               | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridPOEnv-v0_f9c59_00000 | RUNNING  | 192.168.1.65:17432 |     66 |           3840.1 | 264000 | -216.501 |              -188.11 |             -377.554 |                400 |\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridPOEnv-v0_f9c59_00000:\n",
      "  agent_timesteps_total: 268000\n",
      "  custom_metrics: {}\n",
      "  date: 2023-01-01_16-24-34\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -187.8429759961484\n",
      "  episode_reward_mean: -216.46725030898497\n",
      "  episode_reward_min: -377.55376643338803\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 670\n",
      "  experiment_id: 610b8b5226964181899719d25df7617e\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 4.547473508864641e-14\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9529935989528895\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013445199821501358\n",
      "          policy_loss: -0.0010726090607931838\n",
      "          total_loss: 730.2103990077973\n",
      "          vf_explained_var: 4.630535954674997e-07\n",
      "          vf_loss: 730.2114690780639\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 268000\n",
      "    num_steps_sampled: 268000\n",
      "    num_steps_trained: 268000\n",
      "  iterations_since_restore: 67\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.74878048780488\n",
      "    ram_util_percent: 28.69390243902439\n",
      "  pid: 17432\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08838636600128591\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.791859930886073\n",
      "    mean_inference_ms: 1.8351469187485454\n",
      "    mean_raw_obs_processing_ms: 6.5823320912090955\n",
      "  time_since_restore: 3897.493439912796\n",
      "  time_this_iter_s: 57.39828562736511\n",
      "  time_total_s: 3897.493439912796\n",
      "  timers:\n",
      "    learn_throughput: 155.864\n",
      "    learn_time_ms: 25663.384\n",
      "    sample_throughput: 124.485\n",
      "    sample_time_ms: 32132.339\n",
      "    update_time_ms: 3.272\n",
      "  timestamp: 1672611874\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 268000\n",
      "  training_iteration: 67\n",
      "  trial_id: f9c59_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 9.0/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.87 GiB heap, 0.0/14.65 GiB objects (0.0/6.0 CPU_group_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_4_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_1_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_2_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_0_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_3_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_5_6417799359d4e2284884fd3ae2ec9735)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                               | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridPOEnv-v0_f9c59_00000 | RUNNING  | 192.168.1.65:17432 |     67 |          3897.49 | 268000 | -216.467 |             -187.843 |             -377.554 |                400 |\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridPOEnv-v0_f9c59_00000:\n",
      "  agent_timesteps_total: 272000\n",
      "  custom_metrics: {}\n",
      "  date: 2023-01-01_16-25-32\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -187.8429759961484\n",
      "  episode_reward_mean: -217.4717086697183\n",
      "  episode_reward_min: -377.55376643338803\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 680\n",
      "  experiment_id: 610b8b5226964181899719d25df7617e\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 4.547473508864641e-14\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.930832276120782\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010195043198129387\n",
      "          policy_loss: -0.00022936328896321355\n",
      "          total_loss: 674.0095272064209\n",
      "          vf_explained_var: 3.7938357309030835e-06\n",
      "          vf_loss: 674.0097570419312\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 272000\n",
      "    num_steps_sampled: 272000\n",
      "    num_steps_trained: 272000\n",
      "  iterations_since_restore: 68\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.540243902439023\n",
      "    ram_util_percent: 28.809756097560978\n",
      "  pid: 17432\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0883908130311611\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.789657703011116\n",
      "    mean_inference_ms: 1.835243905554206\n",
      "    mean_raw_obs_processing_ms: 6.576003047872824\n",
      "  time_since_restore: 3955.0086195468903\n",
      "  time_this_iter_s: 57.51517963409424\n",
      "  time_total_s: 3955.0086195468903\n",
      "  timers:\n",
      "    learn_throughput: 155.868\n",
      "    learn_time_ms: 25662.815\n",
      "    sample_throughput: 124.72\n",
      "    sample_time_ms: 32071.969\n",
      "    update_time_ms: 3.215\n",
      "  timestamp: 1672611932\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 272000\n",
      "  training_iteration: 68\n",
      "  trial_id: f9c59_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Memory usage on this node: 9.0/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.87 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_2_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_1_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_4_6417799359d4e2284884fd3ae2ec9735, 0.0/6.0 CPU_group_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_3_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_5_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_0_6417799359d4e2284884fd3ae2ec9735)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                               | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridPOEnv-v0_f9c59_00000 | RUNNING  | 192.168.1.65:17432 |     68 |          3955.01 | 272000 | -217.472 |             -187.843 |             -377.554 |                400 |\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridPOEnv-v0_f9c59_00000:\n",
      "  agent_timesteps_total: 276000\n",
      "  custom_metrics: {}\n",
      "  date: 2023-01-01_16-26-29\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -187.8429759961484\n",
      "  episode_reward_mean: -217.29187921545952\n",
      "  episode_reward_min: -377.55376643338803\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 690\n",
      "  experiment_id: 610b8b5226964181899719d25df7617e\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 4.547473508864641e-14\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8970309745520353\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.004716703532744814\n",
      "          policy_loss: -0.0013137060319422743\n",
      "          total_loss: 732.0051804542542\n",
      "          vf_explained_var: 2.282299192302162e-06\n",
      "          vf_loss: 732.0064966201783\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 276000\n",
      "    num_steps_sampled: 276000\n",
      "    num_steps_trained: 276000\n",
      "  iterations_since_restore: 69\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.52048192771084\n",
      "    ram_util_percent: 28.924096385542168\n",
      "  pid: 17432\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08839459233299016\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.787664544492134\n",
      "    mean_inference_ms: 1.8353639298823043\n",
      "    mean_raw_obs_processing_ms: 6.569886807888875\n",
      "  time_since_restore: 4012.4645426273346\n",
      "  time_this_iter_s: 57.455923080444336\n",
      "  time_total_s: 4012.4645426273346\n",
      "  timers:\n",
      "    learn_throughput: 155.979\n",
      "    learn_time_ms: 25644.46\n",
      "    sample_throughput: 124.685\n",
      "    sample_time_ms: 32080.893\n",
      "    update_time_ms: 3.239\n",
      "  timestamp: 1672611989\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 276000\n",
      "  training_iteration: 69\n",
      "  trial_id: f9c59_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 9.0/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.87 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_1_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_5_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_0_6417799359d4e2284884fd3ae2ec9735, 0.0/6.0 CPU_group_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_4_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_2_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_3_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 accelerator_type:G)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                               | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridPOEnv-v0_f9c59_00000 | RUNNING  | 192.168.1.65:17432 |     69 |          4012.46 | 276000 | -217.292 |             -187.843 |             -377.554 |                400 |\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridPOEnv-v0_f9c59_00000:\n",
      "  agent_timesteps_total: 280000\n",
      "  custom_metrics: {}\n",
      "  date: 2023-01-01_16-27-27\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -187.8429759961484\n",
      "  episode_reward_mean: -217.34931986155098\n",
      "  episode_reward_min: -377.55376643338803\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 700\n",
      "  experiment_id: 610b8b5226964181899719d25df7617e\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.2737367544323204e-14\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9086376540362835\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.006939429190629266\n",
      "          policy_loss: -0.0007354407294769772\n",
      "          total_loss: 699.9612163066864\n",
      "          vf_explained_var: 1.6938895441853674e-06\n",
      "          vf_loss: 699.9619515895844\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 280000\n",
      "    num_steps_sampled: 280000\n",
      "    num_steps_trained: 280000\n",
      "  iterations_since_restore: 70\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.586585365853658\n",
      "    ram_util_percent: 28.968292682926826\n",
      "  pid: 17432\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08839778602692043\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.785903546858254\n",
      "    mean_inference_ms: 1.8354207336017436\n",
      "    mean_raw_obs_processing_ms: 6.56180579939391\n",
      "  time_since_restore: 4069.901978969574\n",
      "  time_this_iter_s: 57.43743634223938\n",
      "  time_total_s: 4069.901978969574\n",
      "  timers:\n",
      "    learn_throughput: 155.986\n",
      "    learn_time_ms: 25643.262\n",
      "    sample_throughput: 125.077\n",
      "    sample_time_ms: 31980.185\n",
      "    update_time_ms: 3.197\n",
      "  timestamp: 1672612047\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 280000\n",
      "  training_iteration: 70\n",
      "  trial_id: f9c59_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 9.0/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.87 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_3_6417799359d4e2284884fd3ae2ec9735, 0.0/6.0 CPU_group_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_5_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_0_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_4_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_1_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_2_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 accelerator_type:G)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                               | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridPOEnv-v0_f9c59_00000 | RUNNING  | 192.168.1.65:17432 |     70 |           4069.9 | 280000 | -217.349 |             -187.843 |             -377.554 |                400 |\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_TrafficLightGridPOEnv-v0_f9c59_00000:\n",
      "  agent_timesteps_total: 284000\n",
      "  custom_metrics: {}\n",
      "  date: 2023-01-01_16-28-25\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -187.8429759961484\n",
      "  episode_reward_mean: -216.2782391478542\n",
      "  episode_reward_min: -241.1761862283248\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 710\n",
      "  experiment_id: 610b8b5226964181899719d25df7617e\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.1368683772161602e-14\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.790356320515275\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01613770910101956\n",
      "          policy_loss: -0.003252380728372373\n",
      "          total_loss: 720.2587259292602\n",
      "          vf_explained_var: 3.234483301639557e-06\n",
      "          vf_loss: 720.2619769096375\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 284000\n",
      "    num_steps_sampled: 284000\n",
      "    num_steps_trained: 284000\n",
      "  iterations_since_restore: 71\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.757317073170732\n",
      "    ram_util_percent: 29.053658536585367\n",
      "  pid: 17432\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0884001354267022\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.78465065732913\n",
      "    mean_inference_ms: 1.8354592223313995\n",
      "    mean_raw_obs_processing_ms: 6.553999017383657\n",
      "  time_since_restore: 4127.659029960632\n",
      "  time_this_iter_s: 57.75705099105835\n",
      "  time_total_s: 4127.659029960632\n",
      "  timers:\n",
      "    learn_throughput: 155.699\n",
      "    learn_time_ms: 25690.621\n",
      "    sample_throughput: 124.978\n",
      "    sample_time_ms: 32005.505\n",
      "    update_time_ms: 3.235\n",
      "  timestamp: 1672612105\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 284000\n",
      "  training_iteration: 71\n",
      "  trial_id: f9c59_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 9.0/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.87 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_5_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_3_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_1_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_2_6417799359d4e2284884fd3ae2ec9735, 0.0/6.0 CPU_group_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_4_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_0_6417799359d4e2284884fd3ae2ec9735)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                               | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridPOEnv-v0_f9c59_00000 | RUNNING  | 192.168.1.65:17432 |     71 |          4127.66 | 284000 | -216.278 |             -187.843 |             -241.176 |                400 |\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridPOEnv-v0_f9c59_00000:\n",
      "  agent_timesteps_total: 288000\n",
      "  custom_metrics: {}\n",
      "  date: 2023-01-01_16-29-23\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -187.8429759961484\n",
      "  episode_reward_mean: -216.2275949422999\n",
      "  episode_reward_min: -241.1761862283248\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 720\n",
      "  experiment_id: 610b8b5226964181899719d25df7617e\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.1368683772161602e-14\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.749838125705719\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010150268608856608\n",
      "          policy_loss: -0.0018375688829110003\n",
      "          total_loss: 697.1987298011779\n",
      "          vf_explained_var: 2.5121494218183216e-06\n",
      "          vf_loss: 697.2005676269531\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 288000\n",
      "    num_steps_sampled: 288000\n",
      "    num_steps_trained: 288000\n",
      "  iterations_since_restore: 72\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.486746987951808\n",
      "    ram_util_percent: 29.0578313253012\n",
      "  pid: 17432\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08840268240591781\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.784148248786405\n",
      "    mean_inference_ms: 1.835587119565511\n",
      "    mean_raw_obs_processing_ms: 6.546341845429695\n",
      "  time_since_restore: 4185.686171531677\n",
      "  time_this_iter_s: 58.02714157104492\n",
      "  time_total_s: 4185.686171531677\n",
      "  timers:\n",
      "    learn_throughput: 155.746\n",
      "    learn_time_ms: 25682.831\n",
      "    sample_throughput: 124.825\n",
      "    sample_time_ms: 32044.963\n",
      "    update_time_ms: 3.23\n",
      "  timestamp: 1672612163\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 288000\n",
      "  training_iteration: 72\n",
      "  trial_id: f9c59_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 9.0/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.87 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_2_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_5_6417799359d4e2284884fd3ae2ec9735, 0.0/6.0 CPU_group_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_4_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_3_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_0_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_1_6417799359d4e2284884fd3ae2ec9735)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                               | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridPOEnv-v0_f9c59_00000 | RUNNING  | 192.168.1.65:17432 |     72 |          4185.69 | 288000 | -216.228 |             -187.843 |             -241.176 |                400 |\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridPOEnv-v0_f9c59_00000:\n",
      "  agent_timesteps_total: 292000\n",
      "  custom_metrics: {}\n",
      "  date: 2023-01-01_16-30-21\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -187.8429759961484\n",
      "  episode_reward_mean: -216.33025298326524\n",
      "  episode_reward_min: -241.1761862283248\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 730\n",
      "  experiment_id: 610b8b5226964181899719d25df7617e\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.1368683772161602e-14\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6619527671486138\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009796971811118795\n",
      "          policy_loss: -0.0020554691887809894\n",
      "          total_loss: 637.6225524425506\n",
      "          vf_explained_var: 7.262266990437638e-06\n",
      "          vf_loss: 637.6246081352234\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 292000\n",
      "    num_steps_sampled: 292000\n",
      "    num_steps_trained: 292000\n",
      "  iterations_since_restore: 73\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.80365853658537\n",
      "    ram_util_percent: 29.10365853658537\n",
      "  pid: 17432\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08840651606437248\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.783392546772184\n",
      "    mean_inference_ms: 1.8357485392729376\n",
      "    mean_raw_obs_processing_ms: 6.5389741625292395\n",
      "  time_since_restore: 4243.320711612701\n",
      "  time_this_iter_s: 57.63454008102417\n",
      "  time_total_s: 4243.320711612701\n",
      "  timers:\n",
      "    learn_throughput: 155.86\n",
      "    learn_time_ms: 25664.038\n",
      "    sample_throughput: 124.835\n",
      "    sample_time_ms: 32042.354\n",
      "    update_time_ms: 3.22\n",
      "  timestamp: 1672612221\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 292000\n",
      "  training_iteration: 73\n",
      "  trial_id: f9c59_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Memory usage on this node: 9.1/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.87 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_4_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_2_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_0_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_1_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_5_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_3_6417799359d4e2284884fd3ae2ec9735, 0.0/6.0 CPU_group_6417799359d4e2284884fd3ae2ec9735)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                               | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridPOEnv-v0_f9c59_00000 | RUNNING  | 192.168.1.65:17432 |     73 |          4243.32 | 292000 |  -216.33 |             -187.843 |             -241.176 |                400 |\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridPOEnv-v0_f9c59_00000:\n",
      "  agent_timesteps_total: 296000\n",
      "  custom_metrics: {}\n",
      "  date: 2023-01-01_16-31-19\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -187.8429759961484\n",
      "  episode_reward_mean: -216.96462589533755\n",
      "  episode_reward_min: -241.1761862283248\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 740\n",
      "  experiment_id: 610b8b5226964181899719d25df7617e\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 5.684341886080801e-15\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5799703124910593\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01472032380359572\n",
      "          policy_loss: -0.0037007100894697943\n",
      "          total_loss: 688.1928644657135\n",
      "          vf_explained_var: 3.7981196783221094e-06\n",
      "          vf_loss: 688.1965685367584\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 296000\n",
      "    num_steps_sampled: 296000\n",
      "    num_steps_trained: 296000\n",
      "  iterations_since_restore: 74\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.603614457831327\n",
      "    ram_util_percent: 29.246987951807217\n",
      "  pid: 17432\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08841083960222744\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.78217350562401\n",
      "    mean_inference_ms: 1.8359648992866986\n",
      "    mean_raw_obs_processing_ms: 6.531800971352953\n",
      "  time_since_restore: 4301.243188381195\n",
      "  time_this_iter_s: 57.92247676849365\n",
      "  time_total_s: 4301.243188381195\n",
      "  timers:\n",
      "    learn_throughput: 155.584\n",
      "    learn_time_ms: 25709.534\n",
      "    sample_throughput: 124.973\n",
      "    sample_time_ms: 32006.818\n",
      "    update_time_ms: 3.244\n",
      "  timestamp: 1672612279\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 296000\n",
      "  training_iteration: 74\n",
      "  trial_id: f9c59_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 9.1/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.87 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_1_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_5_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_2_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_0_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_3_6417799359d4e2284884fd3ae2ec9735, 0.0/6.0 CPU_group_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_4_6417799359d4e2284884fd3ae2ec9735)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                               | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridPOEnv-v0_f9c59_00000 | RUNNING  | 192.168.1.65:17432 |     74 |          4301.24 | 296000 | -216.965 |             -187.843 |             -241.176 |                400 |\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridPOEnv-v0_f9c59_00000:\n",
      "  agent_timesteps_total: 300000\n",
      "  custom_metrics: {}\n",
      "  date: 2023-01-01_16-32-17\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -187.8429759961484\n",
      "  episode_reward_mean: -216.5353187547743\n",
      "  episode_reward_min: -241.33388154464458\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 750\n",
      "  experiment_id: 610b8b5226964181899719d25df7617e\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 5.684341886080801e-15\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.496621199697256\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014465873212219576\n",
      "          policy_loss: -0.004321487013658043\n",
      "          total_loss: 733.7690867424011\n",
      "          vf_explained_var: 1.8944963358080713e-06\n",
      "          vf_loss: 733.7734038352967\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 300000\n",
      "    num_steps_sampled: 300000\n",
      "    num_steps_trained: 300000\n",
      "  iterations_since_restore: 75\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.54578313253012\n",
      "    ram_util_percent: 29.259036144578317\n",
      "  pid: 17432\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08841195118617673\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.780732383665313\n",
      "    mean_inference_ms: 1.8361140241466432\n",
      "    mean_raw_obs_processing_ms: 6.524824239279479\n",
      "  time_since_restore: 4359.287802219391\n",
      "  time_this_iter_s: 58.0446138381958\n",
      "  time_total_s: 4359.287802219391\n",
      "  timers:\n",
      "    learn_throughput: 155.684\n",
      "    learn_time_ms: 25693.071\n",
      "    sample_throughput: 125.033\n",
      "    sample_time_ms: 31991.554\n",
      "    update_time_ms: 3.237\n",
      "  timestamp: 1672612337\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 300000\n",
      "  training_iteration: 75\n",
      "  trial_id: f9c59_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 9.1/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.87 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_2_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_5_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_1_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_4_6417799359d4e2284884fd3ae2ec9735, 0.0/6.0 CPU_group_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_3_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_0_6417799359d4e2284884fd3ae2ec9735)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                               | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridPOEnv-v0_f9c59_00000 | RUNNING  | 192.168.1.65:17432 |     75 |          4359.29 | 300000 | -216.535 |             -187.843 |             -241.334 |                400 |\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_TrafficLightGridPOEnv-v0_f9c59_00000:\n",
      "  agent_timesteps_total: 304000\n",
      "  custom_metrics: {}\n",
      "  date: 2023-01-01_16-33-15\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -187.8429759961484\n",
      "  episode_reward_mean: -216.45532622756545\n",
      "  episode_reward_min: -241.33388154464458\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 760\n",
      "  experiment_id: 610b8b5226964181899719d25df7617e\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 5.684341886080801e-15\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.4367705136537552\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.00711092839448525\n",
      "          policy_loss: 0.0011046279556467197\n",
      "          total_loss: 661.5991339206696\n",
      "          vf_explained_var: 3.7241727568471106e-06\n",
      "          vf_loss: 661.5980299949646\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 304000\n",
      "    num_steps_sampled: 304000\n",
      "    num_steps_trained: 304000\n",
      "  iterations_since_restore: 76\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.725301204819274\n",
      "    ram_util_percent: 29.368674698795186\n",
      "  pid: 17432\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08841200764861913\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.779375582977938\n",
      "    mean_inference_ms: 1.8362115058695698\n",
      "    mean_raw_obs_processing_ms: 6.517983026054692\n",
      "  time_since_restore: 4417.098685741425\n",
      "  time_this_iter_s: 57.81088352203369\n",
      "  time_total_s: 4417.098685741425\n",
      "  timers:\n",
      "    learn_throughput: 155.602\n",
      "    learn_time_ms: 25706.623\n",
      "    sample_throughput: 125.081\n",
      "    sample_time_ms: 31979.357\n",
      "    update_time_ms: 3.233\n",
      "  timestamp: 1672612395\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 304000\n",
      "  training_iteration: 76\n",
      "  trial_id: f9c59_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 9.1/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.87 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_5_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_2_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_3_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_1_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_4_6417799359d4e2284884fd3ae2ec9735, 0.0/6.0 CPU_group_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_0_6417799359d4e2284884fd3ae2ec9735)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                               | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridPOEnv-v0_f9c59_00000 | RUNNING  | 192.168.1.65:17432 |     76 |           4417.1 | 304000 | -216.455 |             -187.843 |             -241.334 |                400 |\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridPOEnv-v0_f9c59_00000:\n",
      "  agent_timesteps_total: 308000\n",
      "  custom_metrics: {}\n",
      "  date: 2023-01-01_16-34-13\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -189.04680402595622\n",
      "  episode_reward_mean: -216.8634889382373\n",
      "  episode_reward_min: -241.33388154464458\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 770\n",
      "  experiment_id: 610b8b5226964181899719d25df7617e\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.8421709430404005e-15\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.375923378393054\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0072798780986773155\n",
      "          policy_loss: 0.0013625436782604083\n",
      "          total_loss: 697.9214310646057\n",
      "          vf_explained_var: 9.580701771483291e-06\n",
      "          vf_loss: 697.9200733184814\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 308000\n",
      "    num_steps_sampled: 308000\n",
      "    num_steps_trained: 308000\n",
      "  iterations_since_restore: 77\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.52530120481927\n",
      "    ram_util_percent: 29.36144578313253\n",
      "  pid: 17432\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08841002072092995\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.778129427509313\n",
      "    mean_inference_ms: 1.836233426758995\n",
      "    mean_raw_obs_processing_ms: 6.5112618458079705\n",
      "  time_since_restore: 4475.529012441635\n",
      "  time_this_iter_s: 58.43032670021057\n",
      "  time_total_s: 4475.529012441635\n",
      "  timers:\n",
      "    learn_throughput: 154.944\n",
      "    learn_time_ms: 25815.722\n",
      "    sample_throughput: 125.104\n",
      "    sample_time_ms: 31973.4\n",
      "    update_time_ms: 3.283\n",
      "  timestamp: 1672612453\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 308000\n",
      "  training_iteration: 77\n",
      "  trial_id: f9c59_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 9.1/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.87 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_3_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_0_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_2_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_1_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 accelerator_type:G, 0.0/6.0 CPU_group_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_5_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_4_6417799359d4e2284884fd3ae2ec9735)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                               | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridPOEnv-v0_f9c59_00000 | RUNNING  | 192.168.1.65:17432 |     77 |          4475.53 | 308000 | -216.863 |             -189.047 |             -241.334 |                400 |\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridPOEnv-v0_f9c59_00000:\n",
      "  agent_timesteps_total: 312000\n",
      "  custom_metrics: {}\n",
      "  date: 2023-01-01_16-35-12\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -189.04680402595622\n",
      "  episode_reward_mean: -217.4025650140631\n",
      "  episode_reward_min: -395.50797583825494\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 780\n",
      "  experiment_id: 610b8b5226964181899719d25df7617e\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.4210854715202003e-15\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.4344960428774356\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.006675251197884827\n",
      "          policy_loss: -0.0015397995564853772\n",
      "          total_loss: 804.3683162689209\n",
      "          vf_explained_var: 1.6283058357657865e-05\n",
      "          vf_loss: 804.3698499679565\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 312000\n",
      "    num_steps_sampled: 312000\n",
      "    num_steps_trained: 312000\n",
      "  iterations_since_restore: 78\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.53333333333334\n",
      "    ram_util_percent: 29.448809523809523\n",
      "  pid: 17432\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08840712895053557\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.777718117239164\n",
      "    mean_inference_ms: 1.836182005518969\n",
      "    mean_raw_obs_processing_ms: 6.504694056334042\n",
      "  time_since_restore: 4533.808296918869\n",
      "  time_this_iter_s: 58.27928447723389\n",
      "  time_total_s: 4533.808296918869\n",
      "  timers:\n",
      "    learn_throughput: 154.754\n",
      "    learn_time_ms: 25847.485\n",
      "    sample_throughput: 124.929\n",
      "    sample_time_ms: 32018.087\n",
      "    update_time_ms: 3.303\n",
      "  timestamp: 1672612512\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 312000\n",
      "  training_iteration: 78\n",
      "  trial_id: f9c59_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Memory usage on this node: 9.2/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.87 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_2_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_1_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_0_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_4_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_5_6417799359d4e2284884fd3ae2ec9735, 0.0/6.0 CPU_group_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_3_6417799359d4e2284884fd3ae2ec9735)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                               | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridPOEnv-v0_f9c59_00000 | RUNNING  | 192.168.1.65:17432 |     78 |          4533.81 | 312000 | -217.403 |             -189.047 |             -395.508 |                400 |\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridPOEnv-v0_f9c59_00000:\n",
      "  agent_timesteps_total: 316000\n",
      "  custom_metrics: {}\n",
      "  date: 2023-01-01_16-36-10\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -189.04680402595622\n",
      "  episode_reward_mean: -218.43974163970424\n",
      "  episode_reward_min: -395.50797583825494\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 790\n",
      "  experiment_id: 610b8b5226964181899719d25df7617e\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 7.105427357601001e-16\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.4337287742644549\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009769575719565233\n",
      "          policy_loss: -0.00018520550365792588\n",
      "          total_loss: 681.3652040958405\n",
      "          vf_explained_var: 1.332256942987442e-05\n",
      "          vf_loss: 681.3653876304627\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 316000\n",
      "    num_steps_sampled: 316000\n",
      "    num_steps_trained: 316000\n",
      "  iterations_since_restore: 79\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.516867469879518\n",
      "    ram_util_percent: 29.522891566265056\n",
      "  pid: 17432\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08840428317020052\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.7781682048534\n",
      "    mean_inference_ms: 1.836135502399834\n",
      "    mean_raw_obs_processing_ms: 6.498357092590786\n",
      "  time_since_restore: 4591.929440498352\n",
      "  time_this_iter_s: 58.12114357948303\n",
      "  time_total_s: 4591.929440498352\n",
      "  timers:\n",
      "    learn_throughput: 154.663\n",
      "    learn_time_ms: 25862.644\n",
      "    sample_throughput: 124.729\n",
      "    sample_time_ms: 32069.462\n",
      "    update_time_ms: 3.281\n",
      "  timestamp: 1672612570\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 316000\n",
      "  training_iteration: 79\n",
      "  trial_id: f9c59_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 9.2/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.87 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_2_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_5_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_1_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_0_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_4_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_3_6417799359d4e2284884fd3ae2ec9735, 0.0/6.0 CPU_group_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 accelerator_type:G)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                               | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridPOEnv-v0_f9c59_00000 | RUNNING  | 192.168.1.65:17432 |     79 |          4591.93 | 316000 |  -218.44 |             -189.047 |             -395.508 |                400 |\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridPOEnv-v0_f9c59_00000:\n",
      "  agent_timesteps_total: 320000\n",
      "  custom_metrics: {}\n",
      "  date: 2023-01-01_16-37-08\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -189.04680402595622\n",
      "  episode_reward_mean: -218.61788196753528\n",
      "  episode_reward_min: -395.50797583825494\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 800\n",
      "  experiment_id: 610b8b5226964181899719d25df7617e\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 3.5527136788005006e-16\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.315489725023508\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01528513979918104\n",
      "          policy_loss: -0.0027151656191563233\n",
      "          total_loss: 658.6110906600952\n",
      "          vf_explained_var: 3.013200966961449e-06\n",
      "          vf_loss: 658.6138070106506\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 320000\n",
      "    num_steps_sampled: 320000\n",
      "    num_steps_trained: 320000\n",
      "  iterations_since_restore: 80\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.681707317073172\n",
      "    ram_util_percent: 29.507317073170732\n",
      "  pid: 17432\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08840126949063086\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.778461619561696\n",
      "    mean_inference_ms: 1.8361333913014937\n",
      "    mean_raw_obs_processing_ms: 6.49214880692012\n",
      "  time_since_restore: 4649.720350027084\n",
      "  time_this_iter_s: 57.7909095287323\n",
      "  time_total_s: 4649.720350027084\n",
      "  timers:\n",
      "    learn_throughput: 154.365\n",
      "    learn_time_ms: 25912.64\n",
      "    sample_throughput: 124.786\n",
      "    sample_time_ms: 32054.767\n",
      "    update_time_ms: 3.288\n",
      "  timestamp: 1672612628\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 320000\n",
      "  training_iteration: 80\n",
      "  trial_id: f9c59_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 9.2/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.87 GiB heap, 0.0/14.65 GiB objects (0.0/6.0 CPU_group_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_2_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_1_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_4_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_3_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_5_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_0_6417799359d4e2284884fd3ae2ec9735)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                               | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridPOEnv-v0_f9c59_00000 | RUNNING  | 192.168.1.65:17432 |     80 |          4649.72 | 320000 | -218.618 |             -189.047 |             -395.508 |                400 |\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_TrafficLightGridPOEnv-v0_f9c59_00000:\n",
      "  agent_timesteps_total: 324000\n",
      "  custom_metrics: {}\n",
      "  date: 2023-01-01_16-38-06\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -189.04680402595622\n",
      "  episode_reward_mean: -219.1070793351554\n",
      "  episode_reward_min: -395.50797583825494\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 810\n",
      "  experiment_id: 610b8b5226964181899719d25df7617e\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 3.5527136788005006e-16\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.3653465580195188\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.00719943717822395\n",
      "          policy_loss: -0.001768843324680347\n",
      "          total_loss: 643.3395702838898\n",
      "          vf_explained_var: 1.7656013824307593e-06\n",
      "          vf_loss: 643.3413408756256\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 324000\n",
      "    num_steps_sampled: 324000\n",
      "    num_steps_trained: 324000\n",
      "  iterations_since_restore: 81\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.47951807228916\n",
      "    ram_util_percent: 29.563855421686743\n",
      "  pid: 17432\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08839859986842498\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.778271467292814\n",
      "    mean_inference_ms: 1.8361232180970666\n",
      "    mean_raw_obs_processing_ms: 6.486057128616096\n",
      "  time_since_restore: 4707.653263568878\n",
      "  time_this_iter_s: 57.93291354179382\n",
      "  time_total_s: 4707.653263568878\n",
      "  timers:\n",
      "    learn_throughput: 154.065\n",
      "    learn_time_ms: 25963.108\n",
      "    sample_throughput: 124.914\n",
      "    sample_time_ms: 32021.909\n",
      "    update_time_ms: 3.214\n",
      "  timestamp: 1672612686\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 324000\n",
      "  training_iteration: 81\n",
      "  trial_id: f9c59_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 9.2/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.87 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_4_6417799359d4e2284884fd3ae2ec9735, 0.0/6.0 CPU_group_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_3_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_2_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_0_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_5_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_1_6417799359d4e2284884fd3ae2ec9735)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                               | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridPOEnv-v0_f9c59_00000 | RUNNING  | 192.168.1.65:17432 |     81 |          4707.65 | 324000 | -219.107 |             -189.047 |             -395.508 |                400 |\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridPOEnv-v0_f9c59_00000:\n",
      "  agent_timesteps_total: 328000\n",
      "  custom_metrics: {}\n",
      "  date: 2023-01-01_16-39-03\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -189.04680402595622\n",
      "  episode_reward_mean: -219.76737653051572\n",
      "  episode_reward_min: -395.50797583825494\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 820\n",
      "  experiment_id: 610b8b5226964181899719d25df7617e\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.7763568394002503e-16\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.3116516865789891\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010457353914289197\n",
      "          policy_loss: 0.00108454513392644\n",
      "          total_loss: 651.2897430419922\n",
      "          vf_explained_var: -1.1175871339474952e-09\n",
      "          vf_loss: 651.2886568546295\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 328000\n",
      "    num_steps_sampled: 328000\n",
      "    num_steps_trained: 328000\n",
      "  iterations_since_restore: 82\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.506097560975608\n",
      "    ram_util_percent: 29.64999999999999\n",
      "  pid: 17432\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08839719132244035\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.777374597686205\n",
      "    mean_inference_ms: 1.8361040534407145\n",
      "    mean_raw_obs_processing_ms: 6.480243430047135\n",
      "  time_since_restore: 4764.972269535065\n",
      "  time_this_iter_s: 57.31900596618652\n",
      "  time_total_s: 4764.972269535065\n",
      "  timers:\n",
      "    learn_throughput: 154.221\n",
      "    learn_time_ms: 25936.838\n",
      "    sample_throughput: 125.089\n",
      "    sample_time_ms: 31977.26\n",
      "    update_time_ms: 3.288\n",
      "  timestamp: 1672612743\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 328000\n",
      "  training_iteration: 82\n",
      "  trial_id: f9c59_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 9.2/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.87 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_1_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_0_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_5_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_3_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_2_6417799359d4e2284884fd3ae2ec9735, 0.0/6.0 CPU_group_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_4_6417799359d4e2284884fd3ae2ec9735)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                               | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridPOEnv-v0_f9c59_00000 | RUNNING  | 192.168.1.65:17432 |     82 |          4764.97 | 328000 | -219.767 |             -189.047 |             -395.508 |                400 |\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridPOEnv-v0_f9c59_00000:\n",
      "  agent_timesteps_total: 332000\n",
      "  custom_metrics: {}\n",
      "  date: 2023-01-01_16-40-00\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -190.5022903187344\n",
      "  episode_reward_mean: -219.54252023206303\n",
      "  episode_reward_min: -395.50797583825494\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 830\n",
      "  experiment_id: 610b8b5226964181899719d25df7617e\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.7763568394002503e-16\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.2483368787914515\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014247698728000585\n",
      "          policy_loss: -0.005280467326520011\n",
      "          total_loss: 705.8780611991882\n",
      "          vf_explained_var: 5.684420557372505e-06\n",
      "          vf_loss: 705.8833369255066\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 332000\n",
      "    num_steps_sampled: 332000\n",
      "    num_steps_trained: 332000\n",
      "  iterations_since_restore: 83\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.874074074074077\n",
      "    ram_util_percent: 29.67283950617284\n",
      "  pid: 17432\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08839616256298814\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.775996184675396\n",
      "    mean_inference_ms: 1.8360436739134744\n",
      "    mean_raw_obs_processing_ms: 6.4745006435669215\n",
      "  time_since_restore: 4822.077932357788\n",
      "  time_this_iter_s: 57.10566282272339\n",
      "  time_total_s: 4822.077932357788\n",
      "  timers:\n",
      "    learn_throughput: 154.266\n",
      "    learn_time_ms: 25929.254\n",
      "    sample_throughput: 125.266\n",
      "    sample_time_ms: 31931.947\n",
      "    update_time_ms: 3.298\n",
      "  timestamp: 1672612800\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 332000\n",
      "  training_iteration: 83\n",
      "  trial_id: f9c59_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Memory usage on this node: 9.2/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.87 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_5_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_2_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_4_6417799359d4e2284884fd3ae2ec9735, 0.0/6.0 CPU_group_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_3_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_1_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_0_6417799359d4e2284884fd3ae2ec9735)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                               | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridPOEnv-v0_f9c59_00000 | RUNNING  | 192.168.1.65:17432 |     83 |          4822.08 | 332000 | -219.543 |             -190.502 |             -395.508 |                400 |\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridPOEnv-v0_f9c59_00000:\n",
      "  agent_timesteps_total: 336000\n",
      "  custom_metrics: {}\n",
      "  date: 2023-01-01_16-40-58\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -188.45247696921453\n",
      "  episode_reward_mean: -219.61329987858176\n",
      "  episode_reward_min: -395.50797583825494\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 840\n",
      "  experiment_id: 610b8b5226964181899719d25df7617e\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.7763568394002503e-16\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.1915311999619007\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008788862797325724\n",
      "          policy_loss: -0.00031291738123400136\n",
      "          total_loss: 686.395431137085\n",
      "          vf_explained_var: 3.020651547558373e-06\n",
      "          vf_loss: 686.3957483291626\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 336000\n",
      "    num_steps_sampled: 336000\n",
      "    num_steps_trained: 336000\n",
      "  iterations_since_restore: 84\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.71219512195122\n",
      "    ram_util_percent: 29.669512195121946\n",
      "  pid: 17432\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08839484656459011\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.774725350936315\n",
      "    mean_inference_ms: 1.835929758525539\n",
      "    mean_raw_obs_processing_ms: 6.468938587770904\n",
      "  time_since_restore: 4879.311360359192\n",
      "  time_this_iter_s: 57.23342800140381\n",
      "  time_total_s: 4879.311360359192\n",
      "  timers:\n",
      "    learn_throughput: 154.687\n",
      "    learn_time_ms: 25858.68\n",
      "    sample_throughput: 125.26\n",
      "    sample_time_ms: 31933.628\n",
      "    update_time_ms: 3.27\n",
      "  timestamp: 1672612858\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 336000\n",
      "  training_iteration: 84\n",
      "  trial_id: f9c59_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 9.2/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.87 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_2_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_3_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_5_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_4_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_1_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 accelerator_type:G, 0.0/6.0 CPU_group_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_0_6417799359d4e2284884fd3ae2ec9735)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                               | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridPOEnv-v0_f9c59_00000 | RUNNING  | 192.168.1.65:17432 |     84 |          4879.31 | 336000 | -219.613 |             -188.452 |             -395.508 |                400 |\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridPOEnv-v0_f9c59_00000:\n",
      "  agent_timesteps_total: 340000\n",
      "  custom_metrics: {}\n",
      "  date: 2023-01-01_16-41-55\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -188.45247696921453\n",
      "  episode_reward_mean: -220.75169341436882\n",
      "  episode_reward_min: -395.50797583825494\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 850\n",
      "  experiment_id: 610b8b5226964181899719d25df7617e\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 8.881784197001252e-17\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.196737587824464\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0095343296019566\n",
      "          policy_loss: -6.229484279174358e-06\n",
      "          total_loss: 648.9025332450867\n",
      "          vf_explained_var: 1.4149770322546829e-05\n",
      "          vf_loss: 648.9025411128998\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 340000\n",
      "    num_steps_sampled: 340000\n",
      "    num_steps_trained: 340000\n",
      "  iterations_since_restore: 85\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.93048780487805\n",
      "    ram_util_percent: 29.7109756097561\n",
      "  pid: 17432\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08839546954792515\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.773189109278647\n",
      "    mean_inference_ms: 1.8358651185379307\n",
      "    mean_raw_obs_processing_ms: 6.463462572360611\n",
      "  time_since_restore: 4936.601147651672\n",
      "  time_this_iter_s: 57.28978729248047\n",
      "  time_total_s: 4936.601147651672\n",
      "  timers:\n",
      "    learn_throughput: 155.02\n",
      "    learn_time_ms: 25803.114\n",
      "    sample_throughput: 125.338\n",
      "    sample_time_ms: 31913.799\n",
      "    update_time_ms: 3.271\n",
      "  timestamp: 1672612915\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 340000\n",
      "  training_iteration: 85\n",
      "  trial_id: f9c59_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 9.3/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.87 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_0_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_1_6417799359d4e2284884fd3ae2ec9735, 0.0/6.0 CPU_group_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_4_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_5_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_2_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_3_6417799359d4e2284884fd3ae2ec9735)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                               | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridPOEnv-v0_f9c59_00000 | RUNNING  | 192.168.1.65:17432 |     85 |           4936.6 | 340000 | -220.752 |             -188.452 |             -395.508 |                400 |\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_TrafficLightGridPOEnv-v0_f9c59_00000:\n",
      "  agent_timesteps_total: 344000\n",
      "  custom_metrics: {}\n",
      "  date: 2023-01-01_16-42-53\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -188.41755881943453\n",
      "  episode_reward_mean: -220.26723418893576\n",
      "  episode_reward_min: -395.50797583825494\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 860\n",
      "  experiment_id: 610b8b5226964181899719d25df7617e\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 4.440892098500626e-17\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.1398460131138564\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011477865909901883\n",
      "          policy_loss: -0.0022818356126663277\n",
      "          total_loss: 674.5455276966095\n",
      "          vf_explained_var: 1.2910179975733627e-05\n",
      "          vf_loss: 674.5478140830994\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 344000\n",
      "    num_steps_sampled: 344000\n",
      "    num_steps_trained: 344000\n",
      "  iterations_since_restore: 86\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.510843373493977\n",
      "    ram_util_percent: 29.732530120481936\n",
      "  pid: 17432\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08839557470135131\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.77178567526315\n",
      "    mean_inference_ms: 1.835779173695164\n",
      "    mean_raw_obs_processing_ms: 6.458200976004405\n",
      "  time_since_restore: 4994.266344547272\n",
      "  time_this_iter_s: 57.665196895599365\n",
      "  time_total_s: 4994.266344547272\n",
      "  timers:\n",
      "    learn_throughput: 155.205\n",
      "    learn_time_ms: 25772.382\n",
      "    sample_throughput: 125.274\n",
      "    sample_time_ms: 31929.953\n",
      "    update_time_ms: 3.27\n",
      "  timestamp: 1672612973\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 344000\n",
      "  training_iteration: 86\n",
      "  trial_id: f9c59_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 9.3/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.87 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_5_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_3_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_2_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_0_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_1_6417799359d4e2284884fd3ae2ec9735, 0.0/6.0 CPU_group_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_4_6417799359d4e2284884fd3ae2ec9735)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                               | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridPOEnv-v0_f9c59_00000 | RUNNING  | 192.168.1.65:17432 |     86 |          4994.27 | 344000 | -220.267 |             -188.418 |             -395.508 |                400 |\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridPOEnv-v0_f9c59_00000:\n",
      "  agent_timesteps_total: 348000\n",
      "  custom_metrics: {}\n",
      "  date: 2023-01-01_16-43-51\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -187.65568518856554\n",
      "  episode_reward_mean: -219.94907569978722\n",
      "  episode_reward_min: -395.50797583825494\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 870\n",
      "  experiment_id: 610b8b5226964181899719d25df7617e\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 4.440892098500626e-17\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.0908934917300939\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012062698713620677\n",
      "          policy_loss: 7.880300836404786e-05\n",
      "          total_loss: 660.962451505661\n",
      "          vf_explained_var: 5.440227596409386e-06\n",
      "          vf_loss: 660.962375164032\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 348000\n",
      "    num_steps_sampled: 348000\n",
      "    num_steps_trained: 348000\n",
      "  iterations_since_restore: 87\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.809756097560978\n",
      "    ram_util_percent: 29.929268292682924\n",
      "  pid: 17432\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08839755327693936\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.76982956399138\n",
      "    mean_inference_ms: 1.8357304441383884\n",
      "    mean_raw_obs_processing_ms: 6.453094726598379\n",
      "  time_since_restore: 5052.031193494797\n",
      "  time_this_iter_s: 57.764848947525024\n",
      "  time_total_s: 5052.031193494797\n",
      "  timers:\n",
      "    learn_throughput: 155.436\n",
      "    learn_time_ms: 25734.025\n",
      "    sample_throughput: 125.385\n",
      "    sample_time_ms: 31901.806\n",
      "    update_time_ms: 3.229\n",
      "  timestamp: 1672613031\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 348000\n",
      "  training_iteration: 87\n",
      "  trial_id: f9c59_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 9.3/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.87 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_1_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_5_6417799359d4e2284884fd3ae2ec9735, 0.0/6.0 CPU_group_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_3_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_0_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_4_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_2_6417799359d4e2284884fd3ae2ec9735)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                               | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridPOEnv-v0_f9c59_00000 | RUNNING  | 192.168.1.65:17432 |     87 |          5052.03 | 348000 | -219.949 |             -187.656 |             -395.508 |                400 |\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridPOEnv-v0_f9c59_00000:\n",
      "  agent_timesteps_total: 352000\n",
      "  custom_metrics: {}\n",
      "  date: 2023-01-01_16-44-49\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -187.65568518856554\n",
      "  episode_reward_mean: -217.73073637641608\n",
      "  episode_reward_min: -328.02084166575446\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 880\n",
      "  experiment_id: 610b8b5226964181899719d25df7617e\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 4.440892098500626e-17\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.9697831792756915\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012463286896434108\n",
      "          policy_loss: 4.034033918287605e-06\n",
      "          total_loss: 647.1716136455536\n",
      "          vf_explained_var: 8.712522685527802e-06\n",
      "          vf_loss: 647.1716100215912\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 352000\n",
      "    num_steps_sampled: 352000\n",
      "    num_steps_trained: 352000\n",
      "  iterations_since_restore: 88\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.44404761904762\n",
      "    ram_util_percent: 29.966666666666665\n",
      "  pid: 17432\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0884019081208319\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.768011125984923\n",
      "    mean_inference_ms: 1.8357577025568248\n",
      "    mean_raw_obs_processing_ms: 6.4480822551274946\n",
      "  time_since_restore: 5110.5031316280365\n",
      "  time_this_iter_s: 58.471938133239746\n",
      "  time_total_s: 5110.5031316280365\n",
      "  timers:\n",
      "    learn_throughput: 155.37\n",
      "    learn_time_ms: 25745.025\n",
      "    sample_throughput: 125.353\n",
      "    sample_time_ms: 31909.99\n",
      "    update_time_ms: 3.25\n",
      "  timestamp: 1672613089\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 352000\n",
      "  training_iteration: 88\n",
      "  trial_id: f9c59_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Memory usage on this node: 9.3/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.87 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_2_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_1_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_5_6417799359d4e2284884fd3ae2ec9735, 0.0/6.0 CPU_group_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_4_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_3_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_0_6417799359d4e2284884fd3ae2ec9735)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                               | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridPOEnv-v0_f9c59_00000 | RUNNING  | 192.168.1.65:17432 |     88 |           5110.5 | 352000 | -217.731 |             -187.656 |             -328.021 |                400 |\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridPOEnv-v0_f9c59_00000:\n",
      "  agent_timesteps_total: 356000\n",
      "  custom_metrics: {}\n",
      "  date: 2023-01-01_16-45-48\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -185.62494092293736\n",
      "  episode_reward_mean: -214.39607625467022\n",
      "  episode_reward_min: -241.29814500966654\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 890\n",
      "  experiment_id: 610b8b5226964181899719d25df7617e\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 4.440892098500626e-17\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.9409431660547852\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012347700208782501\n",
      "          policy_loss: -0.004164321810822003\n",
      "          total_loss: 633.0227085590362\n",
      "          vf_explained_var: -1.1511146880138767e-07\n",
      "          vf_loss: 633.0268731594085\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 356000\n",
      "    num_steps_sampled: 356000\n",
      "    num_steps_trained: 356000\n",
      "  iterations_since_restore: 89\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.483132530120482\n",
      "    ram_util_percent: 30.028915662650604\n",
      "  pid: 17432\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0884055388536878\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.766200799506336\n",
      "    mean_inference_ms: 1.8358059390587338\n",
      "    mean_raw_obs_processing_ms: 6.443201806118309\n",
      "  time_since_restore: 5168.681695222855\n",
      "  time_this_iter_s: 58.178563594818115\n",
      "  time_total_s: 5168.681695222855\n",
      "  timers:\n",
      "    learn_throughput: 155.395\n",
      "    learn_time_ms: 25740.805\n",
      "    sample_throughput: 125.313\n",
      "    sample_time_ms: 31920.038\n",
      "    update_time_ms: 3.271\n",
      "  timestamp: 1672613148\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 356000\n",
      "  training_iteration: 89\n",
      "  trial_id: f9c59_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 9.4/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.87 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_0_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_3_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_5_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_2_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_1_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_4_6417799359d4e2284884fd3ae2ec9735, 0.0/6.0 CPU_group_6417799359d4e2284884fd3ae2ec9735)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                               | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridPOEnv-v0_f9c59_00000 | RUNNING  | 192.168.1.65:17432 |     89 |          5168.68 | 356000 | -214.396 |             -185.625 |             -241.298 |                400 |\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridPOEnv-v0_f9c59_00000:\n",
      "  agent_timesteps_total: 360000\n",
      "  custom_metrics: {}\n",
      "  date: 2023-01-01_16-46-46\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -185.61719042683666\n",
      "  episode_reward_mean: -211.57487806138192\n",
      "  episode_reward_min: -241.24804338359957\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 900\n",
      "  experiment_id: 610b8b5226964181899719d25df7617e\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 4.440892098500626e-17\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.7477348625659943\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.024625439488375034\n",
      "          policy_loss: -0.00588833573856391\n",
      "          total_loss: 627.1503927230835\n",
      "          vf_explained_var: -1.4318153489512042e-06\n",
      "          vf_loss: 627.1562816143036\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 360000\n",
      "    num_steps_sampled: 360000\n",
      "    num_steps_trained: 360000\n",
      "  iterations_since_restore: 90\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.715853658536584\n",
      "    ram_util_percent: 30.065853658536586\n",
      "  pid: 17432\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08840790516236158\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.76450459348044\n",
      "    mean_inference_ms: 1.835840268189047\n",
      "    mean_raw_obs_processing_ms: 6.438397140596467\n",
      "  time_since_restore: 5226.45148897171\n",
      "  time_this_iter_s: 57.76979374885559\n",
      "  time_total_s: 5226.45148897171\n",
      "  timers:\n",
      "    learn_throughput: 155.392\n",
      "    learn_time_ms: 25741.393\n",
      "    sample_throughput: 125.323\n",
      "    sample_time_ms: 31917.4\n",
      "    update_time_ms: 3.276\n",
      "  timestamp: 1672613206\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 360000\n",
      "  training_iteration: 90\n",
      "  trial_id: f9c59_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 9.4/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.87 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_1_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_3_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_4_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 accelerator_type:G, 0.0/6.0 CPU_group_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_0_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_5_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_2_6417799359d4e2284884fd3ae2ec9735)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                               | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridPOEnv-v0_f9c59_00000 | RUNNING  | 192.168.1.65:17432 |     90 |          5226.45 | 360000 | -211.575 |             -185.617 |             -241.248 |                400 |\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_TrafficLightGridPOEnv-v0_f9c59_00000:\n",
      "  agent_timesteps_total: 364000\n",
      "  custom_metrics: {}\n",
      "  date: 2023-01-01_16-47-43\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -185.0143396881673\n",
      "  episode_reward_mean: -208.15312713847644\n",
      "  episode_reward_min: -241.24804338359957\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 910\n",
      "  experiment_id: 610b8b5226964181899719d25df7617e\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 4.440892098500626e-17\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.6549114940688014\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016953827355996615\n",
      "          policy_loss: -0.005962740414543077\n",
      "          total_loss: 608.5403343200684\n",
      "          vf_explained_var: 2.8237701599209686e-07\n",
      "          vf_loss: 608.5462985515594\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 364000\n",
      "    num_steps_sampled: 364000\n",
      "    num_steps_trained: 364000\n",
      "  iterations_since_restore: 91\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.756097560975608\n",
      "    ram_util_percent: 30.16585365853659\n",
      "  pid: 17432\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08840975958620181\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.763342024513825\n",
      "    mean_inference_ms: 1.8359262653393764\n",
      "    mean_raw_obs_processing_ms: 6.433722621039036\n",
      "  time_since_restore: 5283.829202413559\n",
      "  time_this_iter_s: 57.377713441848755\n",
      "  time_total_s: 5283.829202413559\n",
      "  timers:\n",
      "    learn_throughput: 155.949\n",
      "    learn_time_ms: 25649.331\n",
      "    sample_throughput: 125.18\n",
      "    sample_time_ms: 31953.95\n",
      "    update_time_ms: 3.269\n",
      "  timestamp: 1672613263\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 364000\n",
      "  training_iteration: 91\n",
      "  trial_id: f9c59_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 9.4/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.87 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_2_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_5_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_1_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_4_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_0_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_3_6417799359d4e2284884fd3ae2ec9735, 0.0/6.0 CPU_group_6417799359d4e2284884fd3ae2ec9735)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                               | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridPOEnv-v0_f9c59_00000 | RUNNING  | 192.168.1.65:17432 |     91 |          5283.83 | 364000 | -208.153 |             -185.014 |             -241.248 |                400 |\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridPOEnv-v0_f9c59_00000:\n",
      "  agent_timesteps_total: 368000\n",
      "  custom_metrics: {}\n",
      "  date: 2023-01-01_16-48-41\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -185.0143396881673\n",
      "  episode_reward_mean: -204.66708618675406\n",
      "  episode_reward_min: -241.24804338359957\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 920\n",
      "  experiment_id: 610b8b5226964181899719d25df7617e\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 4.440892098500626e-17\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.5439459727145731\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015343985644745105\n",
      "          policy_loss: -0.001882042900979286\n",
      "          total_loss: 592.5538495540619\n",
      "          vf_explained_var: 7.456168304997846e-07\n",
      "          vf_loss: 592.555730009079\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 368000\n",
      "    num_steps_sampled: 368000\n",
      "    num_steps_trained: 368000\n",
      "  iterations_since_restore: 92\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.73734939759036\n",
      "    ram_util_percent: 30.151807228915658\n",
      "  pid: 17432\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0884108332999752\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.762710161138898\n",
      "    mean_inference_ms: 1.8360216562681355\n",
      "    mean_raw_obs_processing_ms: 6.429028408910649\n",
      "  time_since_restore: 5341.477015018463\n",
      "  time_this_iter_s: 57.647812604904175\n",
      "  time_total_s: 5341.477015018463\n",
      "  timers:\n",
      "    learn_throughput: 155.958\n",
      "    learn_time_ms: 25647.901\n",
      "    sample_throughput: 125.046\n",
      "    sample_time_ms: 31988.318\n",
      "    update_time_ms: 3.196\n",
      "  timestamp: 1672613321\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 368000\n",
      "  training_iteration: 92\n",
      "  trial_id: f9c59_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 9.4/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.87 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_4_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_5_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_0_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_3_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_1_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 accelerator_type:G, 0.0/6.0 CPU_group_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_2_6417799359d4e2284884fd3ae2ec9735)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                               | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridPOEnv-v0_f9c59_00000 | RUNNING  | 192.168.1.65:17432 |     92 |          5341.48 | 368000 | -204.667 |             -185.014 |             -241.248 |                400 |\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridPOEnv-v0_f9c59_00000:\n",
      "  agent_timesteps_total: 372000\n",
      "  custom_metrics: {}\n",
      "  date: 2023-01-01_16-49-39\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -185.0143396881673\n",
      "  episode_reward_mean: -201.71388745955838\n",
      "  episode_reward_min: -241.24804338359957\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 930\n",
      "  experiment_id: 610b8b5226964181899719d25df7617e\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 4.440892098500626e-17\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.5388438319787383\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0091798986146842\n",
      "          policy_loss: -0.0010488146406714804\n",
      "          total_loss: 583.3492542743683\n",
      "          vf_explained_var: 2.9373913434938004e-07\n",
      "          vf_loss: 583.3503014087677\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 372000\n",
      "    num_steps_sampled: 372000\n",
      "    num_steps_trained: 372000\n",
      "  iterations_since_restore: 93\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.663855421686748\n",
      "    ram_util_percent: 30.250602409638557\n",
      "  pid: 17432\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08841103385018076\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.762251586532212\n",
      "    mean_inference_ms: 1.8361342433826833\n",
      "    mean_raw_obs_processing_ms: 6.424505414119355\n",
      "  time_since_restore: 5399.705137252808\n",
      "  time_this_iter_s: 58.22812223434448\n",
      "  time_total_s: 5399.705137252808\n",
      "  timers:\n",
      "    learn_throughput: 155.438\n",
      "    learn_time_ms: 25733.684\n",
      "    sample_throughput: 124.942\n",
      "    sample_time_ms: 32014.733\n",
      "    update_time_ms: 3.295\n",
      "  timestamp: 1672613379\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 372000\n",
      "  training_iteration: 93\n",
      "  trial_id: f9c59_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Memory usage on this node: 9.4/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.87 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_0_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_5_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_3_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_2_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_1_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_4_6417799359d4e2284884fd3ae2ec9735, 0.0/6.0 CPU_group_6417799359d4e2284884fd3ae2ec9735)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                               | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridPOEnv-v0_f9c59_00000 | RUNNING  | 192.168.1.65:17432 |     93 |          5399.71 | 372000 | -201.714 |             -185.014 |             -241.248 |                400 |\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridPOEnv-v0_f9c59_00000:\n",
      "  agent_timesteps_total: 376000\n",
      "  custom_metrics: {}\n",
      "  date: 2023-01-01_16-50-37\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -185.0143396881673\n",
      "  episode_reward_mean: -198.7143219643907\n",
      "  episode_reward_min: -241.24804338359957\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 940\n",
      "  experiment_id: 610b8b5226964181899719d25df7617e\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.220446049250313e-17\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.5431632804684341\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.00878516648053852\n",
      "          policy_loss: -0.001809811768180225\n",
      "          total_loss: 573.3288701534271\n",
      "          vf_explained_var: 4.597008285145421e-07\n",
      "          vf_loss: 573.3306790351868\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 376000\n",
      "    num_steps_sampled: 376000\n",
      "    num_steps_trained: 376000\n",
      "  iterations_since_restore: 94\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.815853658536586\n",
      "    ram_util_percent: 30.326829268292684\n",
      "  pid: 17432\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08841025889990942\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.76234185503837\n",
      "    mean_inference_ms: 1.8363066764164109\n",
      "    mean_raw_obs_processing_ms: 6.42010473027103\n",
      "  time_since_restore: 5457.320538759232\n",
      "  time_this_iter_s: 57.61540150642395\n",
      "  time_total_s: 5457.320538759232\n",
      "  timers:\n",
      "    learn_throughput: 155.523\n",
      "    learn_time_ms: 25719.656\n",
      "    sample_throughput: 124.739\n",
      "    sample_time_ms: 32066.947\n",
      "    update_time_ms: 3.285\n",
      "  timestamp: 1672613437\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 376000\n",
      "  training_iteration: 94\n",
      "  trial_id: f9c59_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 9.5/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.87 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_0_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_1_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_3_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_4_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_5_6417799359d4e2284884fd3ae2ec9735, 0.0/6.0 CPU_group_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_2_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 accelerator_type:G)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                               | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridPOEnv-v0_f9c59_00000 | RUNNING  | 192.168.1.65:17432 |     94 |          5457.32 | 376000 | -198.714 |             -185.014 |             -241.248 |                400 |\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridPOEnv-v0_f9c59_00000:\n",
      "  agent_timesteps_total: 380000\n",
      "  custom_metrics: {}\n",
      "  date: 2023-01-01_16-51-35\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -185.0143396881673\n",
      "  episode_reward_mean: -194.7606846563766\n",
      "  episode_reward_min: -241.18069391965957\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 950\n",
      "  experiment_id: 610b8b5226964181899719d25df7617e\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.1102230246251564e-17\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.42342838579788805\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014831865670726137\n",
      "          policy_loss: -0.0005996119813062251\n",
      "          total_loss: 559.9957238197327\n",
      "          vf_explained_var: 1.3085268619761337e-05\n",
      "          vf_loss: 559.996331000328\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 380000\n",
      "    num_steps_sampled: 380000\n",
      "    num_steps_trained: 380000\n",
      "  iterations_since_restore: 95\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.60120481927711\n",
      "    ram_util_percent: 30.23614457831325\n",
      "  pid: 17432\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08840933052744003\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.762943539712513\n",
      "    mean_inference_ms: 1.8364468562368736\n",
      "    mean_raw_obs_processing_ms: 6.415832508161682\n",
      "  time_since_restore: 5515.52386879921\n",
      "  time_this_iter_s: 58.20333003997803\n",
      "  time_total_s: 5515.52386879921\n",
      "  timers:\n",
      "    learn_throughput: 155.24\n",
      "    learn_time_ms: 25766.523\n",
      "    sample_throughput: 124.567\n",
      "    sample_time_ms: 32111.35\n",
      "    update_time_ms: 3.305\n",
      "  timestamp: 1672613495\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 380000\n",
      "  training_iteration: 95\n",
      "  trial_id: f9c59_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 9.4/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.87 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_3_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_0_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_4_6417799359d4e2284884fd3ae2ec9735, 0.0/6.0 CPU_group_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_2_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_5_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_1_6417799359d4e2284884fd3ae2ec9735)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                               | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridPOEnv-v0_f9c59_00000 | RUNNING  | 192.168.1.65:17432 |     95 |          5515.52 | 380000 | -194.761 |             -185.014 |             -241.181 |                400 |\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_TrafficLightGridPOEnv-v0_f9c59_00000:\n",
      "  agent_timesteps_total: 384000\n",
      "  custom_metrics: {}\n",
      "  date: 2023-01-01_16-52-34\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -185.0143396881673\n",
      "  episode_reward_mean: -191.98712097237265\n",
      "  episode_reward_min: -240.75569364284144\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 960\n",
      "  experiment_id: 610b8b5226964181899719d25df7617e\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.1102230246251564e-17\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.2620579184032977\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014386094300295069\n",
      "          policy_loss: -0.001769257860723883\n",
      "          total_loss: 551.4431037425995\n",
      "          vf_explained_var: 3.1653792120778235e-06\n",
      "          vf_loss: 551.444871377945\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 384000\n",
      "    num_steps_sampled: 384000\n",
      "    num_steps_trained: 384000\n",
      "  iterations_since_restore: 96\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.340476190476192\n",
      "    ram_util_percent: 30.329761904761913\n",
      "  pid: 17432\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08840866177204443\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.764063957391798\n",
      "    mean_inference_ms: 1.836577752754246\n",
      "    mean_raw_obs_processing_ms: 6.411678122255082\n",
      "  time_since_restore: 5573.973001718521\n",
      "  time_this_iter_s: 58.44913291931152\n",
      "  time_total_s: 5573.973001718521\n",
      "  timers:\n",
      "    learn_throughput: 154.977\n",
      "    learn_time_ms: 25810.356\n",
      "    sample_throughput: 124.432\n",
      "    sample_time_ms: 32145.944\n",
      "    update_time_ms: 3.262\n",
      "  timestamp: 1672613554\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 384000\n",
      "  training_iteration: 96\n",
      "  trial_id: f9c59_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 9.4/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.87 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_0_6417799359d4e2284884fd3ae2ec9735, 0.0/6.0 CPU_group_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_1_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_4_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_3_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_5_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_2_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 accelerator_type:G)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                               | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridPOEnv-v0_f9c59_00000 | RUNNING  | 192.168.1.65:17432 |     96 |          5573.97 | 384000 | -191.987 |             -185.014 |             -240.756 |                400 |\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=17433)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=17431)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=17430)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=17429)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=17428)\u001b[0m  Retrying in 1 seconds\n",
      "Result for PPO_TrafficLightGridPOEnv-v0_f9c59_00000:\n",
      "  agent_timesteps_total: 388000\n",
      "  custom_metrics: {}\n",
      "  date: 2023-01-01_16-53-33\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -185.0143396881673\n",
      "  episode_reward_mean: -189.45323819322374\n",
      "  episode_reward_min: -218.44016746182947\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 970\n",
      "  experiment_id: 610b8b5226964181899719d25df7617e\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.1102230246251564e-17\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.15395986044313759\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01488612119464965\n",
      "          policy_loss: -0.0016182105697225778\n",
      "          total_loss: 549.3358909606934\n",
      "          vf_explained_var: 6.506219278890057e-07\n",
      "          vf_loss: 549.337512588501\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 388000\n",
      "    num_steps_sampled: 388000\n",
      "    num_steps_trained: 388000\n",
      "  iterations_since_restore: 97\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.392857142857146\n",
      "    ram_util_percent: 29.490476190476194\n",
      "  pid: 17432\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0884067091955751\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.765457124933235\n",
      "    mean_inference_ms: 1.8366367276029518\n",
      "    mean_raw_obs_processing_ms: 6.408927981825145\n",
      "  time_since_restore: 5632.978781700134\n",
      "  time_this_iter_s: 59.00577998161316\n",
      "  time_total_s: 5632.978781700134\n",
      "  timers:\n",
      "    learn_throughput: 154.972\n",
      "    learn_time_ms: 25811.155\n",
      "    sample_throughput: 123.957\n",
      "    sample_time_ms: 32269.25\n",
      "    update_time_ms: 3.266\n",
      "  timestamp: 1672613613\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 388000\n",
      "  training_iteration: 97\n",
      "  trial_id: f9c59_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 9.2/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.87 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_3_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_1_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_4_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_5_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_0_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_2_6417799359d4e2284884fd3ae2ec9735, 0.0/6.0 CPU_group_6417799359d4e2284884fd3ae2ec9735)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                               | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridPOEnv-v0_f9c59_00000 | RUNNING  | 192.168.1.65:17432 |     97 |          5632.98 | 388000 | -189.453 |             -185.014 |              -218.44 |                400 |\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridPOEnv-v0_f9c59_00000:\n",
      "  agent_timesteps_total: 392000\n",
      "  custom_metrics: {}\n",
      "  date: 2023-01-01_16-54-31\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -185.0143396881673\n",
      "  episode_reward_mean: -187.4776865140958\n",
      "  episode_reward_min: -215.7403874174877\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 980\n",
      "  experiment_id: 610b8b5226964181899719d25df7617e\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.1102230246251564e-17\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.10551220712950453\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01216970781123341\n",
      "          policy_loss: -0.0005932501022471115\n",
      "          total_loss: 534.8549998521805\n",
      "          vf_explained_var: 7.320195436477661e-07\n",
      "          vf_loss: 534.8555924654007\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 392000\n",
      "    num_steps_sampled: 392000\n",
      "    num_steps_trained: 392000\n",
      "  iterations_since_restore: 98\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.50722891566265\n",
      "    ram_util_percent: 29.379518072289155\n",
      "  pid: 17432\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08840360069458773\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.766233670822967\n",
      "    mean_inference_ms: 1.836682453611436\n",
      "    mean_raw_obs_processing_ms: 6.406186693098781\n",
      "  time_since_restore: 5690.632318735123\n",
      "  time_this_iter_s: 57.6535370349884\n",
      "  time_total_s: 5690.632318735123\n",
      "  timers:\n",
      "    learn_throughput: 155.164\n",
      "    learn_time_ms: 25779.174\n",
      "    sample_throughput: 124.148\n",
      "    sample_time_ms: 32219.505\n",
      "    update_time_ms: 3.24\n",
      "  timestamp: 1672613671\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 392000\n",
      "  training_iteration: 98\n",
      "  trial_id: f9c59_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Memory usage on this node: 9.2/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.87 GiB heap, 0.0/14.65 GiB objects (0.0/6.0 CPU_group_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_3_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_2_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_4_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_1_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_0_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_5_6417799359d4e2284884fd3ae2ec9735)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                               | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridPOEnv-v0_f9c59_00000 | RUNNING  | 192.168.1.65:17432 |     98 |          5690.63 | 392000 | -187.478 |             -185.014 |              -215.74 |                400 |\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridPOEnv-v0_f9c59_00000:\n",
      "  agent_timesteps_total: 396000\n",
      "  custom_metrics: {}\n",
      "  date: 2023-01-01_16-55-29\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -185.0143396881673\n",
      "  episode_reward_mean: -186.96170758008464\n",
      "  episode_reward_min: -211.5492221082095\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 990\n",
      "  experiment_id: 610b8b5226964181899719d25df7617e\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.1102230246251564e-17\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.11039130738936365\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01628768556458917\n",
      "          policy_loss: -0.004006904021662194\n",
      "          total_loss: 530.0329823017121\n",
      "          vf_explained_var: 5.737878382205963e-06\n",
      "          vf_loss: 530.0369903564454\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 396000\n",
      "    num_steps_sampled: 396000\n",
      "    num_steps_trained: 396000\n",
      "  iterations_since_restore: 99\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.75060240963855\n",
      "    ram_util_percent: 29.46024096385542\n",
      "  pid: 17432\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08840168208124165\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.766923425308406\n",
      "    mean_inference_ms: 1.8367686703250072\n",
      "    mean_raw_obs_processing_ms: 6.403547842104697\n",
      "  time_since_restore: 5748.995930194855\n",
      "  time_this_iter_s: 58.363611459732056\n",
      "  time_total_s: 5748.995930194855\n",
      "  timers:\n",
      "    learn_throughput: 155.052\n",
      "    learn_time_ms: 25797.718\n",
      "    sample_throughput: 124.149\n",
      "    sample_time_ms: 32219.463\n",
      "    update_time_ms: 3.229\n",
      "  timestamp: 1672613729\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 396000\n",
      "  training_iteration: 99\n",
      "  trial_id: f9c59_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 9.2/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.87 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_1_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_5_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_2_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_4_6417799359d4e2284884fd3ae2ec9735, 0.0/6.0 CPU_group_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_0_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_3_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 accelerator_type:G)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                               | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridPOEnv-v0_f9c59_00000 | RUNNING  | 192.168.1.65:17432 |     99 |             5749 | 396000 | -186.962 |             -185.014 |             -211.549 |                400 |\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridPOEnv-v0_f9c59_00000:\n",
      "  agent_timesteps_total: 400000\n",
      "  custom_metrics: {}\n",
      "  date: 2023-01-01_16-56-27\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -185.0143396881673\n",
      "  episode_reward_mean: -186.56901425995966\n",
      "  episode_reward_min: -191.54242767588863\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1000\n",
      "  experiment_id: 610b8b5226964181899719d25df7617e\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.1102230246251564e-17\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.0804860720585566\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014066965990150542\n",
      "          policy_loss: -0.00234727029746864\n",
      "          total_loss: 526.6487230062485\n",
      "          vf_explained_var: 1.1586957043618895e-05\n",
      "          vf_loss: 526.6510679960251\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 400000\n",
      "    num_steps_sampled: 400000\n",
      "    num_steps_trained: 400000\n",
      "  iterations_since_restore: 100\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.783132530120483\n",
      "    ram_util_percent: 29.53975903614457\n",
      "  pid: 17432\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08840152571360818\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.76817153242171\n",
      "    mean_inference_ms: 1.8368442266712766\n",
      "    mean_raw_obs_processing_ms: 6.401136726757666\n",
      "  time_since_restore: 5806.757867336273\n",
      "  time_this_iter_s: 57.76193714141846\n",
      "  time_total_s: 5806.757867336273\n",
      "  timers:\n",
      "    learn_throughput: 155.439\n",
      "    learn_time_ms: 25733.639\n",
      "    sample_throughput: 123.905\n",
      "    sample_time_ms: 32282.719\n",
      "    update_time_ms: 3.222\n",
      "  timestamp: 1672613787\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 400000\n",
      "  training_iteration: 100\n",
      "  trial_id: f9c59_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 9.2/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.87 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_1_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_5_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_3_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_4_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_2_6417799359d4e2284884fd3ae2ec9735, 0.0/6.0 CPU_group_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_0_6417799359d4e2284884fd3ae2ec9735)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                               | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridPOEnv-v0_f9c59_00000 | RUNNING  | 192.168.1.65:17432 |    100 |          5806.76 | 400000 | -186.569 |             -185.014 |             -191.542 |                400 |\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_TrafficLightGridPOEnv-v0_f9c59_00000:\n",
      "  agent_timesteps_total: 404000\n",
      "  custom_metrics: {}\n",
      "  date: 2023-01-01_16-57-25\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -185.5357896982804\n",
      "  episode_reward_mean: -186.5203514336758\n",
      "  episode_reward_min: -191.54242767588863\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1010\n",
      "  experiment_id: 610b8b5226964181899719d25df7617e\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.1102230246251564e-17\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -0.05396245272131637\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01647202161730122\n",
      "          policy_loss: -0.0023598351239343175\n",
      "          total_loss: 525.8324508666992\n",
      "          vf_explained_var: 7.1320682764053345e-06\n",
      "          vf_loss: 525.8348111629487\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 404000\n",
      "    num_steps_sampled: 404000\n",
      "    num_steps_trained: 404000\n",
      "  iterations_since_restore: 101\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.56144578313253\n",
      "    ram_util_percent: 29.50722891566265\n",
      "  pid: 17432\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08840262485480381\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.769897799011577\n",
      "    mean_inference_ms: 1.8369007435194589\n",
      "    mean_raw_obs_processing_ms: 6.398819380256368\n",
      "  time_since_restore: 5864.798890829086\n",
      "  time_this_iter_s: 58.04102349281311\n",
      "  time_total_s: 5864.798890829086\n",
      "  timers:\n",
      "    learn_throughput: 155.32\n",
      "    learn_time_ms: 25753.319\n",
      "    sample_throughput: 123.727\n",
      "    sample_time_ms: 32329.337\n",
      "    update_time_ms: 3.25\n",
      "  timestamp: 1672613845\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 404000\n",
      "  training_iteration: 101\n",
      "  trial_id: f9c59_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 9.2/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.87 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_0_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_5_6417799359d4e2284884fd3ae2ec9735, 0.0/6.0 CPU_group_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_4_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_2_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_3_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_1_6417799359d4e2284884fd3ae2ec9735)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                               | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridPOEnv-v0_f9c59_00000 | RUNNING  | 192.168.1.65:17432 |    101 |           5864.8 | 404000 |  -186.52 |             -185.536 |             -191.542 |                400 |\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridPOEnv-v0_f9c59_00000:\n",
      "  agent_timesteps_total: 408000\n",
      "  custom_metrics: {}\n",
      "  date: 2023-01-01_16-58-23\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -185.5357896982804\n",
      "  episode_reward_mean: -186.5724614846708\n",
      "  episode_reward_min: -192.9000575284058\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1020\n",
      "  experiment_id: 610b8b5226964181899719d25df7617e\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.1102230246251564e-17\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -0.1406958007020876\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011306482404416403\n",
      "          policy_loss: 0.0015709972954937256\n",
      "          total_loss: 513.9591911792755\n",
      "          vf_explained_var: 3.4086406230926514e-06\n",
      "          vf_loss: 513.9576209545136\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 408000\n",
      "    num_steps_sampled: 408000\n",
      "    num_steps_trained: 408000\n",
      "  iterations_since_restore: 102\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.85609756097561\n",
      "    ram_util_percent: 29.419512195121946\n",
      "  pid: 17432\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0884031944021139\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.77123295478035\n",
      "    mean_inference_ms: 1.836959071165694\n",
      "    mean_raw_obs_processing_ms: 6.396587620179195\n",
      "  time_since_restore: 5922.274939775467\n",
      "  time_this_iter_s: 57.476048946380615\n",
      "  time_total_s: 5922.274939775467\n",
      "  timers:\n",
      "    learn_throughput: 155.227\n",
      "    learn_time_ms: 25768.715\n",
      "    sample_throughput: 123.852\n",
      "    sample_time_ms: 32296.692\n",
      "    update_time_ms: 3.273\n",
      "  timestamp: 1672613903\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 408000\n",
      "  training_iteration: 102\n",
      "  trial_id: f9c59_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 9.2/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.87 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_4_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_0_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_3_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_1_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_2_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_5_6417799359d4e2284884fd3ae2ec9735, 0.0/6.0 CPU_group_6417799359d4e2284884fd3ae2ec9735)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                               | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridPOEnv-v0_f9c59_00000 | RUNNING  | 192.168.1.65:17432 |    102 |          5922.27 | 408000 | -186.572 |             -185.536 |               -192.9 |                400 |\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridPOEnv-v0_f9c59_00000:\n",
      "  agent_timesteps_total: 412000\n",
      "  custom_metrics: {}\n",
      "  date: 2023-01-01_16-59-20\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -184.744060933608\n",
      "  episode_reward_mean: -186.62938558958362\n",
      "  episode_reward_min: -192.9000575284058\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1030\n",
      "  experiment_id: 610b8b5226964181899719d25df7617e\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.1102230246251564e-17\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -0.2555527751799673\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014855617246621666\n",
      "          policy_loss: -0.0016564902747632005\n",
      "          total_loss: 514.1160861492157\n",
      "          vf_explained_var: -6.891787140972383e-09\n",
      "          vf_loss: 514.1177417755127\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 412000\n",
      "    num_steps_sampled: 412000\n",
      "    num_steps_trained: 412000\n",
      "  iterations_since_restore: 103\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.590243902439024\n",
      "    ram_util_percent: 29.490243902439023\n",
      "  pid: 17432\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08840379372368577\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.772164892500868\n",
      "    mean_inference_ms: 1.836987553983592\n",
      "    mean_raw_obs_processing_ms: 6.394449182408116\n",
      "  time_since_restore: 5979.927011013031\n",
      "  time_this_iter_s: 57.65207123756409\n",
      "  time_total_s: 5979.927011013031\n",
      "  timers:\n",
      "    learn_throughput: 155.33\n",
      "    learn_time_ms: 25751.626\n",
      "    sample_throughput: 124.007\n",
      "    sample_time_ms: 32256.23\n",
      "    update_time_ms: 3.163\n",
      "  timestamp: 1672613960\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 412000\n",
      "  training_iteration: 103\n",
      "  trial_id: f9c59_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Memory usage on this node: 9.2/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.87 GiB heap, 0.0/14.65 GiB objects (0.0/6.0 CPU_group_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_1_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_2_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_3_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_0_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_4_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_5_6417799359d4e2284884fd3ae2ec9735)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                               | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridPOEnv-v0_f9c59_00000 | RUNNING  | 192.168.1.65:17432 |    103 |          5979.93 | 412000 | -186.629 |             -184.744 |               -192.9 |                400 |\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridPOEnv-v0_f9c59_00000:\n",
      "  agent_timesteps_total: 416000\n",
      "  custom_metrics: {}\n",
      "  date: 2023-01-01_17-00-18\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -184.744060933608\n",
      "  episode_reward_mean: -186.70225310924502\n",
      "  episode_reward_min: -192.9000575284058\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1040\n",
      "  experiment_id: 610b8b5226964181899719d25df7617e\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.1102230246251564e-17\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -0.4600128673017025\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.02068073390953519\n",
      "          policy_loss: -0.002463487850036472\n",
      "          total_loss: 505.76691937446594\n",
      "          vf_explained_var: 4.891306275567331e-07\n",
      "          vf_loss: 505.7693834543228\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 416000\n",
      "    num_steps_sampled: 416000\n",
      "    num_steps_trained: 416000\n",
      "  iterations_since_restore: 104\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.425301204819277\n",
      "    ram_util_percent: 29.490361445783126\n",
      "  pid: 17432\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08840353958002488\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.77279439362954\n",
      "    mean_inference_ms: 1.8369008484936358\n",
      "    mean_raw_obs_processing_ms: 6.39233456010533\n",
      "  time_since_restore: 6037.774996757507\n",
      "  time_this_iter_s: 57.84798574447632\n",
      "  time_total_s: 6037.774996757507\n",
      "  timers:\n",
      "    learn_throughput: 154.961\n",
      "    learn_time_ms: 25812.933\n",
      "    sample_throughput: 124.154\n",
      "    sample_time_ms: 32218.157\n",
      "    update_time_ms: 3.196\n",
      "  timestamp: 1672614018\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 416000\n",
      "  training_iteration: 104\n",
      "  trial_id: f9c59_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 9.2/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.87 GiB heap, 0.0/14.65 GiB objects (0.0/6.0 CPU_group_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_2_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_3_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_5_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_4_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_0_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_1_6417799359d4e2284884fd3ae2ec9735)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                               | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridPOEnv-v0_f9c59_00000 | RUNNING  | 192.168.1.65:17432 |    104 |          6037.77 | 416000 | -186.702 |             -184.744 |               -192.9 |                400 |\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridPOEnv-v0_f9c59_00000:\n",
      "  agent_timesteps_total: 420000\n",
      "  custom_metrics: {}\n",
      "  date: 2023-01-01_17-01-16\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -184.744060933608\n",
      "  episode_reward_mean: -186.7407646765171\n",
      "  episode_reward_min: -192.9000575284058\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1050\n",
      "  experiment_id: 610b8b5226964181899719d25df7617e\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.1102230246251564e-17\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -0.49545236891135574\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011134648191273471\n",
      "          policy_loss: 0.002286404510959983\n",
      "          total_loss: 501.6593831539154\n",
      "          vf_explained_var: 2.008490355365211e-06\n",
      "          vf_loss: 501.65709443092345\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 420000\n",
      "    num_steps_sampled: 420000\n",
      "    num_steps_trained: 420000\n",
      "  iterations_since_restore: 105\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.778048780487808\n",
      "    ram_util_percent: 29.51341463414634\n",
      "  pid: 17432\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08840246714206768\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.773001405581823\n",
      "    mean_inference_ms: 1.8367725705763527\n",
      "    mean_raw_obs_processing_ms: 6.390250953774478\n",
      "  time_since_restore: 6095.509868621826\n",
      "  time_this_iter_s: 57.73487186431885\n",
      "  time_total_s: 6095.509868621826\n",
      "  timers:\n",
      "    learn_throughput: 154.942\n",
      "    learn_time_ms: 25816.097\n",
      "    sample_throughput: 124.346\n",
      "    sample_time_ms: 32168.234\n",
      "    update_time_ms: 3.17\n",
      "  timestamp: 1672614076\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 420000\n",
      "  training_iteration: 105\n",
      "  trial_id: f9c59_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 9.2/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.87 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_5_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_1_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_4_6417799359d4e2284884fd3ae2ec9735, 0.0/6.0 CPU_group_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_0_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_2_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_3_6417799359d4e2284884fd3ae2ec9735)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                               | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridPOEnv-v0_f9c59_00000 | RUNNING  | 192.168.1.65:17432 |    105 |          6095.51 | 420000 | -186.741 |             -184.744 |               -192.9 |                400 |\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_TrafficLightGridPOEnv-v0_f9c59_00000:\n",
      "  agent_timesteps_total: 424000\n",
      "  custom_metrics: {}\n",
      "  date: 2023-01-01_17-02-14\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -184.7082005947841\n",
      "  episode_reward_mean: -186.80595206320297\n",
      "  episode_reward_min: -192.9000575284058\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1060\n",
      "  experiment_id: 610b8b5226964181899719d25df7617e\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.1102230246251564e-17\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -0.550512824486941\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017368200496434914\n",
      "          policy_loss: -0.005638065522362012\n",
      "          total_loss: 505.5216746330261\n",
      "          vf_explained_var: 3.8202853147595306e-07\n",
      "          vf_loss: 505.5273108482361\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 424000\n",
      "    num_steps_sampled: 424000\n",
      "    num_steps_trained: 424000\n",
      "  iterations_since_restore: 106\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.71951219512195\n",
      "    ram_util_percent: 29.69146341463414\n",
      "  pid: 17432\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08840099159506627\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.772789757655847\n",
      "    mean_inference_ms: 1.8366709134958847\n",
      "    mean_raw_obs_processing_ms: 6.388204706433164\n",
      "  time_since_restore: 6152.695074558258\n",
      "  time_this_iter_s: 57.185205936431885\n",
      "  time_total_s: 6152.695074558258\n",
      "  timers:\n",
      "    learn_throughput: 155.513\n",
      "    learn_time_ms: 25721.274\n",
      "    sample_throughput: 124.468\n",
      "    sample_time_ms: 32136.702\n",
      "    update_time_ms: 3.215\n",
      "  timestamp: 1672614134\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 424000\n",
      "  training_iteration: 106\n",
      "  trial_id: f9c59_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 9.2/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.87 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_1_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_0_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_5_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_3_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_2_6417799359d4e2284884fd3ae2ec9735, 0.0/6.0 CPU_group_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_4_6417799359d4e2284884fd3ae2ec9735)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                               | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridPOEnv-v0_f9c59_00000 | RUNNING  | 192.168.1.65:17432 |    106 |           6152.7 | 424000 | -186.806 |             -184.708 |               -192.9 |                400 |\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridPOEnv-v0_f9c59_00000:\n",
      "  agent_timesteps_total: 428000\n",
      "  custom_metrics: {}\n",
      "  date: 2023-01-01_17-03-12\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -183.51439346459816\n",
      "  episode_reward_mean: -186.71202819412167\n",
      "  episode_reward_min: -192.9000575284058\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1070\n",
      "  experiment_id: 610b8b5226964181899719d25df7617e\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.1102230246251564e-17\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -0.5883316926658153\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01188431177788522\n",
      "          policy_loss: 0.003223573922878131\n",
      "          total_loss: 496.12834293842315\n",
      "          vf_explained_var: 1.6405432688770816e-05\n",
      "          vf_loss: 496.12512001991274\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 428000\n",
      "    num_steps_sampled: 428000\n",
      "    num_steps_trained: 428000\n",
      "  iterations_since_restore: 107\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.662650602409638\n",
      "    ram_util_percent: 29.57831325301205\n",
      "  pid: 17432\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08839939267385889\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.772111363756295\n",
      "    mean_inference_ms: 1.8366029821378658\n",
      "    mean_raw_obs_processing_ms: 6.384921345793925\n",
      "  time_since_restore: 6210.804334640503\n",
      "  time_this_iter_s: 58.10926008224487\n",
      "  time_total_s: 6210.804334640503\n",
      "  timers:\n",
      "    learn_throughput: 155.179\n",
      "    learn_time_ms: 25776.615\n",
      "    sample_throughput: 125.033\n",
      "    sample_time_ms: 31991.644\n",
      "    update_time_ms: 3.214\n",
      "  timestamp: 1672614192\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 428000\n",
      "  training_iteration: 107\n",
      "  trial_id: f9c59_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 9.2/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.87 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_5_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_0_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_4_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_1_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_3_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 accelerator_type:G, 0.0/6.0 CPU_group_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_2_6417799359d4e2284884fd3ae2ec9735)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                               | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridPOEnv-v0_f9c59_00000 | RUNNING  | 192.168.1.65:17432 |    107 |           6210.8 | 428000 | -186.712 |             -183.514 |               -192.9 |                400 |\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridPOEnv-v0_f9c59_00000:\n",
      "  agent_timesteps_total: 432000\n",
      "  custom_metrics: {}\n",
      "  date: 2023-01-01_17-04-09\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -183.51439346459816\n",
      "  episode_reward_mean: -186.70032580745774\n",
      "  episode_reward_min: -192.9000575284058\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1080\n",
      "  experiment_id: 610b8b5226964181899719d25df7617e\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.1102230246251564e-17\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -0.6388143623247743\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009334763902302425\n",
      "          policy_loss: 0.0007963256910443306\n",
      "          total_loss: 498.8180895090103\n",
      "          vf_explained_var: 7.076188808241568e-07\n",
      "          vf_loss: 498.8172972202301\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 432000\n",
      "    num_steps_sampled: 432000\n",
      "    num_steps_trained: 432000\n",
      "  iterations_since_restore: 108\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.78536585365854\n",
      "    ram_util_percent: 29.6609756097561\n",
      "  pid: 17432\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08839844290662029\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.771439725343512\n",
      "    mean_inference_ms: 1.8365278967784184\n",
      "    mean_raw_obs_processing_ms: 6.381708774369692\n",
      "  time_since_restore: 6268.096026182175\n",
      "  time_this_iter_s: 57.29169154167175\n",
      "  time_total_s: 6268.096026182175\n",
      "  timers:\n",
      "    learn_throughput: 155.369\n",
      "    learn_time_ms: 25745.084\n",
      "    sample_throughput: 125.051\n",
      "    sample_time_ms: 31986.956\n",
      "    update_time_ms: 3.2\n",
      "  timestamp: 1672614249\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 432000\n",
      "  training_iteration: 108\n",
      "  trial_id: f9c59_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Memory usage on this node: 9.3/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.87 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_2_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_0_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_4_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_5_6417799359d4e2284884fd3ae2ec9735, 0.0/6.0 CPU_group_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_3_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_1_6417799359d4e2284884fd3ae2ec9735)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                               | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridPOEnv-v0_f9c59_00000 | RUNNING  | 192.168.1.65:17432 |    108 |           6268.1 | 432000 |   -186.7 |             -183.514 |               -192.9 |                400 |\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridPOEnv-v0_f9c59_00000:\n",
      "  agent_timesteps_total: 436000\n",
      "  custom_metrics: {}\n",
      "  date: 2023-01-01_17-05-08\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -183.3133996585258\n",
      "  episode_reward_mean: -186.690425504665\n",
      "  episode_reward_min: -192.9000575284058\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1090\n",
      "  experiment_id: 610b8b5226964181899719d25df7617e\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 5.551115123125782e-18\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -0.7132511392235756\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008324899725630596\n",
      "          policy_loss: 0.0027954297584074084\n",
      "          total_loss: 495.9500509262085\n",
      "          vf_explained_var: 9.790807780518662e-06\n",
      "          vf_loss: 495.94725980758665\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 436000\n",
      "    num_steps_sampled: 436000\n",
      "    num_steps_trained: 436000\n",
      "  iterations_since_restore: 109\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.65060240963855\n",
      "    ram_util_percent: 29.763855421686753\n",
      "  pid: 17432\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08839745000283954\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.77056050835248\n",
      "    mean_inference_ms: 1.8364055110953688\n",
      "    mean_raw_obs_processing_ms: 6.3784489775438695\n",
      "  time_since_restore: 6326.386046409607\n",
      "  time_this_iter_s: 58.29002022743225\n",
      "  time_total_s: 6326.386046409607\n",
      "  timers:\n",
      "    learn_throughput: 155.261\n",
      "    learn_time_ms: 25763.015\n",
      "    sample_throughput: 125.15\n",
      "    sample_time_ms: 31961.671\n",
      "    update_time_ms: 3.204\n",
      "  timestamp: 1672614308\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 436000\n",
      "  training_iteration: 109\n",
      "  trial_id: f9c59_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 9.3/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.87 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_0_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_1_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_3_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_5_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_2_6417799359d4e2284884fd3ae2ec9735, 0.0/6.0 CPU_group_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_4_6417799359d4e2284884fd3ae2ec9735)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                               | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridPOEnv-v0_f9c59_00000 | RUNNING  | 192.168.1.65:17432 |    109 |          6326.39 | 436000 |  -186.69 |             -183.313 |               -192.9 |                400 |\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridPOEnv-v0_f9c59_00000:\n",
      "  agent_timesteps_total: 440000\n",
      "  custom_metrics: {}\n",
      "  date: 2023-01-01_17-06-05\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -183.3133996585258\n",
      "  episode_reward_mean: -186.7316250952227\n",
      "  episode_reward_min: -192.9000575284058\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1100\n",
      "  experiment_id: 610b8b5226964181899719d25df7617e\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.775557561562891e-18\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -0.7261355269700289\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01399260303019258\n",
      "          policy_loss: -0.0035430790725513362\n",
      "          total_loss: 495.6476851940155\n",
      "          vf_explained_var: 4.78159654448973e-06\n",
      "          vf_loss: 495.65122723579407\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 440000\n",
      "    num_steps_sampled: 440000\n",
      "    num_steps_trained: 440000\n",
      "  iterations_since_restore: 110\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.67710843373494\n",
      "    ram_util_percent: 29.787951807228918\n",
      "  pid: 17432\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0883971052152911\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.769172395787074\n",
      "    mean_inference_ms: 1.8363366312589955\n",
      "    mean_raw_obs_processing_ms: 6.37524113474984\n",
      "  time_since_restore: 6383.8844475746155\n",
      "  time_this_iter_s: 57.498401165008545\n",
      "  time_total_s: 6383.8844475746155\n",
      "  timers:\n",
      "    learn_throughput: 155.169\n",
      "    learn_time_ms: 25778.354\n",
      "    sample_throughput: 125.314\n",
      "    sample_time_ms: 31919.945\n",
      "    update_time_ms: 3.221\n",
      "  timestamp: 1672614365\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 440000\n",
      "  training_iteration: 110\n",
      "  trial_id: f9c59_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 9.3/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.87 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_5_6417799359d4e2284884fd3ae2ec9735, 0.0/6.0 CPU_group_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_2_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_0_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_4_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_1_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_3_6417799359d4e2284884fd3ae2ec9735)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                               | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridPOEnv-v0_f9c59_00000 | RUNNING  | 192.168.1.65:17432 |    110 |          6383.88 | 440000 | -186.732 |             -183.313 |               -192.9 |                400 |\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_TrafficLightGridPOEnv-v0_f9c59_00000:\n",
      "  agent_timesteps_total: 444000\n",
      "  custom_metrics: {}\n",
      "  date: 2023-01-01_17-07-04\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -183.3133996585258\n",
      "  episode_reward_mean: -186.75570833807254\n",
      "  episode_reward_min: -192.9000575284058\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1110\n",
      "  experiment_id: 610b8b5226964181899719d25df7617e\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.775557561562891e-18\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -0.8162103086709976\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01422512515691785\n",
      "          policy_loss: 0.0006077389785787091\n",
      "          total_loss: 491.3872726202011\n",
      "          vf_explained_var: 2.9571353934443323e-06\n",
      "          vf_loss: 491.3866669416428\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 444000\n",
      "    num_steps_sampled: 444000\n",
      "    num_steps_trained: 444000\n",
      "  iterations_since_restore: 111\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.751807228915663\n",
      "    ram_util_percent: 29.85542168674699\n",
      "  pid: 17432\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0883970968445374\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.76747191789816\n",
      "    mean_inference_ms: 1.8362679412577894\n",
      "    mean_raw_obs_processing_ms: 6.372078245987506\n",
      "  time_since_restore: 6442.343851566315\n",
      "  time_this_iter_s: 58.45940399169922\n",
      "  time_total_s: 6442.343851566315\n",
      "  timers:\n",
      "    learn_throughput: 154.759\n",
      "    learn_time_ms: 25846.558\n",
      "    sample_throughput: 125.417\n",
      "    sample_time_ms: 31893.576\n",
      "    update_time_ms: 3.181\n",
      "  timestamp: 1672614424\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 444000\n",
      "  training_iteration: 111\n",
      "  trial_id: f9c59_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 9.3/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.87 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_2_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_0_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_4_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_1_6417799359d4e2284884fd3ae2ec9735, 0.0/6.0 CPU_group_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_3_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_5_6417799359d4e2284884fd3ae2ec9735)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                               | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridPOEnv-v0_f9c59_00000 | RUNNING  | 192.168.1.65:17432 |    111 |          6442.34 | 444000 | -186.756 |             -183.313 |               -192.9 |                400 |\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridPOEnv-v0_f9c59_00000:\n",
      "  agent_timesteps_total: 448000\n",
      "  custom_metrics: {}\n",
      "  date: 2023-01-01_17-08-02\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -183.3133996585258\n",
      "  episode_reward_mean: -186.6447156388752\n",
      "  episode_reward_min: -192.67473684721173\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1120\n",
      "  experiment_id: 610b8b5226964181899719d25df7617e\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.775557561562891e-18\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -0.9246747130528092\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012298544370878694\n",
      "          policy_loss: 0.002541426474635955\n",
      "          total_loss: 487.09129717350004\n",
      "          vf_explained_var: 3.1653792120778235e-06\n",
      "          vf_loss: 487.0887582778931\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 448000\n",
      "    num_steps_sampled: 448000\n",
      "    num_steps_trained: 448000\n",
      "  iterations_since_restore: 112\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.39166666666667\n",
      "    ram_util_percent: 29.85833333333333\n",
      "  pid: 17432\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08839856278867407\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.766423872174684\n",
      "    mean_inference_ms: 1.8362199814856197\n",
      "    mean_raw_obs_processing_ms: 6.369008997640578\n",
      "  time_since_restore: 6500.575036048889\n",
      "  time_this_iter_s: 58.23118448257446\n",
      "  time_total_s: 6500.575036048889\n",
      "  timers:\n",
      "    learn_throughput: 154.673\n",
      "    learn_time_ms: 25861.086\n",
      "    sample_throughput: 125.178\n",
      "    sample_time_ms: 31954.556\n",
      "    update_time_ms: 3.203\n",
      "  timestamp: 1672614482\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 448000\n",
      "  training_iteration: 112\n",
      "  trial_id: f9c59_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 9.3/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.87 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_4_6417799359d4e2284884fd3ae2ec9735, 0.0/6.0 CPU_group_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_2_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_1_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_5_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_0_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_3_6417799359d4e2284884fd3ae2ec9735)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                               | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridPOEnv-v0_f9c59_00000 | RUNNING  | 192.168.1.65:17432 |    112 |          6500.58 | 448000 | -186.645 |             -183.313 |             -192.675 |                400 |\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridPOEnv-v0_f9c59_00000:\n",
      "  agent_timesteps_total: 452000\n",
      "  custom_metrics: {}\n",
      "  date: 2023-01-01_17-09-01\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -183.3133996585258\n",
      "  episode_reward_mean: -186.61516920911555\n",
      "  episode_reward_min: -192.67473684721173\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1130\n",
      "  experiment_id: 610b8b5226964181899719d25df7617e\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.775557561562891e-18\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -0.9322913374751807\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010991785742908508\n",
      "          policy_loss: -0.0004062738502398133\n",
      "          total_loss: 493.660556268692\n",
      "          vf_explained_var: 1.2605823940248229e-05\n",
      "          vf_loss: 493.6609657287598\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 452000\n",
      "    num_steps_sampled: 452000\n",
      "    num_steps_trained: 452000\n",
      "  iterations_since_restore: 113\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.76144578313253\n",
      "    ram_util_percent: 29.846987951807236\n",
      "  pid: 17432\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08840065073352324\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.76594007957143\n",
      "    mean_inference_ms: 1.836226286110064\n",
      "    mean_raw_obs_processing_ms: 6.365973300117539\n",
      "  time_since_restore: 6558.869600534439\n",
      "  time_this_iter_s: 58.29456448554993\n",
      "  time_total_s: 6558.869600534439\n",
      "  timers:\n",
      "    learn_throughput: 154.62\n",
      "    learn_time_ms: 25869.865\n",
      "    sample_throughput: 124.961\n",
      "    sample_time_ms: 32010.055\n",
      "    update_time_ms: 3.197\n",
      "  timestamp: 1672614541\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 452000\n",
      "  training_iteration: 113\n",
      "  trial_id: f9c59_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Memory usage on this node: 9.3/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.87 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_4_6417799359d4e2284884fd3ae2ec9735, 0.0/6.0 CPU_group_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_2_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_3_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_1_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_0_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_5_6417799359d4e2284884fd3ae2ec9735)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                               | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridPOEnv-v0_f9c59_00000 | RUNNING  | 192.168.1.65:17432 |    113 |          6558.87 | 452000 | -186.615 |             -183.313 |             -192.675 |                400 |\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridPOEnv-v0_f9c59_00000:\n",
      "  agent_timesteps_total: 456000\n",
      "  custom_metrics: {}\n",
      "  date: 2023-01-01_17-09-59\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -183.3133996585258\n",
      "  episode_reward_mean: -186.5097076238157\n",
      "  episode_reward_min: -192.67473684721173\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1140\n",
      "  experiment_id: 610b8b5226964181899719d25df7617e\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.775557561562891e-18\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -0.9782331585884094\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012125473716241899\n",
      "          policy_loss: -0.0011151532977237365\n",
      "          total_loss: 489.3737967014313\n",
      "          vf_explained_var: 1.0421313163533341e-05\n",
      "          vf_loss: 489.37490530014037\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 456000\n",
      "    num_steps_sampled: 456000\n",
      "    num_steps_trained: 456000\n",
      "  iterations_since_restore: 114\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.6144578313253\n",
      "    ram_util_percent: 29.887951807228912\n",
      "  pid: 17432\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0884037533279455\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.76602875283905\n",
      "    mean_inference_ms: 1.8362928533473477\n",
      "    mean_raw_obs_processing_ms: 6.362977243223388\n",
      "  time_since_restore: 6617.416618585587\n",
      "  time_this_iter_s: 58.54701805114746\n",
      "  time_total_s: 6617.416618585587\n",
      "  timers:\n",
      "    learn_throughput: 154.557\n",
      "    learn_time_ms: 25880.421\n",
      "    sample_throughput: 124.729\n",
      "    sample_time_ms: 32069.447\n",
      "    update_time_ms: 3.158\n",
      "  timestamp: 1672614599\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 456000\n",
      "  training_iteration: 114\n",
      "  trial_id: f9c59_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 9.3/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.87 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_5_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_3_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_1_6417799359d4e2284884fd3ae2ec9735, 0.0/6.0 CPU_group_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_0_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_4_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_2_6417799359d4e2284884fd3ae2ec9735)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                               | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridPOEnv-v0_f9c59_00000 | RUNNING  | 192.168.1.65:17432 |    114 |          6617.42 | 456000 |  -186.51 |             -183.313 |             -192.675 |                400 |\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridPOEnv-v0_f9c59_00000:\n",
      "  agent_timesteps_total: 460000\n",
      "  custom_metrics: {}\n",
      "  date: 2023-01-01_17-10-57\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -183.3133996585258\n",
      "  episode_reward_mean: -186.41060522977855\n",
      "  episode_reward_min: -192.67473684721173\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1150\n",
      "  experiment_id: 610b8b5226964181899719d25df7617e\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.775557561562891e-18\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -1.0074269609525799\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011594628441139355\n",
      "          policy_loss: 5.234808486420661e-06\n",
      "          total_loss: 485.3038414001465\n",
      "          vf_explained_var: -7.450580596923828e-09\n",
      "          vf_loss: 485.30383319854735\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 460000\n",
      "    num_steps_sampled: 460000\n",
      "    num_steps_trained: 460000\n",
      "  iterations_since_restore: 115\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.67439024390244\n",
      "    ram_util_percent: 29.882926829268293\n",
      "  pid: 17432\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08840866754159336\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.76606175341808\n",
      "    mean_inference_ms: 1.8364012822479532\n",
      "    mean_raw_obs_processing_ms: 6.360047108726478\n",
      "  time_since_restore: 6674.672617197037\n",
      "  time_this_iter_s: 57.255998611450195\n",
      "  time_total_s: 6674.672617197037\n",
      "  timers:\n",
      "    learn_throughput: 154.841\n",
      "    learn_time_ms: 25832.938\n",
      "    sample_throughput: 124.731\n",
      "    sample_time_ms: 32069.031\n",
      "    update_time_ms: 3.157\n",
      "  timestamp: 1672614657\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 460000\n",
      "  training_iteration: 115\n",
      "  trial_id: f9c59_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 9.3/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.87 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_3_6417799359d4e2284884fd3ae2ec9735, 0.0/6.0 CPU_group_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_0_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_4_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_2_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_1_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_5_6417799359d4e2284884fd3ae2ec9735)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                               | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridPOEnv-v0_f9c59_00000 | RUNNING  | 192.168.1.65:17432 |    115 |          6674.67 | 460000 | -186.411 |             -183.313 |             -192.675 |                400 |\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_TrafficLightGridPOEnv-v0_f9c59_00000:\n",
      "  agent_timesteps_total: 464000\n",
      "  custom_metrics: {}\n",
      "  date: 2023-01-01_17-11-54\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -183.3133996585258\n",
      "  episode_reward_mean: -186.28913139827463\n",
      "  episode_reward_min: -188.32320272304185\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1160\n",
      "  experiment_id: 610b8b5226964181899719d25df7617e\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.775557561562891e-18\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -0.9747936913743616\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.00834608493041764\n",
      "          policy_loss: 0.0005155141072464176\n",
      "          total_loss: 485.1474750995636\n",
      "          vf_explained_var: -1.5765428997838171e-06\n",
      "          vf_loss: 485.1469593524933\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 464000\n",
      "    num_steps_sampled: 464000\n",
      "    num_steps_trained: 464000\n",
      "  iterations_since_restore: 116\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.665060240963854\n",
      "    ram_util_percent: 29.886746987951817\n",
      "  pid: 17432\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08841431005430662\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.76601447496646\n",
      "    mean_inference_ms: 1.8365151828143218\n",
      "    mean_raw_obs_processing_ms: 6.35711319072869\n",
      "  time_since_restore: 6732.24994468689\n",
      "  time_this_iter_s: 57.577327489852905\n",
      "  time_total_s: 6732.24994468689\n",
      "  timers:\n",
      "    learn_throughput: 154.51\n",
      "    learn_time_ms: 25888.322\n",
      "    sample_throughput: 124.794\n",
      "    sample_time_ms: 32052.761\n",
      "    update_time_ms: 3.175\n",
      "  timestamp: 1672614714\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 464000\n",
      "  training_iteration: 116\n",
      "  trial_id: f9c59_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 9.3/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.87 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_1_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_5_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_2_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_0_6417799359d4e2284884fd3ae2ec9735, 0.0/6.0 CPU_group_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_3_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_4_6417799359d4e2284884fd3ae2ec9735)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                               | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridPOEnv-v0_f9c59_00000 | RUNNING  | 192.168.1.65:17432 |    116 |          6732.25 | 464000 | -186.289 |             -183.313 |             -188.323 |                400 |\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridPOEnv-v0_f9c59_00000:\n",
      "  agent_timesteps_total: 468000\n",
      "  custom_metrics: {}\n",
      "  date: 2023-01-01_17-12-52\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -183.3133996585258\n",
      "  episode_reward_mean: -186.25163290049693\n",
      "  episode_reward_min: -188.32320272304185\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1170\n",
      "  experiment_id: 610b8b5226964181899719d25df7617e\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.3877787807814456e-18\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -1.0515363715589046\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017127432261804645\n",
      "          policy_loss: -0.003887058254622389\n",
      "          total_loss: 485.57912559509276\n",
      "          vf_explained_var: -7.078051478259795e-09\n",
      "          vf_loss: 485.58301520347595\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 468000\n",
      "    num_steps_sampled: 468000\n",
      "    num_steps_trained: 468000\n",
      "  iterations_since_restore: 117\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.77073170731707\n",
      "    ram_util_percent: 29.90975609756098\n",
      "  pid: 17432\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08841996097181878\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.76618731928298\n",
      "    mean_inference_ms: 1.8366269785705598\n",
      "    mean_raw_obs_processing_ms: 6.354208652997135\n",
      "  time_since_restore: 6789.764577865601\n",
      "  time_this_iter_s: 57.51463317871094\n",
      "  time_total_s: 6789.764577865601\n",
      "  timers:\n",
      "    learn_throughput: 155.015\n",
      "    learn_time_ms: 25804.034\n",
      "    sample_throughput: 124.697\n",
      "    sample_time_ms: 32077.649\n",
      "    update_time_ms: 3.165\n",
      "  timestamp: 1672614772\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 468000\n",
      "  training_iteration: 117\n",
      "  trial_id: f9c59_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 9.3/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.87 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_5_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_0_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_2_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_4_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_1_6417799359d4e2284884fd3ae2ec9735, 0.0/6.0 CPU_group_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_3_6417799359d4e2284884fd3ae2ec9735)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                               | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridPOEnv-v0_f9c59_00000 | RUNNING  | 192.168.1.65:17432 |    117 |          6789.76 | 468000 | -186.252 |             -183.313 |             -188.323 |                400 |\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridPOEnv-v0_f9c59_00000:\n",
      "  agent_timesteps_total: 472000\n",
      "  custom_metrics: {}\n",
      "  date: 2023-01-01_17-13-50\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -183.3133996585258\n",
      "  episode_reward_mean: -186.22418368746247\n",
      "  episode_reward_min: -188.34447736384803\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1180\n",
      "  experiment_id: 610b8b5226964181899719d25df7617e\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.3877787807814456e-18\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -1.0921056812629104\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013021645549406501\n",
      "          policy_loss: 0.0007754467849736101\n",
      "          total_loss: 481.54506545066835\n",
      "          vf_explained_var: 1.952424554474419e-06\n",
      "          vf_loss: 481.54428668022155\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 472000\n",
      "    num_steps_sampled: 472000\n",
      "    num_steps_trained: 472000\n",
      "  iterations_since_restore: 118\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.63975903614458\n",
      "    ram_util_percent: 29.998795180722894\n",
      "  pid: 17432\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08842588500661179\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.76687045582558\n",
      "    mean_inference_ms: 1.8367418226958114\n",
      "    mean_raw_obs_processing_ms: 6.35135421536547\n",
      "  time_since_restore: 6848.079530477524\n",
      "  time_this_iter_s: 58.31495261192322\n",
      "  time_total_s: 6848.079530477524\n",
      "  timers:\n",
      "    learn_throughput: 154.71\n",
      "    learn_time_ms: 25854.786\n",
      "    sample_throughput: 124.497\n",
      "    sample_time_ms: 32129.293\n",
      "    update_time_ms: 3.143\n",
      "  timestamp: 1672614830\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 472000\n",
      "  training_iteration: 118\n",
      "  trial_id: f9c59_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Memory usage on this node: 9.3/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.87 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_4_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_0_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_3_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_2_6417799359d4e2284884fd3ae2ec9735, 0.0/6.0 CPU_group_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_1_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_5_6417799359d4e2284884fd3ae2ec9735)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                               | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridPOEnv-v0_f9c59_00000 | RUNNING  | 192.168.1.65:17432 |    118 |          6848.08 | 472000 | -186.224 |             -183.313 |             -188.344 |                400 |\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridPOEnv-v0_f9c59_00000:\n",
      "  agent_timesteps_total: 476000\n",
      "  custom_metrics: {}\n",
      "  date: 2023-01-01_17-14-48\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -183.33879348635818\n",
      "  episode_reward_mean: -186.2547039617916\n",
      "  episode_reward_min: -192.92389461291413\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1190\n",
      "  experiment_id: 610b8b5226964181899719d25df7617e\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.3877787807814456e-18\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -1.14261353649199\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011632174183068857\n",
      "          policy_loss: 0.0005697594162484166\n",
      "          total_loss: 486.419762802124\n",
      "          vf_explained_var: 5.2874906941724475e-06\n",
      "          vf_loss: 486.4191924095154\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 476000\n",
      "    num_steps_sampled: 476000\n",
      "    num_steps_trained: 476000\n",
      "  iterations_since_restore: 119\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.596385542168672\n",
      "    ram_util_percent: 29.889156626506026\n",
      "  pid: 17432\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08843381415487134\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.76776497893249\n",
      "    mean_inference_ms: 1.8369005908321894\n",
      "    mean_raw_obs_processing_ms: 6.3485403870316475\n",
      "  time_since_restore: 6905.9251499176025\n",
      "  time_this_iter_s: 57.845619440078735\n",
      "  time_total_s: 6905.9251499176025\n",
      "  timers:\n",
      "    learn_throughput: 155.128\n",
      "    learn_time_ms: 25785.134\n",
      "    sample_throughput: 124.399\n",
      "    sample_time_ms: 32154.579\n",
      "    update_time_ms: 3.129\n",
      "  timestamp: 1672614888\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 476000\n",
      "  training_iteration: 119\n",
      "  trial_id: f9c59_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 9.3/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.87 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_0_6417799359d4e2284884fd3ae2ec9735, 0.0/6.0 CPU_group_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_3_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_5_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_4_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_2_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_1_6417799359d4e2284884fd3ae2ec9735)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                               | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridPOEnv-v0_f9c59_00000 | RUNNING  | 192.168.1.65:17432 |    119 |          6905.93 | 476000 | -186.255 |             -183.339 |             -192.924 |                400 |\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridPOEnv-v0_f9c59_00000:\n",
      "  agent_timesteps_total: 480000\n",
      "  custom_metrics: {}\n",
      "  date: 2023-01-01_17-15-46\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -183.33879348635818\n",
      "  episode_reward_mean: -186.23152086268655\n",
      "  episode_reward_min: -192.92389461291413\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1200\n",
      "  experiment_id: 610b8b5226964181899719d25df7617e\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.3877787807814456e-18\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -1.2308270137757062\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014148496519465042\n",
      "          policy_loss: -0.002022119064349681\n",
      "          total_loss: 484.9333647489548\n",
      "          vf_explained_var: 6.730854693159927e-06\n",
      "          vf_loss: 484.93538546562195\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 480000\n",
      "    num_steps_sampled: 480000\n",
      "    num_steps_trained: 480000\n",
      "  iterations_since_restore: 120\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.678048780487806\n",
      "    ram_util_percent: 29.934146341463407\n",
      "  pid: 17432\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08844117869656401\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.769339618825242\n",
      "    mean_inference_ms: 1.8370293027248112\n",
      "    mean_raw_obs_processing_ms: 6.3457470530353985\n",
      "  time_since_restore: 6963.468443870544\n",
      "  time_this_iter_s: 57.543293952941895\n",
      "  time_total_s: 6963.468443870544\n",
      "  timers:\n",
      "    learn_throughput: 155.45\n",
      "    learn_time_ms: 25731.691\n",
      "    sample_throughput: 124.175\n",
      "    sample_time_ms: 32212.601\n",
      "    update_time_ms: 3.105\n",
      "  timestamp: 1672614946\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 480000\n",
      "  training_iteration: 120\n",
      "  trial_id: f9c59_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 9.3/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.87 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_2_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_1_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_5_6417799359d4e2284884fd3ae2ec9735, 0.0/6.0 CPU_group_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_3_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_4_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_0_6417799359d4e2284884fd3ae2ec9735)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                               | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridPOEnv-v0_f9c59_00000 | RUNNING  | 192.168.1.65:17432 |    120 |          6963.47 | 480000 | -186.232 |             -183.339 |             -192.924 |                400 |\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_TrafficLightGridPOEnv-v0_f9c59_00000:\n",
      "  agent_timesteps_total: 484000\n",
      "  custom_metrics: {}\n",
      "  date: 2023-01-01_17-16-44\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -183.33879348635818\n",
      "  episode_reward_mean: -186.20078552255472\n",
      "  episode_reward_min: -192.92389461291413\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1210\n",
      "  experiment_id: 610b8b5226964181899719d25df7617e\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.3877787807814456e-18\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -1.3045419827103615\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016492658459720026\n",
      "          policy_loss: -0.0038529814206412993\n",
      "          total_loss: 482.85361580848695\n",
      "          vf_explained_var: 5.119852630741661e-06\n",
      "          vf_loss: 482.8574691295624\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 484000\n",
      "    num_steps_sampled: 484000\n",
      "    num_steps_trained: 484000\n",
      "  iterations_since_restore: 121\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.787951807228918\n",
      "    ram_util_percent: 30.014457831325295\n",
      "  pid: 17432\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08844754943901706\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.77090295042508\n",
      "    mean_inference_ms: 1.8371498591805624\n",
      "    mean_raw_obs_processing_ms: 6.3430393893842165\n",
      "  time_since_restore: 7021.123440742493\n",
      "  time_this_iter_s: 57.65499687194824\n",
      "  time_total_s: 7021.123440742493\n",
      "  timers:\n",
      "    learn_throughput: 156.005\n",
      "    learn_time_ms: 25640.166\n",
      "    sample_throughput: 124.132\n",
      "    sample_time_ms: 32223.73\n",
      "    update_time_ms: 3.122\n",
      "  timestamp: 1672615004\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 484000\n",
      "  training_iteration: 121\n",
      "  trial_id: f9c59_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 9.4/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.87 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_4_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_1_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_0_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_3_6417799359d4e2284884fd3ae2ec9735, 0.0/6.0 CPU_group_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_2_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_5_6417799359d4e2284884fd3ae2ec9735)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                               | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridPOEnv-v0_f9c59_00000 | RUNNING  | 192.168.1.65:17432 |    121 |          7021.12 | 484000 | -186.201 |             -183.339 |             -192.924 |                400 |\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridPOEnv-v0_f9c59_00000:\n",
      "  agent_timesteps_total: 488000\n",
      "  custom_metrics: {}\n",
      "  date: 2023-01-01_17-17-42\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -183.33879348635818\n",
      "  episode_reward_mean: -186.1800516583105\n",
      "  episode_reward_min: -192.92389461291413\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1220\n",
      "  experiment_id: 610b8b5226964181899719d25df7617e\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.3877787807814456e-18\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -1.288086624071002\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009187037208994617\n",
      "          policy_loss: -0.0008789530402282253\n",
      "          total_loss: 482.5475550889969\n",
      "          vf_explained_var: 6.3043089539860375e-06\n",
      "          vf_loss: 482.54843137264254\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 488000\n",
      "    num_steps_sampled: 488000\n",
      "    num_steps_trained: 488000\n",
      "  iterations_since_restore: 122\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.84878048780488\n",
      "    ram_util_percent: 29.917073170731705\n",
      "  pid: 17432\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08845393941446757\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.77239301736815\n",
      "    mean_inference_ms: 1.837265239343589\n",
      "    mean_raw_obs_processing_ms: 6.340390183392112\n",
      "  time_since_restore: 7079.033207178116\n",
      "  time_this_iter_s: 57.90976643562317\n",
      "  time_total_s: 7079.033207178116\n",
      "  timers:\n",
      "    learn_throughput: 156.172\n",
      "    learn_time_ms: 25612.794\n",
      "    sample_throughput: 124.15\n",
      "    sample_time_ms: 32219.092\n",
      "    update_time_ms: 3.093\n",
      "  timestamp: 1672615062\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 488000\n",
      "  training_iteration: 122\n",
      "  trial_id: f9c59_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 9.3/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.87 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_5_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_1_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_2_6417799359d4e2284884fd3ae2ec9735, 0.0/6.0 CPU_group_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_3_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_0_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_4_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 accelerator_type:G)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                               | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridPOEnv-v0_f9c59_00000 | RUNNING  | 192.168.1.65:17432 |    122 |          7079.03 | 488000 |  -186.18 |             -183.339 |             -192.924 |                400 |\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridPOEnv-v0_f9c59_00000:\n",
      "  agent_timesteps_total: 492000\n",
      "  custom_metrics: {}\n",
      "  date: 2023-01-01_17-18-40\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -183.33879348635818\n",
      "  episode_reward_mean: -186.16213685605942\n",
      "  episode_reward_min: -192.92389461291413\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1230\n",
      "  experiment_id: 610b8b5226964181899719d25df7617e\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 6.938893903907228e-19\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -1.3677978284657002\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01437545815265393\n",
      "          policy_loss: -0.001319767472159583\n",
      "          total_loss: 476.64232523441314\n",
      "          vf_explained_var: 6.031803877704078e-06\n",
      "          vf_loss: 476.6436423778534\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 492000\n",
      "    num_steps_sampled: 492000\n",
      "    num_steps_trained: 492000\n",
      "  iterations_since_restore: 123\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.60722891566265\n",
      "    ram_util_percent: 29.993975903614448\n",
      "  pid: 17432\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08846030557343253\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.773869798283002\n",
      "    mean_inference_ms: 1.8373427657157848\n",
      "    mean_raw_obs_processing_ms: 6.337786402303252\n",
      "  time_since_restore: 7136.9045696258545\n",
      "  time_this_iter_s: 57.87136244773865\n",
      "  time_total_s: 7136.9045696258545\n",
      "  timers:\n",
      "    learn_throughput: 156.434\n",
      "    learn_time_ms: 25569.854\n",
      "    sample_throughput: 124.148\n",
      "    sample_time_ms: 32219.735\n",
      "    update_time_ms: 3.111\n",
      "  timestamp: 1672615120\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 492000\n",
      "  training_iteration: 123\n",
      "  trial_id: f9c59_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Memory usage on this node: 9.4/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.87 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_0_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_5_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_3_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_1_6417799359d4e2284884fd3ae2ec9735, 0.0/6.0 CPU_group_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_4_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_2_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 accelerator_type:G)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                               | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridPOEnv-v0_f9c59_00000 | RUNNING  | 192.168.1.65:17432 |    123 |           7136.9 | 492000 | -186.162 |             -183.339 |             -192.924 |                400 |\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridPOEnv-v0_f9c59_00000:\n",
      "  agent_timesteps_total: 496000\n",
      "  custom_metrics: {}\n",
      "  date: 2023-01-01_17-19-37\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -183.33879348635818\n",
      "  episode_reward_mean: -186.42779505987193\n",
      "  episode_reward_min: -212.5300233919286\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1240\n",
      "  experiment_id: 610b8b5226964181899719d25df7617e\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 6.938893903907228e-19\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -1.3633211586624383\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013869450592176463\n",
      "          policy_loss: -0.002110737221664749\n",
      "          total_loss: 488.28976287841795\n",
      "          vf_explained_var: 2.0209699869155884e-06\n",
      "          vf_loss: 488.2918713092804\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 496000\n",
      "    num_steps_sampled: 496000\n",
      "    num_steps_trained: 496000\n",
      "  iterations_since_restore: 124\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.69512195121951\n",
      "    ram_util_percent: 30.15243902439025\n",
      "  pid: 17432\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08846643838075906\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.77458540091504\n",
      "    mean_inference_ms: 1.8374087012182816\n",
      "    mean_raw_obs_processing_ms: 6.335218409574716\n",
      "  time_since_restore: 7194.002170801163\n",
      "  time_this_iter_s: 57.09760117530823\n",
      "  time_total_s: 7194.002170801163\n",
      "  timers:\n",
      "    learn_throughput: 156.854\n",
      "    learn_time_ms: 25501.477\n",
      "    sample_throughput: 124.443\n",
      "    sample_time_ms: 32143.189\n",
      "    update_time_ms: 3.104\n",
      "  timestamp: 1672615177\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 496000\n",
      "  training_iteration: 124\n",
      "  trial_id: f9c59_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 9.4/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.87 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_5_6417799359d4e2284884fd3ae2ec9735, 0.0/6.0 CPU_group_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_2_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_1_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_0_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_4_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_3_6417799359d4e2284884fd3ae2ec9735)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                               | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridPOEnv-v0_f9c59_00000 | RUNNING  | 192.168.1.65:17432 |    124 |             7194 | 496000 | -186.428 |             -183.339 |              -212.53 |                400 |\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridPOEnv-v0_f9c59_00000:\n",
      "  agent_timesteps_total: 500000\n",
      "  custom_metrics: {}\n",
      "  date: 2023-01-01_17-20-34\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -183.33879348635818\n",
      "  episode_reward_mean: -186.85527832556875\n",
      "  episode_reward_min: -212.5300233919286\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1250\n",
      "  experiment_id: 610b8b5226964181899719d25df7617e\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 6.938893903907228e-19\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -1.460708161070943\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.018498493699803654\n",
      "          policy_loss: -0.0037654229206964375\n",
      "          total_loss: 482.1928359746933\n",
      "          vf_explained_var: 8.324161058226309e-07\n",
      "          vf_loss: 482.1966045856476\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 500000\n",
      "    num_steps_sampled: 500000\n",
      "    num_steps_trained: 500000\n",
      "  iterations_since_restore: 125\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.882716049382715\n",
      "    ram_util_percent: 30.103703703703705\n",
      "  pid: 17432\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08847158956271134\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.775352465810194\n",
      "    mean_inference_ms: 1.8374538995682732\n",
      "    mean_raw_obs_processing_ms: 6.3326216310273455\n",
      "  time_since_restore: 7250.957622528076\n",
      "  time_this_iter_s: 56.95545172691345\n",
      "  time_total_s: 7250.957622528076\n",
      "  timers:\n",
      "    learn_throughput: 156.991\n",
      "    learn_time_ms: 25479.148\n",
      "    sample_throughput: 124.473\n",
      "    sample_time_ms: 32135.442\n",
      "    update_time_ms: 3.117\n",
      "  timestamp: 1672615234\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 500000\n",
      "  training_iteration: 125\n",
      "  trial_id: f9c59_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 9.4/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.87 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_0_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_2_6417799359d4e2284884fd3ae2ec9735, 0.0/6.0 CPU_group_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_5_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_1_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_4_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_3_6417799359d4e2284884fd3ae2ec9735)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                               | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridPOEnv-v0_f9c59_00000 | RUNNING  | 192.168.1.65:17432 |    125 |          7250.96 | 500000 | -186.855 |             -183.339 |              -212.53 |                400 |\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_TrafficLightGridPOEnv-v0_f9c59_00000:\n",
      "  agent_timesteps_total: 504000\n",
      "  custom_metrics: {}\n",
      "  date: 2023-01-01_17-21-32\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -184.52770048513818\n",
      "  episode_reward_mean: -187.75430823633909\n",
      "  episode_reward_min: -215.6457720153414\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1260\n",
      "  experiment_id: 610b8b5226964181899719d25df7617e\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 6.938893903907228e-19\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -1.4288340654224156\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016033754528781096\n",
      "          policy_loss: -0.0030223662091884763\n",
      "          total_loss: 504.71503167152406\n",
      "          vf_explained_var: 1.351907826574461e-06\n",
      "          vf_loss: 504.71805276870725\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 504000\n",
      "    num_steps_sampled: 504000\n",
      "    num_steps_trained: 504000\n",
      "  iterations_since_restore: 126\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.479518072289157\n",
      "    ram_util_percent: 30.201204819277105\n",
      "  pid: 17432\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08847668054251798\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.776315427821427\n",
      "    mean_inference_ms: 1.8374919557325193\n",
      "    mean_raw_obs_processing_ms: 6.330084014193346\n",
      "  time_since_restore: 7308.695199012756\n",
      "  time_this_iter_s: 57.737576484680176\n",
      "  time_total_s: 7308.695199012756\n",
      "  timers:\n",
      "    learn_throughput: 157.058\n",
      "    learn_time_ms: 25468.267\n",
      "    sample_throughput: 124.369\n",
      "    sample_time_ms: 32162.39\n",
      "    update_time_ms: 3.091\n",
      "  timestamp: 1672615292\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 504000\n",
      "  training_iteration: 126\n",
      "  trial_id: f9c59_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 9.4/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.87 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_3_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_5_6417799359d4e2284884fd3ae2ec9735, 0.0/6.0 CPU_group_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_2_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_4_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_0_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_1_6417799359d4e2284884fd3ae2ec9735)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                               | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridPOEnv-v0_f9c59_00000 | RUNNING  | 192.168.1.65:17432 |    126 |           7308.7 | 504000 | -187.754 |             -184.528 |             -215.646 |                400 |\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridPOEnv-v0_f9c59_00000:\n",
      "  agent_timesteps_total: 508000\n",
      "  custom_metrics: {}\n",
      "  date: 2023-01-01_17-22-30\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -184.53996046853342\n",
      "  episode_reward_mean: -188.22008093014796\n",
      "  episode_reward_min: -215.6457720153414\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1270\n",
      "  experiment_id: 610b8b5226964181899719d25df7617e\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 6.938893903907228e-19\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -1.4245546914637088\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008180702704294189\n",
      "          policy_loss: 0.0023438913936843163\n",
      "          total_loss: 483.53098158836366\n",
      "          vf_explained_var: 4.8102810978889465e-06\n",
      "          vf_loss: 483.5286387443542\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 508000\n",
      "    num_steps_sampled: 508000\n",
      "    num_steps_trained: 508000\n",
      "  iterations_since_restore: 127\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.453658536585365\n",
      "    ram_util_percent: 30.245121951219506\n",
      "  pid: 17432\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08848104441914943\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.77714328219182\n",
      "    mean_inference_ms: 1.8374846245039604\n",
      "    mean_raw_obs_processing_ms: 6.327535750209806\n",
      "  time_since_restore: 7366.215271472931\n",
      "  time_this_iter_s: 57.52007246017456\n",
      "  time_total_s: 7366.215271472931\n",
      "  timers:\n",
      "    learn_throughput: 156.835\n",
      "    learn_time_ms: 25504.432\n",
      "    sample_throughput: 124.507\n",
      "    sample_time_ms: 32126.732\n",
      "    update_time_ms: 3.091\n",
      "  timestamp: 1672615350\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 508000\n",
      "  training_iteration: 127\n",
      "  trial_id: f9c59_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 9.4/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.87 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_0_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_5_6417799359d4e2284884fd3ae2ec9735, 0.0/6.0 CPU_group_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_2_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_3_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_1_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_4_6417799359d4e2284884fd3ae2ec9735)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                               | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridPOEnv-v0_f9c59_00000 | RUNNING  | 192.168.1.65:17432 |    127 |          7366.22 | 508000 |  -188.22 |              -184.54 |             -215.646 |                400 |\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridPOEnv-v0_f9c59_00000:\n",
      "  agent_timesteps_total: 512000\n",
      "  custom_metrics: {}\n",
      "  date: 2023-01-01_17-23-28\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -184.5546425659713\n",
      "  episode_reward_mean: -189.23471707324748\n",
      "  episode_reward_min: -217.65935107204243\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1280\n",
      "  experiment_id: 610b8b5226964181899719d25df7617e\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 3.469446951953614e-19\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -1.3872663885354997\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.00961827593805573\n",
      "          policy_loss: 0.0021084472769871353\n",
      "          total_loss: 499.8845046043396\n",
      "          vf_explained_var: 1.855473965406418e-05\n",
      "          vf_loss: 499.88239645957947\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 512000\n",
      "    num_steps_sampled: 512000\n",
      "    num_steps_trained: 512000\n",
      "  iterations_since_restore: 128\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.565060240963852\n",
      "    ram_util_percent: 30.21204819277108\n",
      "  pid: 17432\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08848551980689504\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.77797051333172\n",
      "    mean_inference_ms: 1.8374813503296707\n",
      "    mean_raw_obs_processing_ms: 6.325078077514687\n",
      "  time_since_restore: 7424.24778175354\n",
      "  time_this_iter_s: 58.03251028060913\n",
      "  time_total_s: 7424.24778175354\n",
      "  timers:\n",
      "    learn_throughput: 157.02\n",
      "    learn_time_ms: 25474.394\n",
      "    sample_throughput: 124.5\n",
      "    sample_time_ms: 32128.5\n",
      "    update_time_ms: 3.129\n",
      "  timestamp: 1672615408\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 512000\n",
      "  training_iteration: 128\n",
      "  trial_id: f9c59_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Memory usage on this node: 9.4/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.87 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_5_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_4_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_0_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_1_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_2_6417799359d4e2284884fd3ae2ec9735, 0.0/6.0 CPU_group_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_3_6417799359d4e2284884fd3ae2ec9735)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                               | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridPOEnv-v0_f9c59_00000 | RUNNING  | 192.168.1.65:17432 |    128 |          7424.25 | 512000 | -189.235 |             -184.555 |             -217.659 |                400 |\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridPOEnv-v0_f9c59_00000:\n",
      "  agent_timesteps_total: 516000\n",
      "  custom_metrics: {}\n",
      "  date: 2023-01-01_17-24-26\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -184.5546425659713\n",
      "  episode_reward_mean: -189.54968188920338\n",
      "  episode_reward_min: -217.65935107204243\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1290\n",
      "  experiment_id: 610b8b5226964181899719d25df7617e\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.734723475976807e-19\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -1.322786583006382\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011970397492473504\n",
      "          policy_loss: -0.0022734527636202985\n",
      "          total_loss: 477.4405925989151\n",
      "          vf_explained_var: 1.2947245977557031e-06\n",
      "          vf_loss: 477.4428657531738\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 516000\n",
      "    num_steps_sampled: 516000\n",
      "    num_steps_trained: 516000\n",
      "  iterations_since_restore: 129\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.637349397590363\n",
      "    ram_util_percent: 30.277108433734945\n",
      "  pid: 17432\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08848831109349609\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.778330007576574\n",
      "    mean_inference_ms: 1.837446782857215\n",
      "    mean_raw_obs_processing_ms: 6.322706431894392\n",
      "  time_since_restore: 7482.110691070557\n",
      "  time_this_iter_s: 57.8629093170166\n",
      "  time_total_s: 7482.110691070557\n",
      "  timers:\n",
      "    learn_throughput: 156.697\n",
      "    learn_time_ms: 25526.931\n",
      "    sample_throughput: 124.698\n",
      "    sample_time_ms: 32077.624\n",
      "    update_time_ms: 3.133\n",
      "  timestamp: 1672615466\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 516000\n",
      "  training_iteration: 129\n",
      "  trial_id: f9c59_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 9.4/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.87 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_4_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_0_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_1_6417799359d4e2284884fd3ae2ec9735, 0.0/6.0 CPU_group_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_5_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_3_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_2_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 accelerator_type:G)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                               | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridPOEnv-v0_f9c59_00000 | RUNNING  | 192.168.1.65:17432 |    129 |          7482.11 | 516000 |  -189.55 |             -184.555 |             -217.659 |                400 |\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridPOEnv-v0_f9c59_00000:\n",
      "  agent_timesteps_total: 520000\n",
      "  custom_metrics: {}\n",
      "  date: 2023-01-01_17-25-24\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -184.5546425659713\n",
      "  episode_reward_mean: -190.81193050995935\n",
      "  episode_reward_min: -217.65935107204243\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1300\n",
      "  experiment_id: 610b8b5226964181899719d25df7617e\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.734723475976807e-19\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -1.3624878518283368\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013625725477505396\n",
      "          policy_loss: -1.534146722406149e-05\n",
      "          total_loss: 508.31097934246066\n",
      "          vf_explained_var: 9.924359801516403e-06\n",
      "          vf_loss: 508.3109984397888\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 520000\n",
      "    num_steps_sampled: 520000\n",
      "    num_steps_trained: 520000\n",
      "  iterations_since_restore: 130\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.687951807228913\n",
      "    ram_util_percent: 30.32168674698795\n",
      "  pid: 17432\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08849045499243427\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.77836491741492\n",
      "    mean_inference_ms: 1.8374397178921307\n",
      "    mean_raw_obs_processing_ms: 6.320374294286021\n",
      "  time_since_restore: 7539.994876384735\n",
      "  time_this_iter_s: 57.88418531417847\n",
      "  time_total_s: 7539.994876384735\n",
      "  timers:\n",
      "    learn_throughput: 156.333\n",
      "    learn_time_ms: 25586.329\n",
      "    sample_throughput: 124.796\n",
      "    sample_time_ms: 32052.264\n",
      "    update_time_ms: 3.133\n",
      "  timestamp: 1672615524\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 520000\n",
      "  training_iteration: 130\n",
      "  trial_id: f9c59_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 9.5/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.87 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_3_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_0_6417799359d4e2284884fd3ae2ec9735, 0.0/6.0 CPU_group_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_1_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_5_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_4_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_2_6417799359d4e2284884fd3ae2ec9735)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                               | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridPOEnv-v0_f9c59_00000 | RUNNING  | 192.168.1.65:17432 |    130 |          7539.99 | 520000 | -190.812 |             -184.555 |             -217.659 |                400 |\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=17433)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=17431)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=17429)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=17428)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=17430)\u001b[0m  Retrying in 1 seconds\n",
      "Result for PPO_TrafficLightGridPOEnv-v0_f9c59_00000:\n",
      "  agent_timesteps_total: 524000\n",
      "  custom_metrics: {}\n",
      "  date: 2023-01-01_17-26-23\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -184.5886370268819\n",
      "  episode_reward_mean: -192.52836801761893\n",
      "  episode_reward_min: -217.65935107204243\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1310\n",
      "  experiment_id: 610b8b5226964181899719d25df7617e\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.734723475976807e-19\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -1.3896312583237886\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012472610242275733\n",
      "          policy_loss: -0.0020921059185639024\n",
      "          total_loss: 534.7693375587463\n",
      "          vf_explained_var: 7.69738107919693e-06\n",
      "          vf_loss: 534.7714285850525\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 524000\n",
      "    num_steps_sampled: 524000\n",
      "    num_steps_trained: 524000\n",
      "  iterations_since_restore: 131\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.476190476190474\n",
      "    ram_util_percent: 30.27023809523809\n",
      "  pid: 17432\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08849177323547758\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.77801628123049\n",
      "    mean_inference_ms: 1.8373890233591124\n",
      "    mean_raw_obs_processing_ms: 6.318996398052827\n",
      "  time_since_restore: 7599.273048877716\n",
      "  time_this_iter_s: 59.27817249298096\n",
      "  time_total_s: 7599.273048877716\n",
      "  timers:\n",
      "    learn_throughput: 155.572\n",
      "    learn_time_ms: 25711.644\n",
      "    sample_throughput: 124.652\n",
      "    sample_time_ms: 32089.297\n",
      "    update_time_ms: 3.155\n",
      "  timestamp: 1672615583\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 524000\n",
      "  training_iteration: 131\n",
      "  trial_id: f9c59_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 9.4/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.87 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_4_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_3_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_1_6417799359d4e2284884fd3ae2ec9735, 0.0/6.0 CPU_group_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_2_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_0_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_5_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 accelerator_type:G)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                               | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridPOEnv-v0_f9c59_00000 | RUNNING  | 192.168.1.65:17432 |    131 |          7599.27 | 524000 | -192.528 |             -184.589 |             -217.659 |                400 |\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridPOEnv-v0_f9c59_00000:\n",
      "  agent_timesteps_total: 528000\n",
      "  custom_metrics: {}\n",
      "  date: 2023-01-01_17-27-22\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -184.5886370268819\n",
      "  episode_reward_mean: -194.42050336088545\n",
      "  episode_reward_min: -217.882254556856\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1320\n",
      "  experiment_id: 610b8b5226964181899719d25df7617e\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.734723475976807e-19\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -1.3691293463110923\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012235217593116316\n",
      "          policy_loss: 0.002709279929695185\n",
      "          total_loss: 534.8545945167541\n",
      "          vf_explained_var: 1.6273557776003145e-05\n",
      "          vf_loss: 534.8518856525421\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 528000\n",
      "    num_steps_sampled: 528000\n",
      "    num_steps_trained: 528000\n",
      "  iterations_since_restore: 132\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.472619047619045\n",
      "    ram_util_percent: 30.266666666666673\n",
      "  pid: 17432\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08849195913962196\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.77762746450823\n",
      "    mean_inference_ms: 1.8373098261561334\n",
      "    mean_raw_obs_processing_ms: 6.317683367535669\n",
      "  time_since_restore: 7657.608540058136\n",
      "  time_this_iter_s: 58.33549118041992\n",
      "  time_total_s: 7657.608540058136\n",
      "  timers:\n",
      "    learn_throughput: 155.311\n",
      "    learn_time_ms: 25754.749\n",
      "    sample_throughput: 124.654\n",
      "    sample_time_ms: 32088.723\n",
      "    update_time_ms: 3.14\n",
      "  timestamp: 1672615642\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 528000\n",
      "  training_iteration: 132\n",
      "  trial_id: f9c59_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 9.5/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.87 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_1_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_0_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_2_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_3_6417799359d4e2284884fd3ae2ec9735, 0.0/6.0 CPU_group_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_4_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_5_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 accelerator_type:G)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                               | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridPOEnv-v0_f9c59_00000 | RUNNING  | 192.168.1.65:17432 |    132 |          7657.61 | 528000 | -194.421 |             -184.589 |             -217.882 |                400 |\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridPOEnv-v0_f9c59_00000:\n",
      "  agent_timesteps_total: 532000\n",
      "  custom_metrics: {}\n",
      "  date: 2023-01-01_17-28-19\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -184.62248846070176\n",
      "  episode_reward_mean: -196.6709183028523\n",
      "  episode_reward_min: -217.882254556856\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1330\n",
      "  experiment_id: 610b8b5226964181899719d25df7617e\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.734723475976807e-19\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -1.3691274497658015\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016318716117985188\n",
      "          policy_loss: -0.0037224707841232883\n",
      "          total_loss: 563.497831773758\n",
      "          vf_explained_var: 1.644101030251477e-05\n",
      "          vf_loss: 563.5015522956849\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 532000\n",
      "    num_steps_sampled: 532000\n",
      "    num_steps_trained: 532000\n",
      "  iterations_since_restore: 133\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.856097560975613\n",
      "    ram_util_percent: 30.410975609756097\n",
      "  pid: 17432\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08849121371186015\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.777022826042412\n",
      "    mean_inference_ms: 1.837252847919206\n",
      "    mean_raw_obs_processing_ms: 6.316432744930234\n",
      "  time_since_restore: 7714.776551485062\n",
      "  time_this_iter_s: 57.16801142692566\n",
      "  time_total_s: 7714.776551485062\n",
      "  timers:\n",
      "    learn_throughput: 155.595\n",
      "    learn_time_ms: 25707.766\n",
      "    sample_throughput: 124.746\n",
      "    sample_time_ms: 32065.238\n",
      "    update_time_ms: 3.149\n",
      "  timestamp: 1672615699\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 532000\n",
      "  training_iteration: 133\n",
      "  trial_id: f9c59_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Memory usage on this node: 9.5/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.87 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_3_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_2_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_0_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_5_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_1_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_4_6417799359d4e2284884fd3ae2ec9735, 0.0/6.0 CPU_group_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 accelerator_type:G)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                               | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridPOEnv-v0_f9c59_00000 | RUNNING  | 192.168.1.65:17432 |    133 |          7714.78 | 532000 | -196.671 |             -184.622 |             -217.882 |                400 |\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridPOEnv-v0_f9c59_00000:\n",
      "  agent_timesteps_total: 536000\n",
      "  custom_metrics: {}\n",
      "  date: 2023-01-01_17-29-18\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -185.66518698620797\n",
      "  episode_reward_mean: -199.08391046161216\n",
      "  episode_reward_min: -217.882254556856\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1340\n",
      "  experiment_id: 610b8b5226964181899719d25df7617e\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.734723475976807e-19\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -1.5003622997552157\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016531455168844172\n",
      "          policy_loss: 0.00048528369952691717\n",
      "          total_loss: 584.6177273750305\n",
      "          vf_explained_var: 2.5205687052221037e-05\n",
      "          vf_loss: 584.6172375202179\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 536000\n",
      "    num_steps_sampled: 536000\n",
      "    num_steps_trained: 536000\n",
      "  iterations_since_restore: 134\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.63734939759036\n",
      "    ram_util_percent: 30.381927710843378\n",
      "  pid: 17432\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08848996511829155\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.77708719622492\n",
      "    mean_inference_ms: 1.8371997086046037\n",
      "    mean_raw_obs_processing_ms: 6.315210724605702\n",
      "  time_since_restore: 7773.423260450363\n",
      "  time_this_iter_s: 58.646708965301514\n",
      "  time_total_s: 7773.423260450363\n",
      "  timers:\n",
      "    learn_throughput: 155.101\n",
      "    learn_time_ms: 25789.597\n",
      "    sample_throughput: 124.462\n",
      "    sample_time_ms: 32138.272\n",
      "    update_time_ms: 3.155\n",
      "  timestamp: 1672615758\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 536000\n",
      "  training_iteration: 134\n",
      "  trial_id: f9c59_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 9.4/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.87 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_5_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_0_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_4_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_1_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_3_6417799359d4e2284884fd3ae2ec9735, 0.0/6.0 CPU_group_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_2_6417799359d4e2284884fd3ae2ec9735)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                               | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridPOEnv-v0_f9c59_00000 | RUNNING  | 192.168.1.65:17432 |    134 |          7773.42 | 536000 | -199.084 |             -185.665 |             -217.882 |                400 |\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridPOEnv-v0_f9c59_00000:\n",
      "  agent_timesteps_total: 540000\n",
      "  custom_metrics: {}\n",
      "  date: 2023-01-01_17-30-17\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -185.69388479826821\n",
      "  episode_reward_mean: -201.43179596310026\n",
      "  episode_reward_min: -229.71726505918883\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1350\n",
      "  experiment_id: 610b8b5226964181899719d25df7617e\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.734723475976807e-19\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -1.5591593522578477\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010202276259383414\n",
      "          policy_loss: 0.001866181915102061\n",
      "          total_loss: 576.8188388347626\n",
      "          vf_explained_var: 3.06911751977168e-05\n",
      "          vf_loss: 576.8169726371765\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 540000\n",
      "    num_steps_sampled: 540000\n",
      "    num_steps_trained: 540000\n",
      "  iterations_since_restore: 135\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.503571428571426\n",
      "    ram_util_percent: 30.48214285714285\n",
      "  pid: 17432\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08848818317430325\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.77771824468226\n",
      "    mean_inference_ms: 1.837118586345044\n",
      "    mean_raw_obs_processing_ms: 6.3140939290589\n",
      "  time_since_restore: 7832.026828050613\n",
      "  time_this_iter_s: 58.603567600250244\n",
      "  time_total_s: 7832.026828050613\n",
      "  timers:\n",
      "    learn_throughput: 154.533\n",
      "    learn_time_ms: 25884.444\n",
      "    sample_throughput: 124.192\n",
      "    sample_time_ms: 32208.205\n",
      "    update_time_ms: 3.168\n",
      "  timestamp: 1672615817\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 540000\n",
      "  training_iteration: 135\n",
      "  trial_id: f9c59_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 9.5/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.87 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_3_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_1_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_5_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_2_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_4_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 accelerator_type:G, 0.0/6.0 CPU_group_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_0_6417799359d4e2284884fd3ae2ec9735)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                               | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridPOEnv-v0_f9c59_00000 | RUNNING  | 192.168.1.65:17432 |    135 |          7832.03 | 540000 | -201.432 |             -185.694 |             -229.717 |                400 |\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=17433)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=17430)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=17429)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=17428)\u001b[0m  Retrying in 1 seconds\n",
      "Result for PPO_TrafficLightGridPOEnv-v0_f9c59_00000:\n",
      "  agent_timesteps_total: 544000\n",
      "  custom_metrics: {}\n",
      "  date: 2023-01-01_17-31-15\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -185.9287466005066\n",
      "  episode_reward_mean: -204.96570662867597\n",
      "  episode_reward_min: -259.3916533651185\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1360\n",
      "  experiment_id: 610b8b5226964181899719d25df7617e\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.734723475976807e-19\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -1.5272345881909133\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010736569054017998\n",
      "          policy_loss: -0.00012546313446364364\n",
      "          total_loss: 608.4845476150513\n",
      "          vf_explained_var: 1.1175871339474952e-09\n",
      "          vf_loss: 608.484672832489\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 544000\n",
      "    num_steps_sampled: 544000\n",
      "    num_steps_trained: 544000\n",
      "  iterations_since_restore: 136\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.30952380952381\n",
      "    ram_util_percent: 30.52261904761904\n",
      "  pid: 17432\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08848591359625071\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.77815706179281\n",
      "    mean_inference_ms: 1.837017559710676\n",
      "    mean_raw_obs_processing_ms: 6.313773134188568\n",
      "  time_since_restore: 7890.698982954025\n",
      "  time_this_iter_s: 58.672154903411865\n",
      "  time_total_s: 7890.698982954025\n",
      "  timers:\n",
      "    learn_throughput: 154.429\n",
      "    learn_time_ms: 25901.893\n",
      "    sample_throughput: 123.9\n",
      "    sample_time_ms: 32284.177\n",
      "    update_time_ms: 3.184\n",
      "  timestamp: 1672615875\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 544000\n",
      "  training_iteration: 136\n",
      "  trial_id: f9c59_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 9.5/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.87 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_2_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_5_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_1_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_4_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_3_6417799359d4e2284884fd3ae2ec9735, 0.0/6.0 CPU_group_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_0_6417799359d4e2284884fd3ae2ec9735)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                               | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridPOEnv-v0_f9c59_00000 | RUNNING  | 192.168.1.65:17432 |    136 |           7890.7 | 544000 | -204.966 |             -185.929 |             -259.392 |                400 |\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridPOEnv-v0_f9c59_00000:\n",
      "  agent_timesteps_total: 548000\n",
      "  custom_metrics: {}\n",
      "  date: 2023-01-01_17-32-13\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -186.54926488754316\n",
      "  episode_reward_mean: -208.6504214908234\n",
      "  episode_reward_min: -291.14654481735363\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1370\n",
      "  experiment_id: 610b8b5226964181899719d25df7617e\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.734723475976807e-19\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -1.634105722233653\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016151405671326648\n",
      "          policy_loss: 0.001104728490463458\n",
      "          total_loss: 605.9951320648194\n",
      "          vf_explained_var: 1.4976971215219237e-05\n",
      "          vf_loss: 605.9940237522126\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 548000\n",
      "    num_steps_sampled: 548000\n",
      "    num_steps_trained: 548000\n",
      "  iterations_since_restore: 137\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.792682926829272\n",
      "    ram_util_percent: 30.475609756097562\n",
      "  pid: 17432\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08848464115507625\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.778448279675917\n",
      "    mean_inference_ms: 1.8370154357713813\n",
      "    mean_raw_obs_processing_ms: 6.313490588845223\n",
      "  time_since_restore: 7948.069796323776\n",
      "  time_this_iter_s: 57.37081336975098\n",
      "  time_total_s: 7948.069796323776\n",
      "  timers:\n",
      "    learn_throughput: 154.505\n",
      "    learn_time_ms: 25889.206\n",
      "    sample_throughput: 123.908\n",
      "    sample_time_ms: 32281.917\n",
      "    update_time_ms: 3.222\n",
      "  timestamp: 1672615933\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 548000\n",
      "  training_iteration: 137\n",
      "  trial_id: f9c59_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 9.4/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.87 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_1_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_4_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_2_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_5_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_3_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_0_6417799359d4e2284884fd3ae2ec9735, 0.0/6.0 CPU_group_6417799359d4e2284884fd3ae2ec9735)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                               | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridPOEnv-v0_f9c59_00000 | RUNNING  | 192.168.1.65:17432 |    137 |          7948.07 | 548000 |  -208.65 |             -186.549 |             -291.147 |                400 |\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridPOEnv-v0_f9c59_00000:\n",
      "  agent_timesteps_total: 552000\n",
      "  custom_metrics: {}\n",
      "  date: 2023-01-01_17-33-11\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -186.57625371913502\n",
      "  episode_reward_mean: -210.0003918992195\n",
      "  episode_reward_min: -291.14654481735363\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1380\n",
      "  experiment_id: 610b8b5226964181899719d25df7617e\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.734723475976807e-19\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -1.6020128976553678\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015624514894421243\n",
      "          policy_loss: -0.0021033481694757938\n",
      "          total_loss: 584.9966420173645\n",
      "          vf_explained_var: 8.76951980899321e-06\n",
      "          vf_loss: 584.9987440109253\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 552000\n",
      "    num_steps_sampled: 552000\n",
      "    num_steps_trained: 552000\n",
      "  iterations_since_restore: 138\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.645783132530124\n",
      "    ram_util_percent: 30.46024096385542\n",
      "  pid: 17432\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08848305308161412\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.7782899064032\n",
      "    mean_inference_ms: 1.8370077625250227\n",
      "    mean_raw_obs_processing_ms: 6.313199465159551\n",
      "  time_since_restore: 8006.017758607864\n",
      "  time_this_iter_s: 57.947962284088135\n",
      "  time_total_s: 8006.017758607864\n",
      "  timers:\n",
      "    learn_throughput: 154.272\n",
      "    learn_time_ms: 25928.311\n",
      "    sample_throughput: 124.091\n",
      "    sample_time_ms: 32234.357\n",
      "    update_time_ms: 3.198\n",
      "  timestamp: 1672615991\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 552000\n",
      "  training_iteration: 138\n",
      "  trial_id: f9c59_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Memory usage on this node: 9.5/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.87 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_2_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_5_6417799359d4e2284884fd3ae2ec9735, 0.0/6.0 CPU_group_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_3_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_1_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_0_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_4_6417799359d4e2284884fd3ae2ec9735)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                               | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridPOEnv-v0_f9c59_00000 | RUNNING  | 192.168.1.65:17432 |    138 |          8006.02 | 552000 |     -210 |             -186.576 |             -291.147 |                400 |\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridPOEnv-v0_f9c59_00000:\n",
      "  agent_timesteps_total: 556000\n",
      "  custom_metrics: {}\n",
      "  date: 2023-01-01_17-34-09\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -186.74110498240924\n",
      "  episode_reward_mean: -213.35463614083562\n",
      "  episode_reward_min: -291.14654481735363\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1390\n",
      "  experiment_id: 610b8b5226964181899719d25df7617e\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.734723475976807e-19\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -1.6352242793887855\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012118563305785776\n",
      "          policy_loss: 0.0010515521396882832\n",
      "          total_loss: 622.1750234127045\n",
      "          vf_explained_var: -2.7939677238464355e-09\n",
      "          vf_loss: 622.1739725589753\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 556000\n",
      "    num_steps_sampled: 556000\n",
      "    num_steps_trained: 556000\n",
      "  iterations_since_restore: 139\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.54578313253012\n",
      "    ram_util_percent: 30.51927710843374\n",
      "  pid: 17432\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08848129160040404\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.77812041407472\n",
      "    mean_inference_ms: 1.8369915280839288\n",
      "    mean_raw_obs_processing_ms: 6.312870914393019\n",
      "  time_since_restore: 8063.6944835186005\n",
      "  time_this_iter_s: 57.676724910736084\n",
      "  time_total_s: 8063.6944835186005\n",
      "  timers:\n",
      "    learn_throughput: 154.337\n",
      "    learn_time_ms: 25917.374\n",
      "    sample_throughput: 124.121\n",
      "    sample_time_ms: 32226.67\n",
      "    update_time_ms: 3.206\n",
      "  timestamp: 1672616049\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 556000\n",
      "  training_iteration: 139\n",
      "  trial_id: f9c59_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 9.5/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.87 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_0_6417799359d4e2284884fd3ae2ec9735, 0.0/6.0 CPU_group_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_4_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_5_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_1_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_3_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_2_6417799359d4e2284884fd3ae2ec9735)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                               | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridPOEnv-v0_f9c59_00000 | RUNNING  | 192.168.1.65:17432 |    139 |          8063.69 | 556000 | -213.355 |             -186.741 |             -291.147 |                400 |\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridPOEnv-v0_f9c59_00000:\n",
      "  agent_timesteps_total: 560000\n",
      "  custom_metrics: {}\n",
      "  date: 2023-01-01_17-35-07\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -186.74110498240924\n",
      "  episode_reward_mean: -215.63395219669445\n",
      "  episode_reward_min: -291.14654481735363\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1400\n",
      "  experiment_id: 610b8b5226964181899719d25df7617e\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.734723475976807e-19\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -1.673946747928858\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011230729628732682\n",
      "          policy_loss: -0.0004195739908027463\n",
      "          total_loss: 633.9387853622436\n",
      "          vf_explained_var: 2.3256241547642276e-05\n",
      "          vf_loss: 633.9392064094543\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 560000\n",
      "    num_steps_sampled: 560000\n",
      "    num_steps_trained: 560000\n",
      "  iterations_since_restore: 140\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.771084337349397\n",
      "    ram_util_percent: 30.569879518072288\n",
      "  pid: 17432\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08848009231611442\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.778024840208456\n",
      "    mean_inference_ms: 1.8369429188007098\n",
      "    mean_raw_obs_processing_ms: 6.312541620541522\n",
      "  time_since_restore: 8122.111684083939\n",
      "  time_this_iter_s: 58.417200565338135\n",
      "  time_total_s: 8122.111684083939\n",
      "  timers:\n",
      "    learn_throughput: 154.057\n",
      "    learn_time_ms: 25964.49\n",
      "    sample_throughput: 124.097\n",
      "    sample_time_ms: 32232.822\n",
      "    update_time_ms: 3.241\n",
      "  timestamp: 1672616107\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 560000\n",
      "  training_iteration: 140\n",
      "  trial_id: f9c59_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 9.5/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.87 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_4_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_5_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 accelerator_type:G, 0.0/6.0 CPU_group_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_0_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_1_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_3_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_2_6417799359d4e2284884fd3ae2ec9735)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                               | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridPOEnv-v0_f9c59_00000 | RUNNING  | 192.168.1.65:17432 |    140 |          8122.11 | 560000 | -215.634 |             -186.741 |             -291.147 |                400 |\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_TrafficLightGridPOEnv-v0_f9c59_00000:\n",
      "  agent_timesteps_total: 564000\n",
      "  custom_metrics: {}\n",
      "  date: 2023-01-01_17-36-05\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -190.3064341913851\n",
      "  episode_reward_mean: -216.88633773608905\n",
      "  episode_reward_min: -291.14654481735363\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1410\n",
      "  experiment_id: 610b8b5226964181899719d25df7617e\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.734723475976807e-19\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -1.7270173646509648\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013478260137254233\n",
      "          policy_loss: 0.002355198537406977\n",
      "          total_loss: 644.0137707710267\n",
      "          vf_explained_var: 5.7425349950790405e-06\n",
      "          vf_loss: 644.0114124298095\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 564000\n",
      "    num_steps_sampled: 564000\n",
      "    num_steps_trained: 564000\n",
      "  iterations_since_restore: 141\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.69759036144578\n",
      "    ram_util_percent: 30.63373493975903\n",
      "  pid: 17432\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.088478561089699\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.77825194706569\n",
      "    mean_inference_ms: 1.8368931553442747\n",
      "    mean_raw_obs_processing_ms: 6.311291977509946\n",
      "  time_since_restore: 8179.95804643631\n",
      "  time_this_iter_s: 57.846362352371216\n",
      "  time_total_s: 8179.95804643631\n",
      "  timers:\n",
      "    learn_throughput: 154.593\n",
      "    learn_time_ms: 25874.361\n",
      "    sample_throughput: 124.302\n",
      "    sample_time_ms: 32179.724\n",
      "    update_time_ms: 3.218\n",
      "  timestamp: 1672616165\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 564000\n",
      "  training_iteration: 141\n",
      "  trial_id: f9c59_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 9.6/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.87 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_0_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 accelerator_type:G, 0.0/6.0 CPU_group_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_3_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_4_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_5_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_1_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_2_6417799359d4e2284884fd3ae2ec9735)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                               | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridPOEnv-v0_f9c59_00000 | RUNNING  | 192.168.1.65:17432 |    141 |          8179.96 | 564000 | -216.886 |             -190.306 |             -291.147 |                400 |\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridPOEnv-v0_f9c59_00000:\n",
      "  agent_timesteps_total: 568000\n",
      "  custom_metrics: {}\n",
      "  date: 2023-01-01_17-37-04\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -191.16233316464567\n",
      "  episode_reward_mean: -220.44907931059038\n",
      "  episode_reward_min: -291.14654481735363\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1420\n",
      "  experiment_id: 610b8b5226964181899719d25df7617e\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.734723475976807e-19\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -1.7548034124076366\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011632247351099408\n",
      "          policy_loss: 0.003182581765577197\n",
      "          total_loss: 613.9359989643096\n",
      "          vf_explained_var: 1.7440132069168612e-05\n",
      "          vf_loss: 613.9328155994415\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 568000\n",
      "    num_steps_sampled: 568000\n",
      "    num_steps_trained: 568000\n",
      "  iterations_since_restore: 142\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.666265060240963\n",
      "    ram_util_percent: 30.661445783132535\n",
      "  pid: 17432\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08847666832992378\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.77844579581006\n",
      "    mean_inference_ms: 1.836837898627914\n",
      "    mean_raw_obs_processing_ms: 6.310018977301176\n",
      "  time_since_restore: 8238.369067430496\n",
      "  time_this_iter_s: 58.4110209941864\n",
      "  time_total_s: 8238.369067430496\n",
      "  timers:\n",
      "    learn_throughput: 154.476\n",
      "    learn_time_ms: 25894.019\n",
      "    sample_throughput: 124.349\n",
      "    sample_time_ms: 32167.622\n",
      "    update_time_ms: 3.265\n",
      "  timestamp: 1672616224\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 568000\n",
      "  training_iteration: 142\n",
      "  trial_id: f9c59_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 9.6/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.87 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_1_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_3_6417799359d4e2284884fd3ae2ec9735, 0.0/6.0 CPU_group_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_2_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_5_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_4_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_0_6417799359d4e2284884fd3ae2ec9735)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                               | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridPOEnv-v0_f9c59_00000 | RUNNING  | 192.168.1.65:17432 |    142 |          8238.37 | 568000 | -220.449 |             -191.162 |             -291.147 |                400 |\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridPOEnv-v0_f9c59_00000:\n",
      "  agent_timesteps_total: 572000\n",
      "  custom_metrics: {}\n",
      "  date: 2023-01-01_17-38-02\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -192.3765868854594\n",
      "  episode_reward_mean: -223.64894658300753\n",
      "  episode_reward_min: -291.14654481735363\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1430\n",
      "  experiment_id: 610b8b5226964181899719d25df7617e\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.734723475976807e-19\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -1.8271784503012896\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014209538307250389\n",
      "          policy_loss: 0.0006126021762611344\n",
      "          total_loss: 639.5523648262024\n",
      "          vf_explained_var: 2.435930036881473e-05\n",
      "          vf_loss: 639.5517534732819\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 572000\n",
      "    num_steps_sampled: 572000\n",
      "    num_steps_trained: 572000\n",
      "  iterations_since_restore: 143\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.632530120481928\n",
      "    ram_util_percent: 30.654216867469877\n",
      "  pid: 17432\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08847568870549363\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.77885210022093\n",
      "    mean_inference_ms: 1.8367756791977161\n",
      "    mean_raw_obs_processing_ms: 6.308793640895669\n",
      "  time_since_restore: 8296.329124450684\n",
      "  time_this_iter_s: 57.96005702018738\n",
      "  time_total_s: 8296.329124450684\n",
      "  timers:\n",
      "    learn_throughput: 154.239\n",
      "    learn_time_ms: 25933.76\n",
      "    sample_throughput: 124.196\n",
      "    sample_time_ms: 32207.106\n",
      "    update_time_ms: 3.256\n",
      "  timestamp: 1672616282\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 572000\n",
      "  training_iteration: 143\n",
      "  trial_id: f9c59_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Memory usage on this node: 9.5/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.87 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_2_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_0_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_1_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_4_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_5_6417799359d4e2284884fd3ae2ec9735, 0.0/6.0 CPU_group_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_3_6417799359d4e2284884fd3ae2ec9735)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                               | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridPOEnv-v0_f9c59_00000 | RUNNING  | 192.168.1.65:17432 |    143 |          8296.33 | 572000 | -223.649 |             -192.377 |             -291.147 |                400 |\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridPOEnv-v0_f9c59_00000:\n",
      "  agent_timesteps_total: 576000\n",
      "  custom_metrics: {}\n",
      "  date: 2023-01-01_17-39-01\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -192.3765868854594\n",
      "  episode_reward_mean: -229.54370071256255\n",
      "  episode_reward_min: -313.5395925553535\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1440\n",
      "  experiment_id: 610b8b5226964181899719d25df7617e\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.734723475976807e-19\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -1.8298191335052252\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014379398872302088\n",
      "          policy_loss: -0.0004022395791253075\n",
      "          total_loss: 661.1480628013611\n",
      "          vf_explained_var: 0.00010738354467321187\n",
      "          vf_loss: 661.1484642028809\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 576000\n",
      "    num_steps_sampled: 576000\n",
      "    num_steps_trained: 576000\n",
      "  iterations_since_restore: 144\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.448809523809526\n",
      "    ram_util_percent: 30.72857142857144\n",
      "  pid: 17432\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08847475384424298\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.77901458527528\n",
      "    mean_inference_ms: 1.8367296098412458\n",
      "    mean_raw_obs_processing_ms: 6.307651495391201\n",
      "  time_since_restore: 8354.569123506546\n",
      "  time_this_iter_s: 58.23999905586243\n",
      "  time_total_s: 8354.569123506546\n",
      "  timers:\n",
      "    learn_throughput: 154.355\n",
      "    learn_time_ms: 25914.333\n",
      "    sample_throughput: 124.278\n",
      "    sample_time_ms: 32185.877\n",
      "    update_time_ms: 3.265\n",
      "  timestamp: 1672616341\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 576000\n",
      "  training_iteration: 144\n",
      "  trial_id: f9c59_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 9.6/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.87 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_3_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_1_6417799359d4e2284884fd3ae2ec9735, 0.0/6.0 CPU_group_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_4_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_5_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_0_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_2_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 accelerator_type:G)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                               | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridPOEnv-v0_f9c59_00000 | RUNNING  | 192.168.1.65:17432 |    144 |          8354.57 | 576000 | -229.544 |             -192.377 |              -313.54 |                400 |\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridPOEnv-v0_f9c59_00000:\n",
      "  agent_timesteps_total: 580000\n",
      "  custom_metrics: {}\n",
      "  date: 2023-01-01_17-39-58\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -192.3765868854594\n",
      "  episode_reward_mean: -230.6904409675527\n",
      "  episode_reward_min: -313.5395925553535\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1450\n",
      "  experiment_id: 610b8b5226964181899719d25df7617e\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.734723475976807e-19\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -1.8641198560595513\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014200508022531987\n",
      "          policy_loss: -0.0007400429807603359\n",
      "          total_loss: 709.4406327009201\n",
      "          vf_explained_var: 2.428516836516792e-06\n",
      "          vf_loss: 709.4413764715194\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 580000\n",
      "    num_steps_sampled: 580000\n",
      "    num_steps_trained: 580000\n",
      "  iterations_since_restore: 145\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.797560975609755\n",
      "    ram_util_percent: 30.753658536585373\n",
      "  pid: 17432\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08847380414705577\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.778784387243842\n",
      "    mean_inference_ms: 1.8366772794654225\n",
      "    mean_raw_obs_processing_ms: 6.306515985134791\n",
      "  time_since_restore: 8411.933078289032\n",
      "  time_this_iter_s: 57.36395478248596\n",
      "  time_total_s: 8411.933078289032\n",
      "  timers:\n",
      "    learn_throughput: 154.794\n",
      "    learn_time_ms: 25840.841\n",
      "    sample_throughput: 124.473\n",
      "    sample_time_ms: 32135.466\n",
      "    update_time_ms: 3.251\n",
      "  timestamp: 1672616398\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 580000\n",
      "  training_iteration: 145\n",
      "  trial_id: f9c59_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 9.6/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.87 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_5_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_4_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_2_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_1_6417799359d4e2284884fd3ae2ec9735, 0.0/6.0 CPU_group_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_0_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_3_6417799359d4e2284884fd3ae2ec9735)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                               | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridPOEnv-v0_f9c59_00000 | RUNNING  | 192.168.1.65:17432 |    145 |          8411.93 | 580000 |  -230.69 |             -192.377 |              -313.54 |                400 |\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_TrafficLightGridPOEnv-v0_f9c59_00000:\n",
      "  agent_timesteps_total: 584000\n",
      "  custom_metrics: {}\n",
      "  date: 2023-01-01_17-40-57\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -192.3765868854594\n",
      "  episode_reward_mean: -229.96020594121026\n",
      "  episode_reward_min: -313.5395925553535\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1460\n",
      "  experiment_id: 610b8b5226964181899719d25df7617e\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.734723475976807e-19\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -1.9147825673222543\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012724326946409592\n",
      "          policy_loss: -0.00048751059803180395\n",
      "          total_loss: 715.0339737892151\n",
      "          vf_explained_var: -3.180652811352047e-06\n",
      "          vf_loss: 715.034467458725\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 584000\n",
      "    num_steps_sampled: 584000\n",
      "    num_steps_trained: 584000\n",
      "  iterations_since_restore: 146\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.424999999999997\n",
      "    ram_util_percent: 30.699999999999996\n",
      "  pid: 17432\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08847425649550843\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.779007729207443\n",
      "    mean_inference_ms: 1.8366590058695726\n",
      "    mean_raw_obs_processing_ms: 6.30466401091629\n",
      "  time_since_restore: 8470.60453748703\n",
      "  time_this_iter_s: 58.67145919799805\n",
      "  time_total_s: 8470.60453748703\n",
      "  timers:\n",
      "    learn_throughput: 154.52\n",
      "    learn_time_ms: 25886.645\n",
      "    sample_throughput: 124.651\n",
      "    sample_time_ms: 32089.621\n",
      "    update_time_ms: 3.284\n",
      "  timestamp: 1672616457\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 584000\n",
      "  training_iteration: 146\n",
      "  trial_id: f9c59_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 9.5/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.87 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_5_6417799359d4e2284884fd3ae2ec9735, 0.0/6.0 CPU_group_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_0_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_1_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_2_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_3_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_4_6417799359d4e2284884fd3ae2ec9735)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                               | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridPOEnv-v0_f9c59_00000 | RUNNING  | 192.168.1.65:17432 |    146 |           8470.6 | 584000 |  -229.96 |             -192.377 |              -313.54 |                400 |\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridPOEnv-v0_f9c59_00000:\n",
      "  agent_timesteps_total: 588000\n",
      "  custom_metrics: {}\n",
      "  date: 2023-01-01_17-41-55\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -192.3765868854594\n",
      "  episode_reward_mean: -230.8570251546889\n",
      "  episode_reward_min: -313.5395925553535\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1470\n",
      "  experiment_id: 610b8b5226964181899719d25df7617e\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.734723475976807e-19\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -1.9183464661240577\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010534826074990811\n",
      "          policy_loss: -0.0011727172648534178\n",
      "          total_loss: 715.236626625061\n",
      "          vf_explained_var: 2.3414566385326907e-05\n",
      "          vf_loss: 715.2377985954284\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 588000\n",
      "    num_steps_sampled: 588000\n",
      "    num_steps_trained: 588000\n",
      "  iterations_since_restore: 147\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.75121951219512\n",
      "    ram_util_percent: 30.83780487804878\n",
      "  pid: 17432\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0884744621398076\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.779611795786856\n",
      "    mean_inference_ms: 1.8366002393783232\n",
      "    mean_raw_obs_processing_ms: 6.3028521836293\n",
      "  time_since_restore: 8528.501602172852\n",
      "  time_this_iter_s: 57.89706468582153\n",
      "  time_total_s: 8528.501602172852\n",
      "  timers:\n",
      "    learn_throughput: 154.458\n",
      "    learn_time_ms: 25896.975\n",
      "    sample_throughput: 124.487\n",
      "    sample_time_ms: 32131.998\n",
      "    update_time_ms: 3.263\n",
      "  timestamp: 1672616515\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 588000\n",
      "  training_iteration: 147\n",
      "  trial_id: f9c59_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 9.6/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.87 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_3_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_1_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_4_6417799359d4e2284884fd3ae2ec9735, 0.0/6.0 CPU_group_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_0_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_5_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_2_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 accelerator_type:G)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                               | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridPOEnv-v0_f9c59_00000 | RUNNING  | 192.168.1.65:17432 |    147 |           8528.5 | 588000 | -230.857 |             -192.377 |              -313.54 |                400 |\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridPOEnv-v0_f9c59_00000:\n",
      "  agent_timesteps_total: 592000\n",
      "  custom_metrics: {}\n",
      "  date: 2023-01-01_17-42-53\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -191.09749850956823\n",
      "  episode_reward_mean: -230.85993962341254\n",
      "  episode_reward_min: -313.5395925553535\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1480\n",
      "  experiment_id: 610b8b5226964181899719d25df7617e\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.734723475976807e-19\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -1.9844557303935288\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013436445135721883\n",
      "          policy_loss: -0.0015909667796222492\n",
      "          total_loss: 723.9859693527221\n",
      "          vf_explained_var: 8.476898187836923e-07\n",
      "          vf_loss: 723.9875625610351\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 592000\n",
      "    num_steps_sampled: 592000\n",
      "    num_steps_trained: 592000\n",
      "  iterations_since_restore: 148\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.54047619047619\n",
      "    ram_util_percent: 30.90119047619048\n",
      "  pid: 17432\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08847423669314514\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.78050283008344\n",
      "    mean_inference_ms: 1.8365321674489652\n",
      "    mean_raw_obs_processing_ms: 6.301077453623098\n",
      "  time_since_restore: 8586.631762266159\n",
      "  time_this_iter_s: 58.130160093307495\n",
      "  time_total_s: 8586.631762266159\n",
      "  timers:\n",
      "    learn_throughput: 154.576\n",
      "    learn_time_ms: 25877.199\n",
      "    sample_throughput: 124.34\n",
      "    sample_time_ms: 32169.98\n",
      "    update_time_ms: 3.273\n",
      "  timestamp: 1672616573\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 592000\n",
      "  training_iteration: 148\n",
      "  trial_id: f9c59_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Memory usage on this node: 9.6/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.87 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_0_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_1_6417799359d4e2284884fd3ae2ec9735, 0.0/6.0 CPU_group_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_2_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_5_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_3_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_4_6417799359d4e2284884fd3ae2ec9735)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                               | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridPOEnv-v0_f9c59_00000 | RUNNING  | 192.168.1.65:17432 |    148 |          8586.63 | 592000 |  -230.86 |             -191.097 |              -313.54 |                400 |\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridPOEnv-v0_f9c59_00000:\n",
      "  agent_timesteps_total: 596000\n",
      "  custom_metrics: {}\n",
      "  date: 2023-01-01_17-43-52\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -191.09749850956823\n",
      "  episode_reward_mean: -229.20474649353028\n",
      "  episode_reward_min: -313.5395925553535\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1490\n",
      "  experiment_id: 610b8b5226964181899719d25df7617e\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.734723475976807e-19\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -2.108445281162858\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014684464519132412\n",
      "          policy_loss: 0.000989269302226603\n",
      "          total_loss: 692.5055159568786\n",
      "          vf_explained_var: 2.9103830456733704e-06\n",
      "          vf_loss: 692.5045297145844\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 596000\n",
      "    num_steps_sampled: 596000\n",
      "    num_steps_trained: 596000\n",
      "  iterations_since_restore: 149\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.60963855421687\n",
      "    ram_util_percent: 30.859036144578315\n",
      "  pid: 17432\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08847422863587026\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.781720757428303\n",
      "    mean_inference_ms: 1.8364826649642843\n",
      "    mean_raw_obs_processing_ms: 6.299405907725067\n",
      "  time_since_restore: 8645.090413331985\n",
      "  time_this_iter_s: 58.458651065826416\n",
      "  time_total_s: 8645.090413331985\n",
      "  timers:\n",
      "    learn_throughput: 154.436\n",
      "    learn_time_ms: 25900.771\n",
      "    sample_throughput: 124.129\n",
      "    sample_time_ms: 32224.649\n",
      "    update_time_ms: 3.256\n",
      "  timestamp: 1672616632\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 596000\n",
      "  training_iteration: 149\n",
      "  trial_id: f9c59_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 9.6/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.87 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_3_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_2_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_1_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_4_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_0_6417799359d4e2284884fd3ae2ec9735, 0.0/6.0 CPU_group_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_5_6417799359d4e2284884fd3ae2ec9735)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                               | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridPOEnv-v0_f9c59_00000 | RUNNING  | 192.168.1.65:17432 |    149 |          8645.09 | 596000 | -229.205 |             -191.097 |              -313.54 |                400 |\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridPOEnv-v0_f9c59_00000:\n",
      "  agent_timesteps_total: 600000\n",
      "  custom_metrics: {}\n",
      "  date: 2023-01-01_17-44-50\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -186.58467194067134\n",
      "  episode_reward_mean: -226.81848745731742\n",
      "  episode_reward_min: -313.5395925553535\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1500\n",
      "  experiment_id: 610b8b5226964181899719d25df7617e\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.734723475976807e-19\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -2.1756411612033846\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014668263067596854\n",
      "          policy_loss: 0.0013528645024052822\n",
      "          total_loss: 664.6250769615174\n",
      "          vf_explained_var: 4.160031494393479e-06\n",
      "          vf_loss: 664.6237264633179\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 600000\n",
      "    num_steps_sampled: 600000\n",
      "    num_steps_trained: 600000\n",
      "  iterations_since_restore: 150\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.675903614457834\n",
      "    ram_util_percent: 30.92650602409638\n",
      "  pid: 17432\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08847380770746045\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.78303142957921\n",
      "    mean_inference_ms: 1.8364448267343\n",
      "    mean_raw_obs_processing_ms: 6.29781548033364\n",
      "  time_since_restore: 8703.300125598907\n",
      "  time_this_iter_s: 58.209712266922\n",
      "  time_total_s: 8703.300125598907\n",
      "  timers:\n",
      "    learn_throughput: 154.665\n",
      "    learn_time_ms: 25862.363\n",
      "    sample_throughput: 124.063\n",
      "    sample_time_ms: 32241.762\n",
      "    update_time_ms: 3.351\n",
      "  timestamp: 1672616690\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 600000\n",
      "  training_iteration: 150\n",
      "  trial_id: f9c59_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 9.7/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.87 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_4_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_3_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 accelerator_type:G, 0.0/6.0 CPU_group_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_2_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_0_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_5_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_1_6417799359d4e2284884fd3ae2ec9735)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                               | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridPOEnv-v0_f9c59_00000 | RUNNING  | 192.168.1.65:17432 |    150 |           8703.3 | 600000 | -226.818 |             -186.585 |              -313.54 |                400 |\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_TrafficLightGridPOEnv-v0_f9c59_00000:\n",
      "  agent_timesteps_total: 604000\n",
      "  custom_metrics: {}\n",
      "  date: 2023-01-01_17-45-48\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -186.58467194067134\n",
      "  episode_reward_mean: -225.35870213517984\n",
      "  episode_reward_min: -313.5395925553535\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1510\n",
      "  experiment_id: 610b8b5226964181899719d25df7617e\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.734723475976807e-19\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -2.2003790736198425\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013523822028674104\n",
      "          policy_loss: 0.0006795918743591756\n",
      "          total_loss: 658.4039381980896\n",
      "          vf_explained_var: 4.647299647331238e-06\n",
      "          vf_loss: 658.4032599449158\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 604000\n",
      "    num_steps_sampled: 604000\n",
      "    num_steps_trained: 604000\n",
      "  iterations_since_restore: 151\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.719277108433737\n",
      "    ram_util_percent: 30.998795180722887\n",
      "  pid: 17432\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08847447650355797\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.784267356704742\n",
      "    mean_inference_ms: 1.8364713723591382\n",
      "    mean_raw_obs_processing_ms: 6.296288754939396\n",
      "  time_since_restore: 8761.19279909134\n",
      "  time_this_iter_s: 57.89267349243164\n",
      "  time_total_s: 8761.19279909134\n",
      "  timers:\n",
      "    learn_throughput: 154.638\n",
      "    learn_time_ms: 25866.821\n",
      "    sample_throughput: 124.062\n",
      "    sample_time_ms: 32241.937\n",
      "    update_time_ms: 3.359\n",
      "  timestamp: 1672616748\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 604000\n",
      "  training_iteration: 151\n",
      "  trial_id: f9c59_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 9.7/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.87 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_5_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_0_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_4_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_3_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_1_6417799359d4e2284884fd3ae2ec9735, 0.0/6.0 CPU_group_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_2_6417799359d4e2284884fd3ae2ec9735)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                               | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridPOEnv-v0_f9c59_00000 | RUNNING  | 192.168.1.65:17432 |    151 |          8761.19 | 604000 | -225.359 |             -186.585 |              -313.54 |                400 |\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridPOEnv-v0_f9c59_00000:\n",
      "  agent_timesteps_total: 608000\n",
      "  custom_metrics: {}\n",
      "  date: 2023-01-01_17-46-47\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -186.58467194067134\n",
      "  episode_reward_mean: -220.456577719917\n",
      "  episode_reward_min: -313.5395925553535\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1520\n",
      "  experiment_id: 610b8b5226964181899719d25df7617e\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.734723475976807e-19\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -2.2501154780387878\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.018612206598073094\n",
      "          policy_loss: -0.0017948658467503264\n",
      "          total_loss: 618.714398097992\n",
      "          vf_explained_var: 2.0422041870915564e-06\n",
      "          vf_loss: 618.7161918640137\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 608000\n",
      "    num_steps_sampled: 608000\n",
      "    num_steps_trained: 608000\n",
      "  iterations_since_restore: 152\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.583333333333332\n",
      "    ram_util_percent: 30.927380952380958\n",
      "  pid: 17432\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0884755362296177\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.78560502849051\n",
      "    mean_inference_ms: 1.8365206377538987\n",
      "    mean_raw_obs_processing_ms: 6.2948193638994825\n",
      "  time_since_restore: 8819.78144454956\n",
      "  time_this_iter_s: 58.588645458221436\n",
      "  time_total_s: 8819.78144454956\n",
      "  timers:\n",
      "    learn_throughput: 154.691\n",
      "    learn_time_ms: 25857.92\n",
      "    sample_throughput: 123.96\n",
      "    sample_time_ms: 32268.562\n",
      "    update_time_ms: 3.331\n",
      "  timestamp: 1672616807\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 608000\n",
      "  training_iteration: 152\n",
      "  trial_id: f9c59_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 9.6/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.87 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_5_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_0_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_2_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_3_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_4_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_1_6417799359d4e2284884fd3ae2ec9735, 0.0/6.0 CPU_group_6417799359d4e2284884fd3ae2ec9735)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                               | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridPOEnv-v0_f9c59_00000 | RUNNING  | 192.168.1.65:17432 |    152 |          8819.78 | 608000 | -220.457 |             -186.585 |              -313.54 |                400 |\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridPOEnv-v0_f9c59_00000:\n",
      "  agent_timesteps_total: 612000\n",
      "  custom_metrics: {}\n",
      "  date: 2023-01-01_17-47-45\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -186.51609587469991\n",
      "  episode_reward_mean: -215.6584611720119\n",
      "  episode_reward_min: -313.5395925553535\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1530\n",
      "  experiment_id: 610b8b5226964181899719d25df7617e\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.734723475976807e-19\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -2.3335687525570394\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013140699479390788\n",
      "          policy_loss: 0.0007200241976534017\n",
      "          total_loss: 624.1942215919495\n",
      "          vf_explained_var: 8.758157719057635e-07\n",
      "          vf_loss: 624.1935014247895\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 612000\n",
      "    num_steps_sampled: 612000\n",
      "    num_steps_trained: 612000\n",
      "  iterations_since_restore: 153\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.597590361445782\n",
      "    ram_util_percent: 30.891566265060234\n",
      "  pid: 17432\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08847606120401219\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.786892316076088\n",
      "    mean_inference_ms: 1.83657061414253\n",
      "    mean_raw_obs_processing_ms: 6.293318537337944\n",
      "  time_since_restore: 8878.059575557709\n",
      "  time_this_iter_s: 58.27813100814819\n",
      "  time_total_s: 8878.059575557709\n",
      "  timers:\n",
      "    learn_throughput: 154.445\n",
      "    learn_time_ms: 25899.104\n",
      "    sample_throughput: 123.995\n",
      "    sample_time_ms: 32259.316\n",
      "    update_time_ms: 3.325\n",
      "  timestamp: 1672616865\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 612000\n",
      "  training_iteration: 153\n",
      "  trial_id: f9c59_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Memory usage on this node: 9.7/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.87 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_0_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_4_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_5_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_2_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_1_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 accelerator_type:G, 0.0/6.0 CPU_group_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_3_6417799359d4e2284884fd3ae2ec9735)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                               | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridPOEnv-v0_f9c59_00000 | RUNNING  | 192.168.1.65:17432 |    153 |          8878.06 | 612000 | -215.658 |             -186.516 |              -313.54 |                400 |\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridPOEnv-v0_f9c59_00000:\n",
      "  agent_timesteps_total: 616000\n",
      "  custom_metrics: {}\n",
      "  date: 2023-01-01_17-48-44\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -186.48860157783588\n",
      "  episode_reward_mean: -207.2951127496565\n",
      "  episode_reward_min: -300.6148800919135\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1540\n",
      "  experiment_id: 610b8b5226964181899719d25df7617e\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.734723475976807e-19\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -2.42004576548934\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.018444636956064995\n",
      "          policy_loss: 0.0012105988571420312\n",
      "          total_loss: 593.0032531738282\n",
      "          vf_explained_var: 1.637823856981413e-06\n",
      "          vf_loss: 593.0020467758179\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 616000\n",
      "    num_steps_sampled: 616000\n",
      "    num_steps_trained: 616000\n",
      "  iterations_since_restore: 154\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.579761904761906\n",
      "    ram_util_percent: 31.06785714285713\n",
      "  pid: 17432\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08847718202346375\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.788212143469813\n",
      "    mean_inference_ms: 1.8366091290551145\n",
      "    mean_raw_obs_processing_ms: 6.291798985421897\n",
      "  time_since_restore: 8936.334857463837\n",
      "  time_this_iter_s: 58.27528190612793\n",
      "  time_total_s: 8936.334857463837\n",
      "  timers:\n",
      "    learn_throughput: 154.413\n",
      "    learn_time_ms: 25904.478\n",
      "    sample_throughput: 124.002\n",
      "    sample_time_ms: 32257.509\n",
      "    update_time_ms: 3.315\n",
      "  timestamp: 1672616924\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 616000\n",
      "  training_iteration: 154\n",
      "  trial_id: f9c59_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 9.7/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.87 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_0_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 accelerator_type:G, 0.0/6.0 CPU_group_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_4_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_3_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_2_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_5_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_1_6417799359d4e2284884fd3ae2ec9735)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                               | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridPOEnv-v0_f9c59_00000 | RUNNING  | 192.168.1.65:17432 |    154 |          8936.33 | 616000 | -207.295 |             -186.489 |             -300.615 |                400 |\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridPOEnv-v0_f9c59_00000:\n",
      "  agent_timesteps_total: 620000\n",
      "  custom_metrics: {}\n",
      "  date: 2023-01-01_17-49-43\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -186.48860157783588\n",
      "  episode_reward_mean: -203.71582420504282\n",
      "  episode_reward_min: -300.6148800919135\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1550\n",
      "  experiment_id: 610b8b5226964181899719d25df7617e\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.734723475976807e-19\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -2.4948904253542423\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.019018351450722993\n",
      "          policy_loss: -0.0016995413607219234\n",
      "          total_loss: 591.8749637126923\n",
      "          vf_explained_var: 2.0390375539136585e-06\n",
      "          vf_loss: 591.8766636371613\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 620000\n",
      "    num_steps_sampled: 620000\n",
      "    num_steps_trained: 620000\n",
      "  iterations_since_restore: 155\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.866666666666667\n",
      "    ram_util_percent: 30.983333333333338\n",
      "  pid: 17432\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08847920040810907\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.789819656965776\n",
      "    mean_inference_ms: 1.8367302352068293\n",
      "    mean_raw_obs_processing_ms: 6.290328587407089\n",
      "  time_since_restore: 8995.064639568329\n",
      "  time_this_iter_s: 58.72978210449219\n",
      "  time_total_s: 8995.064639568329\n",
      "  timers:\n",
      "    learn_throughput: 153.899\n",
      "    learn_time_ms: 25991.036\n",
      "    sample_throughput: 123.81\n",
      "    sample_time_ms: 32307.507\n",
      "    update_time_ms: 3.313\n",
      "  timestamp: 1672616983\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 620000\n",
      "  training_iteration: 155\n",
      "  trial_id: f9c59_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 9.6/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.87 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_4_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_0_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_1_6417799359d4e2284884fd3ae2ec9735, 0.0/6.0 CPU_group_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_5_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_3_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_2_6417799359d4e2284884fd3ae2ec9735)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                               | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridPOEnv-v0_f9c59_00000 | RUNNING  | 192.168.1.65:17432 |    155 |          8995.06 | 620000 | -203.716 |             -186.489 |             -300.615 |                400 |\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_TrafficLightGridPOEnv-v0_f9c59_00000:\n",
      "  agent_timesteps_total: 624000\n",
      "  custom_metrics: {}\n",
      "  date: 2023-01-01_17-50-42\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -186.48860157783588\n",
      "  episode_reward_mean: -200.39424924272717\n",
      "  episode_reward_min: -300.6148800919135\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1560\n",
      "  experiment_id: 610b8b5226964181899719d25df7617e\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.734723475976807e-19\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -2.6635240502655506\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.020569298175814855\n",
      "          policy_loss: -6.430732319131493e-05\n",
      "          total_loss: 584.6453175544739\n",
      "          vf_explained_var: 6.543285962834489e-06\n",
      "          vf_loss: 584.6453817844391\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 624000\n",
      "    num_steps_sampled: 624000\n",
      "    num_steps_trained: 624000\n",
      "  iterations_since_restore: 156\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.34642857142857\n",
      "    ram_util_percent: 31.108333333333327\n",
      "  pid: 17432\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08847914819087485\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.791519070863743\n",
      "    mean_inference_ms: 1.836789603566159\n",
      "    mean_raw_obs_processing_ms: 6.288933505848645\n",
      "  time_since_restore: 9053.723722934723\n",
      "  time_this_iter_s: 58.65908336639404\n",
      "  time_total_s: 9053.723722934723\n",
      "  timers:\n",
      "    learn_throughput: 153.982\n",
      "    learn_time_ms: 25977.025\n",
      "    sample_throughput: 123.761\n",
      "    sample_time_ms: 32320.245\n",
      "    update_time_ms: 3.308\n",
      "  timestamp: 1672617042\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 624000\n",
      "  training_iteration: 156\n",
      "  trial_id: f9c59_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 9.7/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.87 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_2_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_0_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_1_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_3_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_5_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_4_6417799359d4e2284884fd3ae2ec9735, 0.0/6.0 CPU_group_6417799359d4e2284884fd3ae2ec9735)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                               | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridPOEnv-v0_f9c59_00000 | RUNNING  | 192.168.1.65:17432 |    156 |          9053.72 | 624000 | -200.394 |             -186.489 |             -300.615 |                400 |\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridPOEnv-v0_f9c59_00000:\n",
      "  agent_timesteps_total: 628000\n",
      "  custom_metrics: {}\n",
      "  date: 2023-01-01_17-51-40\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -185.72195092925975\n",
      "  episode_reward_mean: -195.66391570435917\n",
      "  episode_reward_min: -218.78124352286173\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1570\n",
      "  experiment_id: 610b8b5226964181899719d25df7617e\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.734723475976807e-19\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -2.7740314960479737\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017037447304404367\n",
      "          policy_loss: -0.0009151154328719713\n",
      "          total_loss: 575.7955636501313\n",
      "          vf_explained_var: 6.692670467600692e-06\n",
      "          vf_loss: 575.7964752197265\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 628000\n",
      "    num_steps_sampled: 628000\n",
      "    num_steps_trained: 628000\n",
      "  iterations_since_restore: 157\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.740963855421686\n",
      "    ram_util_percent: 31.136144578313246\n",
      "  pid: 17432\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08847909168966428\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.79329154492855\n",
      "    mean_inference_ms: 1.8368508531324717\n",
      "    mean_raw_obs_processing_ms: 6.287591922987999\n",
      "  time_since_restore: 9112.069642543793\n",
      "  time_this_iter_s: 58.345919609069824\n",
      "  time_total_s: 9112.069642543793\n",
      "  timers:\n",
      "    learn_throughput: 153.84\n",
      "    learn_time_ms: 26001.097\n",
      "    sample_throughput: 123.682\n",
      "    sample_time_ms: 32341.005\n",
      "    update_time_ms: 3.302\n",
      "  timestamp: 1672617100\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 628000\n",
      "  training_iteration: 157\n",
      "  trial_id: f9c59_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 9.7/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.87 GiB heap, 0.0/14.65 GiB objects (0.0/6.0 CPU_group_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_3_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_0_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_5_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_4_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_1_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_2_6417799359d4e2284884fd3ae2ec9735)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                               | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridPOEnv-v0_f9c59_00000 | RUNNING  | 192.168.1.65:17432 |    157 |          9112.07 | 628000 | -195.664 |             -185.722 |             -218.781 |                400 |\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridPOEnv-v0_f9c59_00000:\n",
      "  agent_timesteps_total: 632000\n",
      "  custom_metrics: {}\n",
      "  date: 2023-01-01_17-52-38\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -185.70719966880566\n",
      "  episode_reward_mean: -193.34973213544575\n",
      "  episode_reward_min: -218.41038052261607\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1580\n",
      "  experiment_id: 610b8b5226964181899719d25df7617e\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.734723475976807e-19\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -2.857126247137785\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.02045752260205552\n",
      "          policy_loss: 0.0004882957291556522\n",
      "          total_loss: 558.7052242279053\n",
      "          vf_explained_var: 6.949529165467538e-07\n",
      "          vf_loss: 558.7047384262085\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 632000\n",
      "    num_steps_sampled: 632000\n",
      "    num_steps_trained: 632000\n",
      "  iterations_since_restore: 158\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.69156626506024\n",
      "    ram_util_percent: 31.077108433734935\n",
      "  pid: 17432\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08847962579910593\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.7951703016721\n",
      "    mean_inference_ms: 1.8369443717945442\n",
      "    mean_raw_obs_processing_ms: 6.286230017926233\n",
      "  time_since_restore: 9170.257060527802\n",
      "  time_this_iter_s: 58.18741798400879\n",
      "  time_total_s: 9170.257060527802\n",
      "  timers:\n",
      "    learn_throughput: 153.863\n",
      "    learn_time_ms: 25997.213\n",
      "    sample_throughput: 123.645\n",
      "    sample_time_ms: 32350.647\n",
      "    update_time_ms: 3.285\n",
      "  timestamp: 1672617158\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 632000\n",
      "  training_iteration: 158\n",
      "  trial_id: f9c59_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Memory usage on this node: 9.7/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.87 GiB heap, 0.0/14.65 GiB objects (0.0/6.0 CPU_group_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_3_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_0_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_4_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_1_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_5_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_2_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 accelerator_type:G)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                               | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridPOEnv-v0_f9c59_00000 | RUNNING  | 192.168.1.65:17432 |    158 |          9170.26 | 632000 |  -193.35 |             -185.707 |              -218.41 |                400 |\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridPOEnv-v0_f9c59_00000:\n",
      "  agent_timesteps_total: 636000\n",
      "  custom_metrics: {}\n",
      "  date: 2023-01-01_17-53-37\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -185.70719966880566\n",
      "  episode_reward_mean: -191.36709834681503\n",
      "  episode_reward_min: -218.41038052261607\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1590\n",
      "  experiment_id: 610b8b5226964181899719d25df7617e\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.734723475976807e-19\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -2.8398308977484703\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011122738443282553\n",
      "          policy_loss: 0.005202206409012433\n",
      "          total_loss: 548.3921656608582\n",
      "          vf_explained_var: 6.710179150104523e-06\n",
      "          vf_loss: 548.38695936203\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 636000\n",
      "    num_steps_sampled: 636000\n",
      "    num_steps_trained: 636000\n",
      "  iterations_since_restore: 159\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.51666666666667\n",
      "    ram_util_percent: 31.219047619047625\n",
      "  pid: 17432\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08847965271845112\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.797235985661608\n",
      "    mean_inference_ms: 1.8369851093869252\n",
      "    mean_raw_obs_processing_ms: 6.284874275583332\n",
      "  time_since_restore: 9228.666718482971\n",
      "  time_this_iter_s: 58.40965795516968\n",
      "  time_total_s: 9228.666718482971\n",
      "  timers:\n",
      "    learn_throughput: 154.0\n",
      "    learn_time_ms: 25974.047\n",
      "    sample_throughput: 123.576\n",
      "    sample_time_ms: 32368.857\n",
      "    update_time_ms: 3.313\n",
      "  timestamp: 1672617217\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 636000\n",
      "  training_iteration: 159\n",
      "  trial_id: f9c59_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 9.7/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.87 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_2_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_4_6417799359d4e2284884fd3ae2ec9735, 0.0/6.0 CPU_group_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_1_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_5_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_0_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_3_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 accelerator_type:G)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                               | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridPOEnv-v0_f9c59_00000 | RUNNING  | 192.168.1.65:17432 |    159 |          9228.67 | 636000 | -191.367 |             -185.707 |              -218.41 |                400 |\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridPOEnv-v0_f9c59_00000:\n",
      "  agent_timesteps_total: 640000\n",
      "  custom_metrics: {}\n",
      "  date: 2023-01-01_17-54-35\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -185.70719966880566\n",
      "  episode_reward_mean: -190.23473741459907\n",
      "  episode_reward_min: -218.41038052261607\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1600\n",
      "  experiment_id: 610b8b5226964181899719d25df7617e\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.734723475976807e-19\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -2.7620695762336256\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014125165502628361\n",
      "          policy_loss: 0.0028754300758009775\n",
      "          total_loss: 541.6827163219452\n",
      "          vf_explained_var: 1.1175871339474952e-09\n",
      "          vf_loss: 541.6798400878906\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 640000\n",
      "    num_steps_sampled: 640000\n",
      "    num_steps_trained: 640000\n",
      "  iterations_since_restore: 160\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.549397590361444\n",
      "    ram_util_percent: 31.24939759036145\n",
      "  pid: 17432\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0884799724614242\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.798870138374085\n",
      "    mean_inference_ms: 1.8370102206217842\n",
      "    mean_raw_obs_processing_ms: 6.283491294307782\n",
      "  time_since_restore: 9286.74724817276\n",
      "  time_this_iter_s: 58.08052968978882\n",
      "  time_total_s: 9286.74724817276\n",
      "  timers:\n",
      "    learn_throughput: 153.694\n",
      "    learn_time_ms: 26025.821\n",
      "    sample_throughput: 123.821\n",
      "    sample_time_ms: 32304.749\n",
      "    update_time_ms: 3.177\n",
      "  timestamp: 1672617275\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 640000\n",
      "  training_iteration: 160\n",
      "  trial_id: f9c59_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 9.7/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.87 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_4_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_1_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_5_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_3_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_0_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_2_6417799359d4e2284884fd3ae2ec9735, 0.0/6.0 CPU_group_6417799359d4e2284884fd3ae2ec9735)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                               | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridPOEnv-v0_f9c59_00000 | RUNNING  | 192.168.1.65:17432 |    160 |          9286.75 | 640000 | -190.235 |             -185.707 |              -218.41 |                400 |\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_TrafficLightGridPOEnv-v0_f9c59_00000:\n",
      "  agent_timesteps_total: 644000\n",
      "  custom_metrics: {}\n",
      "  date: 2023-01-01_17-55-34\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -185.70719966880566\n",
      "  episode_reward_mean: -188.81708332291058\n",
      "  episode_reward_min: -218.41038052261607\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1610\n",
      "  experiment_id: 610b8b5226964181899719d25df7617e\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.734723475976807e-19\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -2.8279647126793863\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01551832898703695\n",
      "          policy_loss: -0.0014349788936669939\n",
      "          total_loss: 538.6726373672485\n",
      "          vf_explained_var: 0.0\n",
      "          vf_loss: 538.6740717411042\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 644000\n",
      "    num_steps_sampled: 644000\n",
      "    num_steps_trained: 644000\n",
      "  iterations_since_restore: 161\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.40833333333333\n",
      "    ram_util_percent: 31.176190476190474\n",
      "  pid: 17432\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08848078026827132\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.800447115102944\n",
      "    mean_inference_ms: 1.8370104890086174\n",
      "    mean_raw_obs_processing_ms: 6.282117433698627\n",
      "  time_since_restore: 9345.060161828995\n",
      "  time_this_iter_s: 58.31291365623474\n",
      "  time_total_s: 9345.060161828995\n",
      "  timers:\n",
      "    learn_throughput: 153.402\n",
      "    learn_time_ms: 26075.289\n",
      "    sample_throughput: 123.85\n",
      "    sample_time_ms: 32297.246\n",
      "    update_time_ms: 3.199\n",
      "  timestamp: 1672617334\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 644000\n",
      "  training_iteration: 161\n",
      "  trial_id: f9c59_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 9.7/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.87 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_5_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_0_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_1_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_4_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_2_6417799359d4e2284884fd3ae2ec9735, 0.0/6.0 CPU_group_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_3_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 accelerator_type:G)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                               | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridPOEnv-v0_f9c59_00000 | RUNNING  | 192.168.1.65:17432 |    161 |          9345.06 | 644000 | -188.817 |             -185.707 |              -218.41 |                400 |\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridPOEnv-v0_f9c59_00000:\n",
      "  agent_timesteps_total: 648000\n",
      "  custom_metrics: {}\n",
      "  date: 2023-01-01_17-56-32\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -185.70719966880566\n",
      "  episode_reward_mean: -188.2960248316453\n",
      "  episode_reward_min: -212.6513822181522\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1620\n",
      "  experiment_id: 610b8b5226964181899719d25df7617e\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.734723475976807e-19\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -2.8669466234743597\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012477856761245777\n",
      "          policy_loss: 0.0037152106044231912\n",
      "          total_loss: 526.9814342975617\n",
      "          vf_explained_var: 2.349167971260613e-06\n",
      "          vf_loss: 526.9777173757553\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 648000\n",
      "    num_steps_sampled: 648000\n",
      "    num_steps_trained: 648000\n",
      "  iterations_since_restore: 162\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.419277108433736\n",
      "    ram_util_percent: 31.201204819277116\n",
      "  pid: 17432\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08848148546530599\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.801907922891036\n",
      "    mean_inference_ms: 1.837000667625742\n",
      "    mean_raw_obs_processing_ms: 6.280696873945419\n",
      "  time_since_restore: 9403.257014513016\n",
      "  time_this_iter_s: 58.196852684020996\n",
      "  time_total_s: 9403.257014513016\n",
      "  timers:\n",
      "    learn_throughput: 153.509\n",
      "    learn_time_ms: 26057.142\n",
      "    sample_throughput: 123.93\n",
      "    sample_time_ms: 32276.267\n",
      "    update_time_ms: 3.182\n",
      "  timestamp: 1672617392\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 648000\n",
      "  training_iteration: 162\n",
      "  trial_id: f9c59_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 9.7/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.87 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_2_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_1_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_4_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_5_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_3_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_0_6417799359d4e2284884fd3ae2ec9735, 0.0/6.0 CPU_group_6417799359d4e2284884fd3ae2ec9735)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                               | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridPOEnv-v0_f9c59_00000 | RUNNING  | 192.168.1.65:17432 |    162 |          9403.26 | 648000 | -188.296 |             -185.707 |             -212.651 |                400 |\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridPOEnv-v0_f9c59_00000:\n",
      "  agent_timesteps_total: 652000\n",
      "  custom_metrics: {}\n",
      "  date: 2023-01-01_17-57-31\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -185.66387321876027\n",
      "  episode_reward_mean: -187.66463644751045\n",
      "  episode_reward_min: -212.63416470632484\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1630\n",
      "  experiment_id: 610b8b5226964181899719d25df7617e\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.734723475976807e-19\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -2.9917121812701226\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01691678418519605\n",
      "          policy_loss: 0.001107622000563424\n",
      "          total_loss: 528.3904716014862\n",
      "          vf_explained_var: 3.4069641969836084e-06\n",
      "          vf_loss: 528.3893641471863\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 652000\n",
      "    num_steps_sampled: 652000\n",
      "    num_steps_trained: 652000\n",
      "  iterations_since_restore: 163\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.59518072289157\n",
      "    ram_util_percent: 31.313253012048197\n",
      "  pid: 17432\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08848219885030165\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.803169803903625\n",
      "    mean_inference_ms: 1.8369955102163311\n",
      "    mean_raw_obs_processing_ms: 6.27933596954114\n",
      "  time_since_restore: 9461.484918832779\n",
      "  time_this_iter_s: 58.227904319763184\n",
      "  time_total_s: 9461.484918832779\n",
      "  timers:\n",
      "    learn_throughput: 153.42\n",
      "    learn_time_ms: 26072.302\n",
      "    sample_throughput: 124.008\n",
      "    sample_time_ms: 32256.07\n",
      "    update_time_ms: 3.173\n",
      "  timestamp: 1672617451\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 652000\n",
      "  training_iteration: 163\n",
      "  trial_id: f9c59_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Memory usage on this node: 9.8/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.87 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_0_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_5_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_1_6417799359d4e2284884fd3ae2ec9735, 0.0/6.0 CPU_group_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_4_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_3_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_2_6417799359d4e2284884fd3ae2ec9735)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                               | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridPOEnv-v0_f9c59_00000 | RUNNING  | 192.168.1.65:17432 |    163 |          9461.48 | 652000 | -187.665 |             -185.664 |             -212.634 |                400 |\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridPOEnv-v0_f9c59_00000:\n",
      "  agent_timesteps_total: 656000\n",
      "  custom_metrics: {}\n",
      "  date: 2023-01-01_17-58-29\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -184.63216180093394\n",
      "  episode_reward_mean: -187.467080424141\n",
      "  episode_reward_min: -212.63416470632484\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1640\n",
      "  experiment_id: 610b8b5226964181899719d25df7617e\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.734723475976807e-19\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -2.9640859670937063\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013725846057059388\n",
      "          policy_loss: -8.660573948873206e-05\n",
      "          total_loss: 523.6337721824646\n",
      "          vf_explained_var: 3.3622607134020654e-06\n",
      "          vf_loss: 523.633859539032\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 656000\n",
      "    num_steps_sampled: 656000\n",
      "    num_steps_trained: 656000\n",
      "  iterations_since_restore: 164\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.3952380952381\n",
      "    ram_util_percent: 31.263095238095225\n",
      "  pid: 17432\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0884825106574963\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.804373516778995\n",
      "    mean_inference_ms: 1.8369888613037617\n",
      "    mean_raw_obs_processing_ms: 6.278059643178348\n",
      "  time_since_restore: 9519.779201745987\n",
      "  time_this_iter_s: 58.29428291320801\n",
      "  time_total_s: 9519.779201745987\n",
      "  timers:\n",
      "    learn_throughput: 153.484\n",
      "    learn_time_ms: 26061.282\n",
      "    sample_throughput: 123.958\n",
      "    sample_time_ms: 32268.902\n",
      "    update_time_ms: 3.172\n",
      "  timestamp: 1672617509\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 656000\n",
      "  training_iteration: 164\n",
      "  trial_id: f9c59_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 9.7/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.87 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_5_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_2_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_3_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_4_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 accelerator_type:G, 0.0/6.0 CPU_group_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_1_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_0_6417799359d4e2284884fd3ae2ec9735)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                               | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridPOEnv-v0_f9c59_00000 | RUNNING  | 192.168.1.65:17432 |    164 |          9519.78 | 656000 | -187.467 |             -184.632 |             -212.634 |                400 |\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridPOEnv-v0_f9c59_00000:\n",
      "  agent_timesteps_total: 660000\n",
      "  custom_metrics: {}\n",
      "  date: 2023-01-01_17-59-28\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -184.63216180093394\n",
      "  episode_reward_mean: -187.16557786924332\n",
      "  episode_reward_min: -212.63416470632484\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1650\n",
      "  experiment_id: 610b8b5226964181899719d25df7617e\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.734723475976807e-19\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -3.0338437661528586\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.020182183473207438\n",
      "          policy_loss: -0.000843023530615028\n",
      "          total_loss: 520.8531896114349\n",
      "          vf_explained_var: 1.753494188960758e-06\n",
      "          vf_loss: 520.854035282135\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 660000\n",
      "    num_steps_sampled: 660000\n",
      "    num_steps_trained: 660000\n",
      "  iterations_since_restore: 165\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.760240963855424\n",
      "    ram_util_percent: 31.315662650602412\n",
      "  pid: 17432\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08848279912706303\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.805489660486337\n",
      "    mean_inference_ms: 1.8369670749341631\n",
      "    mean_raw_obs_processing_ms: 6.276784607335871\n",
      "  time_since_restore: 9578.162984609604\n",
      "  time_this_iter_s: 58.38378286361694\n",
      "  time_total_s: 9578.162984609604\n",
      "  timers:\n",
      "    learn_throughput: 153.663\n",
      "    learn_time_ms: 26031.073\n",
      "    sample_throughput: 123.975\n",
      "    sample_time_ms: 32264.54\n",
      "    update_time_ms: 3.173\n",
      "  timestamp: 1672617568\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 660000\n",
      "  training_iteration: 165\n",
      "  trial_id: f9c59_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 9.8/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.87 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_3_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_5_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_2_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_0_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_4_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_1_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 accelerator_type:G, 0.0/6.0 CPU_group_6417799359d4e2284884fd3ae2ec9735)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                               | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridPOEnv-v0_f9c59_00000 | RUNNING  | 192.168.1.65:17432 |    165 |          9578.16 | 660000 | -187.166 |             -184.632 |             -212.634 |                400 |\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_TrafficLightGridPOEnv-v0_f9c59_00000:\n",
      "  agent_timesteps_total: 664000\n",
      "  custom_metrics: {}\n",
      "  date: 2023-01-01_18-00-26\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -184.63216180093394\n",
      "  episode_reward_mean: -186.79795763284972\n",
      "  episode_reward_min: -212.57372567564786\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1660\n",
      "  experiment_id: 610b8b5226964181899719d25df7617e\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.734723475976807e-19\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -3.0477350272238253\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01568546414473531\n",
      "          policy_loss: 0.004460562684835167\n",
      "          total_loss: 510.39734079837797\n",
      "          vf_explained_var: 6.585940809600288e-06\n",
      "          vf_loss: 510.39287943840026\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 664000\n",
      "    num_steps_sampled: 664000\n",
      "    num_steps_trained: 664000\n",
      "  iterations_since_restore: 166\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.584523809523812\n",
      "    ram_util_percent: 31.360714285714298\n",
      "  pid: 17432\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08848443978432292\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.806534047293567\n",
      "    mean_inference_ms: 1.8369983113999677\n",
      "    mean_raw_obs_processing_ms: 6.275506624752727\n",
      "  time_since_restore: 9636.599997758865\n",
      "  time_this_iter_s: 58.437013149261475\n",
      "  time_total_s: 9636.599997758865\n",
      "  timers:\n",
      "    learn_throughput: 153.778\n",
      "    learn_time_ms: 26011.554\n",
      "    sample_throughput: 123.985\n",
      "    sample_time_ms: 32261.886\n",
      "    update_time_ms: 3.149\n",
      "  timestamp: 1672617626\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 664000\n",
      "  training_iteration: 166\n",
      "  trial_id: f9c59_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 9.7/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.87 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_1_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_0_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_2_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_3_6417799359d4e2284884fd3ae2ec9735, 0.0/6.0 CPU_group_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_5_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_4_6417799359d4e2284884fd3ae2ec9735)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                               | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridPOEnv-v0_f9c59_00000 | RUNNING  | 192.168.1.65:17432 |    166 |           9636.6 | 664000 | -186.798 |             -184.632 |             -212.574 |                400 |\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridPOEnv-v0_f9c59_00000:\n",
      "  agent_timesteps_total: 668000\n",
      "  custom_metrics: {}\n",
      "  date: 2023-01-01_18-01-24\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -184.63216180093394\n",
      "  episode_reward_mean: -186.50445582654658\n",
      "  episode_reward_min: -194.02145226063433\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1670\n",
      "  experiment_id: 610b8b5226964181899719d25df7617e\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.734723475976807e-19\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -3.0759994857013226\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0165082029418047\n",
      "          policy_loss: 0.0011113775428384543\n",
      "          total_loss: 509.62245843410494\n",
      "          vf_explained_var: 2.8224662855791394e-06\n",
      "          vf_loss: 509.62134473323823\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 668000\n",
      "    num_steps_sampled: 668000\n",
      "    num_steps_trained: 668000\n",
      "  iterations_since_restore: 167\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.555421686746985\n",
      "    ram_util_percent: 31.383132530120484\n",
      "  pid: 17432\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08848604808143884\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.80761685430858\n",
      "    mean_inference_ms: 1.8370108225616324\n",
      "    mean_raw_obs_processing_ms: 6.274226825501715\n",
      "  time_since_restore: 9694.562859535217\n",
      "  time_this_iter_s: 57.96286177635193\n",
      "  time_total_s: 9694.562859535217\n",
      "  timers:\n",
      "    learn_throughput: 153.992\n",
      "    learn_time_ms: 25975.428\n",
      "    sample_throughput: 123.993\n",
      "    sample_time_ms: 32259.756\n",
      "    update_time_ms: 3.137\n",
      "  timestamp: 1672617684\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 668000\n",
      "  training_iteration: 167\n",
      "  trial_id: f9c59_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 9.8/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.87 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_2_6417799359d4e2284884fd3ae2ec9735, 0.0/6.0 CPU_group_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_4_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_0_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_5_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_1_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_3_6417799359d4e2284884fd3ae2ec9735)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                               | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridPOEnv-v0_f9c59_00000 | RUNNING  | 192.168.1.65:17432 |    167 |          9694.56 | 668000 | -186.504 |             -184.632 |             -194.021 |                400 |\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridPOEnv-v0_f9c59_00000:\n",
      "  agent_timesteps_total: 672000\n",
      "  custom_metrics: {}\n",
      "  date: 2023-01-01_18-02-22\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -184.63216180093394\n",
      "  episode_reward_mean: -186.55160401013308\n",
      "  episode_reward_min: -194.02145226063433\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1680\n",
      "  experiment_id: 610b8b5226964181899719d25df7617e\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.734723475976807e-19\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -3.1553919591009616\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.019538382214705583\n",
      "          policy_loss: 0.0023223566793603824\n",
      "          total_loss: 503.7893113136291\n",
      "          vf_explained_var: 2.0369886897242395e-06\n",
      "          vf_loss: 503.7869919538498\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 672000\n",
      "    num_steps_sampled: 672000\n",
      "    num_steps_trained: 672000\n",
      "  iterations_since_restore: 168\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.565853658536582\n",
      "    ram_util_percent: 31.490243902439023\n",
      "  pid: 17432\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08848707352910604\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.808338105958786\n",
      "    mean_inference_ms: 1.836993840442927\n",
      "    mean_raw_obs_processing_ms: 6.273011809462145\n",
      "  time_since_restore: 9752.253623008728\n",
      "  time_this_iter_s: 57.69076347351074\n",
      "  time_total_s: 9752.253623008728\n",
      "  timers:\n",
      "    learn_throughput: 154.017\n",
      "    learn_time_ms: 25971.098\n",
      "    sample_throughput: 124.168\n",
      "    sample_time_ms: 32214.408\n",
      "    update_time_ms: 3.18\n",
      "  timestamp: 1672617742\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 672000\n",
      "  training_iteration: 168\n",
      "  trial_id: f9c59_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Memory usage on this node: 9.8/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.87 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_1_6417799359d4e2284884fd3ae2ec9735, 0.0/6.0 CPU_group_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_4_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_0_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_2_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_5_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_3_6417799359d4e2284884fd3ae2ec9735)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                               | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridPOEnv-v0_f9c59_00000 | RUNNING  | 192.168.1.65:17432 |    168 |          9752.25 | 672000 | -186.552 |             -184.632 |             -194.021 |                400 |\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridPOEnv-v0_f9c59_00000:\n",
      "  agent_timesteps_total: 676000\n",
      "  custom_metrics: {}\n",
      "  date: 2023-01-01_18-03-21\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -184.63216180093394\n",
      "  episode_reward_mean: -186.4741551497811\n",
      "  episode_reward_min: -194.02145226063433\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1690\n",
      "  experiment_id: 610b8b5226964181899719d25df7617e\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.734723475976807e-19\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -3.1819521620869637\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017041412764048126\n",
      "          policy_loss: 0.00040576930914539846\n",
      "          total_loss: 504.01198596954345\n",
      "          vf_explained_var: 7.992610449036874e-07\n",
      "          vf_loss: 504.01158180236814\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 676000\n",
      "    num_steps_sampled: 676000\n",
      "    num_steps_trained: 676000\n",
      "  iterations_since_restore: 169\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.43452380952381\n",
      "    ram_util_percent: 31.436904761904763\n",
      "  pid: 17432\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08848814453330442\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.808862696406095\n",
      "    mean_inference_ms: 1.8369925525332562\n",
      "    mean_raw_obs_processing_ms: 6.271819107979876\n",
      "  time_since_restore: 9810.963069438934\n",
      "  time_this_iter_s: 58.7094464302063\n",
      "  time_total_s: 9810.963069438934\n",
      "  timers:\n",
      "    learn_throughput: 153.694\n",
      "    learn_time_ms: 26025.658\n",
      "    sample_throughput: 124.263\n",
      "    sample_time_ms: 32189.858\n",
      "    update_time_ms: 3.151\n",
      "  timestamp: 1672617801\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 676000\n",
      "  training_iteration: 169\n",
      "  trial_id: f9c59_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 9.8/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.87 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_0_6417799359d4e2284884fd3ae2ec9735, 0.0/6.0 CPU_group_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_2_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_4_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_1_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_5_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_3_6417799359d4e2284884fd3ae2ec9735)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                               | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridPOEnv-v0_f9c59_00000 | RUNNING  | 192.168.1.65:17432 |    169 |          9810.96 | 676000 | -186.474 |             -184.632 |             -194.021 |                400 |\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridPOEnv-v0_f9c59_00000:\n",
      "  agent_timesteps_total: 680000\n",
      "  custom_metrics: {}\n",
      "  date: 2023-01-01_18-04-19\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -184.63216180093394\n",
      "  episode_reward_mean: -186.39761621318883\n",
      "  episode_reward_min: -194.02145226063433\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1700\n",
      "  experiment_id: 610b8b5226964181899719d25df7617e\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.734723475976807e-19\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -3.1907551564276218\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014237391613460262\n",
      "          policy_loss: -0.0002808999524859246\n",
      "          total_loss: 501.83126332759855\n",
      "          vf_explained_var: 4.825182259082794e-06\n",
      "          vf_loss: 501.8315446853638\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 680000\n",
      "    num_steps_sampled: 680000\n",
      "    num_steps_trained: 680000\n",
      "  iterations_since_restore: 170\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.581927710843377\n",
      "    ram_util_percent: 31.444578313253015\n",
      "  pid: 17432\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08848856967476801\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.809362499214348\n",
      "    mean_inference_ms: 1.8369988936208392\n",
      "    mean_raw_obs_processing_ms: 6.270633654978667\n",
      "  time_since_restore: 9869.154337406158\n",
      "  time_this_iter_s: 58.19126796722412\n",
      "  time_total_s: 9869.154337406158\n",
      "  timers:\n",
      "    learn_throughput: 153.6\n",
      "    learn_time_ms: 26041.712\n",
      "    sample_throughput: 124.282\n",
      "    sample_time_ms: 32184.925\n",
      "    update_time_ms: 3.155\n",
      "  timestamp: 1672617859\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 680000\n",
      "  training_iteration: 170\n",
      "  trial_id: f9c59_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 9.8/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.87 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_2_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_0_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_5_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_1_6417799359d4e2284884fd3ae2ec9735, 0.0/6.0 CPU_group_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_4_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_3_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 accelerator_type:G)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                               | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridPOEnv-v0_f9c59_00000 | RUNNING  | 192.168.1.65:17432 |    170 |          9869.15 | 680000 | -186.398 |             -184.632 |             -194.021 |                400 |\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_TrafficLightGridPOEnv-v0_f9c59_00000:\n",
      "  agent_timesteps_total: 684000\n",
      "  custom_metrics: {}\n",
      "  date: 2023-01-01_18-05-18\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -184.63216180093394\n",
      "  episode_reward_mean: -186.32377333081047\n",
      "  episode_reward_min: -194.02145226063433\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1710\n",
      "  experiment_id: 610b8b5226964181899719d25df7617e\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.734723475976807e-19\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -3.154681386053562\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012468525973600032\n",
      "          policy_loss: -0.001448135198734235\n",
      "          total_loss: 502.5833117485046\n",
      "          vf_explained_var: 3.143027470287052e-06\n",
      "          vf_loss: 502.5847583293915\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 684000\n",
      "    num_steps_sampled: 684000\n",
      "    num_steps_trained: 684000\n",
      "  iterations_since_restore: 171\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.41547619047619\n",
      "    ram_util_percent: 31.541666666666668\n",
      "  pid: 17432\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08848862397461549\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.810032044703963\n",
      "    mean_inference_ms: 1.8370101183224359\n",
      "    mean_raw_obs_processing_ms: 6.269450829667242\n",
      "  time_since_restore: 9927.910832643509\n",
      "  time_this_iter_s: 58.756495237350464\n",
      "  time_total_s: 9927.910832643509\n",
      "  timers:\n",
      "    learn_throughput: 153.471\n",
      "    learn_time_ms: 26063.487\n",
      "    sample_throughput: 124.195\n",
      "    sample_time_ms: 32207.533\n",
      "    update_time_ms: 3.147\n",
      "  timestamp: 1672617918\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 684000\n",
      "  training_iteration: 171\n",
      "  trial_id: f9c59_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 9.8/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.87 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_0_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_2_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_4_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_3_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_1_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_5_6417799359d4e2284884fd3ae2ec9735, 0.0/6.0 CPU_group_6417799359d4e2284884fd3ae2ec9735)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                               | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridPOEnv-v0_f9c59_00000 | RUNNING  | 192.168.1.65:17432 |    171 |          9927.91 | 684000 | -186.324 |             -184.632 |             -194.021 |                400 |\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridPOEnv-v0_f9c59_00000:\n",
      "  agent_timesteps_total: 688000\n",
      "  custom_metrics: {}\n",
      "  date: 2023-01-01_18-06-17\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -184.63216180093394\n",
      "  episode_reward_mean: -186.30393181280945\n",
      "  episode_reward_min: -194.02145226063433\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1720\n",
      "  experiment_id: 610b8b5226964181899719d25df7617e\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.734723475976807e-19\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -3.101454845815897\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013039200418371182\n",
      "          policy_loss: 0.003943761756818276\n",
      "          total_loss: 492.23266072273253\n",
      "          vf_explained_var: 7.297657248273026e-06\n",
      "          vf_loss: 492.2287165164947\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 688000\n",
      "    num_steps_sampled: 688000\n",
      "    num_steps_trained: 688000\n",
      "  iterations_since_restore: 172\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.46428571428571\n",
      "    ram_util_percent: 31.520238095238092\n",
      "  pid: 17432\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08848828061406178\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.81065626085766\n",
      "    mean_inference_ms: 1.8369941426692997\n",
      "    mean_raw_obs_processing_ms: 6.268333536300847\n",
      "  time_since_restore: 9986.351236104965\n",
      "  time_this_iter_s: 58.4404034614563\n",
      "  time_total_s: 9986.351236104965\n",
      "  timers:\n",
      "    learn_throughput: 153.322\n",
      "    learn_time_ms: 26088.904\n",
      "    sample_throughput: 124.199\n",
      "    sample_time_ms: 32206.482\n",
      "    update_time_ms: 3.15\n",
      "  timestamp: 1672617977\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 688000\n",
      "  training_iteration: 172\n",
      "  trial_id: f9c59_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 9.8/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.87 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_0_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_4_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_1_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_2_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_5_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 accelerator_type:G, 0.0/6.0 CPU_group_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_3_6417799359d4e2284884fd3ae2ec9735)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                               | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridPOEnv-v0_f9c59_00000 | RUNNING  | 192.168.1.65:17432 |    172 |          9986.35 | 688000 | -186.304 |             -184.632 |             -194.021 |                400 |\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridPOEnv-v0_f9c59_00000:\n",
      "  agent_timesteps_total: 692000\n",
      "  custom_metrics: {}\n",
      "  date: 2023-01-01_18-07-16\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -184.63216180093394\n",
      "  episode_reward_mean: -186.32372985980385\n",
      "  episode_reward_min: -194.02145226063433\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1730\n",
      "  experiment_id: 610b8b5226964181899719d25df7617e\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.734723475976807e-19\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -3.1252244859933853\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01322393725236018\n",
      "          policy_loss: 0.003997856240312103\n",
      "          total_loss: 488.2336047887802\n",
      "          vf_explained_var: 1.08929352791165e-05\n",
      "          vf_loss: 488.22960624694826\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 692000\n",
      "    num_steps_sampled: 692000\n",
      "    num_steps_trained: 692000\n",
      "  iterations_since_restore: 173\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.523809523809526\n",
      "    ram_util_percent: 31.582142857142852\n",
      "  pid: 17432\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0884881300231286\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.811377400068935\n",
      "    mean_inference_ms: 1.8370018041071854\n",
      "    mean_raw_obs_processing_ms: 6.267233835493297\n",
      "  time_since_restore: 10044.98057961464\n",
      "  time_this_iter_s: 58.62934350967407\n",
      "  time_total_s: 10044.98057961464\n",
      "  timers:\n",
      "    learn_throughput: 153.157\n",
      "    learn_time_ms: 26117.045\n",
      "    sample_throughput: 124.152\n",
      "    sample_time_ms: 32218.443\n",
      "    update_time_ms: 3.162\n",
      "  timestamp: 1672618036\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 692000\n",
      "  training_iteration: 173\n",
      "  trial_id: f9c59_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Memory usage on this node: 9.9/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.87 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_2_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_4_6417799359d4e2284884fd3ae2ec9735, 0.0/6.0 CPU_group_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_0_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_5_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_1_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_3_6417799359d4e2284884fd3ae2ec9735)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                               | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridPOEnv-v0_f9c59_00000 | RUNNING  | 192.168.1.65:17432 |    173 |            10045 | 692000 | -186.324 |             -184.632 |             -194.021 |                400 |\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridPOEnv-v0_f9c59_00000:\n",
      "  agent_timesteps_total: 696000\n",
      "  custom_metrics: {}\n",
      "  date: 2023-01-01_18-08-15\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -184.63868378955166\n",
      "  episode_reward_mean: -186.35514580325716\n",
      "  episode_reward_min: -194.02145226063433\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1740\n",
      "  experiment_id: 610b8b5226964181899719d25df7617e\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.734723475976807e-19\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -3.1203459955751898\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015600849201291919\n",
      "          policy_loss: -0.00011165058094775304\n",
      "          total_loss: 491.7930274248123\n",
      "          vf_explained_var: 3.157555966026848e-06\n",
      "          vf_loss: 491.79313803315165\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 696000\n",
      "    num_steps_sampled: 696000\n",
      "    num_steps_trained: 696000\n",
      "  iterations_since_restore: 174\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.48095238095238\n",
      "    ram_util_percent: 31.615476190476176\n",
      "  pid: 17432\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08848821374994716\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.812112599954027\n",
      "    mean_inference_ms: 1.8370252209675886\n",
      "    mean_raw_obs_processing_ms: 6.2661836479261\n",
      "  time_since_restore: 10103.736256599426\n",
      "  time_this_iter_s: 58.75567698478699\n",
      "  time_total_s: 10103.736256599426\n",
      "  timers:\n",
      "    learn_throughput: 152.917\n",
      "    learn_time_ms: 26158.039\n",
      "    sample_throughput: 124.132\n",
      "    sample_time_ms: 32223.65\n",
      "    update_time_ms: 3.167\n",
      "  timestamp: 1672618095\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 696000\n",
      "  training_iteration: 174\n",
      "  trial_id: f9c59_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 9.9/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.87 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_2_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_5_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_1_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_3_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_4_6417799359d4e2284884fd3ae2ec9735, 0.0/6.0 CPU_group_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_0_6417799359d4e2284884fd3ae2ec9735)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                               | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridPOEnv-v0_f9c59_00000 | RUNNING  | 192.168.1.65:17432 |    174 |          10103.7 | 696000 | -186.355 |             -184.639 |             -194.021 |                400 |\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridPOEnv-v0_f9c59_00000:\n",
      "  agent_timesteps_total: 700000\n",
      "  custom_metrics: {}\n",
      "  date: 2023-01-01_18-09-13\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -184.64489134310963\n",
      "  episode_reward_mean: -186.36357128161507\n",
      "  episode_reward_min: -192.2663677958698\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1750\n",
      "  experiment_id: 610b8b5226964181899719d25df7617e\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.734723475976807e-19\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -3.1111795395612716\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01931251874539186\n",
      "          policy_loss: 0.00124111030891072\n",
      "          total_loss: 490.49635019302366\n",
      "          vf_explained_var: 2.497062041584286e-06\n",
      "          vf_loss: 490.49510927200316\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 700000\n",
      "    num_steps_sampled: 700000\n",
      "    num_steps_trained: 700000\n",
      "  iterations_since_restore: 175\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.508333333333333\n",
      "    ram_util_percent: 31.644047619047612\n",
      "  pid: 17432\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08848814698030148\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.812710582904483\n",
      "    mean_inference_ms: 1.837050395397252\n",
      "    mean_raw_obs_processing_ms: 6.2651111721633175\n",
      "  time_since_restore: 10162.357149839401\n",
      "  time_this_iter_s: 58.620893239974976\n",
      "  time_total_s: 10162.357149839401\n",
      "  timers:\n",
      "    learn_throughput: 152.624\n",
      "    learn_time_ms: 26208.172\n",
      "    sample_throughput: 124.234\n",
      "    sample_time_ms: 32197.222\n",
      "    update_time_ms: 3.158\n",
      "  timestamp: 1672618153\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 700000\n",
      "  training_iteration: 175\n",
      "  trial_id: f9c59_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 9.9/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.87 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_2_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_5_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_3_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_1_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_4_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_0_6417799359d4e2284884fd3ae2ec9735, 0.0/6.0 CPU_group_6417799359d4e2284884fd3ae2ec9735)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                               | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridPOEnv-v0_f9c59_00000 | RUNNING  | 192.168.1.65:17432 |    175 |          10162.4 | 700000 | -186.364 |             -184.645 |             -192.266 |                400 |\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_TrafficLightGridPOEnv-v0_f9c59_00000:\n",
      "  agent_timesteps_total: 704000\n",
      "  custom_metrics: {}\n",
      "  date: 2023-01-01_18-10-12\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -184.64489134310963\n",
      "  episode_reward_mean: -186.34029726694317\n",
      "  episode_reward_min: -192.2663677958698\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1760\n",
      "  experiment_id: 610b8b5226964181899719d25df7617e\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.734723475976807e-19\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -3.2012506783008576\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016064677140457544\n",
      "          policy_loss: 0.003013828877010383\n",
      "          total_loss: 488.8361179828644\n",
      "          vf_explained_var: 6.966479304537643e-06\n",
      "          vf_loss: 488.8331049919128\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 704000\n",
      "    num_steps_sampled: 704000\n",
      "    num_steps_trained: 704000\n",
      "  iterations_since_restore: 176\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.601190476190474\n",
      "    ram_util_percent: 31.68214285714285\n",
      "  pid: 17432\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08848737941933991\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.81322520452603\n",
      "    mean_inference_ms: 1.837070982377258\n",
      "    mean_raw_obs_processing_ms: 6.264066059648642\n",
      "  time_since_restore: 10220.981460094452\n",
      "  time_this_iter_s: 58.62431025505066\n",
      "  time_total_s: 10220.981460094452\n",
      "  timers:\n",
      "    learn_throughput: 152.487\n",
      "    learn_time_ms: 26231.723\n",
      "    sample_throughput: 124.253\n",
      "    sample_time_ms: 32192.4\n",
      "    update_time_ms: 3.143\n",
      "  timestamp: 1672618212\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 704000\n",
      "  training_iteration: 176\n",
      "  trial_id: f9c59_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 9.9/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.87 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_1_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_4_6417799359d4e2284884fd3ae2ec9735, 0.0/6.0 CPU_group_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_5_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_0_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_2_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_3_6417799359d4e2284884fd3ae2ec9735)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                               | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridPOEnv-v0_f9c59_00000 | RUNNING  | 192.168.1.65:17432 |    176 |            10221 | 704000 |  -186.34 |             -184.645 |             -192.266 |                400 |\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridPOEnv-v0_f9c59_00000:\n",
      "  agent_timesteps_total: 708000\n",
      "  custom_metrics: {}\n",
      "  date: 2023-01-01_18-11-11\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -184.64489134310963\n",
      "  episode_reward_mean: -186.34517724263821\n",
      "  episode_reward_min: -192.2663677958698\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1770\n",
      "  experiment_id: 610b8b5226964181899719d25df7617e\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.734723475976807e-19\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -3.287667591124773\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015788044152487135\n",
      "          policy_loss: 0.002137551544001326\n",
      "          total_loss: 488.9315572500229\n",
      "          vf_explained_var: 6.876327006466454e-06\n",
      "          vf_loss: 488.9294191598892\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 708000\n",
      "    num_steps_sampled: 708000\n",
      "    num_steps_trained: 708000\n",
      "  iterations_since_restore: 177\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.48452380952381\n",
      "    ram_util_percent: 31.679761904761897\n",
      "  pid: 17432\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08848640824939266\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.813652406517686\n",
      "    mean_inference_ms: 1.8371054522235273\n",
      "    mean_raw_obs_processing_ms: 6.263066719374544\n",
      "  time_since_restore: 10279.61086845398\n",
      "  time_this_iter_s: 58.62940835952759\n",
      "  time_total_s: 10279.61086845398\n",
      "  timers:\n",
      "    learn_throughput: 152.078\n",
      "    learn_time_ms: 26302.238\n",
      "    sample_throughput: 124.268\n",
      "    sample_time_ms: 32188.521\n",
      "    update_time_ms: 3.18\n",
      "  timestamp: 1672618271\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 708000\n",
      "  training_iteration: 177\n",
      "  trial_id: f9c59_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 9.9/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.87 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_5_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_2_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_1_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_3_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_4_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_0_6417799359d4e2284884fd3ae2ec9735, 0.0/6.0 CPU_group_6417799359d4e2284884fd3ae2ec9735)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                               | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridPOEnv-v0_f9c59_00000 | RUNNING  | 192.168.1.65:17432 |    177 |          10279.6 | 708000 | -186.345 |             -184.645 |             -192.266 |                400 |\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridPOEnv-v0_f9c59_00000:\n",
      "  agent_timesteps_total: 712000\n",
      "  custom_metrics: {}\n",
      "  date: 2023-01-01_18-12-10\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -184.64489134310963\n",
      "  episode_reward_mean: -186.3269585353876\n",
      "  episode_reward_min: -189.1013552314774\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1780\n",
      "  experiment_id: 610b8b5226964181899719d25df7617e\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.734723475976807e-19\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -3.4168984718620776\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0254731961809739\n",
      "          policy_loss: -0.0009480997832724825\n",
      "          total_loss: 484.9090114116669\n",
      "          vf_explained_var: 7.982738679856993e-06\n",
      "          vf_loss: 484.9099586963654\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 712000\n",
      "    num_steps_sampled: 712000\n",
      "    num_steps_trained: 712000\n",
      "  iterations_since_restore: 178\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.382142857142856\n",
      "    ram_util_percent: 31.722619047619055\n",
      "  pid: 17432\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08848537076025384\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.814397996651387\n",
      "    mean_inference_ms: 1.8371497147376044\n",
      "    mean_raw_obs_processing_ms: 6.262082144334015\n",
      "  time_since_restore: 10338.262139081955\n",
      "  time_this_iter_s: 58.651270627975464\n",
      "  time_total_s: 10338.262139081955\n",
      "  timers:\n",
      "    learn_throughput: 151.845\n",
      "    learn_time_ms: 26342.579\n",
      "    sample_throughput: 124.054\n",
      "    sample_time_ms: 32243.903\n",
      "    update_time_ms: 3.201\n",
      "  timestamp: 1672618330\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 712000\n",
      "  training_iteration: 178\n",
      "  trial_id: f9c59_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Memory usage on this node: 9.9/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.87 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_1_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_3_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_5_6417799359d4e2284884fd3ae2ec9735, 0.0/6.0 CPU_group_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_0_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_2_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_4_6417799359d4e2284884fd3ae2ec9735)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                               | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridPOEnv-v0_f9c59_00000 | RUNNING  | 192.168.1.65:17432 |    178 |          10338.3 | 712000 | -186.327 |             -184.645 |             -189.101 |                400 |\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridPOEnv-v0_f9c59_00000:\n",
      "  agent_timesteps_total: 716000\n",
      "  custom_metrics: {}\n",
      "  date: 2023-01-01_18-13-09\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -184.64489134310963\n",
      "  episode_reward_mean: -186.33160260935824\n",
      "  episode_reward_min: -189.1013552314774\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1790\n",
      "  experiment_id: 610b8b5226964181899719d25df7617e\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.734723475976807e-19\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -3.5128703944385054\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.021745781276150285\n",
      "          policy_loss: 0.0020937426917953415\n",
      "          total_loss: 489.14298548698423\n",
      "          vf_explained_var: 3.1888484386399796e-07\n",
      "          vf_loss: 489.14089407920835\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 716000\n",
      "    num_steps_sampled: 716000\n",
      "    num_steps_trained: 716000\n",
      "  iterations_since_restore: 179\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.73975903614458\n",
      "    ram_util_percent: 31.821686746987954\n",
      "  pid: 17432\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08848492992504711\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.815118950450906\n",
      "    mean_inference_ms: 1.8372035712785562\n",
      "    mean_raw_obs_processing_ms: 6.261102602859621\n",
      "  time_since_restore: 10396.727051019669\n",
      "  time_this_iter_s: 58.46491193771362\n",
      "  time_total_s: 10396.727051019669\n",
      "  timers:\n",
      "    learn_throughput: 151.997\n",
      "    learn_time_ms: 26316.343\n",
      "    sample_throughput: 124.048\n",
      "    sample_time_ms: 32245.67\n",
      "    update_time_ms: 3.207\n",
      "  timestamp: 1672618389\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 716000\n",
      "  training_iteration: 179\n",
      "  trial_id: f9c59_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 9.9/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.87 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_2_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_5_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_4_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_1_6417799359d4e2284884fd3ae2ec9735, 0.0/6.0 CPU_group_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_0_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_3_6417799359d4e2284884fd3ae2ec9735)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                               | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridPOEnv-v0_f9c59_00000 | RUNNING  | 192.168.1.65:17432 |    179 |          10396.7 | 716000 | -186.332 |             -184.645 |             -189.101 |                400 |\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=17433)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=17429)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=17431)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=17430)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=17428)\u001b[0m  Retrying in 1 seconds\n",
      "Result for PPO_TrafficLightGridPOEnv-v0_f9c59_00000:\n",
      "  agent_timesteps_total: 720000\n",
      "  custom_metrics: {}\n",
      "  date: 2023-01-01_18-14-08\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -185.63051425819086\n",
      "  episode_reward_mean: -186.3988048809229\n",
      "  episode_reward_min: -189.1013552314774\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1800\n",
      "  experiment_id: 610b8b5226964181899719d25df7617e\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.734723475976807e-19\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -3.617676903307438\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016148232587602507\n",
      "          policy_loss: 0.0015677249612053856\n",
      "          total_loss: 482.97524065971373\n",
      "          vf_explained_var: 7.13001918484224e-06\n",
      "          vf_loss: 482.97366960048674\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 720000\n",
      "    num_steps_sampled: 720000\n",
      "    num_steps_trained: 720000\n",
      "  iterations_since_restore: 180\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.130588235294123\n",
      "    ram_util_percent: 31.844705882352944\n",
      "  pid: 17432\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08848497937647287\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.815921515932185\n",
      "    mean_inference_ms: 1.8372404399591247\n",
      "    mean_raw_obs_processing_ms: 6.2608051599226\n",
      "  time_since_restore: 10455.78075838089\n",
      "  time_this_iter_s: 59.05370736122131\n",
      "  time_total_s: 10455.78075838089\n",
      "  timers:\n",
      "    learn_throughput: 152.138\n",
      "    learn_time_ms: 26291.834\n",
      "    sample_throughput: 123.623\n",
      "    sample_time_ms: 32356.385\n",
      "    update_time_ms: 3.214\n",
      "  timestamp: 1672618448\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 720000\n",
      "  training_iteration: 180\n",
      "  trial_id: f9c59_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 9.9/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.87 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_3_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_4_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_0_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_1_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_2_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_5_6417799359d4e2284884fd3ae2ec9735, 0.0/6.0 CPU_group_6417799359d4e2284884fd3ae2ec9735)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                               | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridPOEnv-v0_f9c59_00000 | RUNNING  | 192.168.1.65:17432 |    180 |          10455.8 | 720000 | -186.399 |             -185.631 |             -189.101 |                400 |\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_TrafficLightGridPOEnv-v0_f9c59_00000:\n",
      "  agent_timesteps_total: 724000\n",
      "  custom_metrics: {}\n",
      "  date: 2023-01-01_18-15-07\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -184.5996058636915\n",
      "  episode_reward_mean: -186.5026843938042\n",
      "  episode_reward_min: -189.1013552314774\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1810\n",
      "  experiment_id: 610b8b5226964181899719d25df7617e\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.734723475976807e-19\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -3.709025950729847\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.021826500367297007\n",
      "          policy_loss: 0.0025398255835170857\n",
      "          total_loss: 479.1164822101593\n",
      "          vf_explained_var: 2.541393087085453e-06\n",
      "          vf_loss: 479.1139401674271\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 724000\n",
      "    num_steps_sampled: 724000\n",
      "    num_steps_trained: 724000\n",
      "  iterations_since_restore: 181\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.651190476190475\n",
      "    ram_util_percent: 31.81071428571429\n",
      "  pid: 17432\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08848513908031279\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.816646400136943\n",
      "    mean_inference_ms: 1.8372869605143263\n",
      "    mean_raw_obs_processing_ms: 6.260549650755057\n",
      "  time_since_restore: 10514.735026836395\n",
      "  time_this_iter_s: 58.95426845550537\n",
      "  time_total_s: 10514.735026836395\n",
      "  timers:\n",
      "    learn_throughput: 151.978\n",
      "    learn_time_ms: 26319.644\n",
      "    sample_throughput: 123.654\n",
      "    sample_time_ms: 32348.366\n",
      "    update_time_ms: 3.189\n",
      "  timestamp: 1672618507\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 724000\n",
      "  training_iteration: 181\n",
      "  trial_id: f9c59_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 9.9/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.87 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_5_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_1_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_4_6417799359d4e2284884fd3ae2ec9735, 0.0/6.0 CPU_group_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_2_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_0_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_3_6417799359d4e2284884fd3ae2ec9735)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                               | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridPOEnv-v0_f9c59_00000 | RUNNING  | 192.168.1.65:17432 |    181 |          10514.7 | 724000 | -186.503 |               -184.6 |             -189.101 |                400 |\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridPOEnv-v0_f9c59_00000:\n",
      "  agent_timesteps_total: 728000\n",
      "  custom_metrics: {}\n",
      "  date: 2023-01-01_18-16-06\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -184.42737361045434\n",
      "  episode_reward_mean: -186.5554964429423\n",
      "  episode_reward_min: -189.1013552314774\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1820\n",
      "  experiment_id: 610b8b5226964181899719d25df7617e\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.734723475976807e-19\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -3.7572754018008707\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.022351985845489555\n",
      "          policy_loss: 0.00037744521396234634\n",
      "          total_loss: 476.0681701898575\n",
      "          vf_explained_var: 6.9595871536876075e-06\n",
      "          vf_loss: 476.0677954673767\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 728000\n",
      "    num_steps_sampled: 728000\n",
      "    num_steps_trained: 728000\n",
      "  iterations_since_restore: 182\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.582142857142856\n",
      "    ram_util_percent: 31.813095238095247\n",
      "  pid: 17432\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0884859996639689\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.81721172973448\n",
      "    mean_inference_ms: 1.837385413707555\n",
      "    mean_raw_obs_processing_ms: 6.260305146544692\n",
      "  time_since_restore: 10573.314942359924\n",
      "  time_this_iter_s: 58.57991552352905\n",
      "  time_total_s: 10573.314942359924\n",
      "  timers:\n",
      "    learn_throughput: 151.783\n",
      "    learn_time_ms: 26353.466\n",
      "    sample_throughput: 123.73\n",
      "    sample_time_ms: 32328.474\n",
      "    update_time_ms: 3.213\n",
      "  timestamp: 1672618566\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 728000\n",
      "  training_iteration: 182\n",
      "  trial_id: f9c59_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 9.9/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.87 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_2_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_5_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_4_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_1_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_0_6417799359d4e2284884fd3ae2ec9735, 0.0/6.0 CPU_group_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_3_6417799359d4e2284884fd3ae2ec9735)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                               | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridPOEnv-v0_f9c59_00000 | RUNNING  | 192.168.1.65:17432 |    182 |          10573.3 | 728000 | -186.555 |             -184.427 |             -189.101 |                400 |\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridPOEnv-v0_f9c59_00000:\n",
      "  agent_timesteps_total: 732000\n",
      "  custom_metrics: {}\n",
      "  date: 2023-01-01_18-17-05\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -184.42278866537856\n",
      "  episode_reward_mean: -186.5235574037424\n",
      "  episode_reward_min: -189.1013552314774\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1830\n",
      "  experiment_id: 610b8b5226964181899719d25df7617e\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.734723475976807e-19\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -3.717101299017668\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.020991872489776277\n",
      "          policy_loss: 0.0016578838884015567\n",
      "          total_loss: 483.1462066411972\n",
      "          vf_explained_var: 1.3143755495548248e-05\n",
      "          vf_loss: 483.14454703330995\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 732000\n",
      "    num_steps_sampled: 732000\n",
      "    num_steps_trained: 732000\n",
      "  iterations_since_restore: 183\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.490476190476187\n",
      "    ram_util_percent: 31.894047619047623\n",
      "  pid: 17432\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0884866031578176\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.817728423555437\n",
      "    mean_inference_ms: 1.8374560026443982\n",
      "    mean_raw_obs_processing_ms: 6.2600528680281435\n",
      "  time_since_restore: 10632.02036857605\n",
      "  time_this_iter_s: 58.70542621612549\n",
      "  time_total_s: 10632.02036857605\n",
      "  timers:\n",
      "    learn_throughput: 151.703\n",
      "    learn_time_ms: 26367.378\n",
      "    sample_throughput: 123.754\n",
      "    sample_time_ms: 32322.179\n",
      "    update_time_ms: 3.207\n",
      "  timestamp: 1672618625\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 732000\n",
      "  training_iteration: 183\n",
      "  trial_id: f9c59_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Memory usage on this node: 9.9/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.87 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_2_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_0_6417799359d4e2284884fd3ae2ec9735, 0.0/6.0 CPU_group_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_1_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_4_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_3_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_5_6417799359d4e2284884fd3ae2ec9735)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                               | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridPOEnv-v0_f9c59_00000 | RUNNING  | 192.168.1.65:17432 |    183 |            10632 | 732000 | -186.524 |             -184.423 |             -189.101 |                400 |\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridPOEnv-v0_f9c59_00000:\n",
      "  agent_timesteps_total: 736000\n",
      "  custom_metrics: {}\n",
      "  date: 2023-01-01_18-18-03\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -184.42247715439822\n",
      "  episode_reward_mean: -186.55165369399208\n",
      "  episode_reward_min: -189.1013552314774\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1840\n",
      "  experiment_id: 610b8b5226964181899719d25df7617e\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.734723475976807e-19\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -3.740420399606228\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016808178520841467\n",
      "          policy_loss: 0.0031295724635128862\n",
      "          total_loss: 476.88031737804414\n",
      "          vf_explained_var: 1.1272170013398863e-05\n",
      "          vf_loss: 476.8771874666214\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 736000\n",
      "    num_steps_sampled: 736000\n",
      "    num_steps_trained: 736000\n",
      "  iterations_since_restore: 184\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.35952380952381\n",
      "    ram_util_percent: 31.904761904761905\n",
      "  pid: 17432\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08848653936606717\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.818306321390466\n",
      "    mean_inference_ms: 1.8375018839234667\n",
      "    mean_raw_obs_processing_ms: 6.259705661700545\n",
      "  time_since_restore: 10690.659871816635\n",
      "  time_this_iter_s: 58.63950324058533\n",
      "  time_total_s: 10690.659871816635\n",
      "  timers:\n",
      "    learn_throughput: 151.744\n",
      "    learn_time_ms: 26360.199\n",
      "    sample_throughput: 123.771\n",
      "    sample_time_ms: 32317.701\n",
      "    update_time_ms: 3.209\n",
      "  timestamp: 1672618683\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 736000\n",
      "  training_iteration: 184\n",
      "  trial_id: f9c59_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 9.9/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.87 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_5_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_1_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_3_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_4_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_0_6417799359d4e2284884fd3ae2ec9735, 0.0/6.0 CPU_group_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_2_6417799359d4e2284884fd3ae2ec9735)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                               | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridPOEnv-v0_f9c59_00000 | RUNNING  | 192.168.1.65:17432 |    184 |          10690.7 | 736000 | -186.552 |             -184.422 |             -189.101 |                400 |\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridPOEnv-v0_f9c59_00000:\n",
      "  agent_timesteps_total: 740000\n",
      "  custom_metrics: {}\n",
      "  date: 2023-01-01_18-19-03\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -184.42247715439822\n",
      "  episode_reward_mean: -186.58944518938088\n",
      "  episode_reward_min: -188.8865407631252\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1850\n",
      "  experiment_id: 610b8b5226964181899719d25df7617e\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.734723475976807e-19\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -3.8323535822331904\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01989427720673132\n",
      "          policy_loss: 0.003843685329775326\n",
      "          total_loss: 475.41805467605593\n",
      "          vf_explained_var: 1.1818483471870422e-06\n",
      "          vf_loss: 475.4142103910446\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 740000\n",
      "    num_steps_sampled: 740000\n",
      "    num_steps_trained: 740000\n",
      "  iterations_since_restore: 185\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.645238095238096\n",
      "    ram_util_percent: 31.865476190476194\n",
      "  pid: 17432\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08848672635074531\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.818986058233758\n",
      "    mean_inference_ms: 1.8375270923317748\n",
      "    mean_raw_obs_processing_ms: 6.259403937686844\n",
      "  time_since_restore: 10749.56282043457\n",
      "  time_this_iter_s: 58.90294861793518\n",
      "  time_total_s: 10749.56282043457\n",
      "  timers:\n",
      "    learn_throughput: 151.705\n",
      "    learn_time_ms: 26367.03\n",
      "    sample_throughput: 123.69\n",
      "    sample_time_ms: 32338.99\n",
      "    update_time_ms: 3.264\n",
      "  timestamp: 1672618743\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 740000\n",
      "  training_iteration: 185\n",
      "  trial_id: f9c59_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 9.9/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.87 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_0_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_4_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_5_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 accelerator_type:G, 0.0/6.0 CPU_group_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_1_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_2_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_3_6417799359d4e2284884fd3ae2ec9735)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                               | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridPOEnv-v0_f9c59_00000 | RUNNING  | 192.168.1.65:17432 |    185 |          10749.6 | 740000 | -186.589 |             -184.422 |             -188.887 |                400 |\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_TrafficLightGridPOEnv-v0_f9c59_00000:\n",
      "  agent_timesteps_total: 744000\n",
      "  custom_metrics: {}\n",
      "  date: 2023-01-01_18-20-02\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -184.42247715439822\n",
      "  episode_reward_mean: -186.63151369657393\n",
      "  episode_reward_min: -188.8865407631252\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1860\n",
      "  experiment_id: 610b8b5226964181899719d25df7617e\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.734723475976807e-19\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -3.844866229593754\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015929485704146623\n",
      "          policy_loss: 0.0029176176089094953\n",
      "          total_loss: 477.49826683998106\n",
      "          vf_explained_var: 5.48921525478363e-06\n",
      "          vf_loss: 477.4953483581543\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 744000\n",
      "    num_steps_sampled: 744000\n",
      "    num_steps_trained: 744000\n",
      "  iterations_since_restore: 186\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.385882352941177\n",
      "    ram_util_percent: 31.924705882352935\n",
      "  pid: 17432\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08848726715124311\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.81977836203746\n",
      "    mean_inference_ms: 1.8375506356681268\n",
      "    mean_raw_obs_processing_ms: 6.259140105738271\n",
      "  time_since_restore: 10809.27922296524\n",
      "  time_this_iter_s: 59.716402530670166\n",
      "  time_total_s: 10809.27922296524\n",
      "  timers:\n",
      "    learn_throughput: 151.219\n",
      "    learn_time_ms: 26451.632\n",
      "    sample_throughput: 123.595\n",
      "    sample_time_ms: 32363.646\n",
      "    update_time_ms: 3.251\n",
      "  timestamp: 1672618802\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 744000\n",
      "  training_iteration: 186\n",
      "  trial_id: f9c59_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 10.0/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.87 GiB heap, 0.0/14.65 GiB objects (0.0/6.0 CPU_group_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_5_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_1_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_0_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_3_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_2_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_4_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 accelerator_type:G)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                               | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridPOEnv-v0_f9c59_00000 | RUNNING  | 192.168.1.65:17432 |    186 |          10809.3 | 744000 | -186.632 |             -184.422 |             -188.887 |                400 |\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridPOEnv-v0_f9c59_00000:\n",
      "  agent_timesteps_total: 748000\n",
      "  custom_metrics: {}\n",
      "  date: 2023-01-01_18-21-02\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -184.42247715439822\n",
      "  episode_reward_mean: -186.67799439714148\n",
      "  episode_reward_min: -188.8865407631252\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1870\n",
      "  experiment_id: 610b8b5226964181899719d25df7617e\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.734723475976807e-19\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -3.8953719161450864\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014046987198891308\n",
      "          policy_loss: 0.0026770237265736794\n",
      "          total_loss: 477.69897918701173\n",
      "          vf_explained_var: 5.74309387957328e-06\n",
      "          vf_loss: 477.6963025093079\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 748000\n",
      "    num_steps_sampled: 748000\n",
      "    num_steps_trained: 748000\n",
      "  iterations_since_restore: 187\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.211627906976744\n",
      "    ram_util_percent: 32.004651162790694\n",
      "  pid: 17432\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08848750788242261\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.820565516399057\n",
      "    mean_inference_ms: 1.8375564412709857\n",
      "    mean_raw_obs_processing_ms: 6.258889727642711\n",
      "  time_since_restore: 10868.868785381317\n",
      "  time_this_iter_s: 59.58956241607666\n",
      "  time_total_s: 10868.868785381317\n",
      "  timers:\n",
      "    learn_throughput: 150.677\n",
      "    learn_time_ms: 26546.764\n",
      "    sample_throughput: 123.592\n",
      "    sample_time_ms: 32364.517\n",
      "    update_time_ms: 3.211\n",
      "  timestamp: 1672618862\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 748000\n",
      "  training_iteration: 187\n",
      "  trial_id: f9c59_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 10.0/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.87 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_1_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_5_6417799359d4e2284884fd3ae2ec9735, 0.0/6.0 CPU_group_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_3_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_2_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_4_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_0_6417799359d4e2284884fd3ae2ec9735)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                               | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridPOEnv-v0_f9c59_00000 | RUNNING  | 192.168.1.65:17432 |    187 |          10868.9 | 748000 | -186.678 |             -184.422 |             -188.887 |                400 |\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridPOEnv-v0_f9c59_00000:\n",
      "  agent_timesteps_total: 752000\n",
      "  custom_metrics: {}\n",
      "  date: 2023-01-01_18-22-02\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -184.42247715439822\n",
      "  episode_reward_mean: -186.55907813425384\n",
      "  episode_reward_min: -188.8865407631252\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1880\n",
      "  experiment_id: 610b8b5226964181899719d25df7617e\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.734723475976807e-19\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -4.013073188066483\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.021636207903793756\n",
      "          policy_loss: 0.0010270250779285562\n",
      "          total_loss: 480.5749492883682\n",
      "          vf_explained_var: 3.462284894339973e-06\n",
      "          vf_loss: 480.57392237186434\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 752000\n",
      "    num_steps_sampled: 752000\n",
      "    num_steps_trained: 752000\n",
      "  iterations_since_restore: 188\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.494117647058825\n",
      "    ram_util_percent: 32.03647058823529\n",
      "  pid: 17432\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08848813091805656\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.821295791907698\n",
      "    mean_inference_ms: 1.8375756396484508\n",
      "    mean_raw_obs_processing_ms: 6.25869251719864\n",
      "  time_since_restore: 10928.665649175644\n",
      "  time_this_iter_s: 59.79686379432678\n",
      "  time_total_s: 10928.665649175644\n",
      "  timers:\n",
      "    learn_throughput: 150.001\n",
      "    learn_time_ms: 26666.493\n",
      "    sample_throughput: 123.611\n",
      "    sample_time_ms: 32359.64\n",
      "    update_time_ms: 3.149\n",
      "  timestamp: 1672618922\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 752000\n",
      "  training_iteration: 188\n",
      "  trial_id: f9c59_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Memory usage on this node: 10.0/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.87 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_4_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_2_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_3_6417799359d4e2284884fd3ae2ec9735, 0.0/6.0 CPU_group_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_5_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_0_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_1_6417799359d4e2284884fd3ae2ec9735)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                               | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridPOEnv-v0_f9c59_00000 | RUNNING  | 192.168.1.65:17432 |    188 |          10928.7 | 752000 | -186.559 |             -184.422 |             -188.887 |                400 |\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridPOEnv-v0_f9c59_00000:\n",
      "  agent_timesteps_total: 756000\n",
      "  custom_metrics: {}\n",
      "  date: 2023-01-01_18-23-02\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -184.42247715439822\n",
      "  episode_reward_mean: -186.57899324910534\n",
      "  episode_reward_min: -188.43861462530293\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1890\n",
      "  experiment_id: 610b8b5226964181899719d25df7617e\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.734723475976807e-19\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -4.07650667950511\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01904616958818265\n",
      "          policy_loss: 0.002545333131274674\n",
      "          total_loss: 479.1903046607971\n",
      "          vf_explained_var: 1.0331347766623367e-05\n",
      "          vf_loss: 479.18775782585146\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 756000\n",
      "    num_steps_sampled: 756000\n",
      "    num_steps_trained: 756000\n",
      "  iterations_since_restore: 189\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.346511627906974\n",
      "    ram_util_percent: 32.06860465116279\n",
      "  pid: 17432\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08848860938214914\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.82204984434101\n",
      "    mean_inference_ms: 1.8376150140417025\n",
      "    mean_raw_obs_processing_ms: 6.258513460739334\n",
      "  time_since_restore: 10988.374182462692\n",
      "  time_this_iter_s: 59.70853328704834\n",
      "  time_total_s: 10988.374182462692\n",
      "  timers:\n",
      "    learn_throughput: 149.327\n",
      "    learn_time_ms: 26786.92\n",
      "    sample_throughput: 123.596\n",
      "    sample_time_ms: 32363.585\n",
      "    update_time_ms: 3.142\n",
      "  timestamp: 1672618982\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 756000\n",
      "  training_iteration: 189\n",
      "  trial_id: f9c59_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 10.0/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.87 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_4_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_1_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_3_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_2_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_0_6417799359d4e2284884fd3ae2ec9735, 0.0/6.0 CPU_group_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_5_6417799359d4e2284884fd3ae2ec9735)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                               | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridPOEnv-v0_f9c59_00000 | RUNNING  | 192.168.1.65:17432 |    189 |          10988.4 | 756000 | -186.579 |             -184.422 |             -188.439 |                400 |\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridPOEnv-v0_f9c59_00000:\n",
      "  agent_timesteps_total: 760000\n",
      "  custom_metrics: {}\n",
      "  date: 2023-01-01_18-24-01\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -184.42247715439822\n",
      "  episode_reward_mean: -186.5652274754071\n",
      "  episode_reward_min: -188.43861462530293\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1900\n",
      "  experiment_id: 610b8b5226964181899719d25df7617e\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.734723475976807e-19\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -4.114266476035118\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016661912282962328\n",
      "          policy_loss: 0.005102889623958618\n",
      "          total_loss: 480.46344311237334\n",
      "          vf_explained_var: 8.940696716308594e-06\n",
      "          vf_loss: 480.45833909511566\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 760000\n",
      "    num_steps_sampled: 760000\n",
      "    num_steps_trained: 760000\n",
      "  iterations_since_restore: 190\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.22470588235294\n",
      "    ram_util_percent: 32.05294117647059\n",
      "  pid: 17432\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08848898571799513\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.823030760525317\n",
      "    mean_inference_ms: 1.8376597426797452\n",
      "    mean_raw_obs_processing_ms: 6.257674524613597\n",
      "  time_since_restore: 11047.579673051834\n",
      "  time_this_iter_s: 59.205490589141846\n",
      "  time_total_s: 11047.579673051834\n",
      "  timers:\n",
      "    learn_throughput: 148.897\n",
      "    learn_time_ms: 26864.278\n",
      "    sample_throughput: 123.834\n",
      "    sample_time_ms: 32301.356\n",
      "    update_time_ms: 3.185\n",
      "  timestamp: 1672619041\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 760000\n",
      "  training_iteration: 190\n",
      "  trial_id: f9c59_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 10.0/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.87 GiB heap, 0.0/14.65 GiB objects (0.0/6.0 CPU_group_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_3_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_2_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_4_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_0_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_1_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_5_6417799359d4e2284884fd3ae2ec9735)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                               | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridPOEnv-v0_f9c59_00000 | RUNNING  | 192.168.1.65:17432 |    190 |          11047.6 | 760000 | -186.565 |             -184.422 |             -188.439 |                400 |\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_TrafficLightGridPOEnv-v0_f9c59_00000:\n",
      "  agent_timesteps_total: 764000\n",
      "  custom_metrics: {}\n",
      "  date: 2023-01-01_18-25-01\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -184.42247715439822\n",
      "  episode_reward_mean: -186.4376514406133\n",
      "  episode_reward_min: -188.43861462530293\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1910\n",
      "  experiment_id: 610b8b5226964181899719d25df7617e\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.734723475976807e-19\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -4.067292466759682\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.018040238104004482\n",
      "          policy_loss: 0.003911229700315743\n",
      "          total_loss: 479.5374312520027\n",
      "          vf_explained_var: 1.913122787300381e-06\n",
      "          vf_loss: 479.5335206747055\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 764000\n",
      "    num_steps_sampled: 764000\n",
      "    num_steps_trained: 764000\n",
      "  iterations_since_restore: 191\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.43333333333333\n",
      "    ram_util_percent: 32.10357142857143\n",
      "  pid: 17432\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08848901064515725\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.823763374972014\n",
      "    mean_inference_ms: 1.8376927624630595\n",
      "    mean_raw_obs_processing_ms: 6.256813219224498\n",
      "  time_since_restore: 11106.812588691711\n",
      "  time_this_iter_s: 59.23291563987732\n",
      "  time_total_s: 11106.812588691711\n",
      "  timers:\n",
      "    learn_throughput: 148.515\n",
      "    learn_time_ms: 26933.318\n",
      "    sample_throughput: 123.992\n",
      "    sample_time_ms: 32260.17\n",
      "    update_time_ms: 3.192\n",
      "  timestamp: 1672619101\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 764000\n",
      "  training_iteration: 191\n",
      "  trial_id: f9c59_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 10.0/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.87 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_1_6417799359d4e2284884fd3ae2ec9735, 0.0/6.0 CPU_group_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_5_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_2_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_3_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_0_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_4_6417799359d4e2284884fd3ae2ec9735)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                               | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridPOEnv-v0_f9c59_00000 | RUNNING  | 192.168.1.65:17432 |    191 |          11106.8 | 764000 | -186.438 |             -184.422 |             -188.439 |                400 |\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridPOEnv-v0_f9c59_00000:\n",
      "  agent_timesteps_total: 768000\n",
      "  custom_metrics: {}\n",
      "  date: 2023-01-01_18-26-01\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -184.42247715439822\n",
      "  episode_reward_mean: -186.3493749715785\n",
      "  episode_reward_min: -188.43861462530293\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1920\n",
      "  experiment_id: 610b8b5226964181899719d25df7617e\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.734723475976807e-19\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -4.085047008097172\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.019480338393623436\n",
      "          policy_loss: 0.0024369553051656113\n",
      "          total_loss: 475.91848797798156\n",
      "          vf_explained_var: 9.741634130477905e-06\n",
      "          vf_loss: 475.91604883670806\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 768000\n",
      "    num_steps_sampled: 768000\n",
      "    num_steps_trained: 768000\n",
      "  iterations_since_restore: 192\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.3046511627907\n",
      "    ram_util_percent: 32.12558139534884\n",
      "  pid: 17432\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08848869197599173\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.82465286693433\n",
      "    mean_inference_ms: 1.8377010222125796\n",
      "    mean_raw_obs_processing_ms: 6.256006031148501\n",
      "  time_since_restore: 11166.666818141937\n",
      "  time_this_iter_s: 59.85422945022583\n",
      "  time_total_s: 11166.666818141937\n",
      "  timers:\n",
      "    learn_throughput: 147.985\n",
      "    learn_time_ms: 27029.764\n",
      "    sample_throughput: 123.873\n",
      "    sample_time_ms: 32291.187\n",
      "    update_time_ms: 3.169\n",
      "  timestamp: 1672619161\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 768000\n",
      "  training_iteration: 192\n",
      "  trial_id: f9c59_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 10.0/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.87 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_3_6417799359d4e2284884fd3ae2ec9735, 0.0/6.0 CPU_group_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_2_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_1_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_0_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_4_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_5_6417799359d4e2284884fd3ae2ec9735)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                               | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridPOEnv-v0_f9c59_00000 | RUNNING  | 192.168.1.65:17432 |    192 |          11166.7 | 768000 | -186.349 |             -184.422 |             -188.439 |                400 |\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridPOEnv-v0_f9c59_00000:\n",
      "  agent_timesteps_total: 772000\n",
      "  custom_metrics: {}\n",
      "  date: 2023-01-01_18-27-00\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -184.42247715439822\n",
      "  episode_reward_mean: -186.3891982895897\n",
      "  episode_reward_min: -189.3404922239468\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1930\n",
      "  experiment_id: 610b8b5226964181899719d25df7617e\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.734723475976807e-19\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -4.179245236515999\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.020464163813994855\n",
      "          policy_loss: 0.005252627824665979\n",
      "          total_loss: 475.5450452566147\n",
      "          vf_explained_var: 4.952587005391251e-06\n",
      "          vf_loss: 475.5397919178009\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 772000\n",
      "    num_steps_sampled: 772000\n",
      "    num_steps_trained: 772000\n",
      "  iterations_since_restore: 193\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.39176470588236\n",
      "    ram_util_percent: 32.148235294117654\n",
      "  pid: 17432\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0884881705864807\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.825292929286576\n",
      "    mean_inference_ms: 1.8377082697293532\n",
      "    mean_raw_obs_processing_ms: 6.255206128629418\n",
      "  time_since_restore: 11225.94728398323\n",
      "  time_this_iter_s: 59.280465841293335\n",
      "  time_total_s: 11225.94728398323\n",
      "  timers:\n",
      "    learn_throughput: 147.411\n",
      "    learn_time_ms: 27134.944\n",
      "    sample_throughput: 124.056\n",
      "    sample_time_ms: 32243.487\n",
      "    update_time_ms: 3.168\n",
      "  timestamp: 1672619220\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 772000\n",
      "  training_iteration: 193\n",
      "  trial_id: f9c59_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Memory usage on this node: 10.0/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.87 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_5_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_1_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_2_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_3_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_4_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_0_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 accelerator_type:G, 0.0/6.0 CPU_group_6417799359d4e2284884fd3ae2ec9735)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                               | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridPOEnv-v0_f9c59_00000 | RUNNING  | 192.168.1.65:17432 |    193 |          11225.9 | 772000 | -186.389 |             -184.422 |              -189.34 |                400 |\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridPOEnv-v0_f9c59_00000:\n",
      "  agent_timesteps_total: 776000\n",
      "  custom_metrics: {}\n",
      "  date: 2023-01-01_18-28-01\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -184.62848963079765\n",
      "  episode_reward_mean: -186.35619414917485\n",
      "  episode_reward_min: -189.3404922239468\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1940\n",
      "  experiment_id: 610b8b5226964181899719d25df7617e\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.734723475976807e-19\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -4.268000040948391\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.023386373308312704\n",
      "          policy_loss: 0.0017649270259425976\n",
      "          total_loss: 478.48221015930176\n",
      "          vf_explained_var: 5.3517519518209156e-06\n",
      "          vf_loss: 478.4804446220398\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 776000\n",
      "    num_steps_sampled: 776000\n",
      "    num_steps_trained: 776000\n",
      "  iterations_since_restore: 194\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.21046511627907\n",
      "    ram_util_percent: 32.199999999999996\n",
      "  pid: 17432\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08848783522818157\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.82590764414007\n",
      "    mean_inference_ms: 1.8377091978735973\n",
      "    mean_raw_obs_processing_ms: 6.254465880012038\n",
      "  time_since_restore: 11286.394974946976\n",
      "  time_this_iter_s: 60.44769096374512\n",
      "  time_total_s: 11286.394974946976\n",
      "  timers:\n",
      "    learn_throughput: 146.469\n",
      "    learn_time_ms: 27309.606\n",
      "    sample_throughput: 124.032\n",
      "    sample_time_ms: 32249.717\n",
      "    update_time_ms: 3.18\n",
      "  timestamp: 1672619281\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 776000\n",
      "  training_iteration: 194\n",
      "  trial_id: f9c59_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 10.0/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.87 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 accelerator_type:G, 0.0/6.0 CPU_group_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_1_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_2_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_3_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_0_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_5_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_4_6417799359d4e2284884fd3ae2ec9735)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                               | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridPOEnv-v0_f9c59_00000 | RUNNING  | 192.168.1.65:17432 |    194 |          11286.4 | 776000 | -186.356 |             -184.628 |              -189.34 |                400 |\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridPOEnv-v0_f9c59_00000:\n",
      "  agent_timesteps_total: 780000\n",
      "  custom_metrics: {}\n",
      "  date: 2023-01-01_18-29-02\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -184.62848963079765\n",
      "  episode_reward_mean: -186.37666440170574\n",
      "  episode_reward_min: -192.7225955633577\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1950\n",
      "  experiment_id: 610b8b5226964181899719d25df7617e\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.734723475976807e-19\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -4.344181022047996\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01731321553998076\n",
      "          policy_loss: 0.0034208717639558016\n",
      "          total_loss: 479.00255432128904\n",
      "          vf_explained_var: 1.1738389957827167e-06\n",
      "          vf_loss: 478.9991309642792\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 780000\n",
      "    num_steps_sampled: 780000\n",
      "    num_steps_trained: 780000\n",
      "  iterations_since_restore: 195\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.956818181818186\n",
      "    ram_util_percent: 32.13522727272727\n",
      "  pid: 17432\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08848673309747604\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.826563797674417\n",
      "    mean_inference_ms: 1.8376864392966048\n",
      "    mean_raw_obs_processing_ms: 6.253773010048222\n",
      "  time_since_restore: 11347.342657089233\n",
      "  time_this_iter_s: 60.94768214225769\n",
      "  time_total_s: 11347.342657089233\n",
      "  timers:\n",
      "    learn_throughput: 145.444\n",
      "    learn_time_ms: 27501.912\n",
      "    sample_throughput: 123.985\n",
      "    sample_time_ms: 32261.895\n",
      "    update_time_ms: 3.123\n",
      "  timestamp: 1672619342\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 780000\n",
      "  training_iteration: 195\n",
      "  trial_id: f9c59_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 10.0/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.87 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_3_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_0_6417799359d4e2284884fd3ae2ec9735, 0.0/6.0 CPU_group_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_4_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_5_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_1_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_2_6417799359d4e2284884fd3ae2ec9735)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                               | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridPOEnv-v0_f9c59_00000 | RUNNING  | 192.168.1.65:17432 |    195 |          11347.3 | 780000 | -186.377 |             -184.628 |             -192.723 |                400 |\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_TrafficLightGridPOEnv-v0_f9c59_00000:\n",
      "  agent_timesteps_total: 784000\n",
      "  custom_metrics: {}\n",
      "  date: 2023-01-01_18-30-03\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -184.64447352235814\n",
      "  episode_reward_mean: -186.37973696924644\n",
      "  episode_reward_min: -192.7225955633577\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1960\n",
      "  experiment_id: 610b8b5226964181899719d25df7617e\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.734723475976807e-19\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -4.397592023015022\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01966816351214239\n",
      "          policy_loss: 0.0054128924413817\n",
      "          total_loss: 472.57166240215304\n",
      "          vf_explained_var: 2.2203103071660735e-05\n",
      "          vf_loss: 472.566246509552\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 784000\n",
      "    num_steps_sampled: 784000\n",
      "    num_steps_trained: 784000\n",
      "  iterations_since_restore: 196\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.388372093023253\n",
      "    ram_util_percent: 32.206976744186036\n",
      "  pid: 17432\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08848544396721135\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.82713931999485\n",
      "    mean_inference_ms: 1.837663183179007\n",
      "    mean_raw_obs_processing_ms: 6.253087105373791\n",
      "  time_since_restore: 11408.027355909348\n",
      "  time_this_iter_s: 60.684698820114136\n",
      "  time_total_s: 11408.027355909348\n",
      "  timers:\n",
      "    learn_throughput: 144.848\n",
      "    learn_time_ms: 27615.214\n",
      "    sample_throughput: 124.049\n",
      "    sample_time_ms: 32245.432\n",
      "    update_time_ms: 3.13\n",
      "  timestamp: 1672619403\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 784000\n",
      "  training_iteration: 196\n",
      "  trial_id: f9c59_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 10.0/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.87 GiB heap, 0.0/14.65 GiB objects (0.0/6.0 CPU_group_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_2_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_5_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_1_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_0_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_3_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_4_6417799359d4e2284884fd3ae2ec9735)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                               | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridPOEnv-v0_f9c59_00000 | RUNNING  | 192.168.1.65:17432 |    196 |            11408 | 784000 |  -186.38 |             -184.644 |             -192.723 |                400 |\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridPOEnv-v0_f9c59_00000:\n",
      "  agent_timesteps_total: 788000\n",
      "  custom_metrics: {}\n",
      "  date: 2023-01-01_18-31-04\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -184.6677030033509\n",
      "  episode_reward_mean: -186.34951920662158\n",
      "  episode_reward_min: -192.7225955633577\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1970\n",
      "  experiment_id: 610b8b5226964181899719d25df7617e\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.734723475976807e-19\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -4.530955421924591\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.02984430396754769\n",
      "          policy_loss: 0.0012731475915643386\n",
      "          total_loss: 475.9969894886017\n",
      "          vf_explained_var: 6.3091515585256275e-06\n",
      "          vf_loss: 475.99571509361266\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 788000\n",
      "    num_steps_sampled: 788000\n",
      "    num_steps_trained: 788000\n",
      "  iterations_since_restore: 197\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.16590909090909\n",
      "    ram_util_percent: 32.32727272727273\n",
      "  pid: 17432\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08848444127594085\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.827714174919134\n",
      "    mean_inference_ms: 1.8376619497790727\n",
      "    mean_raw_obs_processing_ms: 6.252404288929085\n",
      "  time_since_restore: 11469.177431821823\n",
      "  time_this_iter_s: 61.150075912475586\n",
      "  time_total_s: 11469.177431821823\n",
      "  timers:\n",
      "    learn_throughput: 144.066\n",
      "    learn_time_ms: 27765.092\n",
      "    sample_throughput: 124.025\n",
      "    sample_time_ms: 32251.65\n",
      "    update_time_ms: 3.127\n",
      "  timestamp: 1672619464\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 788000\n",
      "  training_iteration: 197\n",
      "  trial_id: f9c59_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 10.1/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.87 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_3_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_2_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_1_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_5_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_4_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 accelerator_type:G, 0.0/6.0 CPU_group_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_0_6417799359d4e2284884fd3ae2ec9735)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                               | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridPOEnv-v0_f9c59_00000 | RUNNING  | 192.168.1.65:17432 |    197 |          11469.2 | 788000 |  -186.35 |             -184.668 |             -192.723 |                400 |\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridPOEnv-v0_f9c59_00000:\n",
      "  agent_timesteps_total: 792000\n",
      "  custom_metrics: {}\n",
      "  date: 2023-01-01_18-32-06\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -184.6677030033509\n",
      "  episode_reward_mean: -186.3984440142883\n",
      "  episode_reward_min: -192.7225955633577\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1980\n",
      "  experiment_id: 610b8b5226964181899719d25df7617e\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.734723475976807e-19\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -4.616660302877426\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.021420949760068097\n",
      "          policy_loss: 0.002902344736503437\n",
      "          total_loss: 472.04248802661897\n",
      "          vf_explained_var: 6.301328539848328e-06\n",
      "          vf_loss: 472.03958628177645\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 792000\n",
      "    num_steps_sampled: 792000\n",
      "    num_steps_trained: 792000\n",
      "  iterations_since_restore: 198\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.23103448275862\n",
      "    ram_util_percent: 32.34252873563219\n",
      "  pid: 17432\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08848402356804497\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.828253711892227\n",
      "    mean_inference_ms: 1.8376849892884422\n",
      "    mean_raw_obs_processing_ms: 6.2516855456946905\n",
      "  time_since_restore: 11530.287852048874\n",
      "  time_this_iter_s: 61.11042022705078\n",
      "  time_total_s: 11530.287852048874\n",
      "  timers:\n",
      "    learn_throughput: 143.336\n",
      "    learn_time_ms: 27906.364\n",
      "    sample_throughput: 124.063\n",
      "    sample_time_ms: 32241.784\n",
      "    update_time_ms: 3.178\n",
      "  timestamp: 1672619526\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 792000\n",
      "  training_iteration: 198\n",
      "  trial_id: f9c59_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Memory usage on this node: 10.1/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.87 GiB heap, 0.0/14.65 GiB objects (0.0/6.0 CPU_group_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_5_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_1_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_3_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_2_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_0_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_4_6417799359d4e2284884fd3ae2ec9735)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                               | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridPOEnv-v0_f9c59_00000 | RUNNING  | 192.168.1.65:17432 |    198 |          11530.3 | 792000 | -186.398 |             -184.668 |             -192.723 |                400 |\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridPOEnv-v0_f9c59_00000:\n",
      "  agent_timesteps_total: 796000\n",
      "  custom_metrics: {}\n",
      "  date: 2023-01-01_18-33-07\n",
      "  done: false\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -184.6677030033509\n",
      "  episode_reward_mean: -186.39949681473496\n",
      "  episode_reward_min: -192.7225955633577\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1990\n",
      "  experiment_id: 610b8b5226964181899719d25df7617e\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.734723475976807e-19\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -4.656775183975697\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.023125787450408997\n",
      "          policy_loss: -0.001504234228923451\n",
      "          total_loss: 481.27614364624026\n",
      "          vf_explained_var: 2.9573216124845203e-06\n",
      "          vf_loss: 481.27764530181884\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 796000\n",
      "    num_steps_sampled: 796000\n",
      "    num_steps_trained: 796000\n",
      "  iterations_since_restore: 199\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.875\n",
      "    ram_util_percent: 32.34772727272727\n",
      "  pid: 17432\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08848359633299628\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.828520921583866\n",
      "    mean_inference_ms: 1.8376799144847542\n",
      "    mean_raw_obs_processing_ms: 6.250958321333901\n",
      "  time_since_restore: 11591.236852645874\n",
      "  time_this_iter_s: 60.94900059700012\n",
      "  time_total_s: 11591.236852645874\n",
      "  timers:\n",
      "    learn_throughput: 142.434\n",
      "    learn_time_ms: 28083.15\n",
      "    sample_throughput: 124.266\n",
      "    sample_time_ms: 32189.05\n",
      "    update_time_ms: 3.167\n",
      "  timestamp: 1672619587\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 796000\n",
      "  training_iteration: 199\n",
      "  trial_id: f9c59_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 10.1/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.87 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_4_6417799359d4e2284884fd3ae2ec9735, 0.0/6.0 CPU_group_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_0_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_2_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_5_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_3_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_1_6417799359d4e2284884fd3ae2ec9735)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                               | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridPOEnv-v0_f9c59_00000 | RUNNING  | 192.168.1.65:17432 |    199 |          11591.2 | 796000 | -186.399 |             -184.668 |             -192.723 |                400 |\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "Result for PPO_TrafficLightGridPOEnv-v0_f9c59_00000:\n",
      "  agent_timesteps_total: 800000\n",
      "  custom_metrics: {}\n",
      "  date: 2023-01-01_18-34-09\n",
      "  done: true\n",
      "  episode_len_mean: 400.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -184.6677030033509\n",
      "  episode_reward_mean: -186.40041809579543\n",
      "  episode_reward_min: -192.7225955633577\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 2000\n",
      "  experiment_id: 610b8b5226964181899719d25df7617e\n",
      "  hostname: michael-villarreal\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.734723475976807e-19\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -4.652950264513493\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.024211731473042373\n",
      "          policy_loss: 0.005868003619252704\n",
      "          total_loss: 471.4408680200577\n",
      "          vf_explained_var: 4.396773874759674e-06\n",
      "          vf_loss: 471.4349949598312\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 800000\n",
      "    num_steps_sampled: 800000\n",
      "    num_steps_trained: 800000\n",
      "  iterations_since_restore: 200\n",
      "  node_ip: 192.168.1.65\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.961363636363636\n",
      "    ram_util_percent: 32.28181818181818\n",
      "  pid: 17432\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08848284609524101\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.828814882182836\n",
      "    mean_inference_ms: 1.8376705320576336\n",
      "    mean_raw_obs_processing_ms: 6.250289016896282\n",
      "  time_since_restore: 11653.00525522232\n",
      "  time_this_iter_s: 61.76840257644653\n",
      "  time_total_s: 11653.00525522232\n",
      "  timers:\n",
      "    learn_throughput: 141.248\n",
      "    learn_time_ms: 28319.014\n",
      "    sample_throughput: 124.187\n",
      "    sample_time_ms: 32209.459\n",
      "    update_time_ms: 3.11\n",
      "  timestamp: 1672619649\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 800000\n",
      "  training_iteration: 200\n",
      "  trial_id: f9c59_00000\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 10.1/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.87 GiB heap, 0.0/14.65 GiB objects (0.0/6.0 CPU_group_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_0_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_4_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_2_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_3_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_1_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_5_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 accelerator_type:G)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                               | status   | loc                |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridPOEnv-v0_f9c59_00000 | RUNNING  | 192.168.1.65:17432 |    200 |            11653 | 800000 |   -186.4 |             -184.668 |             -192.723 |                400 |\n",
      "+------------------------------------------+----------+--------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Memory usage on this node: 9.3/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 0/6 CPUs, 0/6 GPUs, 0.0/7.87 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_1_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_2_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_4_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_5_6417799359d4e2284884fd3ae2ec9735, 0.0/6.0 CPU_group_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 CPU_group_3_6417799359d4e2284884fd3ae2ec9735, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_0_6417799359d4e2284884fd3ae2ec9735)\n",
      "Result logdir: /home/michael/ray_results/grid_0\n",
      "Number of trials: 1/1 (1 TERMINATED)\n",
      "+------------------------------------------+------------+-------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                               | status     | loc   |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|------------------------------------------+------------+-------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_TrafficLightGridPOEnv-v0_f9c59_00000 | TERMINATED |       |    200 |            11653 | 800000 |   -186.4 |             -184.668 |             -192.723 |                400 |\n",
      "+------------------------------------------+------------+-------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "2023-01-01 18:34:09,839\tINFO tune.py:550 -- Total run time: 11681.09 seconds (11680.74 seconds for the tuning loop).\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!python3 examples/train.py grid0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
