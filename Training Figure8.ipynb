{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a5efa5b5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'num_workers': 5, 'num_envs_per_worker': 1, 'create_env_on_driver': False, 'rollout_fragment_length': 200, 'batch_mode': 'truncate_episodes', 'gamma': 0.999, 'lr': 5e-05, 'train_batch_size': 15000, 'model': {'_use_default_native_models': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': [[16, [8, 8], 4], [32, [4, 4], 2], [256, [11, 11], 1]], 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'num_framestacks': 'auto', 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1, 'framestack': True}, 'optimizer': {}, 'horizon': 1500, 'soft_horizon': False, 'no_done_at_end': False, 'env': None, 'observation_space': None, 'action_space': None, 'env_config': {}, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'torch', 'eager_tracing': False, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_num_episodes': 10, 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'collect_metrics_timeout': 180, 'metrics_smoothing_episodes': 100, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'min_iter_time_s': 0, 'timesteps_per_iteration': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {}, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, 'simple_optimizer': -1, 'monitor': -1, 'use_critic': True, 'use_gae': True, 'lambda': 0.97, 'kl_coeff': 0.2, 'sgd_minibatch_size': 128, 'shuffle_sequences': True, 'num_sgd_iter': 10, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.02, 'vf_share_layers': -1}\n",
<<<<<<< HEAD
      "2022-10-24 23:18:25,161\tINFO services.py:1247 -- View the Ray dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265\u001b[39m\u001b[22m\n",
      "== Status ==\n",
      "Memory usage on this node: 7.8/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 0/6 CPUs, 0/6 GPUs, 0.0/7.38 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_figure_eight\n",
=======
      "2022-10-27 15:17:37,179\tINFO services.py:1247 -- View the Ray dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265\u001b[39m\u001b[22m\n",
      "== Status ==\n",
      "Memory usage on this node: 7.0/31.3 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 0/6 CPUs, 0/6 GPUs, 0.0/8.21 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Result logdir: /home/michael/ray_results/singleagent_figure_eight\n",
>>>>>>> 45ba339a35ad2740fb9ec6babd1e5d31fe54e2af
      "Number of trials: 1/1 (1 PENDING)\n",
      "+-----------------------------+----------+-------+\n",
      "| Trial name                  | status   | loc   |\n",
      "|-----------------------------+----------+-------|\n",
<<<<<<< HEAD
      "| PPO_AccelEnv-v0_12173_00000 | PENDING  |       |\n",
      "+-----------------------------+----------+-------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=30781)\u001b[0m 2022-10-24 23:18:27,219\tINFO trainer.py:720 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=30781)\u001b[0m 2022-10-24 23:18:29,758\tWARNING util.py:55 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "Result for PPO_AccelEnv-v0_12173_00000:\n",
      "  agent_timesteps_total: 15000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-24_23-21-02\n",
      "  done: false\n",
      "  episode_len_mean: 1432.5\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -4209.6202350466865\n",
      "  episode_reward_mean: -7292.490496889489\n",
      "  episode_reward_min: -8093.733712774245\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 10\n",
      "  experiment_id: 3b224e0d4aa944209a188d13d8dd894d\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.19999999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.3420944402783603\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.005982196283927759\n",
      "          policy_loss: -0.004344863291450981\n",
      "          total_loss: 18679.691959083688\n",
      "          vf_explained_var: -9.05687542740452e-08\n",
      "          vf_loss: 18679.695151118907\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 15000\n",
      "    num_steps_sampled: 15000\n",
      "    num_steps_trained: 15000\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.544495412844036\n",
      "    ram_util_percent: 32.758715596330276\n",
      "  pid: 30781\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08989184429469961\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.603501932257934\n",
      "    mean_inference_ms: 1.992742747555014\n",
      "    mean_raw_obs_processing_ms: 1.0889888802833136\n",
      "  time_since_restore: 152.76448822021484\n",
      "  time_this_iter_s: 152.76448822021484\n",
      "  time_total_s: 152.76448822021484\n",
      "  timers:\n",
      "    learn_throughput: 334.307\n",
      "    learn_time_ms: 44868.982\n",
      "    sample_throughput: 139.046\n",
      "    sample_time_ms: 107878.136\n",
      "    update_time_ms: 2.292\n",
      "  timestamp: 1666671662\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 15000\n",
      "  training_iteration: 1\n",
      "  trial_id: '12173_00000'\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Memory usage on this node: 10.7/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.38 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_figure_eight\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------+----------+----------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                  | status   | loc                  |   iter |   total time (s) |    ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------+----------+----------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_AccelEnv-v0_12173_00000 | RUNNING  | 141.225.10.229:30781 |      1 |          152.764 | 15000 | -7292.49 |             -4209.62 |             -8093.73 |             1432.5 |\n",
      "+-----------------------------+----------+----------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=30781)\u001b[0m 2022-10-24 23:21:02,574\tWARNING ppo.py:242 -- The magnitude of your environment rewards are more than 729.0x the scale of `vf_clip_param`. This means that it will take more than 729.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "Result for PPO_AccelEnv-v0_12173_00000:\n",
      "  agent_timesteps_total: 30000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-24_23-23-32\n",
      "  done: false\n",
      "  episode_len_mean: 1466.25\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -4209.6202350466865\n",
      "  episode_reward_mean: -7406.1809028969055\n",
      "  episode_reward_min: -8093.733712774245\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 20\n",
      "  experiment_id: 3b224e0d4aa944209a188d13d8dd894d\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.09999999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.2739500195293103\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008646298814602556\n",
      "          policy_loss: -0.004902707623570399\n",
      "          total_loss: 17900.934830177437\n",
      "          vf_explained_var: -8.031473264225042e-09\n",
      "          vf_loss: 17900.938882084218\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 30000\n",
      "    num_steps_sampled: 30000\n",
      "    num_steps_trained: 30000\n",
      "  iterations_since_restore: 2\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.587906976744183\n",
      "    ram_util_percent: 36.244186046511636\n",
      "  pid: 30781\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08997679126753143\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.50246789159886\n",
      "    mean_inference_ms: 1.9919985241962845\n",
      "    mean_raw_obs_processing_ms: 1.0902294505058354\n",
      "  time_since_restore: 303.02851605415344\n",
      "  time_this_iter_s: 150.2640278339386\n",
      "  time_total_s: 303.02851605415344\n",
      "  timers:\n",
      "    learn_throughput: 338.845\n",
      "    learn_time_ms: 44268.082\n",
      "    sample_throughput: 139.887\n",
      "    sample_time_ms: 107229.595\n",
      "    update_time_ms: 2.268\n",
      "  timestamp: 1666671812\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 30000\n",
      "  training_iteration: 2\n",
      "  trial_id: '12173_00000'\n",
      "  \n",
      "\u001b[2m\u001b[36m(pid=30781)\u001b[0m 2022-10-24 23:23:32,892\tWARNING ppo.py:242 -- The magnitude of your environment rewards are more than 741.0x the scale of `vf_clip_param`. This means that it will take more than 741.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "== Status ==\n",
      "Memory usage on this node: 11.5/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.38 GiB heap, 0.0/14.65 GiB objects (0.0/6.0 CPU_group_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_0_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_1_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_5_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_4_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_2_edf1e7152919a3617c29d944024e5221, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_3_edf1e7152919a3617c29d944024e5221)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_figure_eight\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------+----------+----------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                  | status   | loc                  |   iter |   total time (s) |    ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------+----------+----------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_AccelEnv-v0_12173_00000 | RUNNING  | 141.225.10.229:30781 |      2 |          303.029 | 30000 | -7406.18 |             -4209.62 |             -8093.73 |            1466.25 |\n",
      "+-----------------------------+----------+----------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "Result for PPO_AccelEnv-v0_12173_00000:\n",
      "  agent_timesteps_total: 45000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-24_23-26-03\n",
      "  done: false\n",
      "  episode_len_mean: 1477.5\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -4209.6202350466865\n",
      "  episode_reward_mean: -7336.244606438577\n",
      "  episode_reward_min: -8093.733712774245\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 30\n",
      "  experiment_id: 3b224e0d4aa944209a188d13d8dd894d\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.049999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.166225009954582\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011967825625289414\n",
      "          policy_loss: -0.005494131942642695\n",
      "          total_loss: 16017.133639267744\n",
      "          vf_explained_var: -7.728399253892348e-09\n",
      "          vf_loss: 16017.138506355932\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 45000\n",
      "    num_steps_sampled: 45000\n",
      "    num_steps_trained: 45000\n",
      "  iterations_since_restore: 3\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.59767441860465\n",
      "    ram_util_percent: 36.52744186046511\n",
      "  pid: 30781\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09006568388432754\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.450242642963403\n",
      "    mean_inference_ms: 1.9893149916668817\n",
      "    mean_raw_obs_processing_ms: 1.0892522437086958\n",
      "  time_since_restore: 453.42125487327576\n",
      "  time_this_iter_s: 150.39273881912231\n",
      "  time_total_s: 453.42125487327576\n",
      "  timers:\n",
      "    learn_throughput: 339.5\n",
      "    learn_time_ms: 44182.579\n",
      "    sample_throughput: 140.261\n",
      "    sample_time_ms: 106943.849\n",
      "    update_time_ms: 2.369\n",
      "  timestamp: 1666671963\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 45000\n",
      "  training_iteration: 3\n",
      "  trial_id: '12173_00000'\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 11.5/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.38 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_5_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_4_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_3_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_0_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_1_edf1e7152919a3617c29d944024e5221, 0.0/1.0 accelerator_type:G, 0.0/6.0 CPU_group_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_2_edf1e7152919a3617c29d944024e5221)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_figure_eight\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------+----------+----------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                  | status   | loc                  |   iter |   total time (s) |    ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------+----------+----------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_AccelEnv-v0_12173_00000 | RUNNING  | 141.225.10.229:30781 |      3 |          453.421 | 45000 | -7336.24 |             -4209.62 |             -8093.73 |             1477.5 |\n",
      "+-----------------------------+----------+----------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=30781)\u001b[0m 2022-10-24 23:26:03,329\tWARNING ppo.py:242 -- The magnitude of your environment rewards are more than 734.0x the scale of `vf_clip_param`. This means that it will take more than 734.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "Result for PPO_AccelEnv-v0_12173_00000:\n",
      "  agent_timesteps_total: 60000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-24_23-28-34\n",
      "  done: false\n",
      "  episode_len_mean: 1483.125\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -4209.6202350466865\n",
      "  episode_reward_mean: -7134.981588537268\n",
      "  episode_reward_min: -8093.733712774245\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 40\n",
      "  experiment_id: 3b224e0d4aa944209a188d13d8dd894d\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.049999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.0676754307948937\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012861202215875995\n",
      "          policy_loss: -0.006082151502654966\n",
      "          total_loss: 13003.998887711865\n",
      "          vf_explained_var: -8.991209554665147e-09\n",
      "          vf_loss: 13004.004339909958\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 60000\n",
      "    num_steps_sampled: 60000\n",
      "    num_steps_trained: 60000\n",
      "  iterations_since_restore: 4\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.5875\n",
      "    ram_util_percent: 36.545833333333334\n",
      "  pid: 30781\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.090115694664597\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.416364353723974\n",
      "    mean_inference_ms: 1.9878919484614221\n",
      "    mean_raw_obs_processing_ms: 1.0882422776522476\n",
      "  time_since_restore: 604.8035311698914\n",
      "  time_this_iter_s: 151.3822762966156\n",
      "  time_total_s: 604.8035311698914\n",
      "  timers:\n",
      "    learn_throughput: 338.08\n",
      "    learn_time_ms: 44368.154\n",
      "    sample_throughput: 140.423\n",
      "    sample_time_ms: 106820.0\n",
      "    update_time_ms: 2.338\n",
      "  timestamp: 1666672114\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 60000\n",
      "  training_iteration: 4\n",
      "  trial_id: '12173_00000'\n",
      "  \n",
      "\u001b[2m\u001b[36m(pid=30781)\u001b[0m 2022-10-24 23:28:34,750\tWARNING ppo.py:242 -- The magnitude of your environment rewards are more than 713.0x the scale of `vf_clip_param`. This means that it will take more than 713.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "== Status ==\n",
      "Memory usage on this node: 11.5/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.38 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_1_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_5_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_0_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_2_edf1e7152919a3617c29d944024e5221, 0.0/6.0 CPU_group_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_4_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_3_edf1e7152919a3617c29d944024e5221, 0.0/1.0 accelerator_type:G)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_figure_eight\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------+----------+----------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                  | status   | loc                  |   iter |   total time (s) |    ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------+----------+----------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_AccelEnv-v0_12173_00000 | RUNNING  | 141.225.10.229:30781 |      4 |          604.804 | 60000 | -7134.98 |             -4209.62 |             -8093.73 |            1483.12 |\n",
      "+-----------------------------+----------+----------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "Result for PPO_AccelEnv-v0_12173_00000:\n",
      "  agent_timesteps_total: 75000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-24_23-31-04\n",
      "  done: false\n",
      "  episode_len_mean: 1486.5\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -4209.6202350466865\n",
      "  episode_reward_mean: -6854.710408331435\n",
      "  episode_reward_min: -8093.733712774245\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 50\n",
      "  experiment_id: 3b224e0d4aa944209a188d13d8dd894d\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.049999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.9372907911316823\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.020712054901862037\n",
      "          policy_loss: -0.009492516881880998\n",
      "          total_loss: 9991.28376588983\n",
      "          vf_explained_var: -4.040992873655114e-09\n",
      "          vf_loss: 9991.29217508607\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 75000\n",
      "    num_steps_sampled: 75000\n",
      "    num_steps_trained: 75000\n",
      "  iterations_since_restore: 5\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.642990654205608\n",
      "    ram_util_percent: 36.57943925233645\n",
      "  pid: 30781\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0901199469689496\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.392544170442235\n",
      "    mean_inference_ms: 1.985644458039956\n",
      "    mean_raw_obs_processing_ms: 1.0869823007337218\n",
      "  time_since_restore: 754.9290769100189\n",
      "  time_this_iter_s: 150.12554574012756\n",
      "  time_total_s: 754.9290769100189\n",
      "  timers:\n",
      "    learn_throughput: 339.051\n",
      "    learn_time_ms: 44241.115\n",
      "    sample_throughput: 140.538\n",
      "    sample_time_ms: 106732.716\n",
      "    update_time_ms: 2.336\n",
      "  timestamp: 1666672264\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 75000\n",
      "  training_iteration: 5\n",
      "  trial_id: '12173_00000'\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 11.5/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.38 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 accelerator_type:G, 0.0/6.0 CPU_group_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_4_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_2_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_1_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_5_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_0_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_3_edf1e7152919a3617c29d944024e5221)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_figure_eight\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------+----------+----------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                  | status   | loc                  |   iter |   total time (s) |    ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------+----------+----------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_AccelEnv-v0_12173_00000 | RUNNING  | 141.225.10.229:30781 |      5 |          754.929 | 75000 | -6854.71 |             -4209.62 |             -8093.73 |             1486.5 |\n",
      "+-----------------------------+----------+----------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=30781)\u001b[0m 2022-10-24 23:31:04,938\tWARNING ppo.py:242 -- The magnitude of your environment rewards are more than 685.0x the scale of `vf_clip_param`. This means that it will take more than 685.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "Result for PPO_AccelEnv-v0_12173_00000:\n",
      "  agent_timesteps_total: 90000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-24_23-33-35\n",
      "  done: false\n",
      "  episode_len_mean: 1488.75\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -4209.6202350466865\n",
      "  episode_reward_mean: -6562.475368116207\n",
      "  episode_reward_min: -8093.733712774245\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 60\n",
      "  experiment_id: 3b224e0d4aa944209a188d13d8dd894d\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.049999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.8075576233156657\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016746211736119104\n",
      "          policy_loss: -0.008528546476342037\n",
      "          total_loss: 7692.083842856197\n",
      "          vf_explained_var: -8.233523196565784e-09\n",
      "          vf_loss: 7692.09151632018\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 90000\n",
      "    num_steps_sampled: 90000\n",
      "    num_steps_trained: 90000\n",
      "  iterations_since_restore: 6\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.55720930232558\n",
      "    ram_util_percent: 36.55255813953488\n",
      "  pid: 30781\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09010609172642964\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.374711551230483\n",
      "    mean_inference_ms: 1.9838280030538142\n",
      "    mean_raw_obs_processing_ms: 1.0866111889499661\n",
      "  time_since_restore: 905.4877109527588\n",
      "  time_this_iter_s: 150.55863404273987\n",
      "  time_total_s: 905.4877109527588\n",
      "  timers:\n",
      "    learn_throughput: 339.39\n",
      "    learn_time_ms: 44196.937\n",
      "    sample_throughput: 140.573\n",
      "    sample_time_ms: 106706.197\n",
      "    update_time_ms: 2.283\n",
      "  timestamp: 1666672415\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 90000\n",
      "  training_iteration: 6\n",
      "  trial_id: '12173_00000'\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 11.5/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.38 GiB heap, 0.0/14.65 GiB objects (0.0/6.0 CPU_group_edf1e7152919a3617c29d944024e5221, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_5_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_4_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_0_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_2_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_1_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_3_edf1e7152919a3617c29d944024e5221)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_figure_eight\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------+----------+----------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                  | status   | loc                  |   iter |   total time (s) |    ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------+----------+----------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_AccelEnv-v0_12173_00000 | RUNNING  | 141.225.10.229:30781 |      6 |          905.488 | 90000 | -6562.48 |             -4209.62 |             -8093.73 |            1488.75 |\n",
      "+-----------------------------+----------+----------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=30781)\u001b[0m 2022-10-24 23:33:35,534\tWARNING ppo.py:242 -- The magnitude of your environment rewards are more than 656.0x the scale of `vf_clip_param`. This means that it will take more than 656.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "Result for PPO_AccelEnv-v0_12173_00000:\n",
      "  agent_timesteps_total: 105000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-24_23-36-06\n",
      "  done: false\n",
      "  episode_len_mean: 1490.357142857143\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3747.657517654553\n",
      "  episode_reward_mean: -6205.701250823337\n",
      "  episode_reward_min: -8093.733712774245\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 70\n",
      "  experiment_id: 3b224e0d4aa944209a188d13d8dd894d\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.049999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.636301727214102\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.020918518059376688\n",
      "          policy_loss: -0.009971933570785164\n",
      "          total_loss: 4545.78043854277\n",
      "          vf_explained_var: -2.9802322831784522e-09\n",
      "          vf_loss: 4545.789378020723\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 105000\n",
      "    num_steps_sampled: 105000\n",
      "    num_steps_trained: 105000\n",
      "  iterations_since_restore: 7\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.847906976744184\n",
      "    ram_util_percent: 36.574883720930224\n",
      "  pid: 30781\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09010161081826278\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.359570668327244\n",
      "    mean_inference_ms: 1.9837522922547481\n",
      "    mean_raw_obs_processing_ms: 1.086864875064487\n",
      "  time_since_restore: 1056.3776695728302\n",
      "  time_this_iter_s: 150.8899586200714\n",
      "  time_total_s: 1056.3776695728302\n",
      "  timers:\n",
      "    learn_throughput: 339.365\n",
      "    learn_time_ms: 44200.238\n",
      "    sample_throughput: 140.583\n",
      "    sample_time_ms: 106698.84\n",
      "    update_time_ms: 2.303\n",
      "  timestamp: 1666672566\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 105000\n",
      "  training_iteration: 7\n",
      "  trial_id: '12173_00000'\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 11.5/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.38 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_3_edf1e7152919a3617c29d944024e5221, 0.0/6.0 CPU_group_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_0_edf1e7152919a3617c29d944024e5221, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_1_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_5_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_2_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_4_edf1e7152919a3617c29d944024e5221)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_figure_eight\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------+----------+----------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                  | status   | loc                  |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------+----------+----------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_AccelEnv-v0_12173_00000 | RUNNING  | 141.225.10.229:30781 |      7 |          1056.38 | 105000 |  -6205.7 |             -3747.66 |             -8093.73 |            1490.36 |\n",
      "+-----------------------------+----------+----------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=30781)\u001b[0m 2022-10-24 23:36:06,461\tWARNING ppo.py:242 -- The magnitude of your environment rewards are more than 621.0x the scale of `vf_clip_param`. This means that it will take more than 621.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "Result for PPO_AccelEnv-v0_12173_00000:\n",
      "  agent_timesteps_total: 120000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-24_23-38-37\n",
      "  done: false\n",
      "  episode_len_mean: 1491.5625\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3316.8225463389376\n",
      "  episode_reward_mean: -5984.570038272177\n",
      "  episode_reward_min: -8093.733712774245\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 80\n",
      "  experiment_id: 3b224e0d4aa944209a188d13d8dd894d\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.049999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.5845918130066435\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.02049232311753829\n",
      "          policy_loss: -0.007533223078965004\n",
      "          total_loss: 5756.900116277146\n",
      "          vf_explained_var: -7.071737639918751e-10\n",
      "          vf_loss: 5756.90661041777\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 120000\n",
      "    num_steps_sampled: 120000\n",
      "    num_steps_trained: 120000\n",
      "  iterations_since_restore: 8\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.73333333333333\n",
      "    ram_util_percent: 36.6199074074074\n",
      "  pid: 30781\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09010699111353568\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.346592141247804\n",
      "    mean_inference_ms: 1.9842500149464244\n",
      "    mean_raw_obs_processing_ms: 1.0872394368107579\n",
      "  time_since_restore: 1207.5893349647522\n",
      "  time_this_iter_s: 151.211665391922\n",
      "  time_total_s: 1207.5893349647522\n",
      "  timers:\n",
      "    learn_throughput: 338.891\n",
      "    learn_time_ms: 44261.979\n",
      "    sample_throughput: 140.614\n",
      "    sample_time_ms: 106675.052\n",
      "    update_time_ms: 2.271\n",
      "  timestamp: 1666672717\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 120000\n",
      "  training_iteration: 8\n",
      "  trial_id: '12173_00000'\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 11.5/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.38 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_0_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_3_edf1e7152919a3617c29d944024e5221, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_2_edf1e7152919a3617c29d944024e5221, 0.0/6.0 CPU_group_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_1_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_4_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_5_edf1e7152919a3617c29d944024e5221)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_figure_eight\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------+----------+----------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                  | status   | loc                  |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------+----------+----------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_AccelEnv-v0_12173_00000 | RUNNING  | 141.225.10.229:30781 |      8 |          1207.59 | 120000 | -5984.57 |             -3316.82 |             -8093.73 |            1491.56 |\n",
      "+-----------------------------+----------+----------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=30781)\u001b[0m 2022-10-24 23:38:37,728\tWARNING ppo.py:242 -- The magnitude of your environment rewards are more than 598.0x the scale of `vf_clip_param`. This means that it will take more than 598.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "Result for PPO_AccelEnv-v0_12173_00000:\n",
      "  agent_timesteps_total: 135000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-24_23-41-08\n",
      "  done: false\n",
      "  episode_len_mean: 1492.5\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2911.7950258308424\n",
      "  episode_reward_mean: -5698.973862593275\n",
      "  episode_reward_min: -8093.733712774245\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 90\n",
      "  experiment_id: 3b224e0d4aa944209a188d13d8dd894d\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.049999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.4110069289045819\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.032320867150533804\n",
      "          policy_loss: -0.011280837968796871\n",
      "          total_loss: 3351.2162456965043\n",
      "          vf_explained_var: 3.535868819959376e-10\n",
      "          vf_loss: 3351.2259144928494\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 135000\n",
      "    num_steps_sampled: 135000\n",
      "    num_steps_trained: 135000\n",
      "  iterations_since_restore: 9\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.666976744186048\n",
      "    ram_util_percent: 35.51720930232558\n",
      "  pid: 30781\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09009996382036238\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.337694013175255\n",
      "    mean_inference_ms: 1.9838753609283941\n",
      "    mean_raw_obs_processing_ms: 1.0872350435768252\n",
      "  time_since_restore: 1358.6735680103302\n",
      "  time_this_iter_s: 151.084233045578\n",
      "  time_total_s: 1358.6735680103302\n",
      "  timers:\n",
      "    learn_throughput: 338.742\n",
      "    learn_time_ms: 44281.446\n",
      "    sample_throughput: 140.62\n",
      "    sample_time_ms: 106670.5\n",
      "    update_time_ms: 2.338\n",
      "  timestamp: 1666672868\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 135000\n",
      "  training_iteration: 9\n",
      "  trial_id: '12173_00000'\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 10.7/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.38 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_1_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_3_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_0_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_4_edf1e7152919a3617c29d944024e5221, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_2_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_5_edf1e7152919a3617c29d944024e5221, 0.0/6.0 CPU_group_edf1e7152919a3617c29d944024e5221)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_figure_eight\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------+----------+----------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                  | status   | loc                  |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------+----------+----------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_AccelEnv-v0_12173_00000 | RUNNING  | 141.225.10.229:30781 |      9 |          1358.67 | 135000 | -5698.97 |              -2911.8 |             -8093.73 |             1492.5 |\n",
      "+-----------------------------+----------+----------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=30781)\u001b[0m 2022-10-24 23:41:08,857\tWARNING ppo.py:242 -- The magnitude of your environment rewards are more than 570.0x the scale of `vf_clip_param`. This means that it will take more than 570.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "Result for PPO_AccelEnv-v0_12173_00000:\n",
      "  agent_timesteps_total: 150000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-24_23-43-39\n",
      "  done: false\n",
      "  episode_len_mean: 1493.25\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2197.84137846731\n",
      "  episode_reward_mean: -5398.040882951039\n",
      "  episode_reward_min: -8093.733712774245\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 100\n",
      "  experiment_id: 3b224e0d4aa944209a188d13d8dd894d\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.049999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.25966891038973455\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.027033856245296185\n",
      "          policy_loss: -0.006957960614176878\n",
      "          total_loss: 2213.582342529297\n",
      "          vf_explained_var: -1.4143475279837503e-09\n",
      "          vf_loss: 2213.5879507291115\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 150000\n",
      "    num_steps_sampled: 150000\n",
      "    num_steps_trained: 150000\n",
      "  iterations_since_restore: 10\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 17.71481481481481\n",
      "    ram_util_percent: 34.0824074074074\n",
      "  pid: 30781\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09003832730811148\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.332772406558824\n",
      "    mean_inference_ms: 1.9810376815337005\n",
      "    mean_raw_obs_processing_ms: 1.0870284347291095\n",
      "  time_since_restore: 1509.482537984848\n",
      "  time_this_iter_s: 150.80896997451782\n",
      "  time_total_s: 1509.482537984848\n",
      "  timers:\n",
      "    learn_throughput: 338.604\n",
      "    learn_time_ms: 44299.545\n",
      "    sample_throughput: 140.664\n",
      "    sample_time_ms: 106636.807\n",
      "    update_time_ms: 2.327\n",
      "  timestamp: 1666673019\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 150000\n",
      "  training_iteration: 10\n",
      "  trial_id: '12173_00000'\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 10.7/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.38 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_2_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_0_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_3_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_1_edf1e7152919a3617c29d944024e5221, 0.0/6.0 CPU_group_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_4_edf1e7152919a3617c29d944024e5221, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_5_edf1e7152919a3617c29d944024e5221)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_figure_eight\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------+----------+----------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                  | status   | loc                  |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------+----------+----------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_AccelEnv-v0_12173_00000 | RUNNING  | 141.225.10.229:30781 |     10 |          1509.48 | 150000 | -5398.04 |             -2197.84 |             -8093.73 |            1493.25 |\n",
      "+-----------------------------+----------+----------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=30781)\u001b[0m 2022-10-24 23:43:39,724\tWARNING ppo.py:242 -- The magnitude of your environment rewards are more than 540.0x the scale of `vf_clip_param`. This means that it will take more than 540.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "Result for PPO_AccelEnv-v0_12173_00000:\n",
      "  agent_timesteps_total: 165000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-24_23-46-10\n",
      "  done: false\n",
      "  episode_len_mean: 1500.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -1559.6987057446681\n",
      "  episode_reward_mean: -4932.180165022646\n",
      "  episode_reward_min: -7988.752089203307\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 110\n",
      "  experiment_id: 3b224e0d4aa944209a188d13d8dd894d\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.049999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.18775100236348177\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.018752088047496472\n",
      "          policy_loss: -0.004598197852008802\n",
      "          total_loss: 2217.2839002706237\n",
      "          vf_explained_var: 0.0\n",
      "          vf_loss: 2217.2875611386057\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 165000\n",
      "    num_steps_sampled: 165000\n",
      "    num_steps_trained: 165000\n",
      "  iterations_since_restore: 11\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 17.76186046511628\n",
      "    ram_util_percent: 34.13116279069768\n",
      "  pid: 30781\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08993875974550766\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.30320809957907\n",
      "    mean_inference_ms: 1.9751328994648134\n",
      "    mean_raw_obs_processing_ms: 1.0865069423325107\n",
      "  time_since_restore: 1660.2992403507233\n",
      "  time_this_iter_s: 150.81670236587524\n",
      "  time_total_s: 1660.2992403507233\n",
      "  timers:\n",
      "    learn_throughput: 338.934\n",
      "    learn_time_ms: 44256.377\n",
      "    sample_throughput: 140.864\n",
      "    sample_time_ms: 106485.608\n",
      "    update_time_ms: 2.361\n",
      "  timestamp: 1666673170\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 165000\n",
      "  training_iteration: 11\n",
      "  trial_id: '12173_00000'\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 10.7/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.38 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_2_edf1e7152919a3617c29d944024e5221, 0.0/6.0 CPU_group_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_0_edf1e7152919a3617c29d944024e5221, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_5_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_4_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_3_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_1_edf1e7152919a3617c29d944024e5221)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_figure_eight\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------+----------+----------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                  | status   | loc                  |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------+----------+----------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_AccelEnv-v0_12173_00000 | RUNNING  | 141.225.10.229:30781 |     11 |           1660.3 | 165000 | -4932.18 |              -1559.7 |             -7988.75 |               1500 |\n",
      "+-----------------------------+----------+----------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=30781)\u001b[0m 2022-10-24 23:46:10,597\tWARNING ppo.py:242 -- The magnitude of your environment rewards are more than 493.0x the scale of `vf_clip_param`. This means that it will take more than 493.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "Result for PPO_AccelEnv-v0_12173_00000:\n",
      "  agent_timesteps_total: 180000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-24_23-48-41\n",
      "  done: false\n",
      "  episode_len_mean: 1500.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -1020.6972394599281\n",
      "  episode_reward_mean: -4389.78229318674\n",
      "  episode_reward_min: -7405.800479162893\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 120\n",
      "  experiment_id: 3b224e0d4aa944209a188d13d8dd894d\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.049999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -0.00029297324520993534\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.026185687943215897\n",
      "          policy_loss: -0.0055881299944673425\n",
      "          total_loss: 1568.3942974543168\n",
      "          vf_explained_var: -9.092234187768611e-10\n",
      "          vf_loss: 1568.3985742601299\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 180000\n",
      "    num_steps_sampled: 180000\n",
      "    num_steps_trained: 180000\n",
      "  iterations_since_restore: 12\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 17.77627906976744\n",
      "    ram_util_percent: 34.096279069767434\n",
      "  pid: 30781\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08977880805891658\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.295523449631848\n",
      "    mean_inference_ms: 1.967279995995436\n",
      "    mean_raw_obs_processing_ms: 1.085412350248612\n",
      "  time_since_restore: 1811.0067007541656\n",
      "  time_this_iter_s: 150.70746040344238\n",
      "  time_total_s: 1811.0067007541656\n",
      "  timers:\n",
      "    learn_throughput: 338.332\n",
      "    learn_time_ms: 44335.124\n",
      "    sample_throughput: 140.909\n",
      "    sample_time_ms: 106451.462\n",
      "    update_time_ms: 2.4\n",
      "  timestamp: 1666673321\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 180000\n",
      "  training_iteration: 12\n",
      "  trial_id: '12173_00000'\n",
      "  \n",
      "\u001b[2m\u001b[36m(pid=30781)\u001b[0m 2022-10-24 23:48:41,353\tWARNING ppo.py:242 -- The magnitude of your environment rewards are more than 439.0x the scale of `vf_clip_param`. This means that it will take more than 439.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "== Status ==\n",
      "Memory usage on this node: 10.7/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.38 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_0_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_1_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_3_edf1e7152919a3617c29d944024e5221, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_4_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_2_edf1e7152919a3617c29d944024e5221, 0.0/6.0 CPU_group_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_5_edf1e7152919a3617c29d944024e5221)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_figure_eight\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------+----------+----------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                  | status   | loc                  |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------+----------+----------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_AccelEnv-v0_12173_00000 | RUNNING  | 141.225.10.229:30781 |     12 |          1811.01 | 180000 | -4389.78 |              -1020.7 |              -7405.8 |               1500 |\n",
      "+-----------------------------+----------+----------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "Result for PPO_AccelEnv-v0_12173_00000:\n",
      "  agent_timesteps_total: 195000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-24_23-51-12\n",
      "  done: false\n",
      "  episode_len_mean: 1500.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -1020.6972394599281\n",
      "  episode_reward_mean: -3859.643319195622\n",
      "  episode_reward_min: -6896.097454842869\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 130\n",
      "  experiment_id: 3b224e0d4aa944209a188d13d8dd894d\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.049999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -0.10998437839488191\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.026320924249961627\n",
      "          policy_loss: -0.005825091640354466\n",
      "          total_loss: 1368.1459362482622\n",
      "          vf_explained_var: 5.051241022679953e-11\n",
      "          vf_loss: 1368.150443539377\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 195000\n",
      "    num_steps_sampled: 195000\n",
      "    num_steps_trained: 195000\n",
      "  iterations_since_restore: 13\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 17.690277777777776\n",
      "    ram_util_percent: 34.138888888888886\n",
      "  pid: 30781\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08957111503622635\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.294599996482447\n",
      "    mean_inference_ms: 1.9586167908915855\n",
      "    mean_raw_obs_processing_ms: 1.0846024318384047\n",
      "  time_since_restore: 1961.9987511634827\n",
      "  time_this_iter_s: 150.99205040931702\n",
      "  time_total_s: 1961.9987511634827\n",
      "  timers:\n",
      "    learn_throughput: 337.831\n",
      "    learn_time_ms: 44400.87\n",
      "    sample_throughput: 140.918\n",
      "    sample_time_ms: 106445.236\n",
      "    update_time_ms: 2.391\n",
      "  timestamp: 1666673472\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 195000\n",
      "  training_iteration: 13\n",
      "  trial_id: '12173_00000'\n",
      "  \n",
      "\u001b[2m\u001b[36m(pid=30781)\u001b[0m 2022-10-24 23:51:12,395\tWARNING ppo.py:242 -- The magnitude of your environment rewards are more than 386.0x the scale of `vf_clip_param`. This means that it will take more than 386.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "== Status ==\n",
      "Memory usage on this node: 10.7/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.38 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_0_edf1e7152919a3617c29d944024e5221, 0.0/6.0 CPU_group_edf1e7152919a3617c29d944024e5221, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_1_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_4_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_5_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_3_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_2_edf1e7152919a3617c29d944024e5221)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_figure_eight\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------+----------+----------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                  | status   | loc                  |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------+----------+----------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_AccelEnv-v0_12173_00000 | RUNNING  | 141.225.10.229:30781 |     13 |             1962 | 195000 | -3859.64 |              -1020.7 |              -6896.1 |               1500 |\n",
      "+-----------------------------+----------+----------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "Result for PPO_AccelEnv-v0_12173_00000:\n",
      "  agent_timesteps_total: 210000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-24_23-53-43\n",
      "  done: false\n",
      "  episode_len_mean: 1500.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -781.1083635643039\n",
      "  episode_reward_mean: -3322.212865321993\n",
      "  episode_reward_min: -6428.063109202697\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 140\n",
      "  experiment_id: 3b224e0d4aa944209a188d13d8dd894d\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.049999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -0.39145965896925683\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.019140496589065872\n",
      "          policy_loss: -0.0014228509786396713\n",
      "          total_loss: 902.1349283509335\n",
      "          vf_explained_var: -1.5153722721095164e-09\n",
      "          vf_loss: 902.1353930974411\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 210000\n",
      "    num_steps_sampled: 210000\n",
      "    num_steps_trained: 210000\n",
      "  iterations_since_restore: 14\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 17.55720930232558\n",
      "    ram_util_percent: 34.10511627906976\n",
      "  pid: 30781\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08932928204646576\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.297966951784037\n",
      "    mean_inference_ms: 1.9485728717854363\n",
      "    mean_raw_obs_processing_ms: 1.0837749849361515\n",
      "  time_since_restore: 2112.769010066986\n",
      "  time_this_iter_s: 150.77025890350342\n",
      "  time_total_s: 2112.769010066986\n",
      "  timers:\n",
      "    learn_throughput: 338.167\n",
      "    learn_time_ms: 44356.819\n",
      "    sample_throughput: 140.941\n",
      "    sample_time_ms: 106427.712\n",
      "    update_time_ms: 2.373\n",
      "  timestamp: 1666673623\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 210000\n",
      "  training_iteration: 14\n",
      "  trial_id: '12173_00000'\n",
      "  \n",
      "\u001b[2m\u001b[36m(pid=30781)\u001b[0m 2022-10-24 23:53:43,221\tWARNING ppo.py:242 -- The magnitude of your environment rewards are more than 332.0x the scale of `vf_clip_param`. This means that it will take more than 332.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Memory usage on this node: 10.7/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.38 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_5_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_2_edf1e7152919a3617c29d944024e5221, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_0_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_3_edf1e7152919a3617c29d944024e5221, 0.0/6.0 CPU_group_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_1_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_4_edf1e7152919a3617c29d944024e5221)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_figure_eight\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------+----------+----------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                  | status   | loc                  |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------+----------+----------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_AccelEnv-v0_12173_00000 | RUNNING  | 141.225.10.229:30781 |     14 |          2112.77 | 210000 | -3322.21 |             -781.108 |             -6428.06 |               1500 |\n",
      "+-----------------------------+----------+----------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "Result for PPO_AccelEnv-v0_12173_00000:\n",
      "  agent_timesteps_total: 225000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-24_23-56-12\n",
      "  done: false\n",
      "  episode_len_mean: 1500.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -403.6375842949057\n",
      "  episode_reward_mean: -2834.3328955958677\n",
      "  episode_reward_min: -5703.3315436595285\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 150\n",
      "  experiment_id: 3b224e0d4aa944209a188d13d8dd894d\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.049999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -0.5094599260869673\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.024121039041808096\n",
      "          policy_loss: -0.0016920809551651227\n",
      "          total_loss: 863.1202728271485\n",
      "          vf_explained_var: -3.7379184192332104e-09\n",
      "          vf_loss: 863.1207576687053\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 225000\n",
      "    num_steps_sampled: 225000\n",
      "    num_steps_trained: 225000\n",
      "  iterations_since_restore: 15\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 17.901408450704224\n",
      "    ram_util_percent: 34.15023474178404\n",
      "  pid: 30781\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08906949979555759\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.304149675016014\n",
      "    mean_inference_ms: 1.9379601869569887\n",
      "    mean_raw_obs_processing_ms: 1.0831424248006858\n",
      "  time_since_restore: 2262.3432273864746\n",
      "  time_this_iter_s: 149.57421731948853\n",
      "  time_total_s: 2262.3432273864746\n",
      "  timers:\n",
      "    learn_throughput: 338.573\n",
      "    learn_time_ms: 44303.537\n",
      "    sample_throughput: 140.944\n",
      "    sample_time_ms: 106425.207\n",
      "    update_time_ms: 2.44\n",
      "  timestamp: 1666673772\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 225000\n",
      "  training_iteration: 15\n",
      "  trial_id: '12173_00000'\n",
      "  \n",
      "\u001b[2m\u001b[36m(pid=30781)\u001b[0m 2022-10-24 23:56:12,852\tWARNING ppo.py:242 -- The magnitude of your environment rewards are more than 283.0x the scale of `vf_clip_param`. This means that it will take more than 283.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "== Status ==\n",
      "Memory usage on this node: 10.7/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.38 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_3_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_0_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_4_edf1e7152919a3617c29d944024e5221, 0.0/6.0 CPU_group_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_1_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_5_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_2_edf1e7152919a3617c29d944024e5221)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_figure_eight\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------+----------+----------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                  | status   | loc                  |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------+----------+----------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_AccelEnv-v0_12173_00000 | RUNNING  | 141.225.10.229:30781 |     15 |          2262.34 | 225000 | -2834.33 |             -403.638 |             -5703.33 |               1500 |\n",
      "+-----------------------------+----------+----------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_AccelEnv-v0_12173_00000:\n",
      "\u001b[2m\u001b[36m(pid=30781)\u001b[0m 2022-10-24 23:58:51,808\tWARNING ppo.py:242 -- The magnitude of your environment rewards are more than 239.0x the scale of `vf_clip_param`. This means that it will take more than 239.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
      "  agent_timesteps_total: 240000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-24_23-58-51\n",
      "  done: false\n",
      "  episode_len_mean: 1500.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 99.92118533541594\n",
      "  episode_reward_mean: -2392.0505098600825\n",
      "  episode_reward_min: -4926.541632354834\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 160\n",
      "  experiment_id: 3b224e0d4aa944209a188d13d8dd894d\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.049999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -0.5904300971051394\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.02211544497113214\n",
      "          policy_loss: -0.002308849799762464\n",
      "          total_loss: 911.268795621193\n",
      "          vf_explained_var: -2.2730584081642746e-09\n",
      "          vf_loss: 911.2699970310017\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 240000\n",
      "    num_steps_sampled: 240000\n",
      "    num_steps_trained: 240000\n",
      "  iterations_since_restore: 16\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 17.1136563876652\n",
      "    ram_util_percent: 34.21850220264317\n",
      "  pid: 30781\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08879247536529217\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.312107589482306\n",
      "    mean_inference_ms: 1.9265520298563612\n",
      "    mean_raw_obs_processing_ms: 1.0822056371506232\n",
      "  time_since_restore: 2421.255918741226\n",
      "  time_this_iter_s: 158.9126913547516\n",
      "  time_total_s: 2421.255918741226\n",
      "  timers:\n",
      "    learn_throughput: 332.093\n",
      "    learn_time_ms: 45168.11\n",
      "    sample_throughput: 140.983\n",
      "    sample_time_ms: 106395.642\n",
      "    update_time_ms: 2.486\n",
      "  timestamp: 1666673931\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 240000\n",
      "  training_iteration: 16\n",
      "  trial_id: '12173_00000'\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 10.7/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.38 GiB heap, 0.0/14.65 GiB objects (0.0/6.0 CPU_group_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_0_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_5_edf1e7152919a3617c29d944024e5221, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_1_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_2_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_3_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_4_edf1e7152919a3617c29d944024e5221)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_figure_eight\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------+----------+----------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                  | status   | loc                  |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------+----------+----------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_AccelEnv-v0_12173_00000 | RUNNING  | 141.225.10.229:30781 |     16 |          2421.26 | 240000 | -2392.05 |              99.9212 |             -4926.54 |               1500 |\n",
      "+-----------------------------+----------+----------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "Result for PPO_AccelEnv-v0_12173_00000:\n",
      "  agent_timesteps_total: 255000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-25_00-01-35\n",
      "  done: false\n",
      "  episode_len_mean: 1500.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 208.34647352475827\n",
      "  episode_reward_mean: -1985.0918184128875\n",
      "  episode_reward_min: -4926.541632354834\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 170\n",
      "  experiment_id: 3b224e0d4aa944209a188d13d8dd894d\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.049999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -0.7971285211837897\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.022769835677493234\n",
      "          policy_loss: -0.0017258432737188572\n",
      "          total_loss: 734.1423463983051\n",
      "          vf_explained_var: -1.3133226728356817e-09\n",
      "          vf_loss: 734.1429323616675\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 255000\n",
      "    num_steps_sampled: 255000\n",
      "    num_steps_trained: 255000\n",
      "  iterations_since_restore: 17\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 16.833760683760687\n",
      "    ram_util_percent: 34.23974358974359\n",
      "  pid: 30781\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08849288920856786\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.322423194154634\n",
      "    mean_inference_ms: 1.9133784422289697\n",
      "    mean_raw_obs_processing_ms: 1.08089893177658\n",
      "  time_since_restore: 2585.251348733902\n",
      "  time_this_iter_s: 163.99542999267578\n",
      "  time_total_s: 2585.251348733902\n",
      "  timers:\n",
      "    learn_throughput: 322.506\n",
      "    learn_time_ms: 46510.766\n",
      "    sample_throughput: 141.025\n",
      "    sample_time_ms: 106363.987\n",
      "    update_time_ms: 2.571\n",
      "  timestamp: 1666674095\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 255000\n",
      "  training_iteration: 17\n",
      "  trial_id: '12173_00000'\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 10.7/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.38 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_4_edf1e7152919a3617c29d944024e5221, 0.0/6.0 CPU_group_edf1e7152919a3617c29d944024e5221, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_1_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_2_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_0_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_3_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_5_edf1e7152919a3617c29d944024e5221)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_figure_eight\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------+----------+----------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                  | status   | loc                  |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------+----------+----------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_AccelEnv-v0_12173_00000 | RUNNING  | 141.225.10.229:30781 |     17 |          2585.25 | 255000 | -1985.09 |              208.346 |             -4926.54 |               1500 |\n",
      "+-----------------------------+----------+----------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "Result for PPO_AccelEnv-v0_12173_00000:\n",
      "  agent_timesteps_total: 270000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-25_00-04-20\n",
      "  done: false\n",
      "  episode_len_mean: 1500.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 208.34647352475827\n",
      "  episode_reward_mean: -1628.1537694090448\n",
      "  episode_reward_min: -4352.2469404771\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 180\n",
      "  experiment_id: 3b224e0d4aa944209a188d13d8dd894d\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.049999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -0.8082517594604169\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01716574168171135\n",
      "          policy_loss: -0.00018114581767280223\n",
      "          total_loss: 793.1929899829929\n",
      "          vf_explained_var: -2.020496436827557e-09\n",
      "          vf_loss: 793.1923145100221\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 270000\n",
      "    num_steps_sampled: 270000\n",
      "    num_steps_trained: 270000\n",
      "  iterations_since_restore: 18\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 16.870212765957444\n",
      "    ram_util_percent: 34.29829787234042\n",
      "  pid: 30781\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08817173288687968\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.334728793130072\n",
      "    mean_inference_ms: 1.8990267885529386\n",
      "    mean_raw_obs_processing_ms: 1.0794071097210136\n",
      "  time_since_restore: 2749.8070249557495\n",
      "  time_this_iter_s: 164.55567622184753\n",
      "  time_total_s: 2749.8070249557495\n",
      "  timers:\n",
      "    learn_throughput: 313.496\n",
      "    learn_time_ms: 47847.503\n",
      "    sample_throughput: 141.029\n",
      "    sample_time_ms: 106361.158\n",
      "    update_time_ms: 2.616\n",
      "  timestamp: 1666674260\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 270000\n",
      "  training_iteration: 18\n",
      "  trial_id: '12173_00000'\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 10.7/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.38 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_4_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_5_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_2_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_0_edf1e7152919a3617c29d944024e5221, 0.0/1.0 accelerator_type:G, 0.0/6.0 CPU_group_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_1_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_3_edf1e7152919a3617c29d944024e5221)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_figure_eight\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------+----------+----------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                  | status   | loc                  |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------+----------+----------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_AccelEnv-v0_12173_00000 | RUNNING  | 141.225.10.229:30781 |     18 |          2749.81 | 270000 | -1628.15 |              208.346 |             -4352.25 |               1500 |\n",
      "+-----------------------------+----------+----------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "Result for PPO_AccelEnv-v0_12173_00000:\n",
      "  agent_timesteps_total: 285000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-25_00-07-05\n",
      "  done: false\n",
      "  episode_len_mean: 1500.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 208.34647352475827\n",
      "  episode_reward_mean: -1363.768179088735\n",
      "  episode_reward_min: -3707.3673979661053\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 190\n",
      "  experiment_id: 3b224e0d4aa944209a188d13d8dd894d\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.049999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -1.0637041946588937\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.02013020974821203\n",
      "          policy_loss: -0.00037065632522926997\n",
      "          total_loss: 559.8764758093882\n",
      "          vf_explained_var: -2.576132862586178e-09\n",
      "          vf_loss: 559.8758405006538\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 285000\n",
      "    num_steps_sampled: 285000\n",
      "    num_steps_trained: 285000\n",
      "  iterations_since_restore: 19\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 16.840254237288136\n",
      "    ram_util_percent: 34.332627118644076\n",
      "  pid: 30781\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08784455795902217\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.346434874614314\n",
      "    mean_inference_ms: 1.8847311595326088\n",
      "    mean_raw_obs_processing_ms: 1.0781404624719422\n",
      "  time_since_restore: 2915.074118614197\n",
      "  time_this_iter_s: 165.26709365844727\n",
      "  time_total_s: 2915.074118614197\n",
      "  timers:\n",
      "    learn_throughput: 304.284\n",
      "    learn_time_ms: 49296.065\n",
      "    sample_throughput: 141.069\n",
      "    sample_time_ms: 106330.631\n",
      "    update_time_ms: 2.595\n",
      "  timestamp: 1666674425\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 285000\n",
      "  training_iteration: 19\n",
      "  trial_id: '12173_00000'\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Memory usage on this node: 10.8/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.38 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_4_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_3_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_0_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_1_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_5_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_2_edf1e7152919a3617c29d944024e5221, 0.0/6.0 CPU_group_edf1e7152919a3617c29d944024e5221, 0.0/1.0 accelerator_type:G)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_figure_eight\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------+----------+----------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                  | status   | loc                  |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------+----------+----------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_AccelEnv-v0_12173_00000 | RUNNING  | 141.225.10.229:30781 |     19 |          2915.07 | 285000 | -1363.77 |              208.346 |             -3707.37 |               1500 |\n",
      "+-----------------------------+----------+----------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "Result for PPO_AccelEnv-v0_12173_00000:\n",
      "  agent_timesteps_total: 300000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-25_00-09-54\n",
      "  done: false\n",
      "  episode_len_mean: 1500.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 208.34647352475827\n",
      "  episode_reward_mean: -1124.7097625286651\n",
      "  episode_reward_min: -3707.3673979661053\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 200\n",
      "  experiment_id: 3b224e0d4aa944209a188d13d8dd894d\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.049999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -1.5768084272489709\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014785388786487904\n",
      "          policy_loss: 0.0011561002625720734\n",
      "          total_loss: 389.0451728303554\n",
      "          vf_explained_var: -0.004972904920578003\n",
      "          vf_loss: 389.0432761046846\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 300000\n",
      "    num_steps_sampled: 300000\n",
      "    num_steps_trained: 300000\n",
      "  iterations_since_restore: 20\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 16.514522821576765\n",
      "    ram_util_percent: 34.44688796680498\n",
      "  pid: 30781\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08756112977056141\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.356486453531197\n",
      "    mean_inference_ms: 1.8723921697543302\n",
      "    mean_raw_obs_processing_ms: 1.0771147745186929\n",
      "  time_since_restore: 3084.122344017029\n",
      "  time_this_iter_s: 169.04822540283203\n",
      "  time_total_s: 3084.122344017029\n",
      "  timers:\n",
      "    learn_throughput: 293.498\n",
      "    learn_time_ms: 51107.737\n",
      "    sample_throughput: 141.053\n",
      "    sample_time_ms: 106343.196\n",
      "    update_time_ms: 2.585\n",
      "  timestamp: 1666674594\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 300000\n",
      "  training_iteration: 20\n",
      "  trial_id: '12173_00000'\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 10.8/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.38 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_1_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_5_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_3_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_4_edf1e7152919a3617c29d944024e5221, 0.0/6.0 CPU_group_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_0_edf1e7152919a3617c29d944024e5221, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_2_edf1e7152919a3617c29d944024e5221)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_figure_eight\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------+----------+----------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                  | status   | loc                  |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------+----------+----------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_AccelEnv-v0_12173_00000 | RUNNING  | 141.225.10.229:30781 |     20 |          3084.12 | 300000 | -1124.71 |              208.346 |             -3707.37 |               1500 |\n",
      "+-----------------------------+----------+----------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "Result for PPO_AccelEnv-v0_12173_00000:\n",
      "  agent_timesteps_total: 315000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-25_00-12-46\n",
      "  done: false\n",
      "  episode_len_mean: 1500.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 208.34647352475827\n",
      "  episode_reward_mean: -879.763144776041\n",
      "  episode_reward_min: -3011.7484130861685\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 210\n",
      "  experiment_id: 3b224e0d4aa944209a188d13d8dd894d\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.049999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -1.7661389529705047\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014922503640843786\n",
      "          policy_loss: 0.00011922227242424831\n",
      "          total_loss: 359.09250754016944\n",
      "          vf_explained_var: -0.0035712970420718193\n",
      "          vf_loss: 359.09164283235197\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 315000\n",
      "    num_steps_sampled: 315000\n",
      "    num_steps_trained: 315000\n",
      "  iterations_since_restore: 21\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 16.447755102040816\n",
      "    ram_util_percent: 34.46040816326532\n",
      "  pid: 30781\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08732571859543568\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.365181586283292\n",
      "    mean_inference_ms: 1.861599575752232\n",
      "    mean_raw_obs_processing_ms: 1.076206479922062\n",
      "  time_since_restore: 3255.4759023189545\n",
      "  time_this_iter_s: 171.35355830192566\n",
      "  time_total_s: 3255.4759023189545\n",
      "  timers:\n",
      "    learn_throughput: 282.213\n",
      "    learn_time_ms: 53151.264\n",
      "    sample_throughput: 141.039\n",
      "    sample_time_ms: 106353.185\n",
      "    update_time_ms: 2.576\n",
      "  timestamp: 1666674766\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 315000\n",
      "  training_iteration: 21\n",
      "  trial_id: '12173_00000'\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 10.8/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.38 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_0_edf1e7152919a3617c29d944024e5221, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_5_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_4_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_2_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_1_edf1e7152919a3617c29d944024e5221, 0.0/6.0 CPU_group_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_3_edf1e7152919a3617c29d944024e5221)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_figure_eight\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------+----------+----------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                  | status   | loc                  |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------+----------+----------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_AccelEnv-v0_12173_00000 | RUNNING  | 141.225.10.229:30781 |     21 |          3255.48 | 315000 | -879.763 |              208.346 |             -3011.75 |               1500 |\n",
      "+-----------------------------+----------+----------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "Result for PPO_AccelEnv-v0_12173_00000:\n",
      "  agent_timesteps_total: 330000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-25_00-15-38\n",
      "  done: false\n",
      "  episode_len_mean: 1500.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 208.34647352475827\n",
      "  episode_reward_mean: -687.396383835323\n",
      "  episode_reward_min: -2550.020136148449\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 220\n",
      "  experiment_id: 3b224e0d4aa944209a188d13d8dd894d\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.049999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -1.8046499744310218\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013624831915157714\n",
      "          policy_loss: 0.0017528671349035733\n",
      "          total_loss: 343.81641030069125\n",
      "          vf_explained_var: -2.8286950559675006e-09\n",
      "          vf_loss: 343.81397491067145\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 330000\n",
      "    num_steps_sampled: 330000\n",
      "    num_steps_trained: 330000\n",
      "  iterations_since_restore: 22\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 16.318367346938775\n",
      "    ram_util_percent: 34.53428571428572\n",
      "  pid: 30781\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08712412231613545\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.372651516964652\n",
      "    mean_inference_ms: 1.8524543572706564\n",
      "    mean_raw_obs_processing_ms: 1.0756134595281441\n",
      "  time_since_restore: 3427.2293922901154\n",
      "  time_this_iter_s: 171.7534899711609\n",
      "  time_total_s: 3427.2293922901154\n",
      "  timers:\n",
      "    learn_throughput: 271.556\n",
      "    learn_time_ms: 55237.279\n",
      "    sample_throughput: 141.015\n",
      "    sample_time_ms: 106371.824\n",
      "    update_time_ms: 2.535\n",
      "  timestamp: 1666674938\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 330000\n",
      "  training_iteration: 22\n",
      "  trial_id: '12173_00000'\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 10.8/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.38 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_2_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_5_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_0_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_1_edf1e7152919a3617c29d944024e5221, 0.0/6.0 CPU_group_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_3_edf1e7152919a3617c29d944024e5221, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_4_edf1e7152919a3617c29d944024e5221)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_figure_eight\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------+----------+----------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                  | status   | loc                  |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------+----------+----------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_AccelEnv-v0_12173_00000 | RUNNING  | 141.225.10.229:30781 |     22 |          3427.23 | 330000 | -687.396 |              208.346 |             -2550.02 |               1500 |\n",
      "+-----------------------------+----------+----------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "Result for PPO_AccelEnv-v0_12173_00000:\n",
      "  agent_timesteps_total: 345000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-25_00-18-30\n",
      "  done: false\n",
      "  episode_len_mean: 1500.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 208.34647352475827\n",
      "  episode_reward_mean: -510.2516268826478\n",
      "  episode_reward_min: -1670.7193877238087\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 230\n",
      "  experiment_id: 3b224e0d4aa944209a188d13d8dd894d\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.049999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -1.911896765333111\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011205694591023503\n",
      "          policy_loss: 0.001692135736614592\n",
      "          total_loss: 330.94177917221845\n",
      "          vf_explained_var: -0.00292671169154346\n",
      "          vf_loss: 330.9395258434748\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 345000\n",
      "    num_steps_sampled: 345000\n",
      "    num_steps_trained: 345000\n",
      "  iterations_since_restore: 23\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 16.378775510204083\n",
      "    ram_util_percent: 34.513877551020414\n",
      "  pid: 30781\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08694045614957244\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.379207777234832\n",
      "    mean_inference_ms: 1.8444649237734425\n",
      "    mean_raw_obs_processing_ms: 1.0751940040069041\n",
      "  time_since_restore: 3599.2196986675262\n",
      "  time_this_iter_s: 171.9903063774109\n",
      "  time_total_s: 3599.2196986675262\n",
      "  timers:\n",
      "    learn_throughput: 261.7\n",
      "    learn_time_ms: 57317.521\n",
      "    sample_throughput: 140.989\n",
      "    sample_time_ms: 106391.24\n",
      "    update_time_ms: 2.657\n",
      "  timestamp: 1666675110\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 345000\n",
      "  training_iteration: 23\n",
      "  trial_id: '12173_00000'\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 10.8/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.38 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_3_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_2_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_5_edf1e7152919a3617c29d944024e5221, 0.0/1.0 accelerator_type:G, 0.0/6.0 CPU_group_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_0_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_1_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_4_edf1e7152919a3617c29d944024e5221)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_figure_eight\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------+----------+----------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                  | status   | loc                  |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------+----------+----------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_AccelEnv-v0_12173_00000 | RUNNING  | 141.225.10.229:30781 |     23 |          3599.22 | 345000 | -510.252 |              208.346 |             -1670.72 |               1500 |\n",
      "+-----------------------------+----------+----------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "Result for PPO_AccelEnv-v0_12173_00000:\n",
      "  agent_timesteps_total: 360000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-25_00-21-22\n",
      "  done: false\n",
      "  episode_len_mean: 1500.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 208.34647352475827\n",
      "  episode_reward_mean: -404.3257928656943\n",
      "  episode_reward_min: -1670.7193877238087\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 240\n",
      "  experiment_id: 3b224e0d4aa944209a188d13d8dd894d\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.049999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -1.970939219806154\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012339096703062467\n",
      "          policy_loss: 0.0021404544822871685\n",
      "          total_loss: 316.62046559220653\n",
      "          vf_explained_var: -4.243042361906646e-09\n",
      "          vf_loss: 316.61770763720494\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 360000\n",
      "    num_steps_sampled: 360000\n",
      "    num_steps_trained: 360000\n",
      "  iterations_since_restore: 24\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 16.272764227642273\n",
      "    ram_util_percent: 34.47764227642276\n",
      "  pid: 30781\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08677478466735401\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.38494026496406\n",
      "    mean_inference_ms: 1.8374905425134773\n",
      "    mean_raw_obs_processing_ms: 1.0749368078519523\n",
      "  time_since_restore: 3771.2147166728973\n",
      "  time_this_iter_s: 171.9950180053711\n",
      "  time_total_s: 3771.2147166728973\n",
      "  timers:\n",
      "    learn_throughput: 252.399\n",
      "    learn_time_ms: 59429.699\n",
      "    sample_throughput: 140.975\n",
      "    sample_time_ms: 106401.956\n",
      "    update_time_ms: 2.649\n",
      "  timestamp: 1666675282\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 360000\n",
      "  training_iteration: 24\n",
      "  trial_id: '12173_00000'\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Memory usage on this node: 10.8/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.38 GiB heap, 0.0/14.65 GiB objects (0.0/6.0 CPU_group_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_5_edf1e7152919a3617c29d944024e5221, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_2_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_3_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_4_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_1_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_0_edf1e7152919a3617c29d944024e5221)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_figure_eight\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------+----------+----------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                  | status   | loc                  |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------+----------+----------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_AccelEnv-v0_12173_00000 | RUNNING  | 141.225.10.229:30781 |     24 |          3771.21 | 360000 | -404.326 |              208.346 |             -1670.72 |               1500 |\n",
      "+-----------------------------+----------+----------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "Result for PPO_AccelEnv-v0_12173_00000:\n",
      "  agent_timesteps_total: 375000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-25_00-24-13\n",
      "  done: false\n",
      "  episode_len_mean: 1500.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 208.34647352475827\n",
      "  episode_reward_mean: -328.8099412013255\n",
      "  episode_reward_min: -1280.245971695355\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 250\n",
      "  experiment_id: 3b224e0d4aa944209a188d13d8dd894d\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.049999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -2.0292834885039572\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010935420515132093\n",
      "          policy_loss: 0.00227739600231871\n",
      "          total_loss: 302.354816650132\n",
      "          vf_explained_var: 3.030744544219033e-10\n",
      "          vf_loss: 302.3519915758553\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 375000\n",
      "    num_steps_sampled: 375000\n",
      "    num_steps_trained: 375000\n",
      "  iterations_since_restore: 25\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 16.363524590163934\n",
      "    ram_util_percent: 34.50081967213115\n",
      "  pid: 30781\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08663484762041634\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.389806633489666\n",
      "    mean_inference_ms: 1.8314843597873462\n",
      "    mean_raw_obs_processing_ms: 1.074822371016463\n",
      "  time_since_restore: 3942.7395298480988\n",
      "  time_this_iter_s: 171.52481317520142\n",
      "  time_total_s: 3942.7395298480988\n",
      "  timers:\n",
      "    learn_throughput: 243.431\n",
      "    learn_time_ms: 61619.044\n",
      "    sample_throughput: 140.966\n",
      "    sample_time_ms: 106408.273\n",
      "    update_time_ms: 2.605\n",
      "  timestamp: 1666675453\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 375000\n",
      "  training_iteration: 25\n",
      "  trial_id: '12173_00000'\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 10.8/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.38 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_2_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_3_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_1_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_5_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_0_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_4_edf1e7152919a3617c29d944024e5221, 0.0/6.0 CPU_group_edf1e7152919a3617c29d944024e5221, 0.0/1.0 accelerator_type:G)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_figure_eight\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------+----------+----------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                  | status   | loc                  |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------+----------+----------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_AccelEnv-v0_12173_00000 | RUNNING  | 141.225.10.229:30781 |     25 |          3942.74 | 375000 |  -328.81 |              208.346 |             -1280.25 |               1500 |\n",
      "+-----------------------------+----------+----------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_AccelEnv-v0_12173_00000:\n",
      "  agent_timesteps_total: 390000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-25_00-27-05\n",
      "  done: false\n",
      "  episode_len_mean: 1500.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 208.34647352475827\n",
      "  episode_reward_mean: -267.7614175731466\n",
      "  episode_reward_min: -1280.245971695355\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 260\n",
      "  experiment_id: 3b224e0d4aa944209a188d13d8dd894d\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.049999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -2.0872980117797852\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011444020191512534\n",
      "          policy_loss: 0.0026935215624396703\n",
      "          total_loss: 287.38617344225867\n",
      "          vf_explained_var: -5.657390111935001e-09\n",
      "          vf_loss: 287.38290837821313\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 390000\n",
      "    num_steps_sampled: 390000\n",
      "    num_steps_trained: 390000\n",
      "  iterations_since_restore: 26\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 16.315040650406505\n",
      "    ram_util_percent: 34.54146341463415\n",
      "  pid: 30781\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08651367315701763\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.394420221748568\n",
      "    mean_inference_ms: 1.8260593995267635\n",
      "    mean_raw_obs_processing_ms: 1.0747235582709833\n",
      "  time_since_restore: 4114.780859708786\n",
      "  time_this_iter_s: 172.04132986068726\n",
      "  time_total_s: 4114.780859708786\n",
      "  timers:\n",
      "    learn_throughput: 238.418\n",
      "    learn_time_ms: 62914.702\n",
      "    sample_throughput: 140.944\n",
      "    sample_time_ms: 106425.117\n",
      "    update_time_ms: 2.69\n",
      "  timestamp: 1666675625\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 390000\n",
      "  training_iteration: 26\n",
      "  trial_id: '12173_00000'\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 10.8/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.38 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_4_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_2_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_3_edf1e7152919a3617c29d944024e5221, 0.0/6.0 CPU_group_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_0_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_1_edf1e7152919a3617c29d944024e5221, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_5_edf1e7152919a3617c29d944024e5221)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_figure_eight\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------+----------+----------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                  | status   | loc                  |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------+----------+----------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_AccelEnv-v0_12173_00000 | RUNNING  | 141.225.10.229:30781 |     26 |          4114.78 | 390000 | -267.761 |              208.346 |             -1280.25 |               1500 |\n",
      "+-----------------------------+----------+----------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "Result for PPO_AccelEnv-v0_12173_00000:\n",
      "  agent_timesteps_total: 405000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-25_00-29-54\n",
      "  done: false\n",
      "  episode_len_mean: 1500.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -51.772788903532145\n",
      "  episode_reward_mean: -277.9070319091019\n",
      "  episode_reward_min: -1280.245971695355\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 270\n",
      "  experiment_id: 3b224e0d4aa944209a188d13d8dd894d\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.049999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -2.1353881357079847\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012511860912692404\n",
      "          policy_loss: 0.0018671552405960985\n",
      "          total_loss: 280.6253230595993\n",
      "          vf_explained_var: -3.687405936148025e-09\n",
      "          vf_loss: 280.6228287971626\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 405000\n",
      "    num_steps_sampled: 405000\n",
      "    num_steps_trained: 405000\n",
      "  iterations_since_restore: 27\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 16.509128630705394\n",
      "    ram_util_percent: 34.457261410788384\n",
      "  pid: 30781\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08640624648509274\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.398771027204575\n",
      "    mean_inference_ms: 1.821093828394661\n",
      "    mean_raw_obs_processing_ms: 1.074621033037296\n",
      "  time_since_restore: 4283.385625123978\n",
      "  time_this_iter_s: 168.60476541519165\n",
      "  time_total_s: 4283.385625123978\n",
      "  timers:\n",
      "    learn_throughput: 236.701\n",
      "    learn_time_ms: 63371.207\n",
      "    sample_throughput: 140.939\n",
      "    sample_time_ms: 106429.265\n",
      "    update_time_ms: 2.617\n",
      "  timestamp: 1666675794\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 405000\n",
      "  training_iteration: 27\n",
      "  trial_id: '12173_00000'\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 10.8/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.38 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_0_edf1e7152919a3617c29d944024e5221, 0.0/6.0 CPU_group_edf1e7152919a3617c29d944024e5221, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_5_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_1_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_3_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_2_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_4_edf1e7152919a3617c29d944024e5221)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_figure_eight\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------+----------+----------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                  | status   | loc                  |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------+----------+----------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_AccelEnv-v0_12173_00000 | RUNNING  | 141.225.10.229:30781 |     27 |          4283.39 | 405000 | -277.907 |             -51.7728 |             -1280.25 |               1500 |\n",
      "+-----------------------------+----------+----------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "Result for PPO_AccelEnv-v0_12173_00000:\n",
      "  agent_timesteps_total: 420000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-25_00-32-46\n",
      "  done: false\n",
      "  episode_len_mean: 1500.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -55.565880431859526\n",
      "  episode_reward_mean: -199.45190964452232\n",
      "  episode_reward_min: -1125.952844606375\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 280\n",
      "  experiment_id: 3b224e0d4aa944209a188d13d8dd894d\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.049999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -2.1851065621537678\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012665556568610244\n",
      "          policy_loss: 0.0030386047942911163\n",
      "          total_loss: 266.989730375096\n",
      "          vf_explained_var: -2.879207317008081e-09\n",
      "          vf_loss: 266.9860582683046\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 420000\n",
      "    num_steps_sampled: 420000\n",
      "    num_steps_trained: 420000\n",
      "  iterations_since_restore: 28\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 16.34122448979592\n",
      "    ram_util_percent: 34.54489795918367\n",
      "  pid: 30781\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0863037403727278\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.40287060336751\n",
      "    mean_inference_ms: 1.8164705134415555\n",
      "    mean_raw_obs_processing_ms: 1.074629095740192\n",
      "  time_since_restore: 4455.600035190582\n",
      "  time_this_iter_s: 172.21441006660461\n",
      "  time_total_s: 4455.600035190582\n",
      "  timers:\n",
      "    learn_throughput: 233.899\n",
      "    learn_time_ms: 64130.155\n",
      "    sample_throughput: 140.929\n",
      "    sample_time_ms: 106436.354\n",
      "    update_time_ms: 2.585\n",
      "  timestamp: 1666675966\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 420000\n",
      "  training_iteration: 28\n",
      "  trial_id: '12173_00000'\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 10.8/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.38 GiB heap, 0.0/14.65 GiB objects (0.0/6.0 CPU_group_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_3_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_1_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_5_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_2_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_0_edf1e7152919a3617c29d944024e5221, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_4_edf1e7152919a3617c29d944024e5221)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_figure_eight\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------+----------+----------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                  | status   | loc                  |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------+----------+----------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_AccelEnv-v0_12173_00000 | RUNNING  | 141.225.10.229:30781 |     28 |           4455.6 | 420000 | -199.452 |             -55.5659 |             -1125.95 |               1500 |\n",
      "+-----------------------------+----------+----------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "Result for PPO_AccelEnv-v0_12173_00000:\n",
      "  agent_timesteps_total: 435000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-25_00-35-38\n",
      "  done: false\n",
      "  episode_len_mean: 1500.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -55.565880431859526\n",
      "  episode_reward_mean: -129.68543983055827\n",
      "  episode_reward_min: -710.8530068504233\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 290\n",
      "  experiment_id: 3b224e0d4aa944209a188d13d8dd894d\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.049999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -2.1860930519588924\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012837363349388788\n",
      "          policy_loss: 0.0012324622623085724\n",
      "          total_loss: 257.9987921536979\n",
      "          vf_explained_var: -2.1720336640385085e-09\n",
      "          vf_loss: 257.99691813840707\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 435000\n",
      "    num_steps_sampled: 435000\n",
      "    num_steps_trained: 435000\n",
      "  iterations_since_restore: 29\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 16.254065040650406\n",
      "    ram_util_percent: 34.5109756097561\n",
      "  pid: 30781\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08621146929909397\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.406660480222055\n",
      "    mean_inference_ms: 1.8122725645849074\n",
      "    mean_raw_obs_processing_ms: 1.074712534897109\n",
      "  time_since_restore: 4627.62873339653\n",
      "  time_this_iter_s: 172.02869820594788\n",
      "  time_total_s: 4627.62873339653\n",
      "  timers:\n",
      "    learn_throughput: 231.48\n",
      "    learn_time_ms: 64800.38\n",
      "    sample_throughput: 140.921\n",
      "    sample_time_ms: 106442.299\n",
      "    update_time_ms: 2.593\n",
      "  timestamp: 1666676138\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 435000\n",
      "  training_iteration: 29\n",
      "  trial_id: '12173_00000'\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Memory usage on this node: 10.8/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.38 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_1_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_5_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_0_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_2_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_4_edf1e7152919a3617c29d944024e5221, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_3_edf1e7152919a3617c29d944024e5221, 0.0/6.0 CPU_group_edf1e7152919a3617c29d944024e5221)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_figure_eight\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------+----------+----------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                  | status   | loc                  |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------+----------+----------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_AccelEnv-v0_12173_00000 | RUNNING  | 141.225.10.229:30781 |     29 |          4627.63 | 435000 | -129.685 |             -55.5659 |             -710.853 |               1500 |\n",
      "+-----------------------------+----------+----------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "Result for PPO_AccelEnv-v0_12173_00000:\n",
      "  agent_timesteps_total: 450000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-25_00-38-30\n",
      "  done: false\n",
      "  episode_len_mean: 1500.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -33.02476558591097\n",
      "  episode_reward_mean: -103.88516625766778\n",
      "  episode_reward_min: -224.52504349914827\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 300\n",
      "  experiment_id: 3b224e0d4aa944209a188d13d8dd894d\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.049999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -2.2277822450055913\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013127257021850611\n",
      "          policy_loss: 0.0028042464353286237\n",
      "          total_loss: 248.14835385791326\n",
      "          vf_explained_var: -1.4143475279837503e-09\n",
      "          vf_loss: 248.14489317101948\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 450000\n",
      "    num_steps_sampled: 450000\n",
      "    num_steps_trained: 450000\n",
      "  iterations_since_restore: 30\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 16.304489795918368\n",
      "    ram_util_percent: 34.50367346938776\n",
      "  pid: 30781\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0861245207887314\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.410145111372348\n",
      "    mean_inference_ms: 1.808371232748016\n",
      "    mean_raw_obs_processing_ms: 1.0746803124686588\n",
      "  time_since_restore: 4799.681177377701\n",
      "  time_this_iter_s: 172.05244398117065\n",
      "  time_total_s: 4799.681177377701\n",
      "  timers:\n",
      "    learn_throughput: 230.338\n",
      "    learn_time_ms: 65121.826\n",
      "    sample_throughput: 140.95\n",
      "    sample_time_ms: 106420.896\n",
      "    update_time_ms: 2.68\n",
      "  timestamp: 1666676310\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 450000\n",
      "  training_iteration: 30\n",
      "  trial_id: '12173_00000'\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 10.8/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.38 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 accelerator_type:G, 0.0/6.0 CPU_group_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_1_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_5_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_4_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_3_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_0_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_2_edf1e7152919a3617c29d944024e5221)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_figure_eight\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------+----------+----------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                  | status   | loc                  |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------+----------+----------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_AccelEnv-v0_12173_00000 | RUNNING  | 141.225.10.229:30781 |     30 |          4799.68 | 450000 | -103.885 |             -33.0248 |             -224.525 |               1500 |\n",
      "+-----------------------------+----------+----------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "Result for PPO_AccelEnv-v0_12173_00000:\n",
      "  agent_timesteps_total: 465000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-25_00-41-22\n",
      "  done: false\n",
      "  episode_len_mean: 1500.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -10.157640182898845\n",
      "  episode_reward_mean: -87.51331710552746\n",
      "  episode_reward_min: -185.1978742364514\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 310\n",
      "  experiment_id: 3b224e0d4aa944209a188d13d8dd894d\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.049999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -2.201369952953468\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010794352499074993\n",
      "          policy_loss: 0.0020789247779649193\n",
      "          total_loss: 233.1290236198296\n",
      "          vf_explained_var: -2.677157828756549e-09\n",
      "          vf_loss: 233.12640543792207\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 465000\n",
      "    num_steps_sampled: 465000\n",
      "    num_steps_trained: 465000\n",
      "  iterations_since_restore: 31\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 16.330894308943087\n",
      "    ram_util_percent: 34.51747967479676\n",
      "  pid: 30781\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08604111280051065\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.413185150762775\n",
      "    mean_inference_ms: 1.8049430123200745\n",
      "    mean_raw_obs_processing_ms: 1.0747391819193555\n",
      "  time_since_restore: 4971.628675460815\n",
      "  time_this_iter_s: 171.94749808311462\n",
      "  time_total_s: 4971.628675460815\n",
      "  timers:\n",
      "    learn_throughput: 230.108\n",
      "    learn_time_ms: 65186.691\n",
      "    sample_throughput: 140.957\n",
      "    sample_time_ms: 106415.37\n",
      "    update_time_ms: 2.7\n",
      "  timestamp: 1666676482\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 465000\n",
      "  training_iteration: 31\n",
      "  trial_id: '12173_00000'\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 10.8/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.38 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 accelerator_type:G, 0.0/6.0 CPU_group_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_2_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_3_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_1_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_5_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_4_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_0_edf1e7152919a3617c29d944024e5221)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_figure_eight\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------+----------+----------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                  | status   | loc                  |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------+----------+----------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_AccelEnv-v0_12173_00000 | RUNNING  | 141.225.10.229:30781 |     31 |          4971.63 | 465000 | -87.5133 |             -10.1576 |             -185.198 |               1500 |\n",
      "+-----------------------------+----------+----------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "Result for PPO_AccelEnv-v0_12173_00000:\n",
      "  agent_timesteps_total: 480000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-25_00-44-14\n",
      "  done: false\n",
      "  episode_len_mean: 1500.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -10.157640182898845\n",
      "  episode_reward_mean: -73.07869461916637\n",
      "  episode_reward_min: -143.42303318555093\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 320\n",
      "  experiment_id: 3b224e0d4aa944209a188d13d8dd894d\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.049999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -2.21606415308128\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010462331791081799\n",
      "          policy_loss: 0.0024579396601608497\n",
      "          total_loss: 225.00478640734138\n",
      "          vf_explained_var: -0.0012651481665670872\n",
      "          vf_loss: 225.00180518910037\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 480000\n",
      "    num_steps_sampled: 480000\n",
      "    num_steps_trained: 480000\n",
      "  iterations_since_restore: 32\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 16.415510204081635\n",
      "    ram_util_percent: 34.54979591836734\n",
      "  pid: 30781\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08596062957530283\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.415999432226695\n",
      "    mean_inference_ms: 1.8016923001244285\n",
      "    mean_raw_obs_processing_ms: 1.0748338997454194\n",
      "  time_since_restore: 5143.12056517601\n",
      "  time_this_iter_s: 171.4918897151947\n",
      "  time_total_s: 5143.12056517601\n",
      "  timers:\n",
      "    learn_throughput: 230.204\n",
      "    learn_time_ms: 65159.588\n",
      "    sample_throughput: 140.957\n",
      "    sample_time_ms: 106415.783\n",
      "    update_time_ms: 2.712\n",
      "  timestamp: 1666676654\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 480000\n",
      "  training_iteration: 32\n",
      "  trial_id: '12173_00000'\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 10.8/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.38 GiB heap, 0.0/14.65 GiB objects (0.0/6.0 CPU_group_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_2_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_3_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_0_edf1e7152919a3617c29d944024e5221, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_1_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_4_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_5_edf1e7152919a3617c29d944024e5221)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_figure_eight\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------+----------+----------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                  | status   | loc                  |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------+----------+----------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_AccelEnv-v0_12173_00000 | RUNNING  | 141.225.10.229:30781 |     32 |          5143.12 | 480000 | -73.0787 |             -10.1576 |             -143.423 |               1500 |\n",
      "+-----------------------------+----------+----------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "Result for PPO_AccelEnv-v0_12173_00000:\n",
      "  agent_timesteps_total: 495000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-25_00-47-06\n",
      "  done: false\n",
      "  episode_len_mean: 1500.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 13.661468724843632\n",
      "  episode_reward_mean: -60.33237612037558\n",
      "  episode_reward_min: -114.068246370271\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 330\n",
      "  experiment_id: 3b224e0d4aa944209a188d13d8dd894d\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.049999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -2.2893994484917592\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012487321125646501\n",
      "          policy_loss: 0.0024173011892806677\n",
      "          total_loss: 217.77248010797015\n",
      "          vf_explained_var: -1.111273073561847e-09\n",
      "          vf_loss: 217.7694394257109\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 495000\n",
      "    num_steps_sampled: 495000\n",
      "    num_steps_trained: 495000\n",
      "  iterations_since_restore: 33\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 16.346938775510203\n",
      "    ram_util_percent: 34.52489795918368\n",
      "  pid: 30781\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08588546734074465\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.418694009615862\n",
      "    mean_inference_ms: 1.7986082350128654\n",
      "    mean_raw_obs_processing_ms: 1.0749087974047997\n",
      "  time_since_restore: 5314.778031587601\n",
      "  time_this_iter_s: 171.65746641159058\n",
      "  time_total_s: 5314.778031587601\n",
      "  timers:\n",
      "    learn_throughput: 230.254\n",
      "    learn_time_ms: 65145.522\n",
      "    sample_throughput: 140.982\n",
      "    sample_time_ms: 106396.682\n",
      "    update_time_ms: 2.586\n",
      "  timestamp: 1666676826\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 495000\n",
      "  training_iteration: 33\n",
      "  trial_id: '12173_00000'\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 10.8/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.38 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_2_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_5_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_4_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_3_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_1_edf1e7152919a3617c29d944024e5221, 0.0/6.0 CPU_group_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_0_edf1e7152919a3617c29d944024e5221, 0.0/1.0 accelerator_type:G)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_figure_eight\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------+----------+----------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                  | status   | loc                  |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------+----------+----------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_AccelEnv-v0_12173_00000 | RUNNING  | 141.225.10.229:30781 |     33 |          5314.78 | 495000 | -60.3324 |              13.6615 |             -114.068 |               1500 |\n",
      "+-----------------------------+----------+----------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "Result for PPO_AccelEnv-v0_12173_00000:\n",
      "  agent_timesteps_total: 510000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-25_00-49-58\n",
      "  done: false\n",
      "  episode_len_mean: 1500.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 24.232936184354973\n",
      "  episode_reward_mean: -48.6011053102648\n",
      "  episode_reward_min: -114.068246370271\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 340\n",
      "  experiment_id: 3b224e0d4aa944209a188d13d8dd894d\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.049999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -2.340365108797106\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011934601047691425\n",
      "          policy_loss: 0.0025568993213602293\n",
      "          total_loss: 209.0025413319216\n",
      "          vf_explained_var: -2.0204964090719812e-10\n",
      "          vf_loss: 208.99938792212535\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 510000\n",
      "    num_steps_sampled: 510000\n",
      "    num_steps_trained: 510000\n",
      "  iterations_since_restore: 34\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 16.34857142857143\n",
      "    ram_util_percent: 34.50448979591836\n",
      "  pid: 30781\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08581707925365095\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.421330227003452\n",
      "    mean_inference_ms: 1.7956736306599594\n",
      "    mean_raw_obs_processing_ms: 1.0750616707616119\n",
      "  time_since_restore: 5486.829856395721\n",
      "  time_this_iter_s: 172.05182480812073\n",
      "  time_total_s: 5486.829856395721\n",
      "  timers:\n",
      "    learn_throughput: 230.279\n",
      "    learn_time_ms: 65138.331\n",
      "    sample_throughput: 140.966\n",
      "    sample_time_ms: 106408.75\n",
      "    update_time_ms: 2.621\n",
      "  timestamp: 1666676998\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 510000\n",
      "  training_iteration: 34\n",
      "  trial_id: '12173_00000'\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 10.8/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.38 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_4_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_2_edf1e7152919a3617c29d944024e5221, 0.0/6.0 CPU_group_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_5_edf1e7152919a3617c29d944024e5221, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_0_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_1_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_3_edf1e7152919a3617c29d944024e5221)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_figure_eight\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------+----------+----------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                  | status   | loc                  |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------+----------+----------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_AccelEnv-v0_12173_00000 | RUNNING  | 141.225.10.229:30781 |     34 |          5486.83 | 510000 | -48.6011 |              24.2329 |             -114.068 |               1500 |\n",
      "+-----------------------------+----------+----------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "Result for PPO_AccelEnv-v0_12173_00000:\n",
      "  agent_timesteps_total: 525000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-25_00-52-49\n",
      "  done: false\n",
      "  episode_len_mean: 1500.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 39.05761853261171\n",
      "  episode_reward_mean: -35.50098440597394\n",
      "  episode_reward_min: -114.068246370271\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 350\n",
      "  experiment_id: 3b224e0d4aa944209a188d13d8dd894d\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.049999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -2.3707178784629046\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012630316875127702\n",
      "          policy_loss: 0.0035106270601657236\n",
      "          total_loss: 198.4125578928802\n",
      "          vf_explained_var: -2.2730584081642746e-09\n",
      "          vf_loss: 198.40841551635225\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 525000\n",
      "    num_steps_sampled: 525000\n",
      "    num_steps_trained: 525000\n",
      "  iterations_since_restore: 35\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 16.377868852459017\n",
      "    ram_util_percent: 34.45778688524591\n",
      "  pid: 30781\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0857510670453695\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.423888867581287\n",
      "    mean_inference_ms: 1.792866604503676\n",
      "    mean_raw_obs_processing_ms: 1.0752123032537702\n",
      "  time_since_restore: 5657.926442861557\n",
      "  time_this_iter_s: 171.09658646583557\n",
      "  time_total_s: 5657.926442861557\n",
      "  timers:\n",
      "    learn_throughput: 230.449\n",
      "    learn_time_ms: 65090.437\n",
      "    sample_throughput: 140.959\n",
      "    sample_time_ms: 106413.73\n",
      "    update_time_ms: 2.584\n",
      "  timestamp: 1666677169\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 525000\n",
      "  training_iteration: 35\n",
      "  trial_id: '12173_00000'\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 10.8/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.38 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_0_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_5_edf1e7152919a3617c29d944024e5221, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_1_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_2_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_4_edf1e7152919a3617c29d944024e5221, 0.0/6.0 CPU_group_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_3_edf1e7152919a3617c29d944024e5221)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_figure_eight\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------+----------+----------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                  | status   | loc                  |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------+----------+----------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_AccelEnv-v0_12173_00000 | RUNNING  | 141.225.10.229:30781 |     35 |          5657.93 | 525000 |  -35.501 |              39.0576 |             -114.068 |               1500 |\n",
      "+-----------------------------+----------+----------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "Result for PPO_AccelEnv-v0_12173_00000:\n",
      "  agent_timesteps_total: 540000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-25_00-55-38\n",
      "  done: false\n",
      "  episode_len_mean: 1500.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 95.92882879856285\n",
      "  episode_reward_mean: -22.072168429192836\n",
      "  episode_reward_min: -114.068246370271\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 360\n",
      "  experiment_id: 3b224e0d4aa944209a188d13d8dd894d\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.049999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -2.3070968947168122\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012093329413052214\n",
      "          policy_loss: 0.0021114709364818567\n",
      "          total_loss: 186.67182864819543\n",
      "          vf_explained_var: -2.6266453456713634e-09\n",
      "          vf_loss: 186.66911260313907\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 540000\n",
      "    num_steps_sampled: 540000\n",
      "    num_steps_trained: 540000\n",
      "  iterations_since_restore: 36\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 16.533884297520665\n",
      "    ram_util_percent: 34.485537190082646\n",
      "  pid: 30781\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08569133317009925\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.42622451195767\n",
      "    mean_inference_ms: 1.7903225138347827\n",
      "    mean_raw_obs_processing_ms: 1.075348948003778\n",
      "  time_since_restore: 5827.288261175156\n",
      "  time_this_iter_s: 169.36181831359863\n",
      "  time_total_s: 5827.288261175156\n",
      "  timers:\n",
      "    learn_throughput: 231.379\n",
      "    learn_time_ms: 64828.826\n",
      "    sample_throughput: 140.967\n",
      "    sample_time_ms: 106408.056\n",
      "    update_time_ms: 2.494\n",
      "  timestamp: 1666677338\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 540000\n",
      "  training_iteration: 36\n",
      "  trial_id: '12173_00000'\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 10.8/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.38 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_5_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_3_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_4_edf1e7152919a3617c29d944024e5221, 0.0/6.0 CPU_group_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_0_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_1_edf1e7152919a3617c29d944024e5221, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_2_edf1e7152919a3617c29d944024e5221)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_figure_eight\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------+----------+----------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                  | status   | loc                  |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------+----------+----------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_AccelEnv-v0_12173_00000 | RUNNING  | 141.225.10.229:30781 |     36 |          5827.29 | 540000 | -22.0722 |              95.9288 |             -114.068 |               1500 |\n",
      "+-----------------------------+----------+----------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "Result for PPO_AccelEnv-v0_12173_00000:\n",
      "  agent_timesteps_total: 555000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-25_00-58-25\n",
      "  done: false\n",
      "  episode_len_mean: 1500.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 225.0211899046609\n",
      "  episode_reward_mean: 2.620734546587657\n",
      "  episode_reward_min: -97.50648505862664\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 370\n",
      "  experiment_id: 3b224e0d4aa944209a188d13d8dd894d\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.049999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -2.202514096033775\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.020469897601386615\n",
      "          policy_loss: 0.0007995614184730615\n",
      "          total_loss: 177.623789834168\n",
      "          vf_explained_var: -2.0204964090719812e-10\n",
      "          vf_loss: 177.62196638462908\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 555000\n",
      "    num_steps_sampled: 555000\n",
      "    num_steps_trained: 555000\n",
      "  iterations_since_restore: 37\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 16.715546218487397\n",
      "    ram_util_percent: 34.51974789915966\n",
      "  pid: 30781\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08563455582912757\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.428298147686693\n",
      "    mean_inference_ms: 1.7880549258678076\n",
      "    mean_raw_obs_processing_ms: 1.0755088781068762\n",
      "  time_since_restore: 5993.788438796997\n",
      "  time_this_iter_s: 166.50017762184143\n",
      "  time_total_s: 5993.788438796997\n",
      "  timers:\n",
      "    learn_throughput: 232.122\n",
      "    learn_time_ms: 64621.314\n",
      "    sample_throughput: 140.971\n",
      "    sample_time_ms: 106404.881\n",
      "    update_time_ms: 2.548\n",
      "  timestamp: 1666677505\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 555000\n",
      "  training_iteration: 37\n",
      "  trial_id: '12173_00000'\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 10.8/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.38 GiB heap, 0.0/14.65 GiB objects (0.0/6.0 CPU_group_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_0_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_1_edf1e7152919a3617c29d944024e5221, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_5_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_3_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_2_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_4_edf1e7152919a3617c29d944024e5221)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_figure_eight\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------+----------+----------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                  | status   | loc                  |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------+----------+----------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_AccelEnv-v0_12173_00000 | RUNNING  | 141.225.10.229:30781 |     37 |          5993.79 | 555000 |  2.62073 |              225.021 |             -97.5065 |               1500 |\n",
      "+-----------------------------+----------+----------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "Result for PPO_AccelEnv-v0_12173_00000:\n",
      "  agent_timesteps_total: 570000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-25_01-01-10\n",
      "  done: false\n",
      "  episode_len_mean: 1500.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 486.4781022804949\n",
      "  episode_reward_mean: 49.08172323381243\n",
      "  episode_reward_min: -80.83912700093929\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 380\n",
      "  experiment_id: 3b224e0d4aa944209a188d13d8dd894d\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.049999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -2.223297462625019\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.021939128453182945\n",
      "          policy_loss: 0.0027421833260647824\n",
      "          total_loss: 171.34333457300218\n",
      "          vf_explained_var: -5.051241092068892e-10\n",
      "          vf_loss: 171.33949568150408\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 570000\n",
      "    num_steps_sampled: 570000\n",
      "    num_steps_trained: 570000\n",
      "  iterations_since_restore: 38\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 16.81367521367521\n",
      "    ram_util_percent: 34.417094017094016\n",
      "  pid: 30781\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0855823141192082\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.43015766123973\n",
      "    mean_inference_ms: 1.78601614950775\n",
      "    mean_raw_obs_processing_ms: 1.075588137984407\n",
      "  time_since_restore: 6158.3960247039795\n",
      "  time_this_iter_s: 164.60758590698242\n",
      "  time_total_s: 6158.3960247039795\n",
      "  timers:\n",
      "    learn_throughput: 234.823\n",
      "    learn_time_ms: 63877.98\n",
      "    sample_throughput: 140.994\n",
      "    sample_time_ms: 106387.809\n",
      "    update_time_ms: 2.623\n",
      "  timestamp: 1666677670\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 570000\n",
      "  training_iteration: 38\n",
      "  trial_id: '12173_00000'\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Memory usage on this node: 10.8/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.38 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_5_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_1_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_3_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_2_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_4_edf1e7152919a3617c29d944024e5221, 0.0/6.0 CPU_group_edf1e7152919a3617c29d944024e5221, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_0_edf1e7152919a3617c29d944024e5221)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_figure_eight\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------+----------+----------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                  | status   | loc                  |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------+----------+----------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_AccelEnv-v0_12173_00000 | RUNNING  | 141.225.10.229:30781 |     38 |           6158.4 | 570000 |  49.0817 |              486.478 |             -80.8391 |               1500 |\n",
      "+-----------------------------+----------+----------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "Result for PPO_AccelEnv-v0_12173_00000:\n",
      "  agent_timesteps_total: 585000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-25_01-03-56\n",
      "  done: false\n",
      "  episode_len_mean: 1500.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 486.4781022804949\n",
      "  episode_reward_mean: 77.02855341882436\n",
      "  episode_reward_min: -48.63825446852969\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 390\n",
      "  experiment_id: 3b224e0d4aa944209a188d13d8dd894d\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.049999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -2.256072699013403\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015663626022988278\n",
      "          policy_loss: 0.003832430732673255\n",
      "          total_loss: 161.11311509083893\n",
      "          vf_explained_var: -2.32357089124946e-09\n",
      "          vf_loss: 161.10849925542283\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 585000\n",
      "    num_steps_sampled: 585000\n",
      "    num_steps_trained: 585000\n",
      "  iterations_since_restore: 39\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 16.578902953586496\n",
      "    ram_util_percent: 34.4409282700422\n",
      "  pid: 30781\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08553092165230605\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.432115167023394\n",
      "    mean_inference_ms: 1.783910060865924\n",
      "    mean_raw_obs_processing_ms: 1.0756033765381752\n",
      "  time_since_restore: 6324.426405668259\n",
      "  time_this_iter_s: 166.03038096427917\n",
      "  time_total_s: 6324.426405668259\n",
      "  timers:\n",
      "    learn_throughput: 237.034\n",
      "    learn_time_ms: 63282.114\n",
      "    sample_throughput: 140.999\n",
      "    sample_time_ms: 106383.766\n",
      "    update_time_ms: 2.656\n",
      "  timestamp: 1666677836\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 585000\n",
      "  training_iteration: 39\n",
      "  trial_id: '12173_00000'\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 10.8/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.38 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_2_edf1e7152919a3617c29d944024e5221, 0.0/6.0 CPU_group_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_1_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_5_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_4_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_0_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_3_edf1e7152919a3617c29d944024e5221, 0.0/1.0 accelerator_type:G)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_figure_eight\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------+----------+----------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                  | status   | loc                  |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------+----------+----------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_AccelEnv-v0_12173_00000 | RUNNING  | 141.225.10.229:30781 |     39 |          6324.43 | 585000 |  77.0286 |              486.478 |             -48.6383 |               1500 |\n",
      "+-----------------------------+----------+----------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_AccelEnv-v0_12173_00000:\n",
      "  agent_timesteps_total: 600000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-25_01-06-43\n",
      "  done: false\n",
      "  episode_len_mean: 1500.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 486.4781022804949\n",
      "  episode_reward_mean: 108.2256852560647\n",
      "  episode_reward_min: -36.52054132278783\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 400\n",
      "  experiment_id: 3b224e0d4aa944209a188d13d8dd894d\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.049999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -2.317760054135727\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01661312440940557\n",
      "          policy_loss: 0.0038457306133488477\n",
      "          total_loss: 150.3441426099357\n",
      "          vf_explained_var: -1.0102482184137784e-09\n",
      "          vf_loss: 150.33946630186955\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 600000\n",
      "    num_steps_sampled: 600000\n",
      "    num_steps_trained: 600000\n",
      "  iterations_since_restore: 40\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 16.664435146443516\n",
      "    ram_util_percent: 34.49288702928872\n",
      "  pid: 30781\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08548173544748508\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.433996090709357\n",
      "    mean_inference_ms: 1.7819140216042952\n",
      "    mean_raw_obs_processing_ms: 1.0756665082290422\n",
      "  time_since_restore: 6491.341495513916\n",
      "  time_this_iter_s: 166.91508984565735\n",
      "  time_total_s: 6491.341495513916\n",
      "  timers:\n",
      "    learn_throughput: 239.001\n",
      "    learn_time_ms: 62761.33\n",
      "    sample_throughput: 140.99\n",
      "    sample_time_ms: 106390.635\n",
      "    update_time_ms: 2.633\n",
      "  timestamp: 1666678003\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 600000\n",
      "  training_iteration: 40\n",
      "  trial_id: '12173_00000'\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 10.8/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.38 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_4_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_0_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_1_edf1e7152919a3617c29d944024e5221, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_5_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_2_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_3_edf1e7152919a3617c29d944024e5221, 0.0/6.0 CPU_group_edf1e7152919a3617c29d944024e5221)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_figure_eight\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------+----------+----------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                  | status   | loc                  |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------+----------+----------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_AccelEnv-v0_12173_00000 | RUNNING  | 141.225.10.229:30781 |     40 |          6491.34 | 600000 |  108.226 |              486.478 |             -36.5205 |               1500 |\n",
      "+-----------------------------+----------+----------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "Result for PPO_AccelEnv-v0_12173_00000:\n",
      "  agent_timesteps_total: 615000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-25_01-09-30\n",
      "  done: false\n",
      "  episode_len_mean: 1500.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 486.4781022804949\n",
      "  episode_reward_mean: 125.3179861427453\n",
      "  episode_reward_min: -32.87113880218905\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 410\n",
      "  experiment_id: 3b224e0d4aa944209a188d13d8dd894d\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.049999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -2.8702469965158883\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0160428079796508\n",
      "          policy_loss: 0.0049135154819557976\n",
      "          total_loss: 147.66402440798484\n",
      "          vf_explained_var: -1.4648599000466334e-09\n",
      "          vf_loss: 147.65830867573365\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 615000\n",
      "    num_steps_sampled: 615000\n",
      "    num_steps_trained: 615000\n",
      "  iterations_since_restore: 41\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 16.696218487394958\n",
      "    ram_util_percent: 34.48907563025209\n",
      "  pid: 30781\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08543774080714052\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.43585705292592\n",
      "    mean_inference_ms: 1.7799638204409427\n",
      "    mean_raw_obs_processing_ms: 1.075646524491988\n",
      "  time_since_restore: 6658.4082362651825\n",
      "  time_this_iter_s: 167.06674075126648\n",
      "  time_total_s: 6658.4082362651825\n",
      "  timers:\n",
      "    learn_throughput: 240.84\n",
      "    learn_time_ms: 62282.063\n",
      "    sample_throughput: 141.002\n",
      "    sample_time_ms: 106381.447\n",
      "    update_time_ms: 2.679\n",
      "  timestamp: 1666678170\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 615000\n",
      "  training_iteration: 41\n",
      "  trial_id: '12173_00000'\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 10.8/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.38 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_0_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_5_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_4_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_2_edf1e7152919a3617c29d944024e5221, 0.0/1.0 accelerator_type:G, 0.0/6.0 CPU_group_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_3_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_1_edf1e7152919a3617c29d944024e5221)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_figure_eight\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------+----------+----------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                  | status   | loc                  |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------+----------+----------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_AccelEnv-v0_12173_00000 | RUNNING  | 141.225.10.229:30781 |     41 |          6658.41 | 615000 |  125.318 |              486.478 |             -32.8711 |               1500 |\n",
      "+-----------------------------+----------+----------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m Could not connect to TraCI server at localhost:34231 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m Could not connect to TraCI server at localhost:34231 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m  Retrying in 3 seconds\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m Could not connect to TraCI server at localhost:34231 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m  Retrying in 4 seconds\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m Could not connect to TraCI server at localhost:34231 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m  Retrying in 5 seconds\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m Could not connect to TraCI server at localhost:34231 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m  Retrying in 6 seconds\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m Could not connect to TraCI server at localhost:34231 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m  Retrying in 7 seconds\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m Could not connect to TraCI server at localhost:34231 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m  Retrying in 8 seconds\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m Could not connect to TraCI server at localhost:34231 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m  Retrying in 9 seconds\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m Could not connect to TraCI server at localhost:34231 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m  Retrying in 10 seconds\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m Could not connect to TraCI server at localhost:34231 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m  Retrying in 11 seconds\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m Could not connect to TraCI server at localhost:34231 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m  Retrying in 12 seconds\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m Could not connect to TraCI server at localhost:34231 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m  Retrying in 13 seconds\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m Could not connect to TraCI server at localhost:34231 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m  Retrying in 14 seconds\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m Could not connect to TraCI server at localhost:34231 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m  Retrying in 15 seconds\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m Could not connect to TraCI server at localhost:34231 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m  Retrying in 16 seconds\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m Could not connect to TraCI server at localhost:34231 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m  Retrying in 17 seconds\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m Could not connect to TraCI server at localhost:34231 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m  Retrying in 18 seconds\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m Could not connect to TraCI server at localhost:34231 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m  Retrying in 19 seconds\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m Could not connect to TraCI server at localhost:34231 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m  Retrying in 20 seconds\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m Could not connect to TraCI server at localhost:34231 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m  Retrying in 21 seconds\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m Could not connect to TraCI server at localhost:34231 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m  Retrying in 22 seconds\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m Could not connect to TraCI server at localhost:34231 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m  Retrying in 23 seconds\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m Could not connect to TraCI server at localhost:34231 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m  Retrying in 24 seconds\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m Could not connect to TraCI server at localhost:34231 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m  Retrying in 25 seconds\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m Could not connect to TraCI server at localhost:34231 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m  Retrying in 26 seconds\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m Could not connect to TraCI server at localhost:34231 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m  Retrying in 27 seconds\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m Could not connect to TraCI server at localhost:34231 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m  Retrying in 28 seconds\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m Could not connect to TraCI server at localhost:34231 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m  Retrying in 29 seconds\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m Could not connect to TraCI server at localhost:34231 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m  Retrying in 30 seconds\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m Could not connect to TraCI server at localhost:34231 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m  Retrying in 31 seconds\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m Could not connect to TraCI server at localhost:34231 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m  Retrying in 32 seconds\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m Could not connect to TraCI server at localhost:34231 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m  Retrying in 33 seconds\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m Could not connect to TraCI server at localhost:34231 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m  Retrying in 34 seconds\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m Could not connect to TraCI server at localhost:34231 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m  Retrying in 35 seconds\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m Could not connect to TraCI server at localhost:34231 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m  Retrying in 36 seconds\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m Could not connect to TraCI server at localhost:34231 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m  Retrying in 37 seconds\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m Could not connect to TraCI server at localhost:34231 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m  Retrying in 38 seconds\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m Could not connect to TraCI server at localhost:34231 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m  Retrying in 39 seconds\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m Could not connect to TraCI server at localhost:34231 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m  Retrying in 40 seconds\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m Could not connect to TraCI server at localhost:34231 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m  Retrying in 41 seconds\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m Could not connect to TraCI server at localhost:34231 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m  Retrying in 42 seconds\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m Could not connect to TraCI server at localhost:34231 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m  Retrying in 43 seconds\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m Could not connect to TraCI server at localhost:34231 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m  Retrying in 44 seconds\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m Could not connect to TraCI server at localhost:34231 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m  Retrying in 45 seconds\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m Could not connect to TraCI server at localhost:34231 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m  Retrying in 46 seconds\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m Could not connect to TraCI server at localhost:34231 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m  Retrying in 47 seconds\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m Could not connect to TraCI server at localhost:34231 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m  Retrying in 48 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m Could not connect to TraCI server at localhost:34231 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m  Retrying in 49 seconds\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m Could not connect to TraCI server at localhost:34231 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m  Retrying in 50 seconds\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m Could not connect to TraCI server at localhost:34231 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m  Retrying in 51 seconds\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m Could not connect to TraCI server at localhost:34231 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m  Retrying in 52 seconds\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m Could not connect to TraCI server at localhost:34231 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m  Retrying in 53 seconds\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m Could not connect to TraCI server at localhost:34231 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m  Retrying in 54 seconds\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m Could not connect to TraCI server at localhost:34231 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m  Retrying in 55 seconds\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m Could not connect to TraCI server at localhost:34231 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m  Retrying in 56 seconds\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m Could not connect to TraCI server at localhost:34231 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m  Retrying in 57 seconds\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m Could not connect to TraCI server at localhost:34231 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m  Retrying in 58 seconds\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m Could not connect to TraCI server at localhost:34231 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m  Retrying in 59 seconds\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m Could not connect to TraCI server at localhost:34231 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m  Retrying in 60 seconds\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m Could not connect to TraCI server at localhost:34231 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m  Retrying in 61 seconds\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m Could not connect to TraCI server at localhost:34231 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m  Retrying in 62 seconds\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m Could not connect to TraCI server at localhost:34231 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m  Retrying in 63 seconds\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m Could not connect to TraCI server at localhost:34231 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m  Retrying in 64 seconds\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m Could not connect to TraCI server at localhost:34231 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m  Retrying in 65 seconds\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m Could not connect to TraCI server at localhost:34231 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m  Retrying in 66 seconds\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m Could not connect to TraCI server at localhost:34231 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m  Retrying in 67 seconds\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m Could not connect to TraCI server at localhost:34231 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m  Retrying in 68 seconds\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m Could not connect to TraCI server at localhost:34231 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m  Retrying in 69 seconds\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m Could not connect to TraCI server at localhost:34231 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m  Retrying in 70 seconds\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m Could not connect to TraCI server at localhost:34231 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m  Retrying in 71 seconds\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m Could not connect to TraCI server at localhost:34231 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m  Retrying in 72 seconds\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m Could not connect to TraCI server at localhost:34231 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m  Retrying in 73 seconds\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m Could not connect to TraCI server at localhost:34231 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m  Retrying in 74 seconds\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m Could not connect to TraCI server at localhost:34231 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m  Retrying in 75 seconds\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m Could not connect to TraCI server at localhost:34231 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m  Retrying in 76 seconds\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m Could not connect to TraCI server at localhost:34231 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m  Retrying in 77 seconds\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m Could not connect to TraCI server at localhost:34231 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m  Retrying in 78 seconds\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m Could not connect to TraCI server at localhost:34231 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m  Retrying in 79 seconds\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m Could not connect to TraCI server at localhost:34231 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m  Retrying in 80 seconds\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m Could not connect to TraCI server at localhost:34231 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m  Retrying in 81 seconds\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m Could not connect to TraCI server at localhost:34231 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m  Retrying in 82 seconds\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m Could not connect to TraCI server at localhost:34231 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m  Retrying in 83 seconds\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m Could not connect to TraCI server at localhost:34231 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m  Retrying in 84 seconds\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m Could not connect to TraCI server at localhost:34231 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m  Retrying in 85 seconds\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m Could not connect to TraCI server at localhost:34231 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m  Retrying in 86 seconds\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m Could not connect to TraCI server at localhost:34231 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m  Retrying in 87 seconds\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m Could not connect to TraCI server at localhost:34231 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m  Retrying in 88 seconds\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m Could not connect to TraCI server at localhost:34231 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m  Retrying in 89 seconds\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m Could not connect to TraCI server at localhost:34231 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m  Retrying in 90 seconds\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m Could not connect to TraCI server at localhost:34231 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m  Retrying in 91 seconds\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m Could not connect to TraCI server at localhost:34231 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m  Retrying in 92 seconds\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m Could not connect to TraCI server at localhost:34231 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m  Retrying in 93 seconds\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m Could not connect to TraCI server at localhost:34231 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m  Retrying in 94 seconds\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m Could not connect to TraCI server at localhost:34231 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m  Retrying in 95 seconds\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m Could not connect to TraCI server at localhost:34231 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m  Retrying in 96 seconds\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m Could not connect to TraCI server at localhost:34231 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m  Retrying in 97 seconds\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m Could not connect to TraCI server at localhost:34231 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m  Retrying in 98 seconds\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m Could not connect to TraCI server at localhost:34231 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m  Retrying in 99 seconds\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m Could not connect to TraCI server at localhost:34231 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m  Retrying in 100 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m Could not connect to TraCI server at localhost:34231 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m Error during start: Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m   File \"/home/michael-lab/Desktop/flow/flow/core/kernel/simulation/traci.py\", line 296, in start_simulation\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m     traci_connection = traci.connect(port, numRetries=100)\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m   File \"/home/michael-lab/anaconda3/envs/flow/lib/python3.7/site-packages/traci/__init__.py\", line 75, in connect\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m     raise FatalTraCIError(\"Could not connect in %s tries\" % (numRetries + 1))\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m traci.exceptions.FatalTraCIError: Could not connect in 101 tries\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m Error during teardown: [Errno 3] No such process\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "Result for PPO_AccelEnv-v0_12173_00000:\n",
      "  agent_timesteps_total: 630000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-25_02-36-37\n",
      "  done: false\n",
      "  episode_len_mean: 1500.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 486.4781022804949\n",
      "  episode_reward_mean: 142.12508055049784\n",
      "  episode_reward_min: -9.005135640719931\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 420\n",
      "  experiment_id: 3b224e0d4aa944209a188d13d8dd894d\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.049999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -2.954592135599104\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016126417270495273\n",
      "          policy_loss: 0.004392073249002382\n",
      "          total_loss: 137.69726531141896\n",
      "          vf_explained_var: -1.0102482184137784e-09\n",
      "          vf_loss: 137.6920667971595\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 630000\n",
      "    num_steps_sampled: 630000\n",
      "    num_steps_trained: 630000\n",
      "  iterations_since_restore: 42\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 2.126425217974514\n",
      "    ram_util_percent: 33.0610060362173\n",
      "  pid: 30781\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08539752068585987\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.43760153381425\n",
      "    mean_inference_ms: 1.7781543344873052\n",
      "    mean_raw_obs_processing_ms: 1.8779908030148937\n",
      "  time_since_restore: 11885.548644065857\n",
      "  time_this_iter_s: 5227.140407800674\n",
      "  time_total_s: 11885.548644065857\n",
      "  timers:\n",
      "    learn_throughput: 240.691\n",
      "    learn_time_ms: 62320.621\n",
      "    sample_throughput: 24.513\n",
      "    sample_time_ms: 611907.938\n",
      "    update_time_ms: 2.745\n",
      "  timestamp: 1666683397\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 630000\n",
      "  training_iteration: 42\n",
      "  trial_id: '12173_00000'\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 10.8/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.38 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_4_edf1e7152919a3617c29d944024e5221, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_0_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_3_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_5_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_2_edf1e7152919a3617c29d944024e5221, 0.0/6.0 CPU_group_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_1_edf1e7152919a3617c29d944024e5221)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_figure_eight\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------+----------+----------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                  | status   | loc                  |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------+----------+----------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_AccelEnv-v0_12173_00000 | RUNNING  | 141.225.10.229:30781 |     42 |          11885.5 | 630000 |  142.125 |              486.478 |             -9.00514 |               1500 |\n",
      "+-----------------------------+----------+----------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "Result for PPO_AccelEnv-v0_12173_00000:\n",
      "  agent_timesteps_total: 645000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-25_02-39-34\n",
      "  done: false\n",
      "  episode_len_mean: 1500.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 486.4781022804949\n",
      "  episode_reward_mean: 156.4442791430812\n",
      "  episode_reward_min: 13.163691921492907\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 430\n",
      "  experiment_id: 3b224e0d4aa944209a188d13d8dd894d\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.049999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -2.965360552779699\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015192944702628344\n",
      "          policy_loss: 0.0035628715885203267\n",
      "          total_loss: 136.92533932055457\n",
      "          vf_explained_var: -1.16178544562473e-09\n",
      "          vf_loss: 136.92101675777113\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 645000\n",
      "    num_steps_sampled: 645000\n",
      "    num_steps_trained: 645000\n",
      "  iterations_since_restore: 43\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 16.027272727272727\n",
      "    ram_util_percent: 34.630830039525684\n",
      "  pid: 30781\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08535993761448513\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.439227646950417\n",
      "    mean_inference_ms: 1.7764220329833176\n",
      "    mean_raw_obs_processing_ms: 2.6616018619011026\n",
      "  time_since_restore: 12062.988653421402\n",
      "  time_this_iter_s: 177.44000935554504\n",
      "  time_total_s: 12062.988653421402\n",
      "  timers:\n",
      "    learn_throughput: 238.462\n",
      "    learn_time_ms: 62903.046\n",
      "    sample_throughput: 24.514\n",
      "    sample_time_ms: 611903.516\n",
      "    update_time_ms: 2.772\n",
      "  timestamp: 1666683574\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 645000\n",
      "  training_iteration: 43\n",
      "  trial_id: '12173_00000'\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Memory usage on this node: 10.8/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.38 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_1_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_2_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_0_edf1e7152919a3617c29d944024e5221, 0.0/6.0 CPU_group_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_3_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_4_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_5_edf1e7152919a3617c29d944024e5221, 0.0/1.0 accelerator_type:G)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_figure_eight\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------+----------+----------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                  | status   | loc                  |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------+----------+----------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_AccelEnv-v0_12173_00000 | RUNNING  | 141.225.10.229:30781 |     43 |            12063 | 645000 |  156.444 |              486.478 |              13.1637 |               1500 |\n",
      "+-----------------------------+----------+----------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "Result for PPO_AccelEnv-v0_12173_00000:\n",
      "  agent_timesteps_total: 660000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-25_02-42-26\n",
      "  done: false\n",
      "  episode_len_mean: 1500.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 486.4781022804949\n",
      "  episode_reward_mean: 170.34488339052837\n",
      "  episode_reward_min: 24.081610546216922\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 440\n",
      "  experiment_id: 3b224e0d4aa944209a188d13d8dd894d\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.049999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -2.705441763643491\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.021587295518986995\n",
      "          policy_loss: 0.004167241151681391\n",
      "          total_loss: 137.09896065663483\n",
      "          vf_explained_var: 1.16178544562473e-09\n",
      "          vf_loss: 137.09371396404202\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 660000\n",
      "    num_steps_sampled: 660000\n",
      "    num_steps_trained: 660000\n",
      "  iterations_since_restore: 44\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 16.281224489795918\n",
      "    ram_util_percent: 34.58897959183673\n",
      "  pid: 30781\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08532594769546145\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.440624339494757\n",
      "    mean_inference_ms: 1.774829951648207\n",
      "    mean_raw_obs_processing_ms: 3.4274041247087257\n",
      "  time_since_restore: 12234.144243478775\n",
      "  time_this_iter_s: 171.15559005737305\n",
      "  time_total_s: 12234.144243478775\n",
      "  timers:\n",
      "    learn_throughput: 238.746\n",
      "    learn_time_ms: 62828.38\n",
      "    sample_throughput: 24.514\n",
      "    sample_time_ms: 611888.711\n",
      "    update_time_ms: 2.739\n",
      "  timestamp: 1666683746\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 660000\n",
      "  training_iteration: 44\n",
      "  trial_id: '12173_00000'\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 10.8/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.38 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_1_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_0_edf1e7152919a3617c29d944024e5221, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_5_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_4_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_2_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_3_edf1e7152919a3617c29d944024e5221, 0.0/6.0 CPU_group_edf1e7152919a3617c29d944024e5221)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_figure_eight\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------+----------+----------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                  | status   | loc                  |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------+----------+----------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_AccelEnv-v0_12173_00000 | RUNNING  | 141.225.10.229:30781 |     44 |          12234.1 | 660000 |  170.345 |              486.478 |              24.0816 |               1500 |\n",
      "+-----------------------------+----------+----------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_AccelEnv-v0_12173_00000:\n",
      "  agent_timesteps_total: 675000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-25_02-45-14\n",
      "  done: false\n",
      "  episode_len_mean: 1500.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 486.4781022804949\n",
      "  episode_reward_mean: 187.79565251726413\n",
      "  episode_reward_min: 40.976028713920286\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 450\n",
      "  experiment_id: 3b224e0d4aa944209a188d13d8dd894d\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.049999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -2.6460337907581004\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017197399673183563\n",
      "          policy_loss: 0.004118265000061463\n",
      "          total_loss: 127.83945339978752\n",
      "          vf_explained_var: -1.9699839537423713e-09\n",
      "          vf_loss: 127.83447561425677\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 675000\n",
      "    num_steps_sampled: 675000\n",
      "    num_steps_trained: 675000\n",
      "  iterations_since_restore: 45\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 16.5175\n",
      "    ram_util_percent: 34.57000000000001\n",
      "  pid: 30781\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08529397161737297\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.441970011004265\n",
      "    mean_inference_ms: 1.7733032762687793\n",
      "    mean_raw_obs_processing_ms: 4.176189272630125\n",
      "  time_since_restore: 12402.281136274338\n",
      "  time_this_iter_s: 168.13689279556274\n",
      "  time_total_s: 12402.281136274338\n",
      "  timers:\n",
      "    learn_throughput: 239.837\n",
      "    learn_time_ms: 62542.554\n",
      "    sample_throughput: 24.515\n",
      "    sample_time_ms: 611878.123\n",
      "    update_time_ms: 2.774\n",
      "  timestamp: 1666683914\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 675000\n",
      "  training_iteration: 45\n",
      "  trial_id: '12173_00000'\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 10.8/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.38 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_0_edf1e7152919a3617c29d944024e5221, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_1_edf1e7152919a3617c29d944024e5221, 0.0/6.0 CPU_group_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_5_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_2_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_3_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_4_edf1e7152919a3617c29d944024e5221)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_figure_eight\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------+----------+----------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                  | status   | loc                  |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------+----------+----------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_AccelEnv-v0_12173_00000 | RUNNING  | 141.225.10.229:30781 |     45 |          12402.3 | 675000 |  187.796 |              486.478 |               40.976 |               1500 |\n",
      "+-----------------------------+----------+----------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "Result for PPO_AccelEnv-v0_12173_00000:\n",
      "  agent_timesteps_total: 690000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-25_02-48-03\n",
      "  done: false\n",
      "  episode_len_mean: 1500.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 486.4781022804949\n",
      "  episode_reward_mean: 209.96175633971342\n",
      "  episode_reward_min: 59.41670050806429\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 460\n",
      "  experiment_id: 3b224e0d4aa944209a188d13d8dd894d\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.049999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -2.5122789595086696\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.020868442620158807\n",
      "          policy_loss: 0.003492432251430543\n",
      "          total_loss: 122.6347852803893\n",
      "          vf_explained_var: -3.2833067376003555e-09\n",
      "          vf_loss: 122.63024910749016\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 690000\n",
      "    num_steps_sampled: 690000\n",
      "    num_steps_trained: 690000\n",
      "  iterations_since_restore: 46\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 16.543153526970954\n",
      "    ram_util_percent: 34.60746887966805\n",
      "  pid: 30781\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08526187333373203\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.443204418632813\n",
      "    mean_inference_ms: 1.771843073560449\n",
      "    mean_raw_obs_processing_ms: 4.908724396711944\n",
      "  time_since_restore: 12571.736030578613\n",
      "  time_this_iter_s: 169.4548943042755\n",
      "  time_total_s: 12571.736030578613\n",
      "  timers:\n",
      "    learn_throughput: 239.785\n",
      "    learn_time_ms: 62555.929\n",
      "    sample_throughput: 24.515\n",
      "    sample_time_ms: 611873.739\n",
      "    update_time_ms: 2.753\n",
      "  timestamp: 1666684083\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 690000\n",
      "  training_iteration: 46\n",
      "  trial_id: '12173_00000'\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 10.8/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.38 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_2_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_1_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_0_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_3_edf1e7152919a3617c29d944024e5221, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_5_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_4_edf1e7152919a3617c29d944024e5221, 0.0/6.0 CPU_group_edf1e7152919a3617c29d944024e5221)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_figure_eight\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------+----------+----------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                  | status   | loc                  |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------+----------+----------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_AccelEnv-v0_12173_00000 | RUNNING  | 141.225.10.229:30781 |     46 |          12571.7 | 690000 |  209.962 |              486.478 |              59.4167 |               1500 |\n",
      "+-----------------------------+----------+----------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "Result for PPO_AccelEnv-v0_12173_00000:\n",
      "  agent_timesteps_total: 705000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-25_02-50-54\n",
      "  done: false\n",
      "  episode_len_mean: 1500.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 486.4781022804949\n",
      "  episode_reward_mean: 218.7438728874592\n",
      "  episode_reward_min: 121.53364212827405\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 470\n",
      "  experiment_id: 3b224e0d4aa944209a188d13d8dd894d\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.049999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -2.563617117728217\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0162326985417169\n",
      "          policy_loss: 0.006360932426937556\n",
      "          total_loss: 126.06521255202213\n",
      "          vf_explained_var: -2.6266453456713634e-09\n",
      "          vf_loss: 126.05804011942976\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 705000\n",
      "    num_steps_sampled: 705000\n",
      "    num_steps_trained: 705000\n",
      "  iterations_since_restore: 47\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 16.545491803278686\n",
      "    ram_util_percent: 34.63032786885246\n",
      "  pid: 30781\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08523082706896855\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.44436818341991\n",
      "    mean_inference_ms: 1.770516274604588\n",
      "    mean_raw_obs_processing_ms: 5.62564100139278\n",
      "  time_since_restore: 12742.118448257446\n",
      "  time_this_iter_s: 170.382417678833\n",
      "  time_total_s: 12742.118448257446\n",
      "  timers:\n",
      "    learn_throughput: 238.328\n",
      "    learn_time_ms: 62938.346\n",
      "    sample_throughput: 24.515\n",
      "    sample_time_ms: 611879.853\n",
      "    update_time_ms: 2.694\n",
      "  timestamp: 1666684254\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 705000\n",
      "  training_iteration: 47\n",
      "  trial_id: '12173_00000'\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 10.8/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.38 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_0_edf1e7152919a3617c29d944024e5221, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_4_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_1_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_5_edf1e7152919a3617c29d944024e5221, 0.0/6.0 CPU_group_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_3_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_2_edf1e7152919a3617c29d944024e5221)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_figure_eight\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------+----------+----------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                  | status   | loc                  |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------+----------+----------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_AccelEnv-v0_12173_00000 | RUNNING  | 141.225.10.229:30781 |     47 |          12742.1 | 705000 |  218.744 |              486.478 |              121.534 |               1500 |\n",
      "+-----------------------------+----------+----------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "Result for PPO_AccelEnv-v0_12173_00000:\n",
      "  agent_timesteps_total: 720000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-25_02-53-46\n",
      "  done: false\n",
      "  episode_len_mean: 1500.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 449.04834476146857\n",
      "  episode_reward_mean: 197.4996015877501\n",
      "  episode_reward_min: 121.53364212827405\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 480\n",
      "  experiment_id: 3b224e0d4aa944209a188d13d8dd894d\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.049999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -2.4164635201631968\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.021482754549035418\n",
      "          policy_loss: 0.007712071925777571\n",
      "          total_loss: 129.56247255357644\n",
      "          vf_explained_var: -8.320757842739113e-06\n",
      "          vf_loss: 129.5536866980084\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 720000\n",
      "    num_steps_sampled: 720000\n",
      "    num_steps_trained: 720000\n",
      "  iterations_since_restore: 48\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 16.340408163265305\n",
      "    ram_util_percent: 34.60693877551021\n",
      "  pid: 30781\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08520310083876538\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.4454388354941\n",
      "    mean_inference_ms: 1.7692041659246434\n",
      "    mean_raw_obs_processing_ms: 6.327641889662758\n",
      "  time_since_restore: 12913.978519201279\n",
      "  time_this_iter_s: 171.8600709438324\n",
      "  time_total_s: 12913.978519201279\n",
      "  timers:\n",
      "    learn_throughput: 235.605\n",
      "    learn_time_ms: 63665.777\n",
      "    sample_throughput: 24.515\n",
      "    sample_time_ms: 611877.268\n",
      "    update_time_ms: 2.677\n",
      "  timestamp: 1666684426\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 720000\n",
      "  training_iteration: 48\n",
      "  trial_id: '12173_00000'\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Memory usage on this node: 10.8/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.38 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_2_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_0_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_4_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_5_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_1_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_3_edf1e7152919a3617c29d944024e5221, 0.0/1.0 accelerator_type:G, 0.0/6.0 CPU_group_edf1e7152919a3617c29d944024e5221)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_figure_eight\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------+----------+----------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                  | status   | loc                  |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------+----------+----------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_AccelEnv-v0_12173_00000 | RUNNING  | 141.225.10.229:30781 |     48 |            12914 | 720000 |    197.5 |              449.048 |              121.534 |               1500 |\n",
      "+-----------------------------+----------+----------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "Result for PPO_AccelEnv-v0_12173_00000:\n",
      "  agent_timesteps_total: 735000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-25_02-56-43\n",
      "  done: false\n",
      "  episode_len_mean: 1500.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 386.3727769100448\n",
      "  episode_reward_mean: 187.3033686232457\n",
      "  episode_reward_min: 79.42598000994778\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 490\n",
      "  experiment_id: 3b224e0d4aa944209a188d13d8dd894d\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.049999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -2.5269293088023947\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.020150364729797022\n",
      "          policy_loss: 0.0036224018661522354\n",
      "          total_loss: 148.02461376190186\n",
      "          vf_explained_var: -1.0102482184137784e-09\n",
      "          vf_loss: 148.0199840060735\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 735000\n",
      "    num_steps_sampled: 735000\n",
      "    num_steps_trained: 735000\n",
      "  iterations_since_restore: 49\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 16.214285714285715\n",
      "    ram_util_percent: 34.69761904761904\n",
      "  pid: 30781\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08517989791345017\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.44622553249607\n",
      "    mean_inference_ms: 1.7681371122022598\n",
      "    mean_raw_obs_processing_ms: 7.015357202518145\n",
      "  time_since_restore: 13090.980432033539\n",
      "  time_this_iter_s: 177.00191283226013\n",
      "  time_total_s: 13090.980432033539\n",
      "  timers:\n",
      "    learn_throughput: 231.594\n",
      "    learn_time_ms: 64768.626\n",
      "    sample_throughput: 24.515\n",
      "    sample_time_ms: 611871.893\n",
      "    update_time_ms: 2.61\n",
      "  timestamp: 1666684603\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 735000\n",
      "  training_iteration: 49\n",
      "  trial_id: '12173_00000'\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 10.8/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.38 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_0_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_2_edf1e7152919a3617c29d944024e5221, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_5_edf1e7152919a3617c29d944024e5221, 0.0/6.0 CPU_group_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_1_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_3_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_4_edf1e7152919a3617c29d944024e5221)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_figure_eight\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------+----------+----------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                  | status   | loc                  |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------+----------+----------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_AccelEnv-v0_12173_00000 | RUNNING  | 141.225.10.229:30781 |     49 |            13091 | 735000 |  187.303 |              386.373 |               79.426 |               1500 |\n",
      "+-----------------------------+----------+----------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "Result for PPO_AccelEnv-v0_12173_00000:\n",
      "  agent_timesteps_total: 750000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-25_02-59-46\n",
      "  done: false\n",
      "  episode_len_mean: 1500.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 386.3727769100448\n",
      "  episode_reward_mean: 170.49213778367604\n",
      "  episode_reward_min: 78.3535635417804\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 500\n",
      "  experiment_id: 3b224e0d4aa944209a188d13d8dd894d\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.049999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -2.5842757408901798\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.019214828844331323\n",
      "          policy_loss: 0.007976312794075426\n",
      "          total_loss: 149.03085344928806\n",
      "          vf_explained_var: 1.3638350448985648e-09\n",
      "          vf_loss: 149.02191648644916\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 750000\n",
      "    num_steps_sampled: 750000\n",
      "    num_steps_trained: 750000\n",
      "  iterations_since_restore: 50\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 15.76259541984733\n",
      "    ram_util_percent: 34.7324427480916\n",
      "  pid: 30781\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08516189273088354\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.44688091165931\n",
      "    mean_inference_ms: 1.7672277031100154\n",
      "    mean_raw_obs_processing_ms: 7.689352901792293\n",
      "  time_since_restore: 13274.387931346893\n",
      "  time_this_iter_s: 183.4074993133545\n",
      "  time_total_s: 13274.387931346893\n",
      "  timers:\n",
      "    learn_throughput: 225.871\n",
      "    learn_time_ms: 66409.484\n",
      "    sample_throughput: 24.515\n",
      "    sample_time_ms: 611880.951\n",
      "    update_time_ms: 2.557\n",
      "  timestamp: 1666684786\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 750000\n",
      "  training_iteration: 50\n",
      "  trial_id: '12173_00000'\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 10.8/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.38 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_2_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_5_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_0_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_1_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_3_edf1e7152919a3617c29d944024e5221, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_4_edf1e7152919a3617c29d944024e5221, 0.0/6.0 CPU_group_edf1e7152919a3617c29d944024e5221)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_figure_eight\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------+----------+----------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                  | status   | loc                  |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------+----------+----------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_AccelEnv-v0_12173_00000 | RUNNING  | 141.225.10.229:30781 |     50 |          13274.4 | 750000 |  170.492 |              386.373 |              78.3536 |               1500 |\n",
      "+-----------------------------+----------+----------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "Result for PPO_AccelEnv-v0_12173_00000:\n",
      "  agent_timesteps_total: 765000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-25_03-02-53\n",
      "  done: false\n",
      "  episode_len_mean: 1500.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 386.3727769100448\n",
      "  episode_reward_mean: 167.6912593349686\n",
      "  episode_reward_min: 78.3535635417804\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 510\n",
      "  experiment_id: 3b224e0d4aa944209a188d13d8dd894d\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.049999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -2.7615767963862017\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.025915953239931334\n",
      "          policy_loss: 0.0074593838004363795\n",
      "          total_loss: 144.42601296376372\n",
      "          vf_explained_var: -1.9194716927017907e-09\n",
      "          vf_loss: 144.41725725400246\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 765000\n",
      "    num_steps_sampled: 765000\n",
      "    num_steps_trained: 765000\n",
      "  iterations_since_restore: 51\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 15.671535580524342\n",
      "    ram_util_percent: 34.82209737827716\n",
      "  pid: 30781\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0851439855154031\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.44748848116543\n",
      "    mean_inference_ms: 1.7663619267945458\n",
      "    mean_raw_obs_processing_ms: 8.35014204423247\n",
      "  time_since_restore: 13461.551724672318\n",
      "  time_this_iter_s: 187.1637933254242\n",
      "  time_total_s: 13461.551724672318\n",
      "  timers:\n",
      "    learn_throughput: 219.244\n",
      "    learn_time_ms: 68416.836\n",
      "    sample_throughput: 24.514\n",
      "    sample_time_ms: 611883.948\n",
      "    update_time_ms: 2.522\n",
      "  timestamp: 1666684973\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 765000\n",
      "  training_iteration: 51\n",
      "  trial_id: '12173_00000'\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 10.8/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.38 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_5_edf1e7152919a3617c29d944024e5221, 0.0/6.0 CPU_group_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_2_edf1e7152919a3617c29d944024e5221, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_0_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_3_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_4_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_1_edf1e7152919a3617c29d944024e5221)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_figure_eight\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------+----------+----------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                  | status   | loc                  |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------+----------+----------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_AccelEnv-v0_12173_00000 | RUNNING  | 141.225.10.229:30781 |     51 |          13461.6 | 765000 |  167.691 |              386.373 |              78.3536 |               1500 |\n",
      "+-----------------------------+----------+----------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "Result for PPO_AccelEnv-v0_12173_00000:\n",
      "  agent_timesteps_total: 780000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-25_03-06-02\n",
      "  done: false\n",
      "  episode_len_mean: 1500.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 386.3727769100448\n",
      "  episode_reward_mean: 164.0901889451818\n",
      "  episode_reward_min: 69.12894514365657\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 520\n",
      "  experiment_id: 3b224e0d4aa944209a188d13d8dd894d\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.049999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -2.6408847552234844\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.02113025182003766\n",
      "          policy_loss: 0.012544543649290956\n",
      "          total_loss: 146.8725225125329\n",
      "          vf_explained_var: -4.798679231754477e-09\n",
      "          vf_loss: 146.858921387236\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 780000\n",
      "    num_steps_sampled: 780000\n",
      "    num_steps_trained: 780000\n",
      "  iterations_since_restore: 52\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 15.51740740740741\n",
      "    ram_util_percent: 34.79074074074074\n",
      "  pid: 30781\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08512699773721323\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.448097148893762\n",
      "    mean_inference_ms: 1.7655159315367757\n",
      "    mean_raw_obs_processing_ms: 8.195803376611314\n",
      "  time_since_restore: 13650.32033085823\n",
      "  time_this_iter_s: 188.7686061859131\n",
      "  time_total_s: 13650.32033085823\n",
      "  timers:\n",
      "    learn_throughput: 213.917\n",
      "    learn_time_ms: 70120.509\n",
      "    sample_throughput: 141.052\n",
      "    sample_time_ms: 106343.428\n",
      "    update_time_ms: 2.457\n",
      "  timestamp: 1666685162\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 780000\n",
      "  training_iteration: 52\n",
      "  trial_id: '12173_00000'\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 10.8/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.38 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_0_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_2_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_3_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_4_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_5_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_1_edf1e7152919a3617c29d944024e5221, 0.0/6.0 CPU_group_edf1e7152919a3617c29d944024e5221)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_figure_eight\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------+----------+----------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                  | status   | loc                  |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------+----------+----------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_AccelEnv-v0_12173_00000 | RUNNING  | 141.225.10.229:30781 |     52 |          13650.3 | 780000 |   164.09 |              386.373 |              69.1289 |               1500 |\n",
      "+-----------------------------+----------+----------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "Result for PPO_AccelEnv-v0_12173_00000:\n",
      "  agent_timesteps_total: 795000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-25_03-08-59\n",
      "  done: false\n",
      "  episode_len_mean: 1500.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 386.3727769100448\n",
      "  episode_reward_mean: 164.71889589625553\n",
      "  episode_reward_min: 69.12894514365657\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 530\n",
      "  experiment_id: 3b224e0d4aa944209a188d13d8dd894d\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.049999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -2.4047303967556712\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.02099020143534801\n",
      "          policy_loss: 0.009711445670898499\n",
      "          total_loss: 134.78090636366505\n",
      "          vf_explained_var: -1.9194716927017907e-09\n",
      "          vf_loss: 134.77014530957754\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 795000\n",
      "    num_steps_sampled: 795000\n",
      "    num_steps_trained: 795000\n",
      "  iterations_since_restore: 53\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 16.108333333333334\n",
      "    ram_util_percent: 34.676587301587304\n",
      "  pid: 30781\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.085112538197973\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.448707960812236\n",
      "    mean_inference_ms: 1.7647532764756262\n",
      "    mean_raw_obs_processing_ms: 8.047915782080693\n",
      "  time_since_restore: 13827.470943927765\n",
      "  time_this_iter_s: 177.1506130695343\n",
      "  time_total_s: 13827.470943927765\n",
      "  timers:\n",
      "    learn_throughput: 214.017\n",
      "    learn_time_ms: 70087.838\n",
      "    sample_throughput: 141.047\n",
      "    sample_time_ms: 106347.844\n",
      "    update_time_ms: 2.387\n",
      "  timestamp: 1666685339\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 795000\n",
      "  training_iteration: 53\n",
      "  trial_id: '12173_00000'\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Memory usage on this node: 10.8/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.38 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_4_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_3_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_2_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_5_edf1e7152919a3617c29d944024e5221, 0.0/6.0 CPU_group_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_1_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_0_edf1e7152919a3617c29d944024e5221)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_figure_eight\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------+----------+----------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                  | status   | loc                  |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------+----------+----------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_AccelEnv-v0_12173_00000 | RUNNING  | 141.225.10.229:30781 |     53 |          13827.5 | 795000 |  164.719 |              386.373 |              69.1289 |               1500 |\n",
      "+-----------------------------+----------+----------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "Result for PPO_AccelEnv-v0_12173_00000:\n",
      "  agent_timesteps_total: 810000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-25_03-12-03\n",
      "  done: false\n",
      "  episode_len_mean: 1500.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 386.3727769100448\n",
      "  episode_reward_mean: 162.95522279767553\n",
      "  episode_reward_min: 69.12894514365657\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 540\n",
      "  experiment_id: 3b224e0d4aa944209a188d13d8dd894d\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.049999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -2.3883545616925774\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017957359016219525\n",
      "          policy_loss: 0.011743989199305237\n",
      "          total_loss: 139.01561347347194\n",
      "          vf_explained_var: -4.748166748669291e-09\n",
      "          vf_loss: 139.00297130164452\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 810000\n",
      "    num_steps_sampled: 810000\n",
      "    num_steps_trained: 810000\n",
      "  iterations_since_restore: 54\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 15.798473282442751\n",
      "    ram_util_percent: 34.76259541984733\n",
      "  pid: 30781\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08510087163063322\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.449250843242854\n",
      "    mean_inference_ms: 1.7640830011645425\n",
      "    mean_raw_obs_processing_ms: 7.906014065902764\n",
      "  time_since_restore: 14010.629807472229\n",
      "  time_this_iter_s: 183.1588635444641\n",
      "  time_total_s: 14010.629807472229\n",
      "  timers:\n",
      "    learn_throughput: 210.397\n",
      "    learn_time_ms: 71293.937\n",
      "    sample_throughput: 141.054\n",
      "    sample_time_ms: 106342.022\n",
      "    update_time_ms: 2.457\n",
      "  timestamp: 1666685523\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 810000\n",
      "  training_iteration: 54\n",
      "  trial_id: '12173_00000'\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 10.8/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.38 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_5_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_1_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_3_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_0_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_2_edf1e7152919a3617c29d944024e5221, 0.0/6.0 CPU_group_edf1e7152919a3617c29d944024e5221, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_4_edf1e7152919a3617c29d944024e5221)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_figure_eight\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------+----------+----------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                  | status   | loc                  |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------+----------+----------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_AccelEnv-v0_12173_00000 | RUNNING  | 141.225.10.229:30781 |     54 |          14010.6 | 810000 |  162.955 |              386.373 |              69.1289 |               1500 |\n",
      "+-----------------------------+----------+----------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_AccelEnv-v0_12173_00000:\n",
      "  agent_timesteps_total: 825000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-25_03-15-10\n",
      "  done: false\n",
      "  episode_len_mean: 1500.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 386.3727769100448\n",
      "  episode_reward_mean: 149.32280161102327\n",
      "  episode_reward_min: 55.435630364902735\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 550\n",
      "  experiment_id: 3b224e0d4aa944209a188d13d8dd894d\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.049999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -2.512847490431899\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.021694547084941677\n",
      "          policy_loss: 0.00794996085016341\n",
      "          total_loss: 157.71838652723926\n",
      "          vf_explained_var: -1.4194593859429006e-05\n",
      "          vf_loss: 157.70935207625567\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 825000\n",
      "    num_steps_sampled: 825000\n",
      "    num_steps_trained: 825000\n",
      "  iterations_since_restore: 55\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 15.598876404494384\n",
      "    ram_util_percent: 34.849438202247185\n",
      "  pid: 30781\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08508773438372103\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.449758712169274\n",
      "    mean_inference_ms: 1.7634460156170055\n",
      "    mean_raw_obs_processing_ms: 7.769754277620173\n",
      "  time_since_restore: 14197.886409759521\n",
      "  time_this_iter_s: 187.25660228729248\n",
      "  time_total_s: 14197.886409759521\n",
      "  timers:\n",
      "    learn_throughput: 204.871\n",
      "    learn_time_ms: 73216.662\n",
      "    sample_throughput: 141.069\n",
      "    sample_time_ms: 106331.203\n",
      "    update_time_ms: 2.475\n",
      "  timestamp: 1666685710\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 825000\n",
      "  training_iteration: 55\n",
      "  trial_id: '12173_00000'\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 10.8/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.38 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_5_edf1e7152919a3617c29d944024e5221, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_4_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_0_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_1_edf1e7152919a3617c29d944024e5221, 0.0/6.0 CPU_group_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_2_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_3_edf1e7152919a3617c29d944024e5221)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_figure_eight\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------+----------+----------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                  | status   | loc                  |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------+----------+----------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_AccelEnv-v0_12173_00000 | RUNNING  | 141.225.10.229:30781 |     55 |          14197.9 | 825000 |  149.323 |              386.373 |              55.4356 |               1500 |\n",
      "+-----------------------------+----------+----------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "Result for PPO_AccelEnv-v0_12173_00000:\n",
      "  agent_timesteps_total: 840000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-25_03-18-13\n",
      "  done: false\n",
      "  episode_len_mean: 1500.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 292.47781650261953\n",
      "  episode_reward_mean: 128.0132790683096\n",
      "  episode_reward_min: 43.26948130169006\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 560\n",
      "  experiment_id: 3b224e0d4aa944209a188d13d8dd894d\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.049999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -2.4311220983327444\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.021984937524874987\n",
      "          policy_loss: 0.010591644074736258\n",
      "          total_loss: 159.8923472388316\n",
      "          vf_explained_var: -4.899703753835638e-09\n",
      "          vf_loss: 159.88065579786138\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 840000\n",
      "    num_steps_sampled: 840000\n",
      "    num_steps_trained: 840000\n",
      "  iterations_since_restore: 56\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 15.796564885496183\n",
      "    ram_util_percent: 34.83473282442748\n",
      "  pid: 30781\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08507563327915385\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.450277488878132\n",
      "    mean_inference_ms: 1.7628331455134738\n",
      "    mean_raw_obs_processing_ms: 7.638826270998352\n",
      "  time_since_restore: 14381.207845687866\n",
      "  time_this_iter_s: 183.32143592834473\n",
      "  time_total_s: 14381.207845687866\n",
      "  timers:\n",
      "    learn_throughput: 201.063\n",
      "    learn_time_ms: 74603.571\n",
      "    sample_throughput: 141.069\n",
      "    sample_time_ms: 106331.262\n",
      "    update_time_ms: 2.519\n",
      "  timestamp: 1666685893\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 840000\n",
      "  training_iteration: 56\n",
      "  trial_id: '12173_00000'\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 10.8/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.38 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_3_edf1e7152919a3617c29d944024e5221, 0.0/6.0 CPU_group_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_0_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_2_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_4_edf1e7152919a3617c29d944024e5221, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_5_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_1_edf1e7152919a3617c29d944024e5221)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_figure_eight\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------+----------+----------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                  | status   | loc                  |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------+----------+----------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_AccelEnv-v0_12173_00000 | RUNNING  | 141.225.10.229:30781 |     56 |          14381.2 | 840000 |  128.013 |              292.478 |              43.2695 |               1500 |\n",
      "+-----------------------------+----------+----------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "Result for PPO_AccelEnv-v0_12173_00000:\n",
      "  agent_timesteps_total: 855000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-25_03-21-17\n",
      "  done: false\n",
      "  episode_len_mean: 1500.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 234.01459193189223\n",
      "  episode_reward_mean: 112.49016257262845\n",
      "  episode_reward_min: 43.26948130169006\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 570\n",
      "  experiment_id: 3b224e0d4aa944209a188d13d8dd894d\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.049999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -2.4227769273822592\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.04086021257801718\n",
      "          policy_loss: 0.014051530285643698\n",
      "          total_loss: 160.1827066971084\n",
      "          vf_explained_var: -7.447650659742067e-06\n",
      "          vf_loss: 160.1666117522676\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 855000\n",
      "    num_steps_sampled: 855000\n",
      "    num_steps_trained: 855000\n",
      "  iterations_since_restore: 57\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 15.811068702290076\n",
      "    ram_util_percent: 34.777480916030534\n",
      "  pid: 30781\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08506235579871431\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.450825263971186\n",
      "    mean_inference_ms: 1.762178048257624\n",
      "    mean_raw_obs_processing_ms: 7.51288582531045\n",
      "  time_since_restore: 14564.95849275589\n",
      "  time_this_iter_s: 183.75064706802368\n",
      "  time_total_s: 14564.95849275589\n",
      "  timers:\n",
      "    learn_throughput: 197.491\n",
      "    learn_time_ms: 75952.85\n",
      "    sample_throughput: 141.085\n",
      "    sample_time_ms: 106318.819\n",
      "    update_time_ms: 2.521\n",
      "  timestamp: 1666686077\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 855000\n",
      "  training_iteration: 57\n",
      "  trial_id: '12173_00000'\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 10.8/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.38 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_3_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_0_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_2_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_1_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_4_edf1e7152919a3617c29d944024e5221, 0.0/6.0 CPU_group_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_5_edf1e7152919a3617c29d944024e5221)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_figure_eight\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------+----------+----------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                  | status   | loc                  |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------+----------+----------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_AccelEnv-v0_12173_00000 | RUNNING  | 141.225.10.229:30781 |     57 |            14565 | 855000 |   112.49 |              234.015 |              43.2695 |               1500 |\n",
      "+-----------------------------+----------+----------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "Result for PPO_AccelEnv-v0_12173_00000:\n",
      "  agent_timesteps_total: 870000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-25_03-24-22\n",
      "  done: false\n",
      "  episode_len_mean: 1500.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 234.01459193189223\n",
      "  episode_reward_mean: 107.85742616807669\n",
      "  episode_reward_min: 43.26948130169006\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 580\n",
      "  experiment_id: 3b224e0d4aa944209a188d13d8dd894d\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.075\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -2.6584747132608446\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0267116194462674\n",
      "          policy_loss: 0.014472772314418424\n",
      "          total_loss: 156.32138068958864\n",
      "          vf_explained_var: -7.517105586885009e-06\n",
      "          vf_loss: 156.30490486338988\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 870000\n",
      "    num_steps_sampled: 870000\n",
      "    num_steps_trained: 870000\n",
      "  iterations_since_restore: 58\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 15.71634980988593\n",
      "    ram_util_percent: 34.815589353612175\n",
      "  pid: 30781\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08505022867799004\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.451272307668525\n",
      "    mean_inference_ms: 1.7616078728867377\n",
      "    mean_raw_obs_processing_ms: 7.39162995298959\n",
      "  time_since_restore: 14749.323742866516\n",
      "  time_this_iter_s: 184.36525011062622\n",
      "  time_total_s: 14749.323742866516\n",
      "  timers:\n",
      "    learn_throughput: 194.255\n",
      "    learn_time_ms: 77217.976\n",
      "    sample_throughput: 141.105\n",
      "    sample_time_ms: 106304.059\n",
      "    update_time_ms: 2.464\n",
      "  timestamp: 1666686262\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 870000\n",
      "  training_iteration: 58\n",
      "  trial_id: '12173_00000'\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 10.8/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.38 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_0_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_5_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_1_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_2_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_4_edf1e7152919a3617c29d944024e5221, 0.0/6.0 CPU_group_edf1e7152919a3617c29d944024e5221, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_3_edf1e7152919a3617c29d944024e5221)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_figure_eight\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------+----------+----------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                  | status   | loc                  |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------+----------+----------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_AccelEnv-v0_12173_00000 | RUNNING  | 141.225.10.229:30781 |     58 |          14749.3 | 870000 |  107.857 |              234.015 |              43.2695 |               1500 |\n",
      "+-----------------------------+----------+----------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "Result for PPO_AccelEnv-v0_12173_00000:\n",
      "  agent_timesteps_total: 885000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-25_03-27-28\n",
      "  done: false\n",
      "  episode_len_mean: 1500.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 234.01459193189223\n",
      "  episode_reward_mean: 108.6313803739387\n",
      "  episode_reward_min: 43.26948130169006\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 590\n",
      "  experiment_id: 3b224e0d4aa944209a188d13d8dd894d\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.075\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -2.671130964715602\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.024643317517731914\n",
      "          policy_loss: 0.01740787229895339\n",
      "          total_loss: 155.5494127047264\n",
      "          vf_explained_var: -4.697654265584106e-09\n",
      "          vf_loss: 155.5301569728528\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 885000\n",
      "    num_steps_sampled: 885000\n",
      "    num_steps_trained: 885000\n",
      "  iterations_since_restore: 59\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 15.553759398496242\n",
      "    ram_util_percent: 34.87894736842105\n",
      "  pid: 30781\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08503567039425125\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.45182519664568\n",
      "    mean_inference_ms: 1.7609591254063017\n",
      "    mean_raw_obs_processing_ms: 7.274837355833809\n",
      "  time_since_restore: 14935.491589307785\n",
      "  time_this_iter_s: 186.16784644126892\n",
      "  time_total_s: 14935.491589307785\n",
      "  timers:\n",
      "    learn_throughput: 191.97\n",
      "    learn_time_ms: 78137.21\n",
      "    sample_throughput: 141.108\n",
      "    sample_time_ms: 106301.856\n",
      "    update_time_ms: 2.447\n",
      "  timestamp: 1666686448\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 885000\n",
      "  training_iteration: 59\n",
      "  trial_id: '12173_00000'\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 10.8/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.38 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_4_edf1e7152919a3617c29d944024e5221, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_3_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_0_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_5_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_2_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_1_edf1e7152919a3617c29d944024e5221, 0.0/6.0 CPU_group_edf1e7152919a3617c29d944024e5221)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_figure_eight\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------+----------+----------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                  | status   | loc                  |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------+----------+----------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_AccelEnv-v0_12173_00000 | RUNNING  | 141.225.10.229:30781 |     59 |          14935.5 | 885000 |  108.631 |              234.015 |              43.2695 |               1500 |\n",
      "+-----------------------------+----------+----------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "Result for PPO_AccelEnv-v0_12173_00000:\n",
      "  agent_timesteps_total: 900000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-25_03-30-37\n",
      "  done: false\n",
      "  episode_len_mean: 1500.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 234.01459193189223\n",
      "  episode_reward_mean: 111.08377565586451\n",
      "  episode_reward_min: 43.26948130169006\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 600\n",
      "  experiment_id: 3b224e0d4aa944209a188d13d8dd894d\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.075\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -2.88931962372893\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.036140970802290076\n",
      "          policy_loss: 0.014294145476088812\n",
      "          total_loss: 158.33476496227718\n",
      "          vf_explained_var: -3.990480390569928e-09\n",
      "          vf_loss: 158.31776004080044\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 900000\n",
      "    num_steps_sampled: 900000\n",
      "    num_steps_trained: 900000\n",
      "  iterations_since_restore: 60\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 15.536666666666665\n",
      "    ram_util_percent: 34.84481481481481\n",
      "  pid: 30781\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08501941587323365\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.452466277175475\n",
      "    mean_inference_ms: 1.7602528334193017\n",
      "    mean_raw_obs_processing_ms: 7.162264045304085\n",
      "  time_since_restore: 15124.638340234756\n",
      "  time_this_iter_s: 189.14675092697144\n",
      "  time_total_s: 15124.638340234756\n",
      "  timers:\n",
      "    learn_throughput: 190.593\n",
      "    learn_time_ms: 78701.94\n",
      "    sample_throughput: 141.096\n",
      "    sample_time_ms: 106310.562\n",
      "    update_time_ms: 2.473\n",
      "  timestamp: 1666686637\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 900000\n",
      "  training_iteration: 60\n",
      "  trial_id: '12173_00000'\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Memory usage on this node: 10.8/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.38 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 accelerator_type:G, 0.0/6.0 CPU_group_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_4_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_3_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_1_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_0_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_5_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_2_edf1e7152919a3617c29d944024e5221)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_figure_eight\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------+----------+----------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                  | status   | loc                  |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------+----------+----------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_AccelEnv-v0_12173_00000 | RUNNING  | 141.225.10.229:30781 |     60 |          15124.6 | 900000 |  111.084 |              234.015 |              43.2695 |               1500 |\n",
      "+-----------------------------+----------+----------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "Result for PPO_AccelEnv-v0_12173_00000:\n",
      "  agent_timesteps_total: 915000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-25_03-33-50\n",
      "  done: false\n",
      "  episode_len_mean: 1500.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 234.01459193189223\n",
      "  episode_reward_mean: 110.14763194245485\n",
      "  episode_reward_min: 43.26948130169006\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 610\n",
      "  experiment_id: 3b224e0d4aa944209a188d13d8dd894d\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.075\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -2.526347404617374\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.021756986810893112\n",
      "          policy_loss: 0.013954158618225385\n",
      "          total_loss: 153.41098081944352\n",
      "          vf_explained_var: -5.505852662679445e-09\n",
      "          vf_loss: 153.395394975048\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 915000\n",
      "    num_steps_sampled: 915000\n",
      "    num_steps_trained: 915000\n",
      "  iterations_since_restore: 61\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 15.316058394160585\n",
      "    ram_util_percent: 34.885401459854016\n",
      "  pid: 30781\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08500356769492384\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.4530984117743\n",
      "    mean_inference_ms: 1.7595297706677837\n",
      "    mean_raw_obs_processing_ms: 7.0537062731366\n",
      "  time_since_restore: 15317.209177017212\n",
      "  time_this_iter_s: 192.57083678245544\n",
      "  time_total_s: 15317.209177017212\n",
      "  timers:\n",
      "    learn_throughput: 189.287\n",
      "    learn_time_ms: 79244.694\n",
      "    sample_throughput: 141.099\n",
      "    sample_time_ms: 106308.259\n",
      "    update_time_ms: 2.458\n",
      "  timestamp: 1666686830\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 915000\n",
      "  training_iteration: 61\n",
      "  trial_id: '12173_00000'\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 10.8/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.38 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_5_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_0_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_4_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_3_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_1_edf1e7152919a3617c29d944024e5221, 0.0/6.0 CPU_group_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_2_edf1e7152919a3617c29d944024e5221)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_figure_eight\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------+----------+----------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                  | status   | loc                  |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------+----------+----------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_AccelEnv-v0_12173_00000 | RUNNING  | 141.225.10.229:30781 |     61 |          15317.2 | 915000 |  110.148 |              234.015 |              43.2695 |               1500 |\n",
      "+-----------------------------+----------+----------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_AccelEnv-v0_12173_00000:\n",
      "  agent_timesteps_total: 930000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-25_03-36-59\n",
      "  done: false\n",
      "  episode_len_mean: 1500.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 234.01459193189223\n",
      "  episode_reward_mean: 108.61024628908646\n",
      "  episode_reward_min: 43.26948130169006\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 620\n",
      "  experiment_id: 3b224e0d4aa944209a188d13d8dd894d\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.075\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -2.541002504300263\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.030523048374965837\n",
      "          policy_loss: 0.018790679755699584\n",
      "          total_loss: 165.05032569109383\n",
      "          vf_explained_var: -5.253290691342727e-09\n",
      "          vf_loss: 165.02924575159105\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 930000\n",
      "    num_steps_sampled: 930000\n",
      "    num_steps_trained: 930000\n",
      "  iterations_since_restore: 62\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 15.352767527675278\n",
      "    ram_util_percent: 34.869372693726945\n",
      "  pid: 30781\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08498837660174885\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.453663952621667\n",
      "    mean_inference_ms: 1.7588494620977173\n",
      "    mean_raw_obs_processing_ms: 6.9489444114316985\n",
      "  time_since_restore: 15506.735834121704\n",
      "  time_this_iter_s: 189.5266571044922\n",
      "  time_total_s: 15506.735834121704\n",
      "  timers:\n",
      "    learn_throughput: 189.099\n",
      "    learn_time_ms: 79323.471\n",
      "    sample_throughput: 141.103\n",
      "    sample_time_ms: 106305.202\n",
      "    update_time_ms: 2.509\n",
      "  timestamp: 1666687019\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 930000\n",
      "  training_iteration: 62\n",
      "  trial_id: '12173_00000'\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 10.8/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.38 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_5_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_1_edf1e7152919a3617c29d944024e5221, 0.0/6.0 CPU_group_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_3_edf1e7152919a3617c29d944024e5221, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_0_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_2_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_4_edf1e7152919a3617c29d944024e5221)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_figure_eight\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------+----------+----------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                  | status   | loc                  |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------+----------+----------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_AccelEnv-v0_12173_00000 | RUNNING  | 141.225.10.229:30781 |     62 |          15506.7 | 930000 |   108.61 |              234.015 |              43.2695 |               1500 |\n",
      "+-----------------------------+----------+----------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "Result for PPO_AccelEnv-v0_12173_00000:\n",
      "  agent_timesteps_total: 945000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-25_03-40-13\n",
      "  done: false\n",
      "  episode_len_mean: 1500.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 205.80074723501545\n",
      "  episode_reward_mean: 106.14481177369578\n",
      "  episode_reward_min: 43.26948130169006\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 630\n",
      "  experiment_id: 3b224e0d4aa944209a188d13d8dd894d\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.075\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -2.7362569566500388\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.03207678201354841\n",
      "          policy_loss: 0.02291942397961235\n",
      "          total_loss: 156.52843239671094\n",
      "          vf_explained_var: -6.4150760259451545e-09\n",
      "          vf_loss: 156.5031074944189\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 945000\n",
      "    num_steps_sampled: 945000\n",
      "    num_steps_trained: 945000\n",
      "  iterations_since_restore: 63\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 15.280505415162454\n",
      "    ram_util_percent: 34.922021660649825\n",
      "  pid: 30781\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0849716427154452\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.4542824721149\n",
      "    mean_inference_ms: 1.7581549732099\n",
      "    mean_raw_obs_processing_ms: 6.847815769465926\n",
      "  time_since_restore: 15700.819668531418\n",
      "  time_this_iter_s: 194.08383440971375\n",
      "  time_total_s: 15700.819668531418\n",
      "  timers:\n",
      "    learn_throughput: 185.173\n",
      "    learn_time_ms: 81005.487\n",
      "    sample_throughput: 141.089\n",
      "    sample_time_ms: 106316.099\n",
      "    update_time_ms: 2.554\n",
      "  timestamp: 1666687213\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 945000\n",
      "  training_iteration: 63\n",
      "  trial_id: '12173_00000'\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 10.8/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.38 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_2_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_3_edf1e7152919a3617c29d944024e5221, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_1_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_5_edf1e7152919a3617c29d944024e5221, 0.0/6.0 CPU_group_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_4_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_0_edf1e7152919a3617c29d944024e5221)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_figure_eight\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------+----------+----------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                  | status   | loc                  |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------+----------+----------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_AccelEnv-v0_12173_00000 | RUNNING  | 141.225.10.229:30781 |     63 |          15700.8 | 945000 |  106.145 |              205.801 |              43.2695 |               1500 |\n",
      "+-----------------------------+----------+----------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "Result for PPO_AccelEnv-v0_12173_00000:\n",
      "  agent_timesteps_total: 960000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-25_03-43-34\n",
      "  done: false\n",
      "  episode_len_mean: 1500.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 205.80074723501545\n",
      "  episode_reward_mean: 109.20233492215658\n",
      "  episode_reward_min: 43.26948130169006\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 640\n",
      "  experiment_id: 3b224e0d4aa944209a188d13d8dd894d\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.075\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -3.1235099160065087\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.04368729607046264\n",
      "          policy_loss: 0.019611191528580957\n",
      "          total_loss: 153.15105113983154\n",
      "          vf_explained_var: -2.778182572882315e-09\n",
      "          vf_loss: 153.1281634023634\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 960000\n",
      "    num_steps_sampled: 960000\n",
      "    num_steps_trained: 960000\n",
      "  iterations_since_restore: 64\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 14.976573426573427\n",
      "    ram_util_percent: 34.95\n",
      "  pid: 30781\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0849533569474567\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.45494390247901\n",
      "    mean_inference_ms: 1.7574130605484601\n",
      "    mean_raw_obs_processing_ms: 6.750131331929468\n",
      "  time_since_restore: 15900.991700649261\n",
      "  time_this_iter_s: 200.17203211784363\n",
      "  time_total_s: 15900.991700649261\n",
      "  timers:\n",
      "    learn_throughput: 181.372\n",
      "    learn_time_ms: 82702.73\n",
      "    sample_throughput: 141.082\n",
      "    sample_time_ms: 106320.826\n",
      "    update_time_ms: 2.497\n",
      "  timestamp: 1666687414\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 960000\n",
      "  training_iteration: 64\n",
      "  trial_id: '12173_00000'\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 10.8/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.38 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_3_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_0_edf1e7152919a3617c29d944024e5221, 0.0/6.0 CPU_group_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_4_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_5_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_2_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_1_edf1e7152919a3617c29d944024e5221, 0.0/1.0 accelerator_type:G)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_figure_eight\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------+----------+----------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                  | status   | loc                  |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------+----------+----------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_AccelEnv-v0_12173_00000 | RUNNING  | 141.225.10.229:30781 |     64 |            15901 | 960000 |  109.202 |              205.801 |              43.2695 |               1500 |\n",
      "+-----------------------------+----------+----------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "Result for PPO_AccelEnv-v0_12173_00000:\n",
      "  agent_timesteps_total: 975000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-25_03-47-06\n",
      "  done: false\n",
      "  episode_len_mean: 1500.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 304.27238214435056\n",
      "  episode_reward_mean: 116.15541326505719\n",
      "  episode_reward_min: 31.713876526043627\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 650\n",
      "  experiment_id: 3b224e0d4aa944209a188d13d8dd894d\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.11250000000000003\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -2.568899647462166\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.023835078229184764\n",
      "          policy_loss: 0.015432520738755494\n",
      "          total_loss: 158.1611469268799\n",
      "          vf_explained_var: 8.587109912028268e-10\n",
      "          vf_loss: 158.1430328078189\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 975000\n",
      "    num_steps_sampled: 975000\n",
      "    num_steps_trained: 975000\n",
      "  iterations_since_restore: 65\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 14.591749174917489\n",
      "    ram_util_percent: 35.172937293729376\n",
      "  pid: 30781\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08493686679011082\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.455633313408743\n",
      "    mean_inference_ms: 1.7566794429813695\n",
      "    mean_raw_obs_processing_ms: 6.655684063340925\n",
      "  time_since_restore: 16113.640412092209\n",
      "  time_this_iter_s: 212.6487114429474\n",
      "  time_total_s: 16113.640412092209\n",
      "  timers:\n",
      "    learn_throughput: 175.984\n",
      "    learn_time_ms: 85234.876\n",
      "    sample_throughput: 141.073\n",
      "    sample_time_ms: 106327.837\n",
      "    update_time_ms: 2.516\n",
      "  timestamp: 1666687626\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 975000\n",
      "  training_iteration: 65\n",
      "  trial_id: '12173_00000'\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 10.8/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.38 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_1_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_3_edf1e7152919a3617c29d944024e5221, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_4_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_0_edf1e7152919a3617c29d944024e5221, 0.0/6.0 CPU_group_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_2_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_5_edf1e7152919a3617c29d944024e5221)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_figure_eight\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------+----------+----------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                  | status   | loc                  |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------+----------+----------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_AccelEnv-v0_12173_00000 | RUNNING  | 141.225.10.229:30781 |     65 |          16113.6 | 975000 |  116.155 |              304.272 |              31.7139 |               1500 |\n",
      "+-----------------------------+----------+----------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "Result for PPO_AccelEnv-v0_12173_00000:\n",
      "  agent_timesteps_total: 990000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-25_03-50-48\n",
      "  done: false\n",
      "  episode_len_mean: 1500.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 304.27238214435056\n",
      "  episode_reward_mean: 124.42455624484022\n",
      "  episode_reward_min: 31.713876526043627\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 660\n",
      "  experiment_id: 3b224e0d4aa944209a188d13d8dd894d\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.11250000000000003\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -2.8339453925520686\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.027860045490660443\n",
      "          policy_loss: 0.016399591796665444\n",
      "          total_loss: 154.76234318684723\n",
      "          vf_explained_var: -2.222546147123694e-09\n",
      "          vf_loss: 154.74280937000856\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 990000\n",
      "    num_steps_sampled: 990000\n",
      "    num_steps_trained: 990000\n",
      "  iterations_since_restore: 66\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 14.238924050632912\n",
      "    ram_util_percent: 35.1496835443038\n",
      "  pid: 30781\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08492031980678838\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.456395726625313\n",
      "    mean_inference_ms: 1.7559440451213901\n",
      "    mean_raw_obs_processing_ms: 6.5643502352357235\n",
      "  time_since_restore: 16335.158529281616\n",
      "  time_this_iter_s: 221.51811718940735\n",
      "  time_total_s: 16335.158529281616\n",
      "  timers:\n",
      "    learn_throughput: 168.475\n",
      "    learn_time_ms: 89033.759\n",
      "    sample_throughput: 141.046\n",
      "    sample_time_ms: 106348.285\n",
      "    update_time_ms: 2.547\n",
      "  timestamp: 1666687848\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 990000\n",
      "  training_iteration: 66\n",
      "  trial_id: '12173_00000'\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 10.8/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.38 GiB heap, 0.0/14.65 GiB objects (0.0/6.0 CPU_group_edf1e7152919a3617c29d944024e5221, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_5_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_0_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_4_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_2_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_3_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_1_edf1e7152919a3617c29d944024e5221)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_figure_eight\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------+----------+----------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                  | status   | loc                  |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------+----------+----------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_AccelEnv-v0_12173_00000 | RUNNING  | 141.225.10.229:30781 |     66 |          16335.2 | 990000 |  124.425 |              304.272 |              31.7139 |               1500 |\n",
      "+-----------------------------+----------+----------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "Result for PPO_AccelEnv-v0_12173_00000:\n",
      "  agent_timesteps_total: 1005000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-25_03-54-40\n",
      "  done: false\n",
      "  episode_len_mean: 1500.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 304.27238214435056\n",
      "  episode_reward_mean: 129.6591644418494\n",
      "  episode_reward_min: 31.713876526043627\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 670\n",
      "  experiment_id: 3b224e0d4aa944209a188d13d8dd894d\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.11250000000000003\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -2.69063178058398\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.026956901303690106\n",
      "          policy_loss: 0.01533913745197578\n",
      "          total_loss: 158.08256967835507\n",
      "          vf_explained_var: -3.030744544219033e-10\n",
      "          vf_loss: 158.06419757261114\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1005000\n",
      "    num_steps_sampled: 1005000\n",
      "    num_steps_trained: 1005000\n",
      "  iterations_since_restore: 67\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 13.532024169184288\n",
      "    ram_util_percent: 35.186706948640484\n",
      "  pid: 30781\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08490580107007777\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.457091196340112\n",
      "    mean_inference_ms: 1.755255058603395\n",
      "    mean_raw_obs_processing_ms: 6.476023450307422\n",
      "  time_since_restore: 16567.234411239624\n",
      "  time_this_iter_s: 232.0758819580078\n",
      "  time_total_s: 16567.234411239624\n",
      "  timers:\n",
      "    learn_throughput: 159.819\n",
      "    learn_time_ms: 93856.052\n",
      "    sample_throughput: 141.033\n",
      "    sample_time_ms: 106358.446\n",
      "    update_time_ms: 2.575\n",
      "  timestamp: 1666688080\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1005000\n",
      "  training_iteration: 67\n",
      "  trial_id: '12173_00000'\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Memory usage on this node: 10.8/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.38 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_3_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_1_edf1e7152919a3617c29d944024e5221, 0.0/1.0 accelerator_type:G, 0.0/6.0 CPU_group_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_4_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_5_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_2_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_0_edf1e7152919a3617c29d944024e5221)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_figure_eight\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                  | status   | loc                  |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_AccelEnv-v0_12173_00000 | RUNNING  | 141.225.10.229:30781 |     67 |          16567.2 | 1005000 |  129.659 |              304.272 |              31.7139 |               1500 |\n",
      "+-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "Result for PPO_AccelEnv-v0_12173_00000:\n",
      "  agent_timesteps_total: 1020000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-25_03-58-30\n",
      "  done: false\n",
      "  episode_len_mean: 1500.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 304.27238214435056\n",
      "  episode_reward_mean: 140.31098697036376\n",
      "  episode_reward_min: 31.713876526043627\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 680\n",
      "  experiment_id: 3b224e0d4aa944209a188d13d8dd894d\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.11250000000000003\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -2.790387046337128\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.05243049053972526\n",
      "          policy_loss: 0.02378725054105586\n",
      "          total_loss: 144.87039836948202\n",
      "          vf_explained_var: -2.5256206015455973e-09\n",
      "          vf_loss: 144.84071282855535\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1020000\n",
      "    num_steps_sampled: 1020000\n",
      "    num_steps_trained: 1020000\n",
      "  iterations_since_restore: 68\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 13.961702127659573\n",
      "    ram_util_percent: 35.30516717325228\n",
      "  pid: 30781\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08489076636705761\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.457850000919176\n",
      "    mean_inference_ms: 1.754499800393252\n",
      "    mean_raw_obs_processing_ms: 6.390525279850127\n",
      "  time_since_restore: 16797.51477909088\n",
      "  time_this_iter_s: 230.28036785125732\n",
      "  time_total_s: 16797.51477909088\n",
      "  timers:\n",
      "    learn_throughput: 152.386\n",
      "    learn_time_ms: 98434.467\n",
      "    sample_throughput: 141.015\n",
      "    sample_time_ms: 106371.713\n",
      "    update_time_ms: 2.641\n",
      "  timestamp: 1666688310\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1020000\n",
      "  training_iteration: 68\n",
      "  trial_id: '12173_00000'\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 10.9/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.38 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_0_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_2_edf1e7152919a3617c29d944024e5221, 0.0/6.0 CPU_group_edf1e7152919a3617c29d944024e5221, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_3_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_4_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_1_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_5_edf1e7152919a3617c29d944024e5221)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_figure_eight\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                  | status   | loc                  |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_AccelEnv-v0_12173_00000 | RUNNING  | 141.225.10.229:30781 |     68 |          16797.5 | 1020000 |  140.311 |              304.272 |              31.7139 |               1500 |\n",
      "+-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_AccelEnv-v0_12173_00000:\n",
      "  agent_timesteps_total: 1035000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-25_04-02-21\n",
      "  done: false\n",
      "  episode_len_mean: 1500.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 304.27238214435056\n",
      "  episode_reward_mean: 144.06432350060476\n",
      "  episode_reward_min: 31.713876526043627\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 690\n",
      "  experiment_id: 3b224e0d4aa944209a188d13d8dd894d\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.16874999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -2.6522083735061903\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.030968490320922058\n",
      "          policy_loss: 0.02470140412047316\n",
      "          total_loss: 162.21554996684446\n",
      "          vf_explained_var: -2.677157828756549e-09\n",
      "          vf_loss: 162.18562269695735\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1035000\n",
      "    num_steps_sampled: 1035000\n",
      "    num_steps_trained: 1035000\n",
      "  iterations_since_restore: 69\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 13.915151515151516\n",
      "    ram_util_percent: 35.28333333333333\n",
      "  pid: 30781\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08487621588321194\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.458557054651134\n",
      "    mean_inference_ms: 1.7537977226765908\n",
      "    mean_raw_obs_processing_ms: 6.30768716367852\n",
      "  time_since_restore: 17028.535399913788\n",
      "  time_this_iter_s: 231.0206208229065\n",
      "  time_total_s: 17028.535399913788\n",
      "  timers:\n",
      "    learn_throughput: 145.743\n",
      "    learn_time_ms: 102920.986\n",
      "    sample_throughput: 141.017\n",
      "    sample_time_ms: 106370.399\n",
      "    update_time_ms: 2.638\n",
      "  timestamp: 1666688541\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1035000\n",
      "  training_iteration: 69\n",
      "  trial_id: '12173_00000'\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 10.9/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.38 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_3_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_0_edf1e7152919a3617c29d944024e5221, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_1_edf1e7152919a3617c29d944024e5221, 0.0/6.0 CPU_group_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_2_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_5_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_4_edf1e7152919a3617c29d944024e5221)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_figure_eight\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                  | status   | loc                  |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_AccelEnv-v0_12173_00000 | RUNNING  | 141.225.10.229:30781 |     69 |          17028.5 | 1035000 |  144.064 |              304.272 |              31.7139 |               1500 |\n",
      "+-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "Result for PPO_AccelEnv-v0_12173_00000:\n",
      "  agent_timesteps_total: 1050000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-25_04-06-14\n",
      "  done: false\n",
      "  episode_len_mean: 1500.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 304.27238214435056\n",
      "  episode_reward_mean: 145.57909820160273\n",
      "  episode_reward_min: 31.713876526043627\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 700\n",
      "  experiment_id: 3b224e0d4aa944209a188d13d8dd894d\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.16874999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -2.6346515621169138\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.029259435209728072\n",
      "          policy_loss: 0.022346067321447307\n",
      "          total_loss: 167.58498550027105\n",
      "          vf_explained_var: -1.891801002784632e-05\n",
      "          vf_loss: 167.55770173153635\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1050000\n",
      "    num_steps_sampled: 1050000\n",
      "    num_steps_trained: 1050000\n",
      "  iterations_since_restore: 70\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 13.745921450151057\n",
      "    ram_util_percent: 35.235951661631425\n",
      "  pid: 30781\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08486148991790556\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.459206907355885\n",
      "    mean_inference_ms: 1.7531185289646345\n",
      "    mean_raw_obs_processing_ms: 6.227410053831573\n",
      "  time_since_restore: 17260.826793193817\n",
      "  time_this_iter_s: 232.2913932800293\n",
      "  time_total_s: 17260.826793193817\n",
      "  timers:\n",
      "    learn_throughput: 139.868\n",
      "    learn_time_ms: 107243.891\n",
      "    sample_throughput: 141.028\n",
      "    sample_time_ms: 106361.747\n",
      "    update_time_ms: 2.593\n",
      "  timestamp: 1666688774\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1050000\n",
      "  training_iteration: 70\n",
      "  trial_id: '12173_00000'\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 10.8/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.38 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_2_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_1_edf1e7152919a3617c29d944024e5221, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_3_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_0_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_4_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_5_edf1e7152919a3617c29d944024e5221, 0.0/6.0 CPU_group_edf1e7152919a3617c29d944024e5221)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_figure_eight\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                  | status   | loc                  |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_AccelEnv-v0_12173_00000 | RUNNING  | 141.225.10.229:30781 |     70 |          17260.8 | 1050000 |  145.579 |              304.272 |              31.7139 |               1500 |\n",
      "+-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "Result for PPO_AccelEnv-v0_12173_00000:\n",
      "  agent_timesteps_total: 1065000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-25_04-10-06\n",
      "  done: false\n",
      "  episode_len_mean: 1500.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 304.27238214435056\n",
      "  episode_reward_mean: 147.3238007523263\n",
      "  episode_reward_min: 31.713876526043627\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 710\n",
      "  experiment_id: 3b224e0d4aa944209a188d13d8dd894d\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.16874999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -2.5236331016330396\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.03316176253937258\n",
      "          policy_loss: 0.019450278156555367\n",
      "          total_loss: 167.90725242485433\n",
      "          vf_explained_var: -4.9502162369208236e-09\n",
      "          vf_loss: 167.88220604072183\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1065000\n",
      "    num_steps_sampled: 1065000\n",
      "    num_steps_trained: 1065000\n",
      "  iterations_since_restore: 71\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 13.815407854984894\n",
      "    ram_util_percent: 35.20845921450151\n",
      "  pid: 30781\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08484693304959322\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.45983128212613\n",
      "    mean_inference_ms: 1.7524794461098987\n",
      "    mean_raw_obs_processing_ms: 6.149577554620512\n",
      "  time_since_restore: 17493.026415348053\n",
      "  time_this_iter_s: 232.19962215423584\n",
      "  time_total_s: 17493.026415348053\n",
      "  timers:\n",
      "    learn_throughput: 134.891\n",
      "    learn_time_ms: 111201.004\n",
      "    sample_throughput: 141.02\n",
      "    sample_time_ms: 106368.091\n",
      "    update_time_ms: 2.558\n",
      "  timestamp: 1666689006\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1065000\n",
      "  training_iteration: 71\n",
      "  trial_id: '12173_00000'\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 10.9/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.38 GiB heap, 0.0/14.65 GiB objects (0.0/6.0 CPU_group_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_4_edf1e7152919a3617c29d944024e5221, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_2_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_3_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_0_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_1_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_5_edf1e7152919a3617c29d944024e5221)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_figure_eight\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                  | status   | loc                  |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_AccelEnv-v0_12173_00000 | RUNNING  | 141.225.10.229:30781 |     71 |            17493 | 1065000 |  147.324 |              304.272 |              31.7139 |               1500 |\n",
      "+-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "Result for PPO_AccelEnv-v0_12173_00000:\n",
      "  agent_timesteps_total: 1080000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-25_04-13-59\n",
      "  done: false\n",
      "  episode_len_mean: 1500.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 304.27238214435056\n",
      "  episode_reward_mean: 147.74096529430656\n",
      "  episode_reward_min: 31.713876526043627\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 720\n",
      "  experiment_id: 3b224e0d4aa944209a188d13d8dd894d\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.16874999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -3.063819560762179\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.03132620694101483\n",
      "          policy_loss: 0.016212621005252004\n",
      "          total_loss: 176.60779889640162\n",
      "          vf_explained_var: -7.071737639918751e-10\n",
      "          vf_loss: 176.58629981218758\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1080000\n",
      "    num_steps_sampled: 1080000\n",
      "    num_steps_trained: 1080000\n",
      "  iterations_since_restore: 72\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 13.922522522522522\n",
      "    ram_util_percent: 35.21051051051052\n",
      "  pid: 30781\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08483271744126697\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.460476155798666\n",
      "    mean_inference_ms: 1.7518436023519575\n",
      "    mean_raw_obs_processing_ms: 6.07408505720546\n",
      "  time_since_restore: 17726.123509645462\n",
      "  time_this_iter_s: 233.09709429740906\n",
      "  time_total_s: 17726.123509645462\n",
      "  timers:\n",
      "    learn_throughput: 129.826\n",
      "    learn_time_ms: 115538.919\n",
      "    sample_throughput: 140.994\n",
      "    sample_time_ms: 106387.188\n",
      "    update_time_ms: 2.56\n",
      "  timestamp: 1666689239\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1080000\n",
      "  training_iteration: 72\n",
      "  trial_id: '12173_00000'\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Memory usage on this node: 10.8/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.38 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_1_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_5_edf1e7152919a3617c29d944024e5221, 0.0/6.0 CPU_group_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_3_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_0_edf1e7152919a3617c29d944024e5221, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_4_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_2_edf1e7152919a3617c29d944024e5221)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_figure_eight\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                  | status   | loc                  |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_AccelEnv-v0_12173_00000 | RUNNING  | 141.225.10.229:30781 |     72 |          17726.1 | 1080000 |  147.741 |              304.272 |              31.7139 |               1500 |\n",
      "+-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "Result for PPO_AccelEnv-v0_12173_00000:\n",
      "  agent_timesteps_total: 1095000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-25_04-17-54\n",
      "  done: false\n",
      "  episode_len_mean: 1500.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 304.27238214435056\n",
      "  episode_reward_mean: 142.928125111341\n",
      "  episode_reward_min: 26.49520769375184\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 730\n",
      "  experiment_id: 3b224e0d4aa944209a188d13d8dd894d\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.16874999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -2.894860410488258\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.03524555755834428\n",
      "          policy_loss: 0.019795949300119686\n",
      "          total_loss: 184.44207688671048\n",
      "          vf_explained_var: -6.4150760259451545e-09\n",
      "          vf_loss: 184.41633322117693\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1095000\n",
      "    num_steps_sampled: 1095000\n",
      "    num_steps_trained: 1095000\n",
      "  iterations_since_restore: 73\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 13.648511904761904\n",
      "    ram_util_percent: 35.26309523809524\n",
      "  pid: 30781\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08481955729202743\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.461065437984054\n",
      "    mean_inference_ms: 1.751214795513588\n",
      "    mean_raw_obs_processing_ms: 6.000814882302345\n",
      "  time_since_restore: 17961.155772447586\n",
      "  time_this_iter_s: 235.03226280212402\n",
      "  time_total_s: 17961.155772447586\n",
      "  timers:\n",
      "    learn_throughput: 125.369\n",
      "    learn_time_ms: 119646.346\n",
      "    sample_throughput: 141.011\n",
      "    sample_time_ms: 106374.717\n",
      "    update_time_ms: 2.53\n",
      "  timestamp: 1666689474\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1095000\n",
      "  training_iteration: 73\n",
      "  trial_id: '12173_00000'\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 10.8/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.38 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_4_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_2_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_0_edf1e7152919a3617c29d944024e5221, 0.0/6.0 CPU_group_edf1e7152919a3617c29d944024e5221, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_1_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_3_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_5_edf1e7152919a3617c29d944024e5221)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_figure_eight\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                  | status   | loc                  |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_AccelEnv-v0_12173_00000 | RUNNING  | 141.225.10.229:30781 |     73 |          17961.2 | 1095000 |  142.928 |              304.272 |              26.4952 |               1500 |\n",
      "+-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_AccelEnv-v0_12173_00000:\n",
      "  agent_timesteps_total: 1110000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-25_04-21-57\n",
      "  done: false\n",
      "  episode_len_mean: 1500.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 304.27238214435056\n",
      "  episode_reward_mean: 151.0693615461161\n",
      "  episode_reward_min: 26.49520769375184\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 740\n",
      "  experiment_id: 3b224e0d4aa944209a188d13d8dd894d\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.16874999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -4.966979061546972\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.06262348381621449\n",
      "          policy_loss: 0.02341241477672958\n",
      "          total_loss: 146.11950223243844\n",
      "          vf_explained_var: -1.1900420759047847e-05\n",
      "          vf_loss: 146.0855218984313\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1110000\n",
      "    num_steps_sampled: 1110000\n",
      "    num_steps_trained: 1110000\n",
      "  iterations_since_restore: 74\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 13.471676300578038\n",
      "    ram_util_percent: 35.284104046242774\n",
      "  pid: 30781\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08480858712267413\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.461644932025646\n",
      "    mean_inference_ms: 1.7506399231121308\n",
      "    mean_raw_obs_processing_ms: 5.929674745127631\n",
      "  time_since_restore: 18203.839164733887\n",
      "  time_this_iter_s: 242.68339228630066\n",
      "  time_total_s: 18203.839164733887\n",
      "  timers:\n",
      "    learn_throughput: 121.08\n",
      "    learn_time_ms: 123885.379\n",
      "    sample_throughput: 140.995\n",
      "    sample_time_ms: 106386.46\n",
      "    update_time_ms: 2.534\n",
      "  timestamp: 1666689717\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1110000\n",
      "  training_iteration: 74\n",
      "  trial_id: '12173_00000'\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 10.8/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.38 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_1_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_4_edf1e7152919a3617c29d944024e5221, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_2_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_3_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_5_edf1e7152919a3617c29d944024e5221, 0.0/6.0 CPU_group_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_0_edf1e7152919a3617c29d944024e5221)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_figure_eight\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                  | status   | loc                  |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_AccelEnv-v0_12173_00000 | RUNNING  | 141.225.10.229:30781 |     74 |          18203.8 | 1110000 |  151.069 |              304.272 |              26.4952 |               1500 |\n",
      "+-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "Result for PPO_AccelEnv-v0_12173_00000:\n",
      "  agent_timesteps_total: 1125000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-25_04-25-59\n",
      "  done: false\n",
      "  episode_len_mean: 1500.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 271.0169784416338\n",
      "  episode_reward_mean: 163.77978345776424\n",
      "  episode_reward_min: 26.49520769375184\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 750\n",
      "  experiment_id: 3b224e0d4aa944209a188d13d8dd894d\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.253125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -5.111889175237235\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.038756706784364464\n",
      "          policy_loss: 0.01491249818354845\n",
      "          total_loss: 143.47140225555938\n",
      "          vf_explained_var: -1.0102482184137784e-09\n",
      "          vf_loss: 143.44667959051617\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1125000\n",
      "    num_steps_sampled: 1125000\n",
      "    num_steps_trained: 1125000\n",
      "  iterations_since_restore: 75\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 13.320000000000002\n",
      "    ram_util_percent: 35.248695652173915\n",
      "  pid: 30781\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08479821202923564\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.462168635699726\n",
      "    mean_inference_ms: 1.7500941077941716\n",
      "    mean_raw_obs_processing_ms: 5.860591961028783\n",
      "  time_since_restore: 18445.63080406189\n",
      "  time_this_iter_s: 241.79163932800293\n",
      "  time_total_s: 18445.63080406189\n",
      "  timers:\n",
      "    learn_throughput: 118.299\n",
      "    learn_time_ms: 126797.389\n",
      "    sample_throughput: 140.992\n",
      "    sample_time_ms: 106388.986\n",
      "    update_time_ms: 2.452\n",
      "  timestamp: 1666689959\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1125000\n",
      "  training_iteration: 75\n",
      "  trial_id: '12173_00000'\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 10.8/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.38 GiB heap, 0.0/14.65 GiB objects (0.0/6.0 CPU_group_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_1_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_3_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_5_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_2_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_4_edf1e7152919a3617c29d944024e5221, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_0_edf1e7152919a3617c29d944024e5221)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_figure_eight\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                  | status   | loc                  |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_AccelEnv-v0_12173_00000 | RUNNING  | 141.225.10.229:30781 |     75 |          18445.6 | 1125000 |   163.78 |              271.017 |              26.4952 |               1500 |\n",
      "+-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "Result for PPO_AccelEnv-v0_12173_00000:\n",
      "  agent_timesteps_total: 1140000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-25_04-30-00\n",
      "  done: false\n",
      "  episode_len_mean: 1500.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 271.0169784416338\n",
      "  episode_reward_mean: 174.65636140391817\n",
      "  episode_reward_min: 26.49520769375184\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 760\n",
      "  experiment_id: 3b224e0d4aa944209a188d13d8dd894d\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.253125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -5.098940397521197\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.03248763486441696\n",
      "          policy_loss: 0.01187563715660471\n",
      "          total_loss: 140.6543417397192\n",
      "          vf_explained_var: 1.111273073561847e-09\n",
      "          vf_loss: 140.63424320544226\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1140000\n",
      "    num_steps_sampled: 1140000\n",
      "    num_steps_trained: 1140000\n",
      "  iterations_since_restore: 76\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 13.60466472303207\n",
      "    ram_util_percent: 35.26588921282799\n",
      "  pid: 30781\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08478888598593155\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.46260987112716\n",
      "    mean_inference_ms: 1.7495756507232756\n",
      "    mean_raw_obs_processing_ms: 5.793411873607027\n",
      "  time_since_restore: 18686.283987760544\n",
      "  time_this_iter_s: 240.65318369865417\n",
      "  time_total_s: 18686.283987760544\n",
      "  timers:\n",
      "    learn_throughput: 116.521\n",
      "    learn_time_ms: 128731.692\n",
      "    sample_throughput: 141.019\n",
      "    sample_time_ms: 106368.611\n",
      "    update_time_ms: 2.397\n",
      "  timestamp: 1666690200\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1140000\n",
      "  training_iteration: 76\n",
      "  trial_id: '12173_00000'\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 10.9/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.38 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_1_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_4_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_3_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_5_edf1e7152919a3617c29d944024e5221, 0.0/6.0 CPU_group_edf1e7152919a3617c29d944024e5221, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_0_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_2_edf1e7152919a3617c29d944024e5221)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_figure_eight\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                  | status   | loc                  |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_AccelEnv-v0_12173_00000 | RUNNING  | 141.225.10.229:30781 |     76 |          18686.3 | 1140000 |  174.656 |              271.017 |              26.4952 |               1500 |\n",
      "+-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "Result for PPO_AccelEnv-v0_12173_00000:\n",
      "  agent_timesteps_total: 1155000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-25_04-34-00\n",
      "  done: false\n",
      "  episode_len_mean: 1500.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 271.0169784416338\n",
      "  episode_reward_mean: 187.94142346874014\n",
      "  episode_reward_min: 26.49520769375184\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 770\n",
      "  experiment_id: 3b224e0d4aa944209a188d13d8dd894d\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.253125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -5.044064679388272\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.03710871271943459\n",
      "          policy_loss: 0.014606318392759284\n",
      "          total_loss: 146.06692912861453\n",
      "          vf_explained_var: -5.051241022679953e-11\n",
      "          vf_loss: 146.04292925818493\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1155000\n",
      "    num_steps_sampled: 1155000\n",
      "    num_steps_trained: 1155000\n",
      "  iterations_since_restore: 77\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 13.623837209302327\n",
      "    ram_util_percent: 35.288953488372094\n",
      "  pid: 30781\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08477979313689624\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.463040017281955\n",
      "    mean_inference_ms: 1.7490606817199064\n",
      "    mean_raw_obs_processing_ms: 5.728099061968803\n",
      "  time_since_restore: 18926.923738479614\n",
      "  time_this_iter_s: 240.63975071907043\n",
      "  time_total_s: 18926.923738479614\n",
      "  timers:\n",
      "    learn_throughput: 115.753\n",
      "    learn_time_ms: 129586.603\n",
      "    sample_throughput: 141.017\n",
      "    sample_time_ms: 106370.135\n",
      "    update_time_ms: 2.347\n",
      "  timestamp: 1666690440\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1155000\n",
      "  training_iteration: 77\n",
      "  trial_id: '12173_00000'\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Memory usage on this node: 10.8/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.38 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_1_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_2_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_3_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_0_edf1e7152919a3617c29d944024e5221, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_4_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_5_edf1e7152919a3617c29d944024e5221, 0.0/6.0 CPU_group_edf1e7152919a3617c29d944024e5221)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_figure_eight\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                  | status   | loc                  |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_AccelEnv-v0_12173_00000 | RUNNING  | 141.225.10.229:30781 |     77 |          18926.9 | 1155000 |  187.941 |              271.017 |              26.4952 |               1500 |\n",
      "+-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "Result for PPO_AccelEnv-v0_12173_00000:\n",
      "  agent_timesteps_total: 1170000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-25_04-38-01\n",
      "  done: false\n",
      "  episode_len_mean: 1500.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 271.0169784416338\n",
      "  episode_reward_mean: 191.38777213809598\n",
      "  episode_reward_min: 26.49520769375184\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 780\n",
      "  experiment_id: 3b224e0d4aa944209a188d13d8dd894d\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.253125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -5.053733821642601\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.027670361515337232\n",
      "          policy_loss: 0.009020426492522455\n",
      "          total_loss: 149.48216146695412\n",
      "          vf_explained_var: -2.0204964090719812e-10\n",
      "          vf_loss: 149.46613649271302\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1170000\n",
      "    num_steps_sampled: 1170000\n",
      "    num_steps_trained: 1170000\n",
      "  iterations_since_restore: 78\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 13.484302325581396\n",
      "    ram_util_percent: 35.27267441860465\n",
      "  pid: 30781\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0847700090272881\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.463458173148673\n",
      "    mean_inference_ms: 1.748575391305794\n",
      "    mean_raw_obs_processing_ms: 5.6645570226129935\n",
      "  time_since_restore: 19167.888500213623\n",
      "  time_this_iter_s: 240.9647617340088\n",
      "  time_total_s: 19167.888500213623\n",
      "  timers:\n",
      "    learn_throughput: 114.8\n",
      "    learn_time_ms: 130661.981\n",
      "    sample_throughput: 141.026\n",
      "    sample_time_ms: 106363.2\n",
      "    update_time_ms: 2.334\n",
      "  timestamp: 1666690681\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1170000\n",
      "  training_iteration: 78\n",
      "  trial_id: '12173_00000'\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 10.8/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.38 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_4_edf1e7152919a3617c29d944024e5221, 0.0/1.0 accelerator_type:G, 0.0/6.0 CPU_group_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_0_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_1_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_2_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_5_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_3_edf1e7152919a3617c29d944024e5221)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_figure_eight\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                  | status   | loc                  |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_AccelEnv-v0_12173_00000 | RUNNING  | 141.225.10.229:30781 |     78 |          19167.9 | 1170000 |  191.388 |              271.017 |              26.4952 |               1500 |\n",
      "+-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_AccelEnv-v0_12173_00000:\n",
      "  agent_timesteps_total: 1185000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-25_04-42-03\n",
      "  done: false\n",
      "  episode_len_mean: 1500.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 271.0169784416338\n",
      "  episode_reward_mean: 202.2935267048505\n",
      "  episode_reward_min: 26.49520769375184\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 790\n",
      "  experiment_id: 3b224e0d4aa944209a188d13d8dd894d\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.253125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -5.005637177370362\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.027390507475601397\n",
      "          policy_loss: 0.008446834595889737\n",
      "          total_loss: 148.46197610628806\n",
      "          vf_explained_var: -7.071737639918751e-10\n",
      "          vf_loss: 148.4465962426137\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1185000\n",
      "    num_steps_sampled: 1185000\n",
      "    num_steps_trained: 1185000\n",
      "  iterations_since_restore: 79\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 13.574127906976743\n",
      "    ram_util_percent: 35.292732558139534\n",
      "  pid: 30781\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.084762093542696\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.46385504073117\n",
      "    mean_inference_ms: 1.7481375250087328\n",
      "    mean_raw_obs_processing_ms: 5.6027354283288044\n",
      "  time_since_restore: 19409.139098644257\n",
      "  time_this_iter_s: 241.25059843063354\n",
      "  time_total_s: 19409.139098644257\n",
      "  timers:\n",
      "    learn_throughput: 113.921\n",
      "    learn_time_ms: 131670.541\n",
      "    sample_throughput: 141.008\n",
      "    sample_time_ms: 106377.309\n",
      "    update_time_ms: 2.355\n",
      "  timestamp: 1666690923\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1185000\n",
      "  training_iteration: 79\n",
      "  trial_id: '12173_00000'\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 10.8/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.38 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_0_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_3_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_2_edf1e7152919a3617c29d944024e5221, 0.0/6.0 CPU_group_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_5_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_1_edf1e7152919a3617c29d944024e5221, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_4_edf1e7152919a3617c29d944024e5221)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_figure_eight\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                  | status   | loc                  |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_AccelEnv-v0_12173_00000 | RUNNING  | 141.225.10.229:30781 |     79 |          19409.1 | 1185000 |  202.294 |              271.017 |              26.4952 |               1500 |\n",
      "+-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "Result for PPO_AccelEnv-v0_12173_00000:\n",
      "  agent_timesteps_total: 1200000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-25_04-46-04\n",
      "  done: false\n",
      "  episode_len_mean: 1500.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 271.0169784416338\n",
      "  episode_reward_mean: 213.65295625835162\n",
      "  episode_reward_min: 26.49520769375184\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 800\n",
      "  experiment_id: 3b224e0d4aa944209a188d13d8dd894d\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.253125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -5.02610728255773\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.03387650354739598\n",
      "          policy_loss: 0.013221761418538073\n",
      "          total_loss: 155.63304269435042\n",
      "          vf_explained_var: 4.0409928181439625e-10\n",
      "          vf_loss: 155.6112459926282\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1200000\n",
      "    num_steps_sampled: 1200000\n",
      "    num_steps_trained: 1200000\n",
      "  iterations_since_restore: 80\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 13.790116279069768\n",
      "    ram_util_percent: 35.299709302325574\n",
      "  pid: 30781\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0847539012429544\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.464252040988704\n",
      "    mean_inference_ms: 1.747696663006613\n",
      "    mean_raw_obs_processing_ms: 5.542531006272066\n",
      "  time_since_restore: 19650.01046562195\n",
      "  time_this_iter_s: 240.87136697769165\n",
      "  time_total_s: 19650.01046562195\n",
      "  timers:\n",
      "    learn_throughput: 113.172\n",
      "    learn_time_ms: 132542.006\n",
      "    sample_throughput: 141.025\n",
      "    sample_time_ms: 106363.764\n",
      "    update_time_ms: 2.46\n",
      "  timestamp: 1666691164\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1200000\n",
      "  training_iteration: 80\n",
      "  trial_id: '12173_00000'\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 10.8/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.38 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_5_edf1e7152919a3617c29d944024e5221, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_1_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_2_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_4_edf1e7152919a3617c29d944024e5221, 0.0/6.0 CPU_group_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_0_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_3_edf1e7152919a3617c29d944024e5221)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_figure_eight\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                  | status   | loc                  |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_AccelEnv-v0_12173_00000 | RUNNING  | 141.225.10.229:30781 |     80 |            19650 | 1200000 |  213.653 |              271.017 |              26.4952 |               1500 |\n",
      "+-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "Result for PPO_AccelEnv-v0_12173_00000:\n",
      "  agent_timesteps_total: 1215000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-25_04-50-01\n",
      "  done: false\n",
      "  episode_len_mean: 1500.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 271.0169784416338\n",
      "  episode_reward_mean: 226.60816841672147\n",
      "  episode_reward_min: 26.49520769375184\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 810\n",
      "  experiment_id: 3b224e0d4aa944209a188d13d8dd894d\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.253125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -4.929373451006614\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.02701822992264759\n",
      "          policy_loss: 0.009797492396023314\n",
      "          total_loss: 158.14438795962576\n",
      "          vf_explained_var: -9.597358463508954e-10\n",
      "          vf_loss: 158.12775146355062\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1215000\n",
      "    num_steps_sampled: 1215000\n",
      "    num_steps_trained: 1215000\n",
      "  iterations_since_restore: 81\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 13.60473372781065\n",
      "    ram_util_percent: 35.26715976331361\n",
      "  pid: 30781\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08474584644036715\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.464604901485504\n",
      "    mean_inference_ms: 1.7472882512053713\n",
      "    mean_raw_obs_processing_ms: 5.483869174821809\n",
      "  time_since_restore: 19887.18006181717\n",
      "  time_this_iter_s: 237.16959619522095\n",
      "  time_total_s: 19887.18006181717\n",
      "  timers:\n",
      "    learn_throughput: 112.742\n",
      "    learn_time_ms: 133047.252\n",
      "    sample_throughput: 141.036\n",
      "    sample_time_ms: 106355.501\n",
      "    update_time_ms: 2.465\n",
      "  timestamp: 1666691401\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1215000\n",
      "  training_iteration: 81\n",
      "  trial_id: '12173_00000'\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 10.9/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.38 GiB heap, 0.0/14.65 GiB objects (0.0/6.0 CPU_group_edf1e7152919a3617c29d944024e5221, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_3_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_0_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_1_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_5_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_2_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_4_edf1e7152919a3617c29d944024e5221)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_figure_eight\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                  | status   | loc                  |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_AccelEnv-v0_12173_00000 | RUNNING  | 141.225.10.229:30781 |     81 |          19887.2 | 1215000 |  226.608 |              271.017 |              26.4952 |               1500 |\n",
      "+-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "Result for PPO_AccelEnv-v0_12173_00000:\n",
      "  agent_timesteps_total: 1230000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-25_04-54-03\n",
      "  done: false\n",
      "  episode_len_mean: 1500.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 271.0169784416338\n",
      "  episode_reward_mean: 243.4990423150671\n",
      "  episode_reward_min: 26.49520769375184\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 820\n",
      "  experiment_id: 3b224e0d4aa944209a188d13d8dd894d\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.253125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -4.90879817130202\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.02887059718767739\n",
      "          policy_loss: 0.010663986361143574\n",
      "          total_loss: 160.46941350031707\n",
      "          vf_explained_var: -9.739398592500947e-06\n",
      "          vf_loss: 160.45144173412\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1230000\n",
      "    num_steps_sampled: 1230000\n",
      "    num_steps_trained: 1230000\n",
      "  iterations_since_restore: 82\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 13.567536231884059\n",
      "    ram_util_percent: 35.24753623188406\n",
      "  pid: 30781\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08473751737734038\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.464941055644577\n",
      "    mean_inference_ms: 1.7468898058909212\n",
      "    mean_raw_obs_processing_ms: 5.426747039221211\n",
      "  time_since_restore: 20128.923768997192\n",
      "  time_this_iter_s: 241.7437071800232\n",
      "  time_total_s: 20128.923768997192\n",
      "  timers:\n",
      "    learn_throughput: 112.008\n",
      "    learn_time_ms: 133919.369\n",
      "    sample_throughput: 141.046\n",
      "    sample_time_ms: 106348.322\n",
      "    update_time_ms: 2.44\n",
      "  timestamp: 1666691643\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1230000\n",
      "  training_iteration: 82\n",
      "  trial_id: '12173_00000'\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Memory usage on this node: 10.8/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.38 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_0_edf1e7152919a3617c29d944024e5221, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_3_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_5_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_1_edf1e7152919a3617c29d944024e5221, 0.0/6.0 CPU_group_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_4_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_2_edf1e7152919a3617c29d944024e5221)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_figure_eight\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                  | status   | loc                  |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_AccelEnv-v0_12173_00000 | RUNNING  | 141.225.10.229:30781 |     82 |          20128.9 | 1230000 |  243.499 |              271.017 |              26.4952 |               1500 |\n",
      "+-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "Result for PPO_AccelEnv-v0_12173_00000:\n",
      "  agent_timesteps_total: 1245000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-25_04-58-04\n",
      "  done: false\n",
      "  episode_len_mean: 1500.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 271.0169784416338\n",
      "  episode_reward_mean: 260.85067817220227\n",
      "  episode_reward_min: 109.80605703692187\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 830\n",
      "  experiment_id: 3b224e0d4aa944209a188d13d8dd894d\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.253125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -4.265925747661267\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.030640812208324354\n",
      "          policy_loss: 0.006887798898032535\n",
      "          total_loss: 164.13140000812078\n",
      "          vf_explained_var: -6.566613364178409e-10\n",
      "          vf_loss: 164.1167559284275\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1245000\n",
      "    num_steps_sampled: 1245000\n",
      "    num_steps_trained: 1245000\n",
      "  iterations_since_restore: 83\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 13.565797101449276\n",
      "    ram_util_percent: 35.36260869565217\n",
      "  pid: 30781\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08473147461647684\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.46521400201893\n",
      "    mean_inference_ms: 1.746566798915618\n",
      "    mean_raw_obs_processing_ms: 5.371087393846849\n",
      "  time_since_restore: 20370.53575849533\n",
      "  time_this_iter_s: 241.61198949813843\n",
      "  time_total_s: 20370.53575849533\n",
      "  timers:\n",
      "    learn_throughput: 111.468\n",
      "    learn_time_ms: 134568.304\n",
      "    sample_throughput: 141.034\n",
      "    sample_time_ms: 106357.004\n",
      "    update_time_ms: 2.495\n",
      "  timestamp: 1666691884\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1245000\n",
      "  training_iteration: 83\n",
      "  trial_id: '12173_00000'\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 10.8/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.38 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_3_edf1e7152919a3617c29d944024e5221, 0.0/6.0 CPU_group_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_4_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_2_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_5_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_1_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_0_edf1e7152919a3617c29d944024e5221, 0.0/1.0 accelerator_type:G)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_figure_eight\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                  | status   | loc                  |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_AccelEnv-v0_12173_00000 | RUNNING  | 141.225.10.229:30781 |     83 |          20370.5 | 1245000 |  260.851 |              271.017 |              109.806 |               1500 |\n",
      "+-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_AccelEnv-v0_12173_00000:\n",
      "  agent_timesteps_total: 1260000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-25_05-02-06\n",
      "  done: false\n",
      "  episode_len_mean: 1500.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 270.06827963002553\n",
      "  episode_reward_mean: 261.67947066633957\n",
      "  episode_reward_min: 246.79649650863942\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 840\n",
      "  experiment_id: 3b224e0d4aa944209a188d13d8dd894d\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.253125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -4.604687633756864\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.023389257020509542\n",
      "          policy_loss: 0.006123675447809747\n",
      "          total_loss: 161.7662947121313\n",
      "          vf_explained_var: -9.092234187768611e-10\n",
      "          vf_loss: 161.75425058785132\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1260000\n",
      "    num_steps_sampled: 1260000\n",
      "    num_steps_trained: 1260000\n",
      "  iterations_since_restore: 84\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 13.583768115942028\n",
      "    ram_util_percent: 35.292173913043484\n",
      "  pid: 30781\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08472494167696583\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.465502222798786\n",
      "    mean_inference_ms: 1.746231867121889\n",
      "    mean_raw_obs_processing_ms: 5.316833152524352\n",
      "  time_since_restore: 20612.479783535004\n",
      "  time_this_iter_s: 241.94402503967285\n",
      "  time_total_s: 20612.479783535004\n",
      "  timers:\n",
      "    learn_throughput: 111.525\n",
      "    learn_time_ms: 134498.747\n",
      "    sample_throughput: 141.04\n",
      "    sample_time_ms: 106352.592\n",
      "    update_time_ms: 2.524\n",
      "  timestamp: 1666692126\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1260000\n",
      "  training_iteration: 84\n",
      "  trial_id: '12173_00000'\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 10.8/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.38 GiB heap, 0.0/14.65 GiB objects (0.0/6.0 CPU_group_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_1_edf1e7152919a3617c29d944024e5221, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_3_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_2_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_0_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_5_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_4_edf1e7152919a3617c29d944024e5221)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_figure_eight\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                  | status   | loc                  |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_AccelEnv-v0_12173_00000 | RUNNING  | 141.225.10.229:30781 |     84 |          20612.5 | 1260000 |  261.679 |              270.068 |              246.796 |               1500 |\n",
      "+-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "Result for PPO_AccelEnv-v0_12173_00000:\n",
      "  agent_timesteps_total: 1275000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-25_05-06-10\n",
      "  done: false\n",
      "  episode_len_mean: 1500.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 270.06827963002553\n",
      "  episode_reward_mean: 260.98967667070707\n",
      "  episode_reward_min: 246.79649650863942\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 850\n",
      "  experiment_id: 3b224e0d4aa944209a188d13d8dd894d\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.253125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -4.6924616070116985\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.02711822322112095\n",
      "          policy_loss: 0.00828456284075786\n",
      "          total_loss: 169.10714535794017\n",
      "          vf_explained_var: -8.587109912028268e-10\n",
      "          vf_loss: 169.09199630607992\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1275000\n",
      "    num_steps_sampled: 1275000\n",
      "    num_steps_trained: 1275000\n",
      "  iterations_since_restore: 85\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 13.386167146974064\n",
      "    ram_util_percent: 35.28731988472623\n",
      "  pid: 30781\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0847169594157021\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.465850511574835\n",
      "    mean_inference_ms: 1.7458454349361372\n",
      "    mean_raw_obs_processing_ms: 5.2639219212819945\n",
      "  time_since_restore: 20855.73512482643\n",
      "  time_this_iter_s: 243.2553412914276\n",
      "  time_total_s: 20855.73512482643\n",
      "  timers:\n",
      "    learn_throughput: 111.402\n",
      "    learn_time_ms: 134647.322\n",
      "    sample_throughput: 141.043\n",
      "    sample_time_ms: 106350.465\n",
      "    update_time_ms: 2.559\n",
      "  timestamp: 1666692370\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1275000\n",
      "  training_iteration: 85\n",
      "  trial_id: '12173_00000'\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 10.8/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.38 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_0_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_1_edf1e7152919a3617c29d944024e5221, 0.0/6.0 CPU_group_edf1e7152919a3617c29d944024e5221, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_5_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_3_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_4_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_2_edf1e7152919a3617c29d944024e5221)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_figure_eight\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                  | status   | loc                  |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_AccelEnv-v0_12173_00000 | RUNNING  | 141.225.10.229:30781 |     85 |          20855.7 | 1275000 |   260.99 |              270.068 |              246.796 |               1500 |\n",
      "+-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "Result for PPO_AccelEnv-v0_12173_00000:\n",
      "  agent_timesteps_total: 1290000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-25_05-10-14\n",
      "  done: false\n",
      "  episode_len_mean: 1500.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 270.06827963002553\n",
      "  episode_reward_mean: 260.33425430111475\n",
      "  episode_reward_min: 246.79649650863942\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 860\n",
      "  experiment_id: 3b224e0d4aa944209a188d13d8dd894d\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.253125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -4.7286261970713985\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.028337952515764736\n",
      "          policy_loss: 0.009669967178034328\n",
      "          total_loss: 167.4275865975073\n",
      "          vf_explained_var: -9.092234187768611e-10\n",
      "          vf_loss: 167.41074355820479\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1290000\n",
      "    num_steps_sampled: 1290000\n",
      "    num_steps_trained: 1290000\n",
      "  iterations_since_restore: 86\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 13.354727793696274\n",
      "    ram_util_percent: 35.347851002865326\n",
      "  pid: 30781\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08470799422920326\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.46621108453295\n",
      "    mean_inference_ms: 1.7454473206865613\n",
      "    mean_raw_obs_processing_ms: 5.212345137435575\n",
      "  time_since_restore: 21100.09143424034\n",
      "  time_this_iter_s: 244.3563094139099\n",
      "  time_total_s: 21100.09143424034\n",
      "  timers:\n",
      "    learn_throughput: 111.103\n",
      "    learn_time_ms: 135010.149\n",
      "    sample_throughput: 141.034\n",
      "    sample_time_ms: 106357.394\n",
      "    update_time_ms: 2.529\n",
      "  timestamp: 1666692614\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1290000\n",
      "  training_iteration: 86\n",
      "  trial_id: '12173_00000'\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 10.9/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.38 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_0_edf1e7152919a3617c29d944024e5221, 0.0/6.0 CPU_group_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_5_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_2_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_3_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_4_edf1e7152919a3617c29d944024e5221, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_1_edf1e7152919a3617c29d944024e5221)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_figure_eight\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                  | status   | loc                  |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_AccelEnv-v0_12173_00000 | RUNNING  | 141.225.10.229:30781 |     86 |          21100.1 | 1290000 |  260.334 |              270.068 |              246.796 |               1500 |\n",
      "+-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "Result for PPO_AccelEnv-v0_12173_00000:\n",
      "  agent_timesteps_total: 1305000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-25_05-14-19\n",
      "  done: false\n",
      "  episode_len_mean: 1500.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 270.06827963002553\n",
      "  episode_reward_mean: 259.70334381704777\n",
      "  episode_reward_min: 246.79649650863942\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 870\n",
      "  experiment_id: 3b224e0d4aa944209a188d13d8dd894d\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.253125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -4.545957140599267\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.03645963473058083\n",
      "          policy_loss: 0.00688617676654357\n",
      "          total_loss: 175.8616564443556\n",
      "          vf_explained_var: -1.8689592096166052e-09\n",
      "          vf_loss: 175.84554159520036\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1305000\n",
      "    num_steps_sampled: 1305000\n",
      "    num_steps_trained: 1305000\n",
      "  iterations_since_restore: 87\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 13.460744985673351\n",
      "    ram_util_percent: 35.31518624641834\n",
      "  pid: 30781\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08469898261608648\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.46661696045974\n",
      "    mean_inference_ms: 1.7450144446522886\n",
      "    mean_raw_obs_processing_ms: 5.1620064921669915\n",
      "  time_since_restore: 21344.777728796005\n",
      "  time_this_iter_s: 244.68629455566406\n",
      "  time_total_s: 21344.777728796005\n",
      "  timers:\n",
      "    learn_throughput: 110.765\n",
      "    learn_time_ms: 135421.283\n",
      "    sample_throughput: 141.042\n",
      "    sample_time_ms: 106351.146\n",
      "    update_time_ms: 2.582\n",
      "  timestamp: 1666692859\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1305000\n",
      "  training_iteration: 87\n",
      "  trial_id: '12173_00000'\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Memory usage on this node: 10.8/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.38 GiB heap, 0.0/14.65 GiB objects (0.0/6.0 CPU_group_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_0_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_5_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_1_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_3_edf1e7152919a3617c29d944024e5221, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_2_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_4_edf1e7152919a3617c29d944024e5221)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_figure_eight\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                  | status   | loc                  |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_AccelEnv-v0_12173_00000 | RUNNING  | 141.225.10.229:30781 |     87 |          21344.8 | 1305000 |  259.703 |              270.068 |              246.796 |               1500 |\n",
      "+-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "Result for PPO_AccelEnv-v0_12173_00000:\n",
      "  agent_timesteps_total: 1320000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-25_05-18-24\n",
      "  done: false\n",
      "  episode_len_mean: 1500.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 270.06827963002553\n",
      "  episode_reward_mean: 259.3686964670312\n",
      "  episode_reward_min: 246.79649650863942\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 880\n",
      "  experiment_id: 3b224e0d4aa944209a188d13d8dd894d\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.253125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -4.811712553945639\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.03293286000894731\n",
      "          policy_loss: 0.010883711170967895\n",
      "          total_loss: 174.50320613990397\n",
      "          vf_explained_var: -7.071737639918751e-10\n",
      "          vf_loss: 174.48398700972734\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1320000\n",
      "    num_steps_sampled: 1320000\n",
      "    num_steps_trained: 1320000\n",
      "  iterations_since_restore: 88\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 13.417714285714288\n",
      "    ram_util_percent: 35.36600000000001\n",
      "  pid: 30781\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08469115421037372\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.466986208760808\n",
      "    mean_inference_ms: 1.7446270618038011\n",
      "    mean_raw_obs_processing_ms: 5.112906011112637\n",
      "  time_since_restore: 21589.885004997253\n",
      "  time_this_iter_s: 245.10727620124817\n",
      "  time_total_s: 21589.885004997253\n",
      "  timers:\n",
      "    learn_throughput: 110.434\n",
      "    learn_time_ms: 135827.721\n",
      "    sample_throughput: 141.031\n",
      "    sample_time_ms: 106359.281\n",
      "    update_time_ms: 2.582\n",
      "  timestamp: 1666693104\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1320000\n",
      "  training_iteration: 88\n",
      "  trial_id: '12173_00000'\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 10.9/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.38 GiB heap, 0.0/14.65 GiB objects (0.0/6.0 CPU_group_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_1_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_4_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_2_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_0_edf1e7152919a3617c29d944024e5221, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_5_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_3_edf1e7152919a3617c29d944024e5221)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_figure_eight\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                  | status   | loc                  |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_AccelEnv-v0_12173_00000 | RUNNING  | 141.225.10.229:30781 |     88 |          21589.9 | 1320000 |  259.369 |              270.068 |              246.796 |               1500 |\n",
      "+-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "Result for PPO_AccelEnv-v0_12173_00000:\n",
      "  agent_timesteps_total: 1335000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-25_05-22-34\n",
      "  done: false\n",
      "  episode_len_mean: 1500.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 270.06827963002553\n",
      "  episode_reward_mean: 259.36169948323715\n",
      "  episode_reward_min: 246.79649650863942\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 890\n",
      "  experiment_id: 3b224e0d4aa944209a188d13d8dd894d\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.253125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -4.807708557177398\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.029769655551381877\n",
      "          policy_loss: 0.009126384152983457\n",
      "          total_loss: 179.35909479432186\n",
      "          vf_explained_var: -3.434843964811307e-09\n",
      "          vf_loss: 179.34243267188637\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1335000\n",
      "    num_steps_sampled: 1335000\n",
      "    num_steps_trained: 1335000\n",
      "  iterations_since_restore: 89\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 13.318258426966292\n",
      "    ram_util_percent: 35.33820224719101\n",
      "  pid: 30781\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0846827078641769\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.46743987261766\n",
      "    mean_inference_ms: 1.7441726364087566\n",
      "    mean_raw_obs_processing_ms: 5.064965061969538\n",
      "  time_since_restore: 21839.640558958054\n",
      "  time_this_iter_s: 249.75555396080017\n",
      "  time_total_s: 21839.640558958054\n",
      "  timers:\n",
      "    learn_throughput: 109.751\n",
      "    learn_time_ms: 136672.959\n",
      "    sample_throughput: 141.024\n",
      "    sample_time_ms: 106364.828\n",
      "    update_time_ms: 2.568\n",
      "  timestamp: 1666693354\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1335000\n",
      "  training_iteration: 89\n",
      "  trial_id: '12173_00000'\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 10.8/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.38 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_2_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_3_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_0_edf1e7152919a3617c29d944024e5221, 0.0/6.0 CPU_group_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_5_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_4_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_1_edf1e7152919a3617c29d944024e5221)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_figure_eight\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                  | status   | loc                  |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_AccelEnv-v0_12173_00000 | RUNNING  | 141.225.10.229:30781 |     89 |          21839.6 | 1335000 |  259.362 |              270.068 |              246.796 |               1500 |\n",
      "+-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "Result for PPO_AccelEnv-v0_12173_00000:\n",
      "  agent_timesteps_total: 1350000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-25_05-26-40\n",
      "  done: false\n",
      "  episode_len_mean: 1500.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 270.06827963002553\n",
      "  episode_reward_mean: 259.6976287130098\n",
      "  episode_reward_min: 246.79649650863942\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 900\n",
      "  experiment_id: 3b224e0d4aa944209a188d13d8dd894d\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.253125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -4.8197710178666195\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.024779882092321574\n",
      "          policy_loss: 0.007422790468682267\n",
      "          total_loss: 182.5955266758547\n",
      "          vf_explained_var: -5.807058187201619e-06\n",
      "          vf_loss: 182.5818315764605\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1350000\n",
      "    num_steps_sampled: 1350000\n",
      "    num_steps_trained: 1350000\n",
      "  iterations_since_restore: 90\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 13.491477272727273\n",
      "    ram_util_percent: 35.32784090909092\n",
      "  pid: 30781\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08467559999384605\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.467832170160218\n",
      "    mean_inference_ms: 1.7437613276536479\n",
      "    mean_raw_obs_processing_ms: 5.018140028101933\n",
      "  time_since_restore: 22085.981372117996\n",
      "  time_this_iter_s: 246.34081315994263\n",
      "  time_total_s: 22085.981372117996\n",
      "  timers:\n",
      "    learn_throughput: 109.315\n",
      "    learn_time_ms: 137218.448\n",
      "    sample_throughput: 141.022\n",
      "    sample_time_ms: 106366.401\n",
      "    update_time_ms: 2.505\n",
      "  timestamp: 1666693600\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1350000\n",
      "  training_iteration: 90\n",
      "  trial_id: '12173_00000'\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 10.8/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.38 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_3_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_5_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_0_edf1e7152919a3617c29d944024e5221, 0.0/6.0 CPU_group_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_2_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_1_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_4_edf1e7152919a3617c29d944024e5221)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_figure_eight\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                  | status   | loc                  |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_AccelEnv-v0_12173_00000 | RUNNING  | 141.225.10.229:30781 |     90 |            22086 | 1350000 |  259.698 |              270.068 |              246.796 |               1500 |\n",
      "+-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "Result for PPO_AccelEnv-v0_12173_00000:\n",
      "  agent_timesteps_total: 1365000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-25_05-30-47\n",
      "  done: false\n",
      "  episode_len_mean: 1500.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 270.06827963002553\n",
      "  episode_reward_mean: 259.43951275520806\n",
      "  episode_reward_min: 246.79649650863942\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 910\n",
      "  experiment_id: 3b224e0d4aa944209a188d13d8dd894d\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.253125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -4.795917229733225\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.024311871644853825\n",
      "          policy_loss: 0.009037931815926301\n",
      "          total_loss: 183.24467247461868\n",
      "          vf_explained_var: -3.2833067376003555e-09\n",
      "          vf_loss: 183.22948009684936\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1365000\n",
      "    num_steps_sampled: 1365000\n",
      "    num_steps_trained: 1365000\n",
      "  iterations_since_restore: 91\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 13.405965909090908\n",
      "    ram_util_percent: 35.423011363636355\n",
      "  pid: 30781\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0846688240161863\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.468267403929318\n",
      "    mean_inference_ms: 1.743336668109404\n",
      "    mean_raw_obs_processing_ms: 4.972411958672636\n",
      "  time_since_restore: 22333.125984191895\n",
      "  time_this_iter_s: 247.14461207389832\n",
      "  time_total_s: 22333.125984191895\n",
      "  timers:\n",
      "    learn_throughput: 108.535\n",
      "    learn_time_ms: 138203.727\n",
      "    sample_throughput: 141.006\n",
      "    sample_time_ms: 106378.296\n",
      "    update_time_ms: 2.499\n",
      "  timestamp: 1666693847\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1365000\n",
      "  training_iteration: 91\n",
      "  trial_id: '12173_00000'\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 10.9/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.38 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_3_edf1e7152919a3617c29d944024e5221, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_2_edf1e7152919a3617c29d944024e5221, 0.0/6.0 CPU_group_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_5_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_4_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_1_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_0_edf1e7152919a3617c29d944024e5221)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_figure_eight\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                  | status   | loc                  |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_AccelEnv-v0_12173_00000 | RUNNING  | 141.225.10.229:30781 |     91 |          22333.1 | 1365000 |   259.44 |              270.068 |              246.796 |               1500 |\n",
      "+-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "Result for PPO_AccelEnv-v0_12173_00000:\n",
      "  agent_timesteps_total: 1380000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-25_05-34-58\n",
      "  done: false\n",
      "  episode_len_mean: 1500.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 270.06827963002553\n",
      "  episode_reward_mean: 258.6825251507231\n",
      "  episode_reward_min: 246.79649650863942\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 920\n",
      "  experiment_id: 3b224e0d4aa944209a188d13d8dd894d\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.253125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -4.778583669662476\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.022454166787595615\n",
      "          policy_loss: 0.007187267023304446\n",
      "          total_loss: 181.34980260558046\n",
      "          vf_explained_var: -5.556365367809235e-10\n",
      "          vf_loss: 181.33693175881595\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1380000\n",
      "    num_steps_sampled: 1380000\n",
      "    num_steps_trained: 1380000\n",
      "  iterations_since_restore: 92\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 13.231843575418994\n",
      "    ram_util_percent: 35.357541899441344\n",
      "  pid: 30781\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08466233157927348\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.468665410965535\n",
      "    mean_inference_ms: 1.7429557321803621\n",
      "    mean_raw_obs_processing_ms: 4.927710804894633\n",
      "  time_since_restore: 22583.806052207947\n",
      "  time_this_iter_s: 250.68006801605225\n",
      "  time_total_s: 22583.806052207947\n",
      "  timers:\n",
      "    learn_throughput: 107.837\n",
      "    learn_time_ms: 139098.253\n",
      "    sample_throughput: 141.008\n",
      "    sample_time_ms: 106376.932\n",
      "    update_time_ms: 2.451\n",
      "  timestamp: 1666694098\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1380000\n",
      "  training_iteration: 92\n",
      "  trial_id: '12173_00000'\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Memory usage on this node: 10.9/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.38 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_0_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_5_edf1e7152919a3617c29d944024e5221, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_1_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_3_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_2_edf1e7152919a3617c29d944024e5221, 0.0/6.0 CPU_group_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_4_edf1e7152919a3617c29d944024e5221)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_figure_eight\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                  | status   | loc                  |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_AccelEnv-v0_12173_00000 | RUNNING  | 141.225.10.229:30781 |     92 |          22583.8 | 1380000 |  258.683 |              270.068 |              246.796 |               1500 |\n",
      "+-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "Result for PPO_AccelEnv-v0_12173_00000:\n",
      "  agent_timesteps_total: 1395000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-25_05-39-08\n",
      "  done: false\n",
      "  episode_len_mean: 1500.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 270.06827963002553\n",
      "  episode_reward_mean: 258.65947903692637\n",
      "  episode_reward_min: 253.19500693479281\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 930\n",
      "  experiment_id: 3b224e0d4aa944209a188d13d8dd894d\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.253125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -4.7294779559313245\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.02462565267212106\n",
      "          policy_loss: 0.006608440312438534\n",
      "          total_loss: 189.03942641403717\n",
      "          vf_explained_var: -2.677157828756549e-09\n",
      "          vf_loss: 189.02658462524414\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1395000\n",
      "    num_steps_sampled: 1395000\n",
      "    num_steps_trained: 1395000\n",
      "  iterations_since_restore: 93\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 13.33669467787115\n",
      "    ram_util_percent: 35.35826330532213\n",
      "  pid: 30781\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0846550595832492\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.46910356215619\n",
      "    mean_inference_ms: 1.7425366521824974\n",
      "    mean_raw_obs_processing_ms: 4.884011887737474\n",
      "  time_since_restore: 22833.695207357407\n",
      "  time_this_iter_s: 249.88915514945984\n",
      "  time_total_s: 22833.695207357407\n",
      "  timers:\n",
      "    learn_throughput: 107.193\n",
      "    learn_time_ms: 139934.615\n",
      "    sample_throughput: 141.019\n",
      "    sample_time_ms: 106368.342\n",
      "    update_time_ms: 2.427\n",
      "  timestamp: 1666694348\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1395000\n",
      "  training_iteration: 93\n",
      "  trial_id: '12173_00000'\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 10.8/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.38 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_2_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_1_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_5_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_3_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_0_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_4_edf1e7152919a3617c29d944024e5221, 0.0/6.0 CPU_group_edf1e7152919a3617c29d944024e5221)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_figure_eight\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                  | status   | loc                  |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_AccelEnv-v0_12173_00000 | RUNNING  | 141.225.10.229:30781 |     93 |          22833.7 | 1395000 |  258.659 |              270.068 |              253.195 |               1500 |\n",
      "+-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "Result for PPO_AccelEnv-v0_12173_00000:\n",
      "  agent_timesteps_total: 1410000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-25_05-43-18\n",
      "  done: false\n",
      "  episode_len_mean: 1500.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 265.45307135773413\n",
      "  episode_reward_mean: 257.4160558617469\n",
      "  episode_reward_min: 248.04194853135394\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 940\n",
      "  experiment_id: 3b224e0d4aa944209a188d13d8dd894d\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.253125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -4.712218146809077\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.020071555143498454\n",
      "          policy_loss: 0.005591266557882082\n",
      "          total_loss: 187.56852932946157\n",
      "          vf_explained_var: -1.8689592096166052e-09\n",
      "          vf_loss: 187.55785743584067\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1410000\n",
      "    num_steps_sampled: 1410000\n",
      "    num_steps_trained: 1410000\n",
      "  iterations_since_restore: 94\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 13.384593837535014\n",
      "    ram_util_percent: 35.398039215686275\n",
      "  pid: 30781\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08464719788595884\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.469510283064572\n",
      "    mean_inference_ms: 1.742105230166628\n",
      "    mean_raw_obs_processing_ms: 4.841296799913968\n",
      "  time_since_restore: 23083.7509226799\n",
      "  time_this_iter_s: 250.0557153224945\n",
      "  time_total_s: 23083.7509226799\n",
      "  timers:\n",
      "    learn_throughput: 106.569\n",
      "    learn_time_ms: 140754.292\n",
      "    sample_throughput: 141.031\n",
      "    sample_time_ms: 106359.885\n",
      "    update_time_ms: 2.417\n",
      "  timestamp: 1666694598\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1410000\n",
      "  training_iteration: 94\n",
      "  trial_id: '12173_00000'\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 10.9/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.38 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_5_edf1e7152919a3617c29d944024e5221, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_3_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_1_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_0_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_2_edf1e7152919a3617c29d944024e5221, 0.0/6.0 CPU_group_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_4_edf1e7152919a3617c29d944024e5221)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_figure_eight\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                  | status   | loc                  |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_AccelEnv-v0_12173_00000 | RUNNING  | 141.225.10.229:30781 |     94 |          23083.8 | 1410000 |  257.416 |              265.453 |              248.042 |               1500 |\n",
      "+-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "Result for PPO_AccelEnv-v0_12173_00000:\n",
      "  agent_timesteps_total: 1425000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-25_05-47-29\n",
      "  done: false\n",
      "  episode_len_mean: 1500.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 265.45307135773413\n",
      "  episode_reward_mean: 256.9743652697843\n",
      "  episode_reward_min: 248.04194853135394\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 950\n",
      "  experiment_id: 3b224e0d4aa944209a188d13d8dd894d\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.253125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -4.703384740473861\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01868021823027359\n",
      "          policy_loss: 0.005283788091234736\n",
      "          total_loss: 188.49409539174226\n",
      "          vf_explained_var: -3.2833067376003555e-09\n",
      "          vf_loss: 188.4840832079871\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1425000\n",
      "    num_steps_sampled: 1425000\n",
      "    num_steps_trained: 1425000\n",
      "  iterations_since_restore: 95\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 13.371988795518206\n",
      "    ram_util_percent: 35.42296918767507\n",
      "  pid: 30781\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.084639127386557\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.46984133507405\n",
      "    mean_inference_ms: 1.7417376606086672\n",
      "    mean_raw_obs_processing_ms: 4.799514552263211\n",
      "  time_since_restore: 23334.173558712006\n",
      "  time_this_iter_s: 250.4226360321045\n",
      "  time_total_s: 23334.173558712006\n",
      "  timers:\n",
      "    learn_throughput: 106.02\n",
      "    learn_time_ms: 141483.076\n",
      "    sample_throughput: 141.046\n",
      "    sample_time_ms: 106348.148\n",
      "    update_time_ms: 2.397\n",
      "  timestamp: 1666694849\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1425000\n",
      "  training_iteration: 95\n",
      "  trial_id: '12173_00000'\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 10.9/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.38 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_4_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_1_edf1e7152919a3617c29d944024e5221, 0.0/6.0 CPU_group_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_3_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_0_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_2_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_5_edf1e7152919a3617c29d944024e5221)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_figure_eight\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                  | status   | loc                  |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_AccelEnv-v0_12173_00000 | RUNNING  | 141.225.10.229:30781 |     95 |          23334.2 | 1425000 |  256.974 |              265.453 |              248.042 |               1500 |\n",
      "+-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "Result for PPO_AccelEnv-v0_12173_00000:\n",
      "  agent_timesteps_total: 1440000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-25_05-51-39\n",
      "  done: false\n",
      "  episode_len_mean: 1500.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 265.45307135773413\n",
      "  episode_reward_mean: 256.1484775464611\n",
      "  episode_reward_min: 248.04194853135394\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 960\n",
      "  experiment_id: 3b224e0d4aa944209a188d13d8dd894d\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.253125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -4.671519690448955\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.02451350248190997\n",
      "          policy_loss: 0.007585070071884005\n",
      "          total_loss: 191.13210870451846\n",
      "          vf_explained_var: -2.5554734293109505e-06\n",
      "          vf_loss: 191.11831826032218\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1440000\n",
      "    num_steps_sampled: 1440000\n",
      "    num_steps_trained: 1440000\n",
      "  iterations_since_restore: 96\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 13.296089385474861\n",
      "    ram_util_percent: 35.39720670391062\n",
      "  pid: 30781\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0846318057949605\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.470206745779997\n",
      "    mean_inference_ms: 1.7413845257647507\n",
      "    mean_raw_obs_processing_ms: 4.75863350689999\n",
      "  time_since_restore: 23584.74280142784\n",
      "  time_this_iter_s: 250.56924271583557\n",
      "  time_total_s: 23584.74280142784\n",
      "  timers:\n",
      "    learn_throughput: 105.566\n",
      "    learn_time_ms: 142090.576\n",
      "    sample_throughput: 141.027\n",
      "    sample_time_ms: 106362.423\n",
      "    update_time_ms: 2.436\n",
      "  timestamp: 1666695099\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1440000\n",
      "  training_iteration: 96\n",
      "  trial_id: '12173_00000'\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 10.9/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.38 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_3_edf1e7152919a3617c29d944024e5221, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_4_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_5_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_2_edf1e7152919a3617c29d944024e5221, 0.0/6.0 CPU_group_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_1_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_0_edf1e7152919a3617c29d944024e5221)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_figure_eight\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                  | status   | loc                  |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_AccelEnv-v0_12173_00000 | RUNNING  | 141.225.10.229:30781 |     96 |          23584.7 | 1440000 |  256.148 |              265.453 |              248.042 |               1500 |\n",
      "+-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "Result for PPO_AccelEnv-v0_12173_00000:\n",
      "  agent_timesteps_total: 1455000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-25_05-55-50\n",
      "  done: false\n",
      "  episode_len_mean: 1500.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 263.3728668781773\n",
      "  episode_reward_mean: 255.3921048630491\n",
      "  episode_reward_min: 248.04194853135394\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 970\n",
      "  experiment_id: 3b224e0d4aa944209a188d13d8dd894d\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.253125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -4.690136509426569\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.02936512349636666\n",
      "          policy_loss: 0.008856764289361062\n",
      "          total_loss: 193.82370743509065\n",
      "          vf_explained_var: -1.2621536598089733e-06\n",
      "          vf_loss: 193.80741735878638\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1455000\n",
      "    num_steps_sampled: 1455000\n",
      "    num_steps_trained: 1455000\n",
      "  iterations_since_restore: 97\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 13.36731843575419\n",
      "    ram_util_percent: 35.457821229050275\n",
      "  pid: 30781\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0846245635256716\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.47047326580405\n",
      "    mean_inference_ms: 1.7411034074037002\n",
      "    mean_raw_obs_processing_ms: 4.71861675624032\n",
      "  time_since_restore: 23835.762391090393\n",
      "  time_this_iter_s: 251.01958966255188\n",
      "  time_total_s: 23835.762391090393\n",
      "  timers:\n",
      "    learn_throughput: 105.091\n",
      "    learn_time_ms: 142733.68\n",
      "    sample_throughput: 141.04\n",
      "    sample_time_ms: 106352.443\n",
      "    update_time_ms: 2.366\n",
      "  timestamp: 1666695350\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1455000\n",
      "  training_iteration: 97\n",
      "  trial_id: '12173_00000'\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Memory usage on this node: 10.9/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.38 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_5_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_2_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_3_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_1_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_0_edf1e7152919a3617c29d944024e5221, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_4_edf1e7152919a3617c29d944024e5221, 0.0/6.0 CPU_group_edf1e7152919a3617c29d944024e5221)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_figure_eight\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                  | status   | loc                  |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_AccelEnv-v0_12173_00000 | RUNNING  | 141.225.10.229:30781 |     97 |          23835.8 | 1455000 |  255.392 |              263.373 |              248.042 |               1500 |\n",
      "+-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "Result for PPO_AccelEnv-v0_12173_00000:\n",
      "  agent_timesteps_total: 1470000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-25_06-00-02\n",
      "  done: false\n",
      "  episode_len_mean: 1500.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 259.8963611294749\n",
      "  episode_reward_mean: 254.720003440201\n",
      "  episode_reward_min: 248.04194853135394\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 980\n",
      "  experiment_id: 3b224e0d4aa944209a188d13d8dd894d\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.253125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -4.723793387008926\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.02310403085978905\n",
      "          policy_loss: 0.005996440137120879\n",
      "          total_loss: 193.9715168290219\n",
      "          vf_explained_var: 1.0102482045359906e-10\n",
      "          vf_loss: 193.95967337074927\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1470000\n",
      "    num_steps_sampled: 1470000\n",
      "    num_steps_trained: 1470000\n",
      "  iterations_since_restore: 98\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 13.152646239554318\n",
      "    ram_util_percent: 35.42813370473538\n",
      "  pid: 30781\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08461791412242432\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.470670353918\n",
      "    mean_inference_ms: 1.7408903673583\n",
      "    mean_raw_obs_processing_ms: 4.679449389953679\n",
      "  time_since_restore: 24087.747938871384\n",
      "  time_this_iter_s: 251.9855477809906\n",
      "  time_total_s: 24087.747938871384\n",
      "  timers:\n",
      "    learn_throughput: 104.588\n",
      "    learn_time_ms: 143419.895\n",
      "    sample_throughput: 141.039\n",
      "    sample_time_ms: 106353.504\n",
      "    update_time_ms: 2.349\n",
      "  timestamp: 1666695602\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1470000\n",
      "  training_iteration: 98\n",
      "  trial_id: '12173_00000'\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 10.9/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.38 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_5_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_4_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_2_edf1e7152919a3617c29d944024e5221, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_0_edf1e7152919a3617c29d944024e5221, 0.0/6.0 CPU_group_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_3_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_1_edf1e7152919a3617c29d944024e5221)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_figure_eight\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                  | status   | loc                  |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_AccelEnv-v0_12173_00000 | RUNNING  | 141.225.10.229:30781 |     98 |          24087.7 | 1470000 |   254.72 |              259.896 |              248.042 |               1500 |\n",
      "+-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_AccelEnv-v0_12173_00000:\n",
      "  agent_timesteps_total: 1485000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-25_06-04-14\n",
      "  done: false\n",
      "  episode_len_mean: 1500.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 259.83535670310545\n",
      "  episode_reward_mean: 254.49668800297863\n",
      "  episode_reward_min: 248.04194853135394\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 990\n",
      "  experiment_id: 3b224e0d4aa944209a188d13d8dd894d\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.253125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -4.708158804198443\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.02393974722634671\n",
      "          policy_loss: 0.007850664415617742\n",
      "          total_loss: 195.24433336742854\n",
      "          vf_explained_var: -2.576132862586178e-09\n",
      "          vf_loss: 195.23042277966516\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1485000\n",
      "    num_steps_sampled: 1485000\n",
      "    num_steps_trained: 1485000\n",
      "  iterations_since_restore: 99\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 13.25\n",
      "    ram_util_percent: 35.38333333333333\n",
      "  pid: 30781\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08461189894125262\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.470761381007378\n",
      "    mean_inference_ms: 1.7407373351823663\n",
      "    mean_raw_obs_processing_ms: 4.641121135983334\n",
      "  time_since_restore: 24339.455492019653\n",
      "  time_this_iter_s: 251.70755314826965\n",
      "  time_total_s: 24339.455492019653\n",
      "  timers:\n",
      "    learn_throughput: 104.433\n",
      "    learn_time_ms: 143633.158\n",
      "    sample_throughput: 141.063\n",
      "    sample_time_ms: 106335.484\n",
      "    update_time_ms: 2.362\n",
      "  timestamp: 1666695854\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1485000\n",
      "  training_iteration: 99\n",
      "  trial_id: '12173_00000'\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 10.8/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.38 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_0_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_5_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_1_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_2_edf1e7152919a3617c29d944024e5221, 0.0/6.0 CPU_group_edf1e7152919a3617c29d944024e5221, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_4_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_3_edf1e7152919a3617c29d944024e5221)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_figure_eight\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                  | status   | loc                  |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_AccelEnv-v0_12173_00000 | RUNNING  | 141.225.10.229:30781 |     99 |          24339.5 | 1485000 |  254.497 |              259.835 |              248.042 |               1500 |\n",
      "+-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "Result for PPO_AccelEnv-v0_12173_00000:\n",
      "  agent_timesteps_total: 1500000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-25_06-08-26\n",
      "  done: false\n",
      "  episode_len_mean: 1500.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 258.5397849930053\n",
      "  episode_reward_mean: 254.24432282995593\n",
      "  episode_reward_min: 248.04194853135394\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1000\n",
      "  experiment_id: 3b224e0d4aa944209a188d13d8dd894d\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.253125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -4.70769203800266\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.020723509049023082\n",
      "          policy_loss: 0.0063514555899142205\n",
      "          total_loss: 198.23438997107036\n",
      "          vf_explained_var: -1.565884755194702e-09\n",
      "          vf_loss: 198.22279261249608\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1500000\n",
      "    num_steps_sampled: 1500000\n",
      "    num_steps_trained: 1500000\n",
      "  iterations_since_restore: 100\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 13.320670391061453\n",
      "    ram_util_percent: 35.43100558659218\n",
      "  pid: 30781\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08460600602739742\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.470850074300216\n",
      "    mean_inference_ms: 1.7405911359388375\n",
      "    mean_raw_obs_processing_ms: 4.603635503264505\n",
      "  time_since_restore: 24590.876834869385\n",
      "  time_this_iter_s: 251.42134284973145\n",
      "  time_total_s: 24590.876834869385\n",
      "  timers:\n",
      "    learn_throughput: 104.072\n",
      "    learn_time_ms: 144130.826\n",
      "    sample_throughput: 141.049\n",
      "    sample_time_ms: 106346.128\n",
      "    update_time_ms: 2.33\n",
      "  timestamp: 1666696106\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1500000\n",
      "  training_iteration: 100\n",
      "  trial_id: '12173_00000'\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 10.9/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.38 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_0_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_4_edf1e7152919a3617c29d944024e5221, 0.0/6.0 CPU_group_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_5_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_3_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_2_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_1_edf1e7152919a3617c29d944024e5221)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_figure_eight\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                  | status   | loc                  |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_AccelEnv-v0_12173_00000 | RUNNING  | 141.225.10.229:30781 |    100 |          24590.9 | 1500000 |  254.244 |               258.54 |              248.042 |               1500 |\n",
      "+-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "Result for PPO_AccelEnv-v0_12173_00000:\n",
      "  agent_timesteps_total: 1515000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-25_06-12-40\n",
      "  done: false\n",
      "  episode_len_mean: 1500.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 257.9928740614547\n",
      "  episode_reward_mean: 254.13626488483132\n",
      "  episode_reward_min: 248.04194853135394\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1010\n",
      "  experiment_id: 3b224e0d4aa944209a188d13d8dd894d\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.253125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -4.7168008970001996\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.018740147889165802\n",
      "          policy_loss: 0.006926151026056101\n",
      "          total_loss: 201.51298616053694\n",
      "          vf_explained_var: -2.6266453456713634e-09\n",
      "          vf_loss: 201.50131616107487\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1515000\n",
      "    num_steps_sampled: 1515000\n",
      "    num_steps_trained: 1515000\n",
      "  iterations_since_restore: 101\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 13.187878787878788\n",
      "    ram_util_percent: 35.45096418732782\n",
      "  pid: 30781\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08460106299519012\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.470912080922417\n",
      "    mean_inference_ms: 1.7404766157616347\n",
      "    mean_raw_obs_processing_ms: 4.566933021379095\n",
      "  time_since_restore: 24845.275610923767\n",
      "  time_this_iter_s: 254.39877605438232\n",
      "  time_total_s: 24845.275610923767\n",
      "  timers:\n",
      "    learn_throughput: 103.547\n",
      "    learn_time_ms: 144862.086\n",
      "    sample_throughput: 141.057\n",
      "    sample_time_ms: 106340.285\n",
      "    update_time_ms: 2.355\n",
      "  timestamp: 1666696360\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1515000\n",
      "  training_iteration: 101\n",
      "  trial_id: '12173_00000'\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 10.9/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.38 GiB heap, 0.0/14.65 GiB objects (0.0/6.0 CPU_group_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_2_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_1_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_0_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_5_edf1e7152919a3617c29d944024e5221, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_4_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_3_edf1e7152919a3617c29d944024e5221)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_figure_eight\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                  | status   | loc                  |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_AccelEnv-v0_12173_00000 | RUNNING  | 141.225.10.229:30781 |    101 |          24845.3 | 1515000 |  254.136 |              257.993 |              248.042 |               1500 |\n",
      "+-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "Result for PPO_AccelEnv-v0_12173_00000:\n",
      "  agent_timesteps_total: 1530000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-25_06-16-55\n",
      "  done: false\n",
      "  episode_len_mean: 1500.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 257.9928740614547\n",
      "  episode_reward_mean: 253.75889154349906\n",
      "  episode_reward_min: 248.04194853135394\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1020\n",
      "  experiment_id: 3b224e0d4aa944209a188d13d8dd894d\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.253125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -4.670340238183232\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01865939160503195\n",
      "          policy_loss: 0.004817444917556466\n",
      "          total_loss: 201.20083961486816\n",
      "          vf_explained_var: -1.0654077868821332e-06\n",
      "          vf_loss: 201.19129923157772\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1530000\n",
      "    num_steps_sampled: 1530000\n",
      "    num_steps_trained: 1530000\n",
      "  iterations_since_restore: 102\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 13.233241758241757\n",
      "    ram_util_percent: 35.46181318681318\n",
      "  pid: 30781\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08459648216667795\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.470982744331693\n",
      "    mean_inference_ms: 1.7403538889477022\n",
      "    mean_raw_obs_processing_ms: 4.530971393911488\n",
      "  time_since_restore: 25099.856618881226\n",
      "  time_this_iter_s: 254.5810079574585\n",
      "  time_total_s: 25099.856618881226\n",
      "  timers:\n",
      "    learn_throughput: 103.259\n",
      "    learn_time_ms: 145265.848\n",
      "    sample_throughput: 141.075\n",
      "    sample_time_ms: 106326.581\n",
      "    update_time_ms: 2.37\n",
      "  timestamp: 1666696615\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1530000\n",
      "  training_iteration: 102\n",
      "  trial_id: '12173_00000'\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Memory usage on this node: 10.9/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.38 GiB heap, 0.0/14.65 GiB objects (0.0/6.0 CPU_group_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_5_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_0_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_1_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_3_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_4_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_2_edf1e7152919a3617c29d944024e5221, 0.0/1.0 accelerator_type:G)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_figure_eight\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                  | status   | loc                  |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_AccelEnv-v0_12173_00000 | RUNNING  | 141.225.10.229:30781 |    102 |          25099.9 | 1530000 |  253.759 |              257.993 |              248.042 |               1500 |\n",
      "+-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "Result for PPO_AccelEnv-v0_12173_00000:\n",
      "  agent_timesteps_total: 1545000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-25_06-21-13\n",
      "  done: false\n",
      "  episode_len_mean: 1500.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 257.9928740614547\n",
      "  episode_reward_mean: 252.88275802006308\n",
      "  episode_reward_min: 243.98058918224868\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1030\n",
      "  experiment_id: 3b224e0d4aa944209a188d13d8dd894d\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.253125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -4.6330190444396715\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01993001332095804\n",
      "          policy_loss: 0.005419671252148889\n",
      "          total_loss: 204.30590720419156\n",
      "          vf_explained_var: -1.0607605904766615e-09\n",
      "          vf_loss: 204.29544310165664\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1545000\n",
      "    num_steps_sampled: 1545000\n",
      "    num_steps_trained: 1545000\n",
      "  iterations_since_restore: 103\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 13.214130434782607\n",
      "    ram_util_percent: 35.43994565217392\n",
      "  pid: 30781\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08459154339191523\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.47108372541582\n",
      "    mean_inference_ms: 1.740218179080603\n",
      "    mean_raw_obs_processing_ms: 4.495725981049054\n",
      "  time_since_restore: 25357.635885238647\n",
      "  time_this_iter_s: 257.7792663574219\n",
      "  time_total_s: 25357.635885238647\n",
      "  timers:\n",
      "    learn_throughput: 102.701\n",
      "    learn_time_ms: 146054.493\n",
      "    sample_throughput: 141.074\n",
      "    sample_time_ms: 106327.512\n",
      "    update_time_ms: 2.368\n",
      "  timestamp: 1666696873\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1545000\n",
      "  training_iteration: 103\n",
      "  trial_id: '12173_00000'\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 10.8/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.38 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_4_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_3_edf1e7152919a3617c29d944024e5221, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_2_edf1e7152919a3617c29d944024e5221, 0.0/6.0 CPU_group_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_1_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_0_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_5_edf1e7152919a3617c29d944024e5221)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_figure_eight\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                  | status   | loc                  |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_AccelEnv-v0_12173_00000 | RUNNING  | 141.225.10.229:30781 |    103 |          25357.6 | 1545000 |  252.883 |              257.993 |              243.981 |               1500 |\n",
      "+-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "Result for PPO_AccelEnv-v0_12173_00000:\n",
      "  agent_timesteps_total: 1560000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-25_06-25-33\n",
      "  done: false\n",
      "  episode_len_mean: 1500.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 257.9928740614547\n",
      "  episode_reward_mean: 251.82315958697552\n",
      "  episode_reward_min: 237.26991158772572\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1040\n",
      "  experiment_id: 3b224e0d4aa944209a188d13d8dd894d\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.253125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -4.619252803366063\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.018088700112334894\n",
      "          policy_loss: 0.004315000424414115\n",
      "          total_loss: 201.61158300820043\n",
      "          vf_explained_var: -1.9194716927017907e-09\n",
      "          vf_loss: 201.60268930014917\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1560000\n",
      "    num_steps_sampled: 1560000\n",
      "    num_steps_trained: 1560000\n",
      "  iterations_since_restore: 104\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 13.067924528301885\n",
      "    ram_util_percent: 35.460916442048514\n",
      "  pid: 30781\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08458703744134383\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.47119679573045\n",
      "    mean_inference_ms: 1.740094483859453\n",
      "    mean_raw_obs_processing_ms: 4.461183712735895\n",
      "  time_since_restore: 25618.09016442299\n",
      "  time_this_iter_s: 260.45427918434143\n",
      "  time_total_s: 25618.09016442299\n",
      "  timers:\n",
      "    learn_throughput: 101.982\n",
      "    learn_time_ms: 147085.129\n",
      "    sample_throughput: 141.061\n",
      "    sample_time_ms: 106336.636\n",
      "    update_time_ms: 2.427\n",
      "  timestamp: 1666697133\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1560000\n",
      "  training_iteration: 104\n",
      "  trial_id: '12173_00000'\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 10.9/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.38 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_0_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_1_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_4_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_5_edf1e7152919a3617c29d944024e5221, 0.0/6.0 CPU_group_edf1e7152919a3617c29d944024e5221, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_3_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_2_edf1e7152919a3617c29d944024e5221)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_figure_eight\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                  | status   | loc                  |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_AccelEnv-v0_12173_00000 | RUNNING  | 141.225.10.229:30781 |    104 |          25618.1 | 1560000 |  251.823 |              257.993 |               237.27 |               1500 |\n",
      "+-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "Result for PPO_AccelEnv-v0_12173_00000:\n",
      "  agent_timesteps_total: 1575000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-25_06-30-02\n",
      "  done: false\n",
      "  episode_len_mean: 1500.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 257.9928740614547\n",
      "  episode_reward_mean: 250.54265985427207\n",
      "  episode_reward_min: 237.26991158772572\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1050\n",
      "  experiment_id: 3b224e0d4aa944209a188d13d8dd894d\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.253125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -4.626146447860588\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.023557342671970762\n",
      "          policy_loss: 0.007173898268956691\n",
      "          total_loss: 206.03187957214098\n",
      "          vf_explained_var: -6.061489088438066e-10\n",
      "          vf_loss: 206.0187428668394\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1575000\n",
      "    num_steps_sampled: 1575000\n",
      "    num_steps_trained: 1575000\n",
      "  iterations_since_restore: 105\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 12.782506527415142\n",
      "    ram_util_percent: 35.527676240208876\n",
      "  pid: 30781\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08458425767385978\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.471313468969065\n",
      "    mean_inference_ms: 1.739977280162691\n",
      "    mean_raw_obs_processing_ms: 4.4273399897328405\n",
      "  time_since_restore: 25886.50744152069\n",
      "  time_this_iter_s: 268.417277097702\n",
      "  time_total_s: 25886.50744152069\n",
      "  timers:\n",
      "    learn_throughput: 100.75\n",
      "    learn_time_ms: 148883.25\n",
      "    sample_throughput: 141.06\n",
      "    sample_time_ms: 106337.63\n",
      "    update_time_ms: 2.462\n",
      "  timestamp: 1666697402\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1575000\n",
      "  training_iteration: 105\n",
      "  trial_id: '12173_00000'\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 10.9/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.38 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_3_edf1e7152919a3617c29d944024e5221, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_1_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_0_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_5_edf1e7152919a3617c29d944024e5221, 0.0/6.0 CPU_group_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_2_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_4_edf1e7152919a3617c29d944024e5221)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_figure_eight\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                  | status   | loc                  |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_AccelEnv-v0_12173_00000 | RUNNING  | 141.225.10.229:30781 |    105 |          25886.5 | 1575000 |  250.543 |              257.993 |               237.27 |               1500 |\n",
      "+-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "Result for PPO_AccelEnv-v0_12173_00000:\n",
      "  agent_timesteps_total: 1590000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-25_06-34-31\n",
      "  done: false\n",
      "  episode_len_mean: 1500.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 257.9928740614547\n",
      "  episode_reward_mean: 249.55349188081007\n",
      "  episode_reward_min: 237.26991158772572\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1060\n",
      "  experiment_id: 3b224e0d4aa944209a188d13d8dd894d\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.253125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -4.645730086908502\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.020488619808507517\n",
      "          policy_loss: 0.0052730279560279794\n",
      "          total_loss: 203.05831464347193\n",
      "          vf_explained_var: -1.111273073561847e-09\n",
      "          vf_loss: 203.04785584271966\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1590000\n",
      "    num_steps_sampled: 1590000\n",
      "    num_steps_trained: 1590000\n",
      "  iterations_since_restore: 106\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 12.851041666666665\n",
      "    ram_util_percent: 35.531770833333326\n",
      "  pid: 30781\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08458122758733856\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.471358434599956\n",
      "    mean_inference_ms: 1.7398701593000996\n",
      "    mean_raw_obs_processing_ms: 4.394154017228951\n",
      "  time_since_restore: 26155.27168393135\n",
      "  time_this_iter_s: 268.7642424106598\n",
      "  time_total_s: 26155.27168393135\n",
      "  timers:\n",
      "    learn_throughput: 99.516\n",
      "    learn_time_ms: 150730.051\n",
      "    sample_throughput: 141.097\n",
      "    sample_time_ms: 106310.18\n",
      "    update_time_ms: 2.463\n",
      "  timestamp: 1666697671\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1590000\n",
      "  training_iteration: 106\n",
      "  trial_id: '12173_00000'\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 10.9/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.38 GiB heap, 0.0/14.65 GiB objects (0.0/6.0 CPU_group_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_1_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_5_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_4_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_0_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_3_edf1e7152919a3617c29d944024e5221, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_2_edf1e7152919a3617c29d944024e5221)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_figure_eight\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                  | status   | loc                  |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_AccelEnv-v0_12173_00000 | RUNNING  | 141.225.10.229:30781 |    106 |          26155.3 | 1590000 |  249.553 |              257.993 |               237.27 |               1500 |\n",
      "+-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "Result for PPO_AccelEnv-v0_12173_00000:\n",
      "  agent_timesteps_total: 1605000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-25_06-38-58\n",
      "  done: false\n",
      "  episode_len_mean: 1500.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 257.9928740614547\n",
      "  episode_reward_mean: 247.5744335820457\n",
      "  episode_reward_min: 232.92041970912604\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1070\n",
      "  experiment_id: 3b224e0d4aa944209a188d13d8dd894d\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.253125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -4.621317618580187\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.020320703736982627\n",
      "          policy_loss: 0.0053906699620834475\n",
      "          total_loss: 204.61986679465085\n",
      "          vf_explained_var: -1.8320851324915566e-07\n",
      "          vf_loss: 204.60933219780355\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1605000\n",
      "    num_steps_sampled: 1605000\n",
      "    num_steps_trained: 1605000\n",
      "  iterations_since_restore: 107\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 13.295811518324607\n",
      "    ram_util_percent: 35.61413612565445\n",
      "  pid: 30781\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08457749231573856\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.471419749847872\n",
      "    mean_inference_ms: 1.7397481471376495\n",
      "    mean_raw_obs_processing_ms: 4.361620046580539\n",
      "  time_since_restore: 26423.00814652443\n",
      "  time_this_iter_s: 267.7364625930786\n",
      "  time_total_s: 26423.00814652443\n",
      "  timers:\n",
      "    learn_throughput: 98.428\n",
      "    learn_time_ms: 152395.192\n",
      "    sample_throughput: 141.088\n",
      "    sample_time_ms: 106316.675\n",
      "    update_time_ms: 2.533\n",
      "  timestamp: 1666697938\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1605000\n",
      "  training_iteration: 107\n",
      "  trial_id: '12173_00000'\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Memory usage on this node: 10.9/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.38 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_2_edf1e7152919a3617c29d944024e5221, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_0_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_4_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_5_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_3_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_1_edf1e7152919a3617c29d944024e5221, 0.0/6.0 CPU_group_edf1e7152919a3617c29d944024e5221)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_figure_eight\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                  | status   | loc                  |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_AccelEnv-v0_12173_00000 | RUNNING  | 141.225.10.229:30781 |    107 |            26423 | 1605000 |  247.574 |              257.993 |               232.92 |               1500 |\n",
      "+-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "Result for PPO_AccelEnv-v0_12173_00000:\n",
      "  agent_timesteps_total: 1620000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-25_06-43-29\n",
      "  done: false\n",
      "  episode_len_mean: 1500.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 257.9928740614547\n",
      "  episode_reward_mean: 245.78939591735303\n",
      "  episode_reward_min: 232.92041970912604\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1080\n",
      "  experiment_id: 3b224e0d4aa944209a188d13d8dd894d\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.253125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -4.6109155040676315\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.02161404312743\n",
      "          policy_loss: 0.0066165030507718105\n",
      "          total_loss: 205.96067021256786\n",
      "          vf_explained_var: -2.2730584081642746e-09\n",
      "          vf_loss: 205.9485836352332\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1620000\n",
      "    num_steps_sampled: 1620000\n",
      "    num_steps_trained: 1620000\n",
      "  iterations_since_restore: 108\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 12.805958549222796\n",
      "    ram_util_percent: 35.58367875647668\n",
      "  pid: 30781\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08457383996983182\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.471512863125145\n",
      "    mean_inference_ms: 1.7396001100562881\n",
      "    mean_raw_obs_processing_ms: 4.32972565520892\n",
      "  time_since_restore: 26693.27544760704\n",
      "  time_this_iter_s: 270.2673010826111\n",
      "  time_total_s: 26693.27544760704\n",
      "  timers:\n",
      "    learn_throughput: 97.263\n",
      "    learn_time_ms: 154220.424\n",
      "    sample_throughput: 141.084\n",
      "    sample_time_ms: 106319.833\n",
      "    update_time_ms: 2.537\n",
      "  timestamp: 1666698209\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1620000\n",
      "  training_iteration: 108\n",
      "  trial_id: '12173_00000'\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 10.9/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.38 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_4_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_0_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_3_edf1e7152919a3617c29d944024e5221, 0.0/6.0 CPU_group_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_5_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_1_edf1e7152919a3617c29d944024e5221, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_2_edf1e7152919a3617c29d944024e5221)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_figure_eight\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                  | status   | loc                  |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_AccelEnv-v0_12173_00000 | RUNNING  | 141.225.10.229:30781 |    108 |          26693.3 | 1620000 |  245.789 |              257.993 |               232.92 |               1500 |\n",
      "+-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_AccelEnv-v0_12173_00000:\n",
      "  agent_timesteps_total: 1635000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-25_06-47-59\n",
      "  done: false\n",
      "  episode_len_mean: 1500.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 257.9928740614547\n",
      "  episode_reward_mean: 243.6908950092489\n",
      "  episode_reward_min: 232.92041970912604\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1090\n",
      "  experiment_id: 3b224e0d4aa944209a188d13d8dd894d\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.253125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -4.616890812324265\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.02121191785824494\n",
      "          policy_loss: 0.0062381833756216245\n",
      "          total_loss: 205.43926679643533\n",
      "          vf_explained_var: -3.687405936148025e-09\n",
      "          vf_loss: 205.42765945499227\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1635000\n",
      "    num_steps_sampled: 1635000\n",
      "    num_steps_trained: 1635000\n",
      "  iterations_since_restore: 109\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 12.847150259067357\n",
      "    ram_util_percent: 35.55362694300518\n",
      "  pid: 30781\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08456979468144532\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.47160814114128\n",
      "    mean_inference_ms: 1.7394574544435104\n",
      "    mean_raw_obs_processing_ms: 4.29843524325535\n",
      "  time_since_restore: 26963.827766656876\n",
      "  time_this_iter_s: 270.5523190498352\n",
      "  time_total_s: 26963.827766656876\n",
      "  timers:\n",
      "    learn_throughput: 96.091\n",
      "    learn_time_ms: 156101.409\n",
      "    sample_throughput: 141.08\n",
      "    sample_time_ms: 106322.678\n",
      "    update_time_ms: 2.553\n",
      "  timestamp: 1666698479\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1635000\n",
      "  training_iteration: 109\n",
      "  trial_id: '12173_00000'\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 10.9/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.38 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_4_edf1e7152919a3617c29d944024e5221, 0.0/6.0 CPU_group_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_1_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_5_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_0_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_3_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_2_edf1e7152919a3617c29d944024e5221, 0.0/1.0 accelerator_type:G)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_figure_eight\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                  | status   | loc                  |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_AccelEnv-v0_12173_00000 | RUNNING  | 141.225.10.229:30781 |    109 |          26963.8 | 1635000 |  243.691 |              257.993 |               232.92 |               1500 |\n",
      "+-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "Result for PPO_AccelEnv-v0_12173_00000:\n",
      "  agent_timesteps_total: 1650000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-25_06-52-32\n",
      "  done: false\n",
      "  episode_len_mean: 1500.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 256.59371398879824\n",
      "  episode_reward_mean: 242.04395260016113\n",
      "  episode_reward_min: 232.92041970912604\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1100\n",
      "  experiment_id: 3b224e0d4aa944209a188d13d8dd894d\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.253125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -4.613951893984261\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.019113238495764892\n",
      "          policy_loss: 0.00504926421030393\n",
      "          total_loss: 205.47072984727762\n",
      "          vf_explained_var: -2.9802322831784522e-09\n",
      "          vf_loss: 205.46084237826074\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1650000\n",
      "    num_steps_sampled: 1650000\n",
      "    num_steps_trained: 1650000\n",
      "  iterations_since_restore: 110\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 12.665552699228794\n",
      "    ram_util_percent: 35.55758354755784\n",
      "  pid: 30781\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08456512620969178\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.47174955204823\n",
      "    mean_inference_ms: 1.739280594738986\n",
      "    mean_raw_obs_processing_ms: 4.267725361661788\n",
      "  time_since_restore: 27236.741216897964\n",
      "  time_this_iter_s: 272.91345024108887\n",
      "  time_total_s: 27236.741216897964\n",
      "  timers:\n",
      "    learn_throughput: 94.786\n",
      "    learn_time_ms: 158250.439\n",
      "    sample_throughput: 141.079\n",
      "    sample_time_ms: 106323.191\n",
      "    update_time_ms: 2.614\n",
      "  timestamp: 1666698752\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1650000\n",
      "  training_iteration: 110\n",
      "  trial_id: '12173_00000'\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 10.9/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.38 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_5_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_3_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_4_edf1e7152919a3617c29d944024e5221, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_2_edf1e7152919a3617c29d944024e5221, 0.0/6.0 CPU_group_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_0_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_1_edf1e7152919a3617c29d944024e5221)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_figure_eight\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                  | status   | loc                  |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_AccelEnv-v0_12173_00000 | RUNNING  | 141.225.10.229:30781 |    110 |          27236.7 | 1650000 |  242.044 |              256.594 |               232.92 |               1500 |\n",
      "+-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "Result for PPO_AccelEnv-v0_12173_00000:\n",
      "  agent_timesteps_total: 1665000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-25_06-56-54\n",
      "  done: false\n",
      "  episode_len_mean: 1500.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 252.5541416981537\n",
      "  episode_reward_mean: 240.84478318805014\n",
      "  episode_reward_min: 232.92041970912604\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1110\n",
      "  experiment_id: 3b224e0d4aa944209a188d13d8dd894d\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.253125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -4.631958724684634\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.02112074321259214\n",
      "          policy_loss: 0.005605062968847421\n",
      "          total_loss: 206.85316320516296\n",
      "          vf_explained_var: -1.8184468375537222e-09\n",
      "          vf_loss: 206.84221216298766\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1665000\n",
      "    num_steps_sampled: 1665000\n",
      "    num_steps_trained: 1665000\n",
      "  iterations_since_restore: 111\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 13.068096514745307\n",
      "    ram_util_percent: 35.55361930294906\n",
      "  pid: 30781\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08456059601611816\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.47189589925628\n",
      "    mean_inference_ms: 1.7390853107370652\n",
      "    mean_raw_obs_processing_ms: 4.237589780533244\n",
      "  time_since_restore: 27498.056153535843\n",
      "  time_this_iter_s: 261.3149366378784\n",
      "  time_total_s: 27498.056153535843\n",
      "  timers:\n",
      "    learn_throughput: 94.369\n",
      "    learn_time_ms: 158950.739\n",
      "    sample_throughput: 141.091\n",
      "    sample_time_ms: 106314.3\n",
      "    update_time_ms: 2.558\n",
      "  timestamp: 1666699014\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1665000\n",
      "  training_iteration: 111\n",
      "  trial_id: '12173_00000'\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 10.9/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.38 GiB heap, 0.0/14.65 GiB objects (0.0/6.0 CPU_group_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_5_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_3_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_2_edf1e7152919a3617c29d944024e5221, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_0_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_1_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_4_edf1e7152919a3617c29d944024e5221)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_figure_eight\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                  | status   | loc                  |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_AccelEnv-v0_12173_00000 | RUNNING  | 141.225.10.229:30781 |    111 |          27498.1 | 1665000 |  240.845 |              252.554 |               232.92 |               1500 |\n",
      "+-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "Result for PPO_AccelEnv-v0_12173_00000:\n",
      "  agent_timesteps_total: 1680000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-25_07-01-25\n",
      "  done: false\n",
      "  episode_len_mean: 1500.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 247.9636323201205\n",
      "  episode_reward_mean: 239.80921053047226\n",
      "  episode_reward_min: 232.92041970912604\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1120\n",
      "  experiment_id: 3b224e0d4aa944209a188d13d8dd894d\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.253125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -4.624042274184146\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.022132974876223285\n",
      "          policy_loss: 0.006560654116605045\n",
      "          total_loss: 207.2549290770191\n",
      "          vf_explained_var: -4.1925298788214604e-09\n",
      "          vf_loss: 207.24276606349622\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1680000\n",
      "    num_steps_sampled: 1680000\n",
      "    num_steps_trained: 1680000\n",
      "  iterations_since_restore: 112\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 12.832299741602068\n",
      "    ram_util_percent: 35.63695090439276\n",
      "  pid: 30781\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0845559618439075\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.47208185292645\n",
      "    mean_inference_ms: 1.7388788437435563\n",
      "    mean_raw_obs_processing_ms: 4.208036987761051\n",
      "  time_since_restore: 27769.082751750946\n",
      "  time_this_iter_s: 271.02659821510315\n",
      "  time_total_s: 27769.082751750946\n",
      "  timers:\n",
      "    learn_throughput: 93.414\n",
      "    learn_time_ms: 160575.423\n",
      "    sample_throughput: 141.064\n",
      "    sample_time_ms: 106334.592\n",
      "    update_time_ms: 2.527\n",
      "  timestamp: 1666699285\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1680000\n",
      "  training_iteration: 112\n",
      "  trial_id: '12173_00000'\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Memory usage on this node: 10.9/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.38 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_3_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_4_edf1e7152919a3617c29d944024e5221, 0.0/6.0 CPU_group_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_2_edf1e7152919a3617c29d944024e5221, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_5_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_0_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_1_edf1e7152919a3617c29d944024e5221)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_figure_eight\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                  | status   | loc                  |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_AccelEnv-v0_12173_00000 | RUNNING  | 141.225.10.229:30781 |    112 |          27769.1 | 1680000 |  239.809 |              247.964 |               232.92 |               1500 |\n",
      "+-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "Result for PPO_AccelEnv-v0_12173_00000:\n",
      "  agent_timesteps_total: 1695000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-25_07-05-59\n",
      "  done: false\n",
      "  episode_len_mean: 1500.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 245.0606776852276\n",
      "  episode_reward_mean: 239.500351078599\n",
      "  episode_reward_min: 232.92041970912604\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1130\n",
      "  experiment_id: 3b224e0d4aa944209a188d13d8dd894d\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.253125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -4.648827221838094\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.02211041915141301\n",
      "          policy_loss: 0.006084161795581044\n",
      "          total_loss: 209.59578885870465\n",
      "          vf_explained_var: -1.0607605904766615e-09\n",
      "          vf_loss: 209.5841081457623\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1695000\n",
      "    num_steps_sampled: 1695000\n",
      "    num_steps_trained: 1695000\n",
      "  iterations_since_restore: 113\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 12.734015345268542\n",
      "    ram_util_percent: 35.62020460358056\n",
      "  pid: 30781\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08455127400808359\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.472218336976876\n",
      "    mean_inference_ms: 1.7387173825982136\n",
      "    mean_raw_obs_processing_ms: 4.1790429538644505\n",
      "  time_since_restore: 28043.03620648384\n",
      "  time_this_iter_s: 273.9534547328949\n",
      "  time_total_s: 28043.03620648384\n",
      "  timers:\n",
      "    learn_throughput: 92.487\n",
      "    learn_time_ms: 162185.687\n",
      "    sample_throughput: 141.055\n",
      "    sample_time_ms: 106341.465\n",
      "    update_time_ms: 2.496\n",
      "  timestamp: 1666699559\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1695000\n",
      "  training_iteration: 113\n",
      "  trial_id: '12173_00000'\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 10.9/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.38 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_4_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_2_edf1e7152919a3617c29d944024e5221, 0.0/6.0 CPU_group_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_1_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_5_edf1e7152919a3617c29d944024e5221, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_3_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_0_edf1e7152919a3617c29d944024e5221)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_figure_eight\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                  | status   | loc                  |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_AccelEnv-v0_12173_00000 | RUNNING  | 141.225.10.229:30781 |    113 |            28043 | 1695000 |    239.5 |              245.061 |               232.92 |               1500 |\n",
      "+-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "Result for PPO_AccelEnv-v0_12173_00000:\n",
      "  agent_timesteps_total: 1710000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-25_07-10-36\n",
      "  done: false\n",
      "  episode_len_mean: 1500.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 246.01467587055842\n",
      "  episode_reward_mean: 240.15598470140304\n",
      "  episode_reward_min: 232.92041970912604\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1140\n",
      "  experiment_id: 3b224e0d4aa944209a188d13d8dd894d\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.253125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -4.670120327755556\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.018598204075662917\n",
      "          policy_loss: 0.005817696103486785\n",
      "          total_loss: 210.23710140454568\n",
      "          vf_explained_var: -6.753509040891004e-08\n",
      "          vf_loss: 210.22657638808428\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1710000\n",
      "    num_steps_sampled: 1710000\n",
      "    num_steps_trained: 1710000\n",
      "  iterations_since_restore: 114\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 12.660101010101009\n",
      "    ram_util_percent: 35.62272727272727\n",
      "  pid: 30781\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08454802806087638\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.472304111583625\n",
      "    mean_inference_ms: 1.7385902572420213\n",
      "    mean_raw_obs_processing_ms: 4.15056585265638\n",
      "  time_since_restore: 28320.467002630234\n",
      "  time_this_iter_s: 277.4307961463928\n",
      "  time_total_s: 28320.467002630234\n",
      "  timers:\n",
      "    learn_throughput: 91.52\n",
      "    learn_time_ms: 163899.158\n",
      "    sample_throughput: 141.076\n",
      "    sample_time_ms: 106325.615\n",
      "    update_time_ms: 2.472\n",
      "  timestamp: 1666699836\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1710000\n",
      "  training_iteration: 114\n",
      "  trial_id: '12173_00000'\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 10.9/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.38 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_5_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_0_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_1_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_4_edf1e7152919a3617c29d944024e5221, 0.0/6.0 CPU_group_edf1e7152919a3617c29d944024e5221, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_3_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_2_edf1e7152919a3617c29d944024e5221)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_figure_eight\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                  | status   | loc                  |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_AccelEnv-v0_12173_00000 | RUNNING  | 141.225.10.229:30781 |    114 |          28320.5 | 1710000 |  240.156 |              246.015 |               232.92 |               1500 |\n",
      "+-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "Result for PPO_AccelEnv-v0_12173_00000:\n",
      "  agent_timesteps_total: 1725000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-25_07-15-11\n",
      "  done: false\n",
      "  episode_len_mean: 1500.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 248.29052881103692\n",
      "  episode_reward_mean: 240.69558025586073\n",
      "  episode_reward_min: 232.92041970912604\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1150\n",
      "  experiment_id: 3b224e0d4aa944209a188d13d8dd894d\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.253125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -4.654973167888189\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01941197387615398\n",
      "          policy_loss: 0.006368929027358733\n",
      "          total_loss: 210.2565797450179\n",
      "          vf_explained_var: -3.687405936148025e-09\n",
      "          vf_loss: 210.24529713776153\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1725000\n",
      "    num_steps_sampled: 1725000\n",
      "    num_steps_trained: 1725000\n",
      "  iterations_since_restore: 115\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 12.742091836734692\n",
      "    ram_util_percent: 35.610969387755105\n",
      "  pid: 30781\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08454376548021511\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.472389318773608\n",
      "    mean_inference_ms: 1.7384500530578901\n",
      "    mean_raw_obs_processing_ms: 4.122591805708953\n",
      "  time_since_restore: 28595.17076230049\n",
      "  time_this_iter_s: 274.70375967025757\n",
      "  time_total_s: 28595.17076230049\n",
      "  timers:\n",
      "    learn_throughput: 91.168\n",
      "    learn_time_ms: 164531.272\n",
      "    sample_throughput: 141.08\n",
      "    sample_time_ms: 106322.501\n",
      "    update_time_ms: 2.476\n",
      "  timestamp: 1666700111\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1725000\n",
      "  training_iteration: 115\n",
      "  trial_id: '12173_00000'\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 10.9/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.38 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_3_edf1e7152919a3617c29d944024e5221, 0.0/1.0 accelerator_type:G, 0.0/6.0 CPU_group_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_0_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_1_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_4_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_2_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_5_edf1e7152919a3617c29d944024e5221)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_figure_eight\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                  | status   | loc                  |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_AccelEnv-v0_12173_00000 | RUNNING  | 141.225.10.229:30781 |    115 |          28595.2 | 1725000 |  240.696 |              248.291 |               232.92 |               1500 |\n",
      "+-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "Result for PPO_AccelEnv-v0_12173_00000:\n",
      "  agent_timesteps_total: 1740000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-25_07-19-53\n",
      "  done: false\n",
      "  episode_len_mean: 1500.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 251.9559253758407\n",
      "  episode_reward_mean: 241.61988661971648\n",
      "  episode_reward_min: 232.92041970912604\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1160\n",
      "  experiment_id: 3b224e0d4aa944209a188d13d8dd894d\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.253125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -4.632010263911749\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016878278241776023\n",
      "          policy_loss: 0.0037540243899102433\n",
      "          total_loss: 211.07563018798828\n",
      "          vf_explained_var: -1.2122978176876131e-09\n",
      "          vf_loss: 211.06760381924903\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1740000\n",
      "    num_steps_sampled: 1740000\n",
      "    num_steps_trained: 1740000\n",
      "  iterations_since_restore: 116\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 12.228855721393035\n",
      "    ram_util_percent: 35.68606965174129\n",
      "  pid: 30781\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08453988636992565\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.472480711129446\n",
      "    mean_inference_ms: 1.738311964100356\n",
      "    mean_raw_obs_processing_ms: 4.095129469962666\n",
      "  time_since_restore: 28876.764624118805\n",
      "  time_this_iter_s: 281.5938618183136\n",
      "  time_total_s: 28876.764624118805\n",
      "  timers:\n",
      "    learn_throughput: 90.469\n",
      "    learn_time_ms: 165802.209\n",
      "    sample_throughput: 141.064\n",
      "    sample_time_ms: 106334.681\n",
      "    update_time_ms: 2.451\n",
      "  timestamp: 1666700393\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1740000\n",
      "  training_iteration: 116\n",
      "  trial_id: '12173_00000'\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 10.9/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.38 GiB heap, 0.0/14.65 GiB objects (0.0/6.0 CPU_group_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_0_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_2_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_4_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_5_edf1e7152919a3617c29d944024e5221, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_1_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_3_edf1e7152919a3617c29d944024e5221)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_figure_eight\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                  | status   | loc                  |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_AccelEnv-v0_12173_00000 | RUNNING  | 141.225.10.229:30781 |    116 |          28876.8 | 1740000 |   241.62 |              251.956 |               232.92 |               1500 |\n",
      "+-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "Result for PPO_AccelEnv-v0_12173_00000:\n",
      "  agent_timesteps_total: 1755000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-25_07-24-31\n",
      "  done: false\n",
      "  episode_len_mean: 1500.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 251.9559253758407\n",
      "  episode_reward_mean: 242.87553126025145\n",
      "  episode_reward_min: 234.8779603059999\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1170\n",
      "  experiment_id: 3b224e0d4aa944209a188d13d8dd894d\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.253125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -4.618456110307726\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.018873139971303656\n",
      "          policy_loss: 0.004382218156696566\n",
      "          total_loss: 212.6224600840423\n",
      "          vf_explained_var: -5.4048281405982834e-09\n",
      "          vf_loss: 212.61330046572928\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1755000\n",
      "    num_steps_sampled: 1755000\n",
      "    num_steps_trained: 1755000\n",
      "  iterations_since_restore: 117\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 12.723677581863983\n",
      "    ram_util_percent: 35.6176322418136\n",
      "  pid: 30781\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08453659692748813\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.47256278897138\n",
      "    mean_inference_ms: 1.7381817582456063\n",
      "    mean_raw_obs_processing_ms: 4.0681836530712925\n",
      "  time_since_restore: 29155.071346521378\n",
      "  time_this_iter_s: 278.30672240257263\n",
      "  time_total_s: 29155.071346521378\n",
      "  timers:\n",
      "    learn_throughput: 89.891\n",
      "    learn_time_ms: 166867.836\n",
      "    sample_throughput: 141.075\n",
      "    sample_time_ms: 106326.047\n",
      "    update_time_ms: 2.41\n",
      "  timestamp: 1666700671\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1755000\n",
      "  training_iteration: 117\n",
      "  trial_id: '12173_00000'\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Memory usage on this node: 10.9/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.38 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_2_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_4_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_1_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_3_edf1e7152919a3617c29d944024e5221, 0.0/6.0 CPU_group_edf1e7152919a3617c29d944024e5221, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_5_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_0_edf1e7152919a3617c29d944024e5221)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_figure_eight\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                  | status   | loc                  |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_AccelEnv-v0_12173_00000 | RUNNING  | 141.225.10.229:30781 |    117 |          29155.1 | 1755000 |  242.876 |              251.956 |              234.878 |               1500 |\n",
      "+-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "Result for PPO_AccelEnv-v0_12173_00000:\n",
      "  agent_timesteps_total: 1770000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-25_07-29-08\n",
      "  done: false\n",
      "  episode_len_mean: 1500.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 251.9559253758407\n",
      "  episode_reward_mean: 243.51714315210708\n",
      "  episode_reward_min: 235.1396688215381\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1180\n",
      "  experiment_id: 3b224e0d4aa944209a188d13d8dd894d\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.253125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -4.648547132944657\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.02082588548143925\n",
      "          policy_loss: 0.007366359528248845\n",
      "          total_loss: 210.4614641868462\n",
      "          vf_explained_var: -3.434843964811307e-09\n",
      "          vf_loss: 210.44882633403196\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1770000\n",
      "    num_steps_sampled: 1770000\n",
      "    num_steps_trained: 1770000\n",
      "  iterations_since_restore: 118\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 12.618227848101268\n",
      "    ram_util_percent: 35.651898734177216\n",
      "  pid: 30781\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08453275007765614\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.472638886023642\n",
      "    mean_inference_ms: 1.7380428125584861\n",
      "    mean_raw_obs_processing_ms: 4.0416948178086045\n",
      "  time_since_restore: 29432.121443510056\n",
      "  time_this_iter_s: 277.050096988678\n",
      "  time_total_s: 29432.121443510056\n",
      "  timers:\n",
      "    learn_throughput: 89.519\n",
      "    learn_time_ms: 167561.272\n",
      "    sample_throughput: 141.096\n",
      "    sample_time_ms: 106310.729\n",
      "    update_time_ms: 2.382\n",
      "  timestamp: 1666700948\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1770000\n",
      "  training_iteration: 118\n",
      "  trial_id: '12173_00000'\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 10.9/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.38 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_3_edf1e7152919a3617c29d944024e5221, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_5_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_2_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_4_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_0_edf1e7152919a3617c29d944024e5221, 0.0/6.0 CPU_group_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_1_edf1e7152919a3617c29d944024e5221)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_figure_eight\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                  | status   | loc                  |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_AccelEnv-v0_12173_00000 | RUNNING  | 141.225.10.229:30781 |    118 |          29432.1 | 1770000 |  243.517 |              251.956 |               235.14 |               1500 |\n",
      "+-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "Result for PPO_AccelEnv-v0_12173_00000:\n",
      "  agent_timesteps_total: 1785000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-25_07-33-50\n",
      "  done: false\n",
      "  episode_len_mean: 1500.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 251.9559253758407\n",
      "  episode_reward_mean: 243.12737929066753\n",
      "  episode_reward_min: 229.74500995736673\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1190\n",
      "  experiment_id: 3b224e0d4aa944209a188d13d8dd894d\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.253125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -4.647773688122378\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.02072977157030951\n",
      "          policy_loss: 0.0071234505592008765\n",
      "          total_loss: 212.0025301092762\n",
      "          vf_explained_var: -2.32357089124946e-09\n",
      "          vf_loss: 211.99015963602875\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1785000\n",
      "    num_steps_sampled: 1785000\n",
      "    num_steps_trained: 1785000\n",
      "  iterations_since_restore: 119\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 12.61641791044776\n",
      "    ram_util_percent: 35.63880597014925\n",
      "  pid: 30781\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08452938701855355\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.472722640201617\n",
      "    mean_inference_ms: 1.737891300769103\n",
      "    mean_raw_obs_processing_ms: 4.015672608928332\n",
      "  time_since_restore: 29713.500484466553\n",
      "  time_this_iter_s: 281.3790409564972\n",
      "  time_total_s: 29713.500484466553\n",
      "  timers:\n",
      "    learn_throughput: 88.943\n",
      "    learn_time_ms: 168648.199\n",
      "    sample_throughput: 141.101\n",
      "    sample_time_ms: 106306.51\n",
      "    update_time_ms: 2.393\n",
      "  timestamp: 1666701230\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1785000\n",
      "  training_iteration: 119\n",
      "  trial_id: '12173_00000'\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 10.9/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.38 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_4_edf1e7152919a3617c29d944024e5221, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_1_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_5_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_2_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_3_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_0_edf1e7152919a3617c29d944024e5221, 0.0/6.0 CPU_group_edf1e7152919a3617c29d944024e5221)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_figure_eight\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                  | status   | loc                  |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_AccelEnv-v0_12173_00000 | RUNNING  | 141.225.10.229:30781 |    119 |          29713.5 | 1785000 |  243.127 |              251.956 |              229.745 |               1500 |\n",
      "+-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "Result for PPO_AccelEnv-v0_12173_00000:\n",
      "  agent_timesteps_total: 1800000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-25_07-38-27\n",
      "  done: false\n",
      "  episode_len_mean: 1500.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 251.9559253758407\n",
      "  episode_reward_mean: 242.2624434490608\n",
      "  episode_reward_min: 229.74500995736673\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1200\n",
      "  experiment_id: 3b224e0d4aa944209a188d13d8dd894d\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.253125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -4.62041199692225\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015806354689829723\n",
      "          policy_loss: 0.00442677270897301\n",
      "          total_loss: 210.80513842631194\n",
      "          vf_explained_var: -1.5153722721095164e-10\n",
      "          vf_loss: 210.7967112428051\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1800000\n",
      "    num_steps_sampled: 1800000\n",
      "    num_steps_trained: 1800000\n",
      "  iterations_since_restore: 120\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 12.389898989898992\n",
      "    ram_util_percent: 35.64772727272727\n",
      "  pid: 30781\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08452633818552893\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.472756683984\n",
      "    mean_inference_ms: 1.7377700577007846\n",
      "    mean_raw_obs_processing_ms: 3.9900841948134804\n",
      "  time_since_restore: 29991.16721892357\n",
      "  time_this_iter_s: 277.666734457016\n",
      "  time_total_s: 29991.16721892357\n",
      "  timers:\n",
      "    learn_throughput: 88.686\n",
      "    learn_time_ms: 169135.834\n",
      "    sample_throughput: 141.118\n",
      "    sample_time_ms: 106293.82\n",
      "    update_time_ms: 2.351\n",
      "  timestamp: 1666701507\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1800000\n",
      "  training_iteration: 120\n",
      "  trial_id: '12173_00000'\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 10.9/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.38 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_5_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_1_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_2_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_4_edf1e7152919a3617c29d944024e5221, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_0_edf1e7152919a3617c29d944024e5221, 0.0/6.0 CPU_group_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_3_edf1e7152919a3617c29d944024e5221)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_figure_eight\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                  | status   | loc                  |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_AccelEnv-v0_12173_00000 | RUNNING  | 141.225.10.229:30781 |    120 |          29991.2 | 1800000 |  242.262 |              251.956 |              229.745 |               1500 |\n",
      "+-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "Result for PPO_AccelEnv-v0_12173_00000:\n",
      "  agent_timesteps_total: 1815000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-25_07-43-07\n",
      "  done: false\n",
      "  episode_len_mean: 1500.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 251.9559253758407\n",
      "  episode_reward_mean: 241.2619609368298\n",
      "  episode_reward_min: 229.74500995736673\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1210\n",
      "  experiment_id: 3b224e0d4aa944209a188d13d8dd894d\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.253125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -4.586649587598898\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01938835018222253\n",
      "          policy_loss: 0.004461796797606778\n",
      "          total_loss: 215.74920977576303\n",
      "          vf_explained_var: -2.9751809904610127e-08\n",
      "          vf_loss: 215.73984004521773\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1815000\n",
      "    num_steps_sampled: 1815000\n",
      "    num_steps_trained: 1815000\n",
      "  iterations_since_restore: 121\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 12.244110275689222\n",
      "    ram_util_percent: 35.6671679197995\n",
      "  pid: 30781\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08452230894580333\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.472797870471144\n",
      "    mean_inference_ms: 1.7376415275304682\n",
      "    mean_raw_obs_processing_ms: 3.964927832243983\n",
      "  time_since_restore: 30270.659484148026\n",
      "  time_this_iter_s: 279.4922652244568\n",
      "  time_total_s: 30270.659484148026\n",
      "  timers:\n",
      "    learn_throughput: 87.743\n",
      "    learn_time_ms: 170953.369\n",
      "    sample_throughput: 141.118\n",
      "    sample_time_ms: 106294.154\n",
      "    update_time_ms: 2.411\n",
      "  timestamp: 1666701787\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1815000\n",
      "  training_iteration: 121\n",
      "  trial_id: '12173_00000'\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 10.9/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.38 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_3_edf1e7152919a3617c29d944024e5221, 0.0/6.0 CPU_group_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_4_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_5_edf1e7152919a3617c29d944024e5221, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_1_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_0_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_2_edf1e7152919a3617c29d944024e5221)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_figure_eight\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                  | status   | loc                  |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_AccelEnv-v0_12173_00000 | RUNNING  | 141.225.10.229:30781 |    121 |          30270.7 | 1815000 |  241.262 |              251.956 |              229.745 |               1500 |\n",
      "+-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "Result for PPO_AccelEnv-v0_12173_00000:\n",
      "  agent_timesteps_total: 1830000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-25_07-47-45\n",
      "  done: false\n",
      "  episode_len_mean: 1500.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 251.9559253758407\n",
      "  episode_reward_mean: 240.18589111344988\n",
      "  episode_reward_min: 227.13846427989722\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1220\n",
      "  experiment_id: 3b224e0d4aa944209a188d13d8dd894d\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.253125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -4.5844616106000995\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.018782844623690686\n",
      "          policy_loss: 0.005123470063317346\n",
      "          total_loss: 213.064216229067\n",
      "          vf_explained_var: -1.5153722721095164e-09\n",
      "          vf_loss: 213.05433971922275\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1830000\n",
      "    num_steps_sampled: 1830000\n",
      "    num_steps_trained: 1830000\n",
      "  iterations_since_restore: 122\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 12.563727959697733\n",
      "    ram_util_percent: 35.71209068010076\n",
      "  pid: 30781\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08451853941485248\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.472803534801713\n",
      "    mean_inference_ms: 1.7375190653741768\n",
      "    mean_raw_obs_processing_ms: 3.940185510654465\n",
      "  time_since_restore: 30548.56646513939\n",
      "  time_this_iter_s: 277.9069809913635\n",
      "  time_total_s: 30548.56646513939\n",
      "  timers:\n",
      "    learn_throughput: 87.381\n",
      "    learn_time_ms: 171661.89\n",
      "    sample_throughput: 141.145\n",
      "    sample_time_ms: 106273.571\n",
      "    update_time_ms: 2.447\n",
      "  timestamp: 1666702065\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1830000\n",
      "  training_iteration: 122\n",
      "  trial_id: '12173_00000'\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Memory usage on this node: 10.9/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.38 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_3_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_1_edf1e7152919a3617c29d944024e5221, 0.0/6.0 CPU_group_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_2_edf1e7152919a3617c29d944024e5221, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_0_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_4_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_5_edf1e7152919a3617c29d944024e5221)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_figure_eight\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                  | status   | loc                  |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_AccelEnv-v0_12173_00000 | RUNNING  | 141.225.10.229:30781 |    122 |          30548.6 | 1830000 |  240.186 |              251.956 |              227.138 |               1500 |\n",
      "+-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m  Retrying in 1 seconds\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m Could not connect to TraCI server at localhost:34231 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m  Retrying in 2 seconds\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m Could not connect to TraCI server at localhost:34231 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m  Retrying in 3 seconds\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m Could not connect to TraCI server at localhost:34231 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m  Retrying in 4 seconds\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m Could not connect to TraCI server at localhost:34231 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m  Retrying in 5 seconds\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m Could not connect to TraCI server at localhost:34231 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m  Retrying in 6 seconds\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m Could not connect to TraCI server at localhost:34231 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m  Retrying in 7 seconds\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m Could not connect to TraCI server at localhost:34231 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m  Retrying in 8 seconds\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m Could not connect to TraCI server at localhost:34231 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m  Retrying in 9 seconds\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m Could not connect to TraCI server at localhost:34231 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m  Retrying in 10 seconds\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m Could not connect to TraCI server at localhost:34231 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m  Retrying in 11 seconds\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m Could not connect to TraCI server at localhost:34231 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m  Retrying in 12 seconds\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m Could not connect to TraCI server at localhost:34231 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m  Retrying in 13 seconds\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m Could not connect to TraCI server at localhost:34231 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m  Retrying in 14 seconds\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m Could not connect to TraCI server at localhost:34231 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m  Retrying in 15 seconds\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m Could not connect to TraCI server at localhost:34231 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m  Retrying in 16 seconds\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m Could not connect to TraCI server at localhost:34231 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m  Retrying in 17 seconds\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m Could not connect to TraCI server at localhost:34231 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m  Retrying in 18 seconds\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m Could not connect to TraCI server at localhost:34231 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m  Retrying in 19 seconds\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m Could not connect to TraCI server at localhost:34231 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m  Retrying in 20 seconds\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m Could not connect to TraCI server at localhost:34231 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m  Retrying in 21 seconds\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m Could not connect to TraCI server at localhost:34231 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m  Retrying in 22 seconds\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m Could not connect to TraCI server at localhost:34231 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m  Retrying in 23 seconds\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m Could not connect to TraCI server at localhost:34231 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m  Retrying in 24 seconds\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m Could not connect to TraCI server at localhost:34231 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m  Retrying in 25 seconds\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m Could not connect to TraCI server at localhost:34231 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m  Retrying in 26 seconds\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m Could not connect to TraCI server at localhost:34231 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m  Retrying in 27 seconds\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m Could not connect to TraCI server at localhost:34231 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m  Retrying in 28 seconds\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m Could not connect to TraCI server at localhost:34231 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m  Retrying in 29 seconds\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m Could not connect to TraCI server at localhost:34231 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m  Retrying in 30 seconds\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m Could not connect to TraCI server at localhost:34231 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m  Retrying in 31 seconds\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m Could not connect to TraCI server at localhost:34231 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m  Retrying in 32 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m Could not connect to TraCI server at localhost:34231 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m  Retrying in 33 seconds\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m Could not connect to TraCI server at localhost:34231 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m  Retrying in 34 seconds\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m Could not connect to TraCI server at localhost:34231 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m  Retrying in 35 seconds\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m Could not connect to TraCI server at localhost:34231 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m  Retrying in 36 seconds\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m Could not connect to TraCI server at localhost:34231 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m  Retrying in 37 seconds\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m Could not connect to TraCI server at localhost:34231 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m  Retrying in 38 seconds\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m Could not connect to TraCI server at localhost:34231 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m  Retrying in 39 seconds\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m Could not connect to TraCI server at localhost:34231 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m  Retrying in 40 seconds\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m Could not connect to TraCI server at localhost:34231 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m  Retrying in 41 seconds\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m Could not connect to TraCI server at localhost:34231 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m  Retrying in 42 seconds\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m Could not connect to TraCI server at localhost:34231 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m  Retrying in 43 seconds\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m Could not connect to TraCI server at localhost:34231 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m  Retrying in 44 seconds\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m Could not connect to TraCI server at localhost:34231 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m  Retrying in 45 seconds\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m Could not connect to TraCI server at localhost:34231 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m  Retrying in 46 seconds\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m Could not connect to TraCI server at localhost:34231 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m  Retrying in 47 seconds\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m Could not connect to TraCI server at localhost:34231 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m  Retrying in 48 seconds\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m Could not connect to TraCI server at localhost:34231 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m  Retrying in 49 seconds\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m Could not connect to TraCI server at localhost:34231 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m  Retrying in 50 seconds\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m Could not connect to TraCI server at localhost:34231 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m  Retrying in 51 seconds\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m Could not connect to TraCI server at localhost:34231 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m  Retrying in 52 seconds\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m Could not connect to TraCI server at localhost:34231 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m  Retrying in 53 seconds\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m Could not connect to TraCI server at localhost:34231 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m  Retrying in 54 seconds\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m Could not connect to TraCI server at localhost:34231 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m  Retrying in 55 seconds\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m Could not connect to TraCI server at localhost:34231 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m  Retrying in 56 seconds\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m Could not connect to TraCI server at localhost:34231 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m  Retrying in 57 seconds\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m Could not connect to TraCI server at localhost:34231 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m  Retrying in 58 seconds\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m Could not connect to TraCI server at localhost:34231 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m  Retrying in 59 seconds\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m Could not connect to TraCI server at localhost:34231 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m  Retrying in 60 seconds\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m Could not connect to TraCI server at localhost:34231 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m  Retrying in 61 seconds\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m Could not connect to TraCI server at localhost:34231 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m  Retrying in 62 seconds\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m Could not connect to TraCI server at localhost:34231 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m  Retrying in 63 seconds\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m Could not connect to TraCI server at localhost:34231 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m  Retrying in 64 seconds\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m Could not connect to TraCI server at localhost:34231 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m  Retrying in 65 seconds\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m Could not connect to TraCI server at localhost:34231 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m  Retrying in 66 seconds\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m Could not connect to TraCI server at localhost:34231 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m  Retrying in 67 seconds\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m Could not connect to TraCI server at localhost:34231 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m  Retrying in 68 seconds\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m Could not connect to TraCI server at localhost:34231 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m  Retrying in 69 seconds\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m Could not connect to TraCI server at localhost:34231 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m  Retrying in 70 seconds\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m Could not connect to TraCI server at localhost:34231 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m  Retrying in 71 seconds\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m Could not connect to TraCI server at localhost:34231 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m  Retrying in 72 seconds\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m Could not connect to TraCI server at localhost:34231 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m  Retrying in 73 seconds\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m Could not connect to TraCI server at localhost:34231 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m  Retrying in 74 seconds\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m Could not connect to TraCI server at localhost:34231 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m  Retrying in 75 seconds\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m Could not connect to TraCI server at localhost:34231 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m  Retrying in 76 seconds\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m Could not connect to TraCI server at localhost:34231 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m  Retrying in 77 seconds\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m Could not connect to TraCI server at localhost:34231 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m  Retrying in 78 seconds\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m Could not connect to TraCI server at localhost:34231 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m  Retrying in 79 seconds\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m Could not connect to TraCI server at localhost:34231 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m  Retrying in 80 seconds\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m Could not connect to TraCI server at localhost:34231 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m  Retrying in 81 seconds\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m Could not connect to TraCI server at localhost:34231 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m  Retrying in 82 seconds\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m Could not connect to TraCI server at localhost:34231 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m  Retrying in 83 seconds\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m Could not connect to TraCI server at localhost:34231 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m  Retrying in 84 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m Could not connect to TraCI server at localhost:34231 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m  Retrying in 85 seconds\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m Could not connect to TraCI server at localhost:34231 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m  Retrying in 86 seconds\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m Could not connect to TraCI server at localhost:34231 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m  Retrying in 87 seconds\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m Could not connect to TraCI server at localhost:34231 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m  Retrying in 88 seconds\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m Could not connect to TraCI server at localhost:34231 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m  Retrying in 89 seconds\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m Could not connect to TraCI server at localhost:34231 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m  Retrying in 90 seconds\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m Could not connect to TraCI server at localhost:34231 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m  Retrying in 91 seconds\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m Could not connect to TraCI server at localhost:34231 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m  Retrying in 92 seconds\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m Could not connect to TraCI server at localhost:34231 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m  Retrying in 93 seconds\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m Could not connect to TraCI server at localhost:34231 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m  Retrying in 94 seconds\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m Could not connect to TraCI server at localhost:34231 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m  Retrying in 95 seconds\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m Could not connect to TraCI server at localhost:34231 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m  Retrying in 96 seconds\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m Could not connect to TraCI server at localhost:34231 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m  Retrying in 97 seconds\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m Could not connect to TraCI server at localhost:34231 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m  Retrying in 98 seconds\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m Could not connect to TraCI server at localhost:34231 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m  Retrying in 99 seconds\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m Could not connect to TraCI server at localhost:34231 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m  Retrying in 100 seconds\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m Could not connect to TraCI server at localhost:34231 [Errno 111] Connection refused\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m Error during start: Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m   File \"/home/michael-lab/Desktop/flow/flow/core/kernel/simulation/traci.py\", line 296, in start_simulation\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m     traci_connection = traci.connect(port, numRetries=100)\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m   File \"/home/michael-lab/anaconda3/envs/flow/lib/python3.7/site-packages/traci/__init__.py\", line 75, in connect\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m     raise FatalTraCIError(\"Could not connect in %s tries\" % (numRetries + 1))\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m traci.exceptions.FatalTraCIError: Could not connect in 101 tries\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m Error during teardown: [Errno 3] No such process\n",
      "Result for PPO_AccelEnv-v0_12173_00000:\n",
      "  agent_timesteps_total: 1845000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-25_09-16-40\n",
      "  done: false\n",
      "  episode_len_mean: 1500.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 251.9559253758407\n",
      "  episode_reward_mean: 238.3296469182385\n",
      "  episode_reward_min: 222.09373178185868\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1230\n",
      "  experiment_id: 3b224e0d4aa944209a188d13d8dd894d\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.253125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -4.59273976229005\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.018472456066031602\n",
      "          policy_loss: 0.004769988746729569\n",
      "          total_loss: 210.06202296887415\n",
      "          vf_explained_var: -1.5254748575443955e-08\n",
      "          vf_loss: 210.05257732262046\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1845000\n",
      "    num_steps_sampled: 1845000\n",
      "    num_steps_trained: 1845000\n",
      "  iterations_since_restore: 123\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 2.247969509791037\n",
      "    ram_util_percent: 34.79968458404521\n",
      "  pid: 30781\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0845148837752334\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.472828090306155\n",
      "    mean_inference_ms: 1.7373878121735675\n",
      "    mean_raw_obs_processing_ms: 4.1898635128652995\n",
      "  time_since_restore: 35883.96171045303\n",
      "  time_this_iter_s: 5335.395245313644\n",
      "  time_total_s: 35883.96171045303\n",
      "  timers:\n",
      "    learn_throughput: 87.071\n",
      "    learn_time_ms: 172273.122\n",
      "    sample_throughput: 24.518\n",
      "    sample_time_ms: 611806.137\n",
      "    update_time_ms: 2.525\n",
      "  timestamp: 1666707400\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1845000\n",
      "  training_iteration: 123\n",
      "  trial_id: '12173_00000'\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 10.9/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.38 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_0_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_2_edf1e7152919a3617c29d944024e5221, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_3_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_1_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_4_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_5_edf1e7152919a3617c29d944024e5221, 0.0/6.0 CPU_group_edf1e7152919a3617c29d944024e5221)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_figure_eight\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                  | status   | loc                  |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_AccelEnv-v0_12173_00000 | RUNNING  | 141.225.10.229:30781 |    123 |            35884 | 1845000 |   238.33 |              251.956 |              222.094 |               1500 |\n",
      "+-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_AccelEnv-v0_12173_00000:\n",
      "  agent_timesteps_total: 1860000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-25_09-21-19\n",
      "  done: false\n",
      "  episode_len_mean: 1500.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 251.9559253758407\n",
      "  episode_reward_mean: 236.70623040651705\n",
      "  episode_reward_min: 222.09373178185868\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1240\n",
      "  experiment_id: 3b224e0d4aa944209a188d13d8dd894d\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.253125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -4.594204612909738\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.020866949581034848\n",
      "          policy_loss: 0.005785646697237961\n",
      "          total_loss: 213.27272871793326\n",
      "          vf_explained_var: -2.1720336640385085e-09\n",
      "          vf_loss: 213.26166172350867\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1860000\n",
      "    num_steps_sampled: 1860000\n",
      "    num_steps_trained: 1860000\n",
      "  iterations_since_restore: 124\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 12.651507537688442\n",
      "    ram_util_percent: 35.72814070351759\n",
      "  pid: 30781\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08451014206909982\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.472859980314023\n",
      "    mean_inference_ms: 1.7372427320260824\n",
      "    mean_raw_obs_processing_ms: 4.437752283681335\n",
      "  time_since_restore: 36162.8499455452\n",
      "  time_this_iter_s: 278.8882350921631\n",
      "  time_total_s: 36162.8499455452\n",
      "  timers:\n",
      "    learn_throughput: 87.0\n",
      "    learn_time_ms: 172414.356\n",
      "    sample_throughput: 24.517\n",
      "    sample_time_ms: 611810.783\n",
      "    update_time_ms: 2.501\n",
      "  timestamp: 1666707679\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1860000\n",
      "  training_iteration: 124\n",
      "  trial_id: '12173_00000'\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 10.9/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.38 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_2_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_4_edf1e7152919a3617c29d944024e5221, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_3_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_0_edf1e7152919a3617c29d944024e5221, 0.0/6.0 CPU_group_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_5_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_1_edf1e7152919a3617c29d944024e5221)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_figure_eight\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                  | status   | loc                  |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_AccelEnv-v0_12173_00000 | RUNNING  | 141.225.10.229:30781 |    124 |          36162.8 | 1860000 |  236.706 |              251.956 |              222.094 |               1500 |\n",
      "+-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "Result for PPO_AccelEnv-v0_12173_00000:\n",
      "  agent_timesteps_total: 1875000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-25_09-25-53\n",
      "  done: false\n",
      "  episode_len_mean: 1500.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 251.9559253758407\n",
      "  episode_reward_mean: 234.8468097995636\n",
      "  episode_reward_min: 222.09373178185868\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1250\n",
      "  experiment_id: 3b224e0d4aa944209a188d13d8dd894d\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.253125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -4.596445937479957\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.021046018560048393\n",
      "          policy_loss: 0.005509435530294175\n",
      "          total_loss: 207.6422634609675\n",
      "          vf_explained_var: -6.667638441371082e-09\n",
      "          vf_loss: 207.6314270051859\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1875000\n",
      "    num_steps_sampled: 1875000\n",
      "    num_steps_trained: 1875000\n",
      "  iterations_since_restore: 125\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 12.720460358056268\n",
      "    ram_util_percent: 35.71202046035806\n",
      "  pid: 30781\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08450590861582967\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.472909986289014\n",
      "    mean_inference_ms: 1.7370885777590002\n",
      "    mean_raw_obs_processing_ms: 4.683883293674959\n",
      "  time_since_restore: 36436.71539187431\n",
      "  time_this_iter_s: 273.8654463291168\n",
      "  time_total_s: 36436.71539187431\n",
      "  timers:\n",
      "    learn_throughput: 87.045\n",
      "    learn_time_ms: 172324.164\n",
      "    sample_throughput: 24.517\n",
      "    sample_time_ms: 611817.217\n",
      "    update_time_ms: 2.446\n",
      "  timestamp: 1666707953\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1875000\n",
      "  training_iteration: 125\n",
      "  trial_id: '12173_00000'\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 10.9/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.38 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_5_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_0_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_4_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_2_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_1_edf1e7152919a3617c29d944024e5221, 0.0/6.0 CPU_group_edf1e7152919a3617c29d944024e5221, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_3_edf1e7152919a3617c29d944024e5221)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_figure_eight\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                  | status   | loc                  |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_AccelEnv-v0_12173_00000 | RUNNING  | 141.225.10.229:30781 |    125 |          36436.7 | 1875000 |  234.847 |              251.956 |              222.094 |               1500 |\n",
      "+-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "Result for PPO_AccelEnv-v0_12173_00000:\n",
      "  agent_timesteps_total: 1890000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-25_09-30-31\n",
      "  done: false\n",
      "  episode_len_mean: 1500.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 248.23712495447947\n",
      "  episode_reward_mean: 232.17009452613914\n",
      "  episode_reward_min: 222.09373178185868\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1260\n",
      "  experiment_id: 3b224e0d4aa944209a188d13d8dd894d\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.253125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -4.581367134239714\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01984665347537607\n",
      "          policy_loss: 0.005922606151937744\n",
      "          total_loss: 209.89316703424615\n",
      "          vf_explained_var: -3.7379184192332104e-09\n",
      "          vf_loss: 209.88222161713293\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1890000\n",
      "    num_steps_sampled: 1890000\n",
      "    num_steps_trained: 1890000\n",
      "  iterations_since_restore: 126\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 12.733333333333333\n",
      "    ram_util_percent: 35.63939393939393\n",
      "  pid: 30781\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08450206260026512\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.472955270569283\n",
      "    mean_inference_ms: 1.7369410059634742\n",
      "    mean_raw_obs_processing_ms: 4.92824352955301\n",
      "  time_since_restore: 36714.36622405052\n",
      "  time_this_iter_s: 277.6508321762085\n",
      "  time_total_s: 36714.36622405052\n",
      "  timers:\n",
      "    learn_throughput: 87.237\n",
      "    learn_time_ms: 171945.813\n",
      "    sample_throughput: 24.518\n",
      "    sample_time_ms: 611801.234\n",
      "    update_time_ms: 2.542\n",
      "  timestamp: 1666708231\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1890000\n",
      "  training_iteration: 126\n",
      "  trial_id: '12173_00000'\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 10.9/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.38 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_5_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_1_edf1e7152919a3617c29d944024e5221, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_4_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_3_edf1e7152919a3617c29d944024e5221, 0.0/6.0 CPU_group_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_0_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_2_edf1e7152919a3617c29d944024e5221)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_figure_eight\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                  | status   | loc                  |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_AccelEnv-v0_12173_00000 | RUNNING  | 141.225.10.229:30781 |    126 |          36714.4 | 1890000 |   232.17 |              248.237 |              222.094 |               1500 |\n",
      "+-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "Result for PPO_AccelEnv-v0_12173_00000:\n",
      "  agent_timesteps_total: 1905000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-25_09-35-09\n",
      "  done: false\n",
      "  episode_len_mean: 1500.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 244.24027738391368\n",
      "  episode_reward_mean: 228.87084237542294\n",
      "  episode_reward_min: 212.67323061535683\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1270\n",
      "  experiment_id: 3b224e0d4aa944209a188d13d8dd894d\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.253125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -4.535054660651643\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01902709452313317\n",
      "          policy_loss: 0.005502265733587792\n",
      "          total_loss: 212.44656015169824\n",
      "          vf_explained_var: -6.46558850903034e-09\n",
      "          vf_loss: 212.4362420906455\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1905000\n",
      "    num_steps_sampled: 1905000\n",
      "    num_steps_trained: 1905000\n",
      "  iterations_since_restore: 127\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 12.660453400503778\n",
      "    ram_util_percent: 35.723425692695216\n",
      "  pid: 30781\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.084498360685566\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.47299403209601\n",
      "    mean_inference_ms: 1.736797260269455\n",
      "    mean_raw_obs_processing_ms: 5.170845348031375\n",
      "  time_since_restore: 36992.46845960617\n",
      "  time_this_iter_s: 278.1022355556488\n",
      "  time_total_s: 36992.46845960617\n",
      "  timers:\n",
      "    learn_throughput: 87.244\n",
      "    learn_time_ms: 171931.343\n",
      "    sample_throughput: 24.518\n",
      "    sample_time_ms: 611795.611\n",
      "    update_time_ms: 2.521\n",
      "  timestamp: 1666708509\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1905000\n",
      "  training_iteration: 127\n",
      "  trial_id: '12173_00000'\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Memory usage on this node: 10.9/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.38 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_3_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_2_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_0_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_5_edf1e7152919a3617c29d944024e5221, 0.0/6.0 CPU_group_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_1_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_4_edf1e7152919a3617c29d944024e5221, 0.0/1.0 accelerator_type:G)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_figure_eight\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                  | status   | loc                  |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_AccelEnv-v0_12173_00000 | RUNNING  | 141.225.10.229:30781 |    127 |          36992.5 | 1905000 |  228.871 |               244.24 |              212.673 |               1500 |\n",
      "+-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "Result for PPO_AccelEnv-v0_12173_00000:\n",
      "  agent_timesteps_total: 1920000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-25_09-39-51\n",
      "  done: false\n",
      "  episode_len_mean: 1500.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 236.4362011038765\n",
      "  episode_reward_mean: 225.41978084233878\n",
      "  episode_reward_min: 205.7852107830523\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1280\n",
      "  experiment_id: 3b224e0d4aa944209a188d13d8dd894d\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.253125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -4.555371129310737\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01647089840321102\n",
      "          policy_loss: 0.00484456291927372\n",
      "          total_loss: 211.97029455799168\n",
      "          vf_explained_var: -3.687405936148025e-09\n",
      "          vf_loss: 211.96127967187914\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1920000\n",
      "    num_steps_sampled: 1920000\n",
      "    num_steps_trained: 1920000\n",
      "  iterations_since_restore: 128\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 12.394029850746268\n",
      "    ram_util_percent: 35.73009950248757\n",
      "  pid: 30781\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0844954175710859\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.473028769176825\n",
      "    mean_inference_ms: 1.736671992140706\n",
      "    mean_raw_obs_processing_ms: 5.411756664337048\n",
      "  time_since_restore: 37273.78135752678\n",
      "  time_this_iter_s: 281.3128979206085\n",
      "  time_total_s: 37273.78135752678\n",
      "  timers:\n",
      "    learn_throughput: 87.031\n",
      "    learn_time_ms: 172352.842\n",
      "    sample_throughput: 24.518\n",
      "    sample_time_ms: 611800.436\n",
      "    update_time_ms: 2.503\n",
      "  timestamp: 1666708791\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1920000\n",
      "  training_iteration: 128\n",
      "  trial_id: '12173_00000'\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 10.9/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.38 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_0_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_5_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_2_edf1e7152919a3617c29d944024e5221, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_4_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_1_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_3_edf1e7152919a3617c29d944024e5221, 0.0/6.0 CPU_group_edf1e7152919a3617c29d944024e5221)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_figure_eight\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                  | status   | loc                  |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_AccelEnv-v0_12173_00000 | RUNNING  | 141.225.10.229:30781 |    128 |          37273.8 | 1920000 |   225.42 |              236.436 |              205.785 |               1500 |\n",
      "+-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_AccelEnv-v0_12173_00000:\n",
      "  agent_timesteps_total: 1935000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-25_09-44-29\n",
      "  done: false\n",
      "  episode_len_mean: 1500.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 234.8815598260951\n",
      "  episode_reward_mean: 223.43622958196713\n",
      "  episode_reward_min: 205.7852107830523\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1290\n",
      "  experiment_id: 3b224e0d4aa944209a188d13d8dd894d\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.253125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -4.537965456509994\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.018315811475134\n",
      "          policy_loss: 0.004376897304634548\n",
      "          total_loss: 210.74666449336684\n",
      "          vf_explained_var: -2.778182572882315e-09\n",
      "          vf_loss: 210.73765170857058\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1935000\n",
      "    num_steps_sampled: 1935000\n",
      "    num_steps_trained: 1935000\n",
      "  iterations_since_restore: 129\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 12.664231738035264\n",
      "    ram_util_percent: 35.70604534005038\n",
      "  pid: 30781\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08449269434788505\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.473085952646827\n",
      "    mean_inference_ms: 1.7365340617783784\n",
      "    mean_raw_obs_processing_ms: 5.650983070190859\n",
      "  time_since_restore: 37552.417463064194\n",
      "  time_this_iter_s: 278.63610553741455\n",
      "  time_total_s: 37552.417463064194\n",
      "  timers:\n",
      "    learn_throughput: 87.169\n",
      "    learn_time_ms: 172080.043\n",
      "    sample_throughput: 24.518\n",
      "    sample_time_ms: 611799.512\n",
      "    update_time_ms: 2.489\n",
      "  timestamp: 1666709069\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1935000\n",
      "  training_iteration: 129\n",
      "  trial_id: '12173_00000'\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 10.9/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.38 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_1_edf1e7152919a3617c29d944024e5221, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_0_edf1e7152919a3617c29d944024e5221, 0.0/6.0 CPU_group_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_5_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_3_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_2_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_4_edf1e7152919a3617c29d944024e5221)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_figure_eight\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                  | status   | loc                  |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_AccelEnv-v0_12173_00000 | RUNNING  | 141.225.10.229:30781 |    129 |          37552.4 | 1935000 |  223.436 |              234.882 |              205.785 |               1500 |\n",
      "+-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "Result for PPO_AccelEnv-v0_12173_00000:\n",
      "  agent_timesteps_total: 1950000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-25_09-49-08\n",
      "  done: false\n",
      "  episode_len_mean: 1500.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 234.8815598260951\n",
      "  episode_reward_mean: 221.2392218832705\n",
      "  episode_reward_min: 205.7852107830523\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1300\n",
      "  experiment_id: 3b224e0d4aa944209a188d13d8dd894d\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.253125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -4.529074590893115\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017530130894993265\n",
      "          policy_loss: 0.004775816834358088\n",
      "          total_loss: 207.63852090350653\n",
      "          vf_explained_var: -3.333819220685541e-09\n",
      "          vf_loss: 207.62930809926178\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1950000\n",
      "    num_steps_sampled: 1950000\n",
      "    num_steps_trained: 1950000\n",
      "  iterations_since_restore: 130\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 12.678894472361808\n",
      "    ram_util_percent: 35.721608040201005\n",
      "  pid: 30781\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08449019933164978\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.473137659607005\n",
      "    mean_inference_ms: 1.7364026819221692\n",
      "    mean_raw_obs_processing_ms: 5.888558405068887\n",
      "  time_since_restore: 37831.20480513573\n",
      "  time_this_iter_s: 278.7873420715332\n",
      "  time_total_s: 37831.20480513573\n",
      "  timers:\n",
      "    learn_throughput: 87.112\n",
      "    learn_time_ms: 172193.064\n",
      "    sample_throughput: 24.518\n",
      "    sample_time_ms: 611798.584\n",
      "    update_time_ms: 2.496\n",
      "  timestamp: 1666709348\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1950000\n",
      "  training_iteration: 130\n",
      "  trial_id: '12173_00000'\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 10.9/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.38 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_4_edf1e7152919a3617c29d944024e5221, 0.0/6.0 CPU_group_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_3_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_0_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_1_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_5_edf1e7152919a3617c29d944024e5221, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_2_edf1e7152919a3617c29d944024e5221)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_figure_eight\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                  | status   | loc                  |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_AccelEnv-v0_12173_00000 | RUNNING  | 141.225.10.229:30781 |    130 |          37831.2 | 1950000 |  221.239 |              234.882 |              205.785 |               1500 |\n",
      "+-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "Result for PPO_AccelEnv-v0_12173_00000:\n",
      "  agent_timesteps_total: 1965000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-25_09-53-47\n",
      "  done: false\n",
      "  episode_len_mean: 1500.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 231.18218361401398\n",
      "  episode_reward_mean: 218.86926694113336\n",
      "  episode_reward_min: 205.7852107830523\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1310\n",
      "  experiment_id: 3b224e0d4aa944209a188d13d8dd894d\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.253125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -4.549983618623119\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017539825411692044\n",
      "          policy_loss: 0.004755650610876096\n",
      "          total_loss: 210.03066432678094\n",
      "          vf_explained_var: -2.879207317008081e-09\n",
      "          vf_loss: 210.02146883253323\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1965000\n",
      "    num_steps_sampled: 1965000\n",
      "    num_steps_trained: 1965000\n",
      "  iterations_since_restore: 131\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 12.474371859296483\n",
      "    ram_util_percent: 35.643718592964824\n",
      "  pid: 30781\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08448788902567518\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.47316980214281\n",
      "    mean_inference_ms: 1.736282429633835\n",
      "    mean_raw_obs_processing_ms: 6.124508182700725\n",
      "  time_since_restore: 38110.10402774811\n",
      "  time_this_iter_s: 278.899222612381\n",
      "  time_total_s: 38110.10402774811\n",
      "  timers:\n",
      "    learn_throughput: 87.143\n",
      "    learn_time_ms: 172130.591\n",
      "    sample_throughput: 24.518\n",
      "    sample_time_ms: 611802.148\n",
      "    update_time_ms: 2.468\n",
      "  timestamp: 1666709627\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1965000\n",
      "  training_iteration: 131\n",
      "  trial_id: '12173_00000'\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 10.9/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.38 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_3_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_4_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_0_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_5_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_2_edf1e7152919a3617c29d944024e5221, 0.0/1.0 accelerator_type:G, 0.0/6.0 CPU_group_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_1_edf1e7152919a3617c29d944024e5221)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_figure_eight\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                  | status   | loc                  |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_AccelEnv-v0_12173_00000 | RUNNING  | 141.225.10.229:30781 |    131 |          38110.1 | 1965000 |  218.869 |              231.182 |              205.785 |               1500 |\n",
      "+-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "Result for PPO_AccelEnv-v0_12173_00000:\n",
      "  agent_timesteps_total: 1980000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-25_09-58-26\n",
      "  done: false\n",
      "  episode_len_mean: 1500.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 230.38126938982995\n",
      "  episode_reward_mean: 216.56192353456373\n",
      "  episode_reward_min: 204.79643020920196\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1320\n",
      "  experiment_id: 3b224e0d4aa944209a188d13d8dd894d\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.253125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -4.557517498630588\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0160552039082061\n",
      "          policy_loss: 0.004423789892314097\n",
      "          total_loss: 210.01542882111113\n",
      "          vf_explained_var: -9.597358463508954e-10\n",
      "          vf_loss: 210.00694105503922\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1980000\n",
      "    num_steps_sampled: 1980000\n",
      "    num_steps_trained: 1980000\n",
      "  iterations_since_restore: 132\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 12.513567839195979\n",
      "    ram_util_percent: 35.75376884422111\n",
      "  pid: 30781\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08448536140765991\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.473177772050207\n",
      "    mean_inference_ms: 1.7361714357258495\n",
      "    mean_raw_obs_processing_ms: 6.358835077127583\n",
      "  time_since_restore: 38389.13436794281\n",
      "  time_this_iter_s: 279.03034019470215\n",
      "  time_total_s: 38389.13436794281\n",
      "  timers:\n",
      "    learn_throughput: 87.082\n",
      "    learn_time_ms: 172252.395\n",
      "    sample_throughput: 24.518\n",
      "    sample_time_ms: 611792.984\n",
      "    update_time_ms: 2.488\n",
      "  timestamp: 1666709906\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1980000\n",
      "  training_iteration: 132\n",
      "  trial_id: '12173_00000'\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Memory usage on this node: 10.9/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.38 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_1_edf1e7152919a3617c29d944024e5221, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_0_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_4_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_2_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_5_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_3_edf1e7152919a3617c29d944024e5221, 0.0/6.0 CPU_group_edf1e7152919a3617c29d944024e5221)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_figure_eight\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                  | status   | loc                  |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_AccelEnv-v0_12173_00000 | RUNNING  | 141.225.10.229:30781 |    132 |          38389.1 | 1980000 |  216.562 |              230.381 |              204.796 |               1500 |\n",
      "+-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "Result for PPO_AccelEnv-v0_12173_00000:\n",
      "  agent_timesteps_total: 1995000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-25_10-03-06\n",
      "  done: false\n",
      "  episode_len_mean: 1500.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 230.38126938982995\n",
      "  episode_reward_mean: 214.6986603607315\n",
      "  episode_reward_min: 203.6146341987762\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1330\n",
      "  experiment_id: 3b224e0d4aa944209a188d13d8dd894d\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.253125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -4.5363309185383685\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.020552935445399344\n",
      "          policy_loss: 0.0053019576335041705\n",
      "          total_loss: 209.45032450142554\n",
      "          vf_explained_var: -3.889455424399557e-09\n",
      "          vf_loss: 209.439819956634\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 1995000\n",
      "    num_steps_sampled: 1995000\n",
      "    num_steps_trained: 1995000\n",
      "  iterations_since_restore: 133\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 12.602756892230577\n",
      "    ram_util_percent: 35.75087719298246\n",
      "  pid: 30781\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08448217043323368\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.473189287838846\n",
      "    mean_inference_ms: 1.7360513683496885\n",
      "    mean_raw_obs_processing_ms: 6.317557390853198\n",
      "  time_since_restore: 38668.28340387344\n",
      "  time_this_iter_s: 279.14903593063354\n",
      "  time_total_s: 38668.28340387344\n",
      "  timers:\n",
      "    learn_throughput: 87.127\n",
      "    learn_time_ms: 172162.988\n",
      "    sample_throughput: 141.166\n",
      "    sample_time_ms: 106257.848\n",
      "    update_time_ms: 2.458\n",
      "  timestamp: 1666710186\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1995000\n",
      "  training_iteration: 133\n",
      "  trial_id: '12173_00000'\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 10.9/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.38 GiB heap, 0.0/14.65 GiB objects (0.0/6.0 CPU_group_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_0_edf1e7152919a3617c29d944024e5221, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_5_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_4_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_3_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_1_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_2_edf1e7152919a3617c29d944024e5221)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_figure_eight\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                  | status   | loc                  |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_AccelEnv-v0_12173_00000 | RUNNING  | 141.225.10.229:30781 |    133 |          38668.3 | 1995000 |  214.699 |              230.381 |              203.615 |               1500 |\n",
      "+-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_AccelEnv-v0_12173_00000:\n",
      "  agent_timesteps_total: 2010000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-25_10-07-44\n",
      "  done: false\n",
      "  episode_len_mean: 1500.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 229.76431136602955\n",
      "  episode_reward_mean: 208.43879789624967\n",
      "  episode_reward_min: 163.2310552870223\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1340\n",
      "  experiment_id: 3b224e0d4aa944209a188d13d8dd894d\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.253125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -4.49195947525865\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.02397246705030646\n",
      "          policy_loss: 0.007387156743098492\n",
      "          total_loss: 204.99769938194146\n",
      "          vf_explained_var: -2.879207317008081e-09\n",
      "          vf_loss: 204.9842448153738\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 2010000\n",
      "    num_steps_sampled: 2010000\n",
      "    num_steps_trained: 2010000\n",
      "  iterations_since_restore: 134\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 12.150629722921915\n",
      "    ram_util_percent: 35.73904282115869\n",
      "  pid: 30781\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08447913860480248\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.473205333724135\n",
      "    mean_inference_ms: 1.7359285202491648\n",
      "    mean_raw_obs_processing_ms: 6.276901524274216\n",
      "  time_since_restore: 38946.92304086685\n",
      "  time_this_iter_s: 278.6396369934082\n",
      "  time_total_s: 38946.92304086685\n",
      "  timers:\n",
      "    learn_throughput: 87.138\n",
      "    learn_time_ms: 172141.579\n",
      "    sample_throughput: 141.171\n",
      "    sample_time_ms: 106254.353\n",
      "    update_time_ms: 2.433\n",
      "  timestamp: 1666710464\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2010000\n",
      "  training_iteration: 134\n",
      "  trial_id: '12173_00000'\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 10.9/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.38 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_5_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_4_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_3_edf1e7152919a3617c29d944024e5221, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_1_edf1e7152919a3617c29d944024e5221, 0.0/6.0 CPU_group_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_2_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_0_edf1e7152919a3617c29d944024e5221)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_figure_eight\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                  | status   | loc                  |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_AccelEnv-v0_12173_00000 | RUNNING  | 141.225.10.229:30781 |    134 |          38946.9 | 2010000 |  208.439 |              229.764 |              163.231 |               1500 |\n",
      "+-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "Result for PPO_AccelEnv-v0_12173_00000:\n",
      "  agent_timesteps_total: 2025000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-25_10-12-23\n",
      "  done: false\n",
      "  episode_len_mean: 1500.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 226.19322170988966\n",
      "  episode_reward_mean: 201.68514181361795\n",
      "  episode_reward_min: 159.94506399952448\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1350\n",
      "  experiment_id: 3b224e0d4aa944209a188d13d8dd894d\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.253125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -4.517837975792966\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015374120288988484\n",
      "          policy_loss: 0.0036352789483301466\n",
      "          total_loss: 204.27088599770755\n",
      "          vf_explained_var: -1.4648599000466334e-09\n",
      "          vf_loss: 204.2633581840386\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 2025000\n",
      "    num_steps_sampled: 2025000\n",
      "    num_steps_trained: 2025000\n",
      "  iterations_since_restore: 135\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 12.40427135678392\n",
      "    ram_util_percent: 35.698241206030154\n",
      "  pid: 30781\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08447658791359654\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.473218562025366\n",
      "    mean_inference_ms: 1.7358345597968856\n",
      "    mean_raw_obs_processing_ms: 6.2368637851852\n",
      "  time_since_restore: 39225.722677230835\n",
      "  time_this_iter_s: 278.79963636398315\n",
      "  time_total_s: 39225.722677230835\n",
      "  timers:\n",
      "    learn_throughput: 86.894\n",
      "    learn_time_ms: 172623.387\n",
      "    sample_throughput: 141.156\n",
      "    sample_time_ms: 106265.354\n",
      "    update_time_ms: 2.461\n",
      "  timestamp: 1666710743\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2025000\n",
      "  training_iteration: 135\n",
      "  trial_id: '12173_00000'\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 10.9/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.38 GiB heap, 0.0/14.65 GiB objects (0.0/6.0 CPU_group_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_0_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_1_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_3_edf1e7152919a3617c29d944024e5221, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_2_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_5_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_4_edf1e7152919a3617c29d944024e5221)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_figure_eight\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                  | status   | loc                  |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_AccelEnv-v0_12173_00000 | RUNNING  | 141.225.10.229:30781 |    135 |          39225.7 | 2025000 |  201.685 |              226.193 |              159.945 |               1500 |\n",
      "+-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "Result for PPO_AccelEnv-v0_12173_00000:\n",
      "  agent_timesteps_total: 2040000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-25_10-17-02\n",
      "  done: false\n",
      "  episode_len_mean: 1500.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 217.0053838676726\n",
      "  episode_reward_mean: 195.6034535195087\n",
      "  episode_reward_min: 159.94506399952448\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1360\n",
      "  experiment_id: 3b224e0d4aa944209a188d13d8dd894d\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.253125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -4.524459287271661\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014012110467269773\n",
      "          policy_loss: 0.0037066906055246117\n",
      "          total_loss: 202.01286482406874\n",
      "          vf_explained_var: -3.0812570273042184e-09\n",
      "          vf_loss: 202.00561068583343\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 2040000\n",
      "    num_steps_sampled: 2040000\n",
      "    num_steps_trained: 2040000\n",
      "  iterations_since_restore: 136\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 12.673618090452262\n",
      "    ram_util_percent: 35.74522613065327\n",
      "  pid: 30781\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08447381482663817\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.47325500844141\n",
      "    mean_inference_ms: 1.7357223539440063\n",
      "    mean_raw_obs_processing_ms: 6.1974308405413465\n",
      "  time_since_restore: 39504.61314845085\n",
      "  time_this_iter_s: 278.8904712200165\n",
      "  time_total_s: 39504.61314845085\n",
      "  timers:\n",
      "    learn_throughput: 86.832\n",
      "    learn_time_ms: 172746.523\n",
      "    sample_throughput: 141.156\n",
      "    sample_time_ms: 106265.716\n",
      "    update_time_ms: 2.328\n",
      "  timestamp: 1666711022\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2040000\n",
      "  training_iteration: 136\n",
      "  trial_id: '12173_00000'\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 10.9/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.38 GiB heap, 0.0/14.65 GiB objects (0.0/6.0 CPU_group_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_0_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_1_edf1e7152919a3617c29d944024e5221, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_5_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_2_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_4_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_3_edf1e7152919a3617c29d944024e5221)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_figure_eight\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                  | status   | loc                  |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_AccelEnv-v0_12173_00000 | RUNNING  | 141.225.10.229:30781 |    136 |          39504.6 | 2040000 |  195.603 |              217.005 |              159.945 |               1500 |\n",
      "+-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "Result for PPO_AccelEnv-v0_12173_00000:\n",
      "  agent_timesteps_total: 2055000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-25_10-21-41\n",
      "  done: false\n",
      "  episode_len_mean: 1500.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 213.11648055994743\n",
      "  episode_reward_mean: 190.65470891880136\n",
      "  episode_reward_min: 159.94506399952448\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1370\n",
      "  experiment_id: 3b224e0d4aa944209a188d13d8dd894d\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.253125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -4.5469862052949805\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01666108725218096\n",
      "          policy_loss: 0.004581379876037965\n",
      "          total_loss: 197.16734590692036\n",
      "          vf_explained_var: -2.121521180953323e-09\n",
      "          vf_loss: 197.15854708461438\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 2055000\n",
      "    num_steps_sampled: 2055000\n",
      "    num_steps_trained: 2055000\n",
      "  iterations_since_restore: 137\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 12.65100502512563\n",
      "    ram_util_percent: 35.6824120603015\n",
      "  pid: 30781\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0844704598636921\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.473348886549207\n",
      "    mean_inference_ms: 1.7355762277758393\n",
      "    mean_raw_obs_processing_ms: 6.158596223667667\n",
      "  time_since_restore: 39783.695127010345\n",
      "  time_this_iter_s: 279.081978559494\n",
      "  time_total_s: 39783.695127010345\n",
      "  timers:\n",
      "    learn_throughput: 86.792\n",
      "    learn_time_ms: 172827.355\n",
      "    sample_throughput: 141.133\n",
      "    sample_time_ms: 106282.538\n",
      "    update_time_ms: 2.356\n",
      "  timestamp: 1666711301\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2055000\n",
      "  training_iteration: 137\n",
      "  trial_id: '12173_00000'\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Memory usage on this node: 10.9/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.38 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_3_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_4_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_2_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_5_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_0_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_1_edf1e7152919a3617c29d944024e5221, 0.0/1.0 accelerator_type:G, 0.0/6.0 CPU_group_edf1e7152919a3617c29d944024e5221)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_figure_eight\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                  | status   | loc                  |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_AccelEnv-v0_12173_00000 | RUNNING  | 141.225.10.229:30781 |    137 |          39783.7 | 2055000 |  190.655 |              213.116 |              159.945 |               1500 |\n",
      "+-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "Result for PPO_AccelEnv-v0_12173_00000:\n",
      "  agent_timesteps_total: 2070000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-25_10-26-26\n",
      "  done: false\n",
      "  episode_len_mean: 1500.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 213.11648055994743\n",
      "  episode_reward_mean: 187.42875395769602\n",
      "  episode_reward_min: 159.94506399952448\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1380\n",
      "  experiment_id: 3b224e0d4aa944209a188d13d8dd894d\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.253125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -4.551294819783356\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012902578799024868\n",
      "          policy_loss: 0.0030039926788619706\n",
      "          total_loss: 195.9548234486984\n",
      "          vf_explained_var: -2.6266453456713634e-09\n",
      "          vf_loss: 195.94855377714512\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 2070000\n",
      "    num_steps_sampled: 2070000\n",
      "    num_steps_trained: 2070000\n",
      "  iterations_since_restore: 138\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 12.299507389162562\n",
      "    ram_util_percent: 35.73349753694581\n",
      "  pid: 30781\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08446684905153738\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.4734578746022\n",
      "    mean_inference_ms: 1.7354086127379083\n",
      "    mean_raw_obs_processing_ms: 6.120339584819938\n",
      "  time_since_restore: 40068.20094060898\n",
      "  time_this_iter_s: 284.5058135986328\n",
      "  time_total_s: 40068.20094060898\n",
      "  timers:\n",
      "    learn_throughput: 86.631\n",
      "    learn_time_ms: 173148.041\n",
      "    sample_throughput: 141.134\n",
      "    sample_time_ms: 106281.598\n",
      "    update_time_ms: 2.405\n",
      "  timestamp: 1666711586\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2070000\n",
      "  training_iteration: 138\n",
      "  trial_id: '12173_00000'\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 10.9/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.38 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_2_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_0_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_5_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_4_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_1_edf1e7152919a3617c29d944024e5221, 0.0/6.0 CPU_group_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_3_edf1e7152919a3617c29d944024e5221)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_figure_eight\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                  | status   | loc                  |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_AccelEnv-v0_12173_00000 | RUNNING  | 141.225.10.229:30781 |    138 |          40068.2 | 2070000 |  187.429 |              213.116 |              159.945 |               1500 |\n",
      "+-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_AccelEnv-v0_12173_00000:\n",
      "  agent_timesteps_total: 2085000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-25_10-31-05\n",
      "  done: false\n",
      "  episode_len_mean: 1500.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 211.86886711815296\n",
      "  episode_reward_mean: 183.72140193125895\n",
      "  episode_reward_min: 159.94506399952448\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1390\n",
      "  experiment_id: 3b224e0d4aa944209a188d13d8dd894d\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.253125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -4.552802447141227\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.018338915030774117\n",
      "          policy_loss: 0.0049724953310264245\n",
      "          total_loss: 198.2039772227659\n",
      "          vf_explained_var: -1.616397127257585e-09\n",
      "          vf_loss: 198.19436366194387\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 2085000\n",
      "    num_steps_sampled: 2085000\n",
      "    num_steps_trained: 2085000\n",
      "  iterations_since_restore: 139\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 12.631578947368421\n",
      "    ram_util_percent: 35.697243107769424\n",
      "  pid: 30781\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08446334326928713\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.47353684336531\n",
      "    mean_inference_ms: 1.7352716228244014\n",
      "    mean_raw_obs_processing_ms: 6.082652614384836\n",
      "  time_since_restore: 40347.61680817604\n",
      "  time_this_iter_s: 279.4158675670624\n",
      "  time_total_s: 40347.61680817604\n",
      "  timers:\n",
      "    learn_throughput: 86.594\n",
      "    learn_time_ms: 173222.527\n",
      "    sample_throughput: 141.13\n",
      "    sample_time_ms: 106285.158\n",
      "    update_time_ms: 2.362\n",
      "  timestamp: 1666711865\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2085000\n",
      "  training_iteration: 139\n",
      "  trial_id: '12173_00000'\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 11.0/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.38 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_0_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_3_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_5_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_4_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_2_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_1_edf1e7152919a3617c29d944024e5221, 0.0/6.0 CPU_group_edf1e7152919a3617c29d944024e5221)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_figure_eight\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                  | status   | loc                  |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_AccelEnv-v0_12173_00000 | RUNNING  | 141.225.10.229:30781 |    139 |          40347.6 | 2085000 |  183.721 |              211.869 |              159.945 |               1500 |\n",
      "+-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "Result for PPO_AccelEnv-v0_12173_00000:\n",
      "  agent_timesteps_total: 2100000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-25_10-35-45\n",
      "  done: false\n",
      "  episode_len_mean: 1500.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 211.86886711815296\n",
      "  episode_reward_mean: 180.25462765466426\n",
      "  episode_reward_min: 159.94506399952448\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1400\n",
      "  experiment_id: 3b224e0d4aa944209a188d13d8dd894d\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.253125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -4.5519993394108145\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01729775761213671\n",
      "          policy_loss: 0.005268936394319191\n",
      "          total_loss: 196.2371373661494\n",
      "          vf_explained_var: -5.354315657513098e-09\n",
      "          vf_loss: 196.22748982704292\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 2100000\n",
      "    num_steps_sampled: 2100000\n",
      "    num_steps_trained: 2100000\n",
      "  iterations_since_restore: 140\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 12.686683417085426\n",
      "    ram_util_percent: 35.783668341708534\n",
      "  pid: 30781\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08445968463203216\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.4736167952517\n",
      "    mean_inference_ms: 1.7351330001034047\n",
      "    mean_raw_obs_processing_ms: 6.045530238035179\n",
      "  time_since_restore: 40626.61764883995\n",
      "  time_this_iter_s: 279.0008406639099\n",
      "  time_total_s: 40626.61764883995\n",
      "  timers:\n",
      "    learn_throughput: 86.58\n",
      "    learn_time_ms: 173249.9\n",
      "    sample_throughput: 141.138\n",
      "    sample_time_ms: 106278.77\n",
      "    update_time_ms: 2.401\n",
      "  timestamp: 1666712145\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2100000\n",
      "  training_iteration: 140\n",
      "  trial_id: '12173_00000'\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 10.9/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.38 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_3_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_2_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_0_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_1_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_5_edf1e7152919a3617c29d944024e5221, 0.0/1.0 accelerator_type:G, 0.0/6.0 CPU_group_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_4_edf1e7152919a3617c29d944024e5221)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_figure_eight\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                  | status   | loc                  |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_AccelEnv-v0_12173_00000 | RUNNING  | 141.225.10.229:30781 |    140 |          40626.6 | 2100000 |  180.255 |              211.869 |              159.945 |               1500 |\n",
      "+-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "Result for PPO_AccelEnv-v0_12173_00000:\n",
      "  agent_timesteps_total: 2115000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-25_10-40-23\n",
      "  done: false\n",
      "  episode_len_mean: 1500.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 208.82556848708836\n",
      "  episode_reward_mean: 177.24950744126966\n",
      "  episode_reward_min: 159.94506399952448\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1410\n",
      "  experiment_id: 3b224e0d4aa944209a188d13d8dd894d\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.253125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -4.549552081803144\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017148250265450343\n",
      "          policy_loss: 0.004179218624939479\n",
      "          total_loss: 196.32177478984252\n",
      "          vf_explained_var: 4.5461170938843054e-10\n",
      "          vf_loss: 196.31325474755238\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 2115000\n",
      "    num_steps_sampled: 2115000\n",
      "    num_steps_trained: 2115000\n",
      "  iterations_since_restore: 141\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 12.708542713567839\n",
      "    ram_util_percent: 35.80954773869346\n",
      "  pid: 30781\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08445738602011488\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.473680596907716\n",
      "    mean_inference_ms: 1.7350184520135696\n",
      "    mean_raw_obs_processing_ms: 6.008925007513389\n",
      "  time_since_restore: 40905.25938606262\n",
      "  time_this_iter_s: 278.6417372226715\n",
      "  time_total_s: 40905.25938606262\n",
      "  timers:\n",
      "    learn_throughput: 86.586\n",
      "    learn_time_ms: 173237.25\n",
      "    sample_throughput: 141.156\n",
      "    sample_time_ms: 106265.621\n",
      "    update_time_ms: 2.417\n",
      "  timestamp: 1666712423\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2115000\n",
      "  training_iteration: 141\n",
      "  trial_id: '12173_00000'\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 10.9/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.38 GiB heap, 0.0/14.65 GiB objects (0.0/6.0 CPU_group_edf1e7152919a3617c29d944024e5221, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_0_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_3_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_1_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_2_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_5_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_4_edf1e7152919a3617c29d944024e5221)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_figure_eight\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                  | status   | loc                  |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_AccelEnv-v0_12173_00000 | RUNNING  | 141.225.10.229:30781 |    141 |          40905.3 | 2115000 |   177.25 |              208.826 |              159.945 |               1500 |\n",
      "+-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "Result for PPO_AccelEnv-v0_12173_00000:\n",
      "  agent_timesteps_total: 2130000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-25_10-44-57\n",
      "  done: false\n",
      "  episode_len_mean: 1500.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 206.33251663291762\n",
      "  episode_reward_mean: 174.36124356194293\n",
      "  episode_reward_min: 159.94506399952448\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1420\n",
      "  experiment_id: 3b224e0d4aa944209a188d13d8dd894d\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.253125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -4.540807027331853\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015312340407515845\n",
      "          policy_loss: 0.004404548303839766\n",
      "          total_loss: 193.34956419346696\n",
      "          vf_explained_var: -2.020496436827557e-09\n",
      "          vf_loss: 193.34128308700303\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 2130000\n",
      "    num_steps_sampled: 2130000\n",
      "    num_steps_trained: 2130000\n",
      "  iterations_since_restore: 142\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 12.714066496163682\n",
      "    ram_util_percent: 35.74271099744245\n",
      "  pid: 30781\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08445549033068374\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.473769178872928\n",
      "    mean_inference_ms: 1.7348979638251931\n",
      "    mean_raw_obs_processing_ms: 5.972872262383127\n",
      "  time_since_restore: 41178.900164842606\n",
      "  time_this_iter_s: 273.6407787799835\n",
      "  time_total_s: 41178.900164842606\n",
      "  timers:\n",
      "    learn_throughput: 86.868\n",
      "    learn_time_ms: 172675.813\n",
      "    sample_throughput: 141.126\n",
      "    sample_time_ms: 106287.835\n",
      "    update_time_ms: 2.367\n",
      "  timestamp: 1666712697\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2130000\n",
      "  training_iteration: 142\n",
      "  trial_id: '12173_00000'\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Memory usage on this node: 10.9/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.38 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_3_edf1e7152919a3617c29d944024e5221, 0.0/6.0 CPU_group_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_2_edf1e7152919a3617c29d944024e5221, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_0_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_1_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_4_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_5_edf1e7152919a3617c29d944024e5221)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_figure_eight\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                  | status   | loc                  |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_AccelEnv-v0_12173_00000 | RUNNING  | 141.225.10.229:30781 |    142 |          41178.9 | 2130000 |  174.361 |              206.333 |              159.945 |               1500 |\n",
      "+-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "Result for PPO_AccelEnv-v0_12173_00000:\n",
      "  agent_timesteps_total: 2145000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-25_10-49-40\n",
      "  done: false\n",
      "  episode_len_mean: 1500.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 190.01472866287745\n",
      "  episode_reward_mean: 172.6703279703953\n",
      "  episode_reward_min: 159.94506399952448\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1430\n",
      "  experiment_id: 3b224e0d4aa944209a188d13d8dd894d\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.253125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -4.558464606737687\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014504508023389153\n",
      "          policy_loss: 0.004077068333300935\n",
      "          total_loss: 194.61113198490466\n",
      "          vf_explained_var: -3.4853564478964927e-09\n",
      "          vf_loss: 194.6033836429402\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 2145000\n",
      "    num_steps_sampled: 2145000\n",
      "    num_steps_trained: 2145000\n",
      "  iterations_since_restore: 143\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 12.55210918114144\n",
      "    ram_util_percent: 35.770471464019856\n",
      "  pid: 30781\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08445475571491215\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.47383711647958\n",
      "    mean_inference_ms: 1.7347798203016194\n",
      "    mean_raw_obs_processing_ms: 5.937337613507646\n",
      "  time_since_restore: 41461.593891620636\n",
      "  time_this_iter_s: 282.6937267780304\n",
      "  time_total_s: 41461.593891620636\n",
      "  timers:\n",
      "    learn_throughput: 86.684\n",
      "    learn_time_ms: 173041.631\n",
      "    sample_throughput: 141.141\n",
      "    sample_time_ms: 106276.503\n",
      "    update_time_ms: 2.311\n",
      "  timestamp: 1666712980\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2145000\n",
      "  training_iteration: 143\n",
      "  trial_id: '12173_00000'\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 10.9/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.38 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_2_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_0_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_4_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_3_edf1e7152919a3617c29d944024e5221, 0.0/1.0 accelerator_type:G, 0.0/6.0 CPU_group_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_5_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_1_edf1e7152919a3617c29d944024e5221)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_figure_eight\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                  | status   | loc                  |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_AccelEnv-v0_12173_00000 | RUNNING  | 141.225.10.229:30781 |    143 |          41461.6 | 2145000 |   172.67 |              190.015 |              159.945 |               1500 |\n",
      "+-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "Result for PPO_AccelEnv-v0_12173_00000:\n",
      "  agent_timesteps_total: 2160000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-25_10-54-20\n",
      "  done: false\n",
      "  episode_len_mean: 1500.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 190.54495708407836\n",
      "  episode_reward_mean: 174.95201227389472\n",
      "  episode_reward_min: 159.94506399952448\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1440\n",
      "  experiment_id: 3b224e0d4aa944209a188d13d8dd894d\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.253125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -4.5773203550759005\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01545658041426513\n",
      "          policy_loss: 0.00354441382598637\n",
      "          total_loss: 191.96229487273652\n",
      "          vf_explained_var: -1.111273073561847e-09\n",
      "          vf_loss: 191.95483881093688\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 2160000\n",
      "    num_steps_sampled: 2160000\n",
      "    num_steps_trained: 2160000\n",
      "  iterations_since_restore: 144\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 12.65775\n",
      "    ram_util_percent: 35.786750000000005\n",
      "  pid: 30781\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08445445029850554\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.4739050841823\n",
      "    mean_inference_ms: 1.7346583637327493\n",
      "    mean_raw_obs_processing_ms: 5.902295446662529\n",
      "  time_since_restore: 41741.511445999146\n",
      "  time_this_iter_s: 279.9175543785095\n",
      "  time_total_s: 41741.511445999146\n",
      "  timers:\n",
      "    learn_throughput: 86.612\n",
      "    learn_time_ms: 173185.905\n",
      "    sample_throughput: 141.163\n",
      "    sample_time_ms: 106260.374\n",
      "    update_time_ms: 2.306\n",
      "  timestamp: 1666713260\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2160000\n",
      "  training_iteration: 144\n",
      "  trial_id: '12173_00000'\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 10.9/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.38 GiB heap, 0.0/14.65 GiB objects (0.0/6.0 CPU_group_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_4_edf1e7152919a3617c29d944024e5221, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_1_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_5_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_0_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_2_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_3_edf1e7152919a3617c29d944024e5221)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_figure_eight\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                  | status   | loc                  |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_AccelEnv-v0_12173_00000 | RUNNING  | 141.225.10.229:30781 |    144 |          41741.5 | 2160000 |  174.952 |              190.545 |              159.945 |               1500 |\n",
      "+-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "Result for PPO_AccelEnv-v0_12173_00000:\n",
      "  agent_timesteps_total: 2175000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-25_10-58-59\n",
      "  done: false\n",
      "  episode_len_mean: 1500.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 190.54495708407836\n",
      "  episode_reward_mean: 177.6257744986324\n",
      "  episode_reward_min: 161.72803961171743\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1450\n",
      "  experiment_id: 3b224e0d4aa944209a188d13d8dd894d\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.253125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -4.558690359228748\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015979878841858794\n",
      "          policy_loss: 0.004210720705465311\n",
      "          total_loss: 195.53908681707867\n",
      "          vf_explained_var: -5.303803174427912e-09\n",
      "          vf_loss: 195.5308312238273\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 2175000\n",
      "    num_steps_sampled: 2175000\n",
      "    num_steps_trained: 2175000\n",
      "  iterations_since_restore: 145\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 12.551507537688442\n",
      "    ram_util_percent: 35.78065326633166\n",
      "  pid: 30781\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08445329376767553\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.473970266210927\n",
      "    mean_inference_ms: 1.7345308530304981\n",
      "    mean_raw_obs_processing_ms: 5.867752432332898\n",
      "  time_since_restore: 42021.07061767578\n",
      "  time_this_iter_s: 279.55917167663574\n",
      "  time_total_s: 42021.07061767578\n",
      "  timers:\n",
      "    learn_throughput: 86.57\n",
      "    learn_time_ms: 173269.648\n",
      "    sample_throughput: 141.172\n",
      "    sample_time_ms: 106253.084\n",
      "    update_time_ms: 2.314\n",
      "  timestamp: 1666713539\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2175000\n",
      "  training_iteration: 145\n",
      "  trial_id: '12173_00000'\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 10.9/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.38 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_0_edf1e7152919a3617c29d944024e5221, 0.0/6.0 CPU_group_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_4_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_5_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_1_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_3_edf1e7152919a3617c29d944024e5221, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_2_edf1e7152919a3617c29d944024e5221)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_figure_eight\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                  | status   | loc                  |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_AccelEnv-v0_12173_00000 | RUNNING  | 141.225.10.229:30781 |    145 |          42021.1 | 2175000 |  177.626 |              190.545 |              161.728 |               1500 |\n",
      "+-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "Result for PPO_AccelEnv-v0_12173_00000:\n",
      "  agent_timesteps_total: 2190000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-25_11-03-42\n",
      "  done: false\n",
      "  episode_len_mean: 1500.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 190.54495708407836\n",
      "  episode_reward_mean: 179.74726228357696\n",
      "  episode_reward_min: 162.6634623875909\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1460\n",
      "  experiment_id: 3b224e0d4aa944209a188d13d8dd894d\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.253125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -4.599122804706379\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016629620907416216\n",
      "          policy_loss: 0.00423228265540043\n",
      "          total_loss: 193.02827830880375\n",
      "          vf_explained_var: -2.7276700897971295e-09\n",
      "          vf_loss: 193.01983661005053\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 2190000\n",
      "    num_steps_sampled: 2190000\n",
      "    num_steps_trained: 2190000\n",
      "  iterations_since_restore: 146\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 12.487623762376238\n",
      "    ram_util_percent: 35.74183168316832\n",
      "  pid: 30781\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08445261892826629\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.47402636699276\n",
      "    mean_inference_ms: 1.734417398829201\n",
      "    mean_raw_obs_processing_ms: 5.83368517093861\n",
      "  time_since_restore: 42303.978929042816\n",
      "  time_this_iter_s: 282.9083113670349\n",
      "  time_total_s: 42303.978929042816\n",
      "  timers:\n",
      "    learn_throughput: 86.371\n",
      "    learn_time_ms: 173669.967\n",
      "    sample_throughput: 141.17\n",
      "    sample_time_ms: 106255.008\n",
      "    update_time_ms: 2.348\n",
      "  timestamp: 1666713822\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2190000\n",
      "  training_iteration: 146\n",
      "  trial_id: '12173_00000'\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 10.9/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.38 GiB heap, 0.0/14.65 GiB objects (0.0/6.0 CPU_group_edf1e7152919a3617c29d944024e5221, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_2_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_4_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_3_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_0_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_1_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_5_edf1e7152919a3617c29d944024e5221)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_figure_eight\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                  | status   | loc                  |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_AccelEnv-v0_12173_00000 | RUNNING  | 141.225.10.229:30781 |    146 |            42304 | 2190000 |  179.747 |              190.545 |              162.663 |               1500 |\n",
      "+-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "Result for PPO_AccelEnv-v0_12173_00000:\n",
      "  agent_timesteps_total: 2205000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-25_11-08-24\n",
      "  done: false\n",
      "  episode_len_mean: 1500.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 190.54495708407836\n",
      "  episode_reward_mean: 182.13699937983384\n",
      "  episode_reward_min: 171.67235842012678\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1470\n",
      "  experiment_id: 3b224e0d4aa944209a188d13d8dd894d\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.253125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -4.579072468159563\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017505133585785388\n",
      "          policy_loss: 0.004415640631153927\n",
      "          total_loss: 193.45603878861766\n",
      "          vf_explained_var: -2.121521180953323e-09\n",
      "          vf_loss: 193.44719218237924\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 2205000\n",
      "    num_steps_sampled: 2205000\n",
      "    num_steps_trained: 2205000\n",
      "  iterations_since_restore: 147\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 12.64278606965174\n",
      "    ram_util_percent: 35.71940298507463\n",
      "  pid: 30781\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08445280717313806\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.47406100198111\n",
      "    mean_inference_ms: 1.7343361508556412\n",
      "    mean_raw_obs_processing_ms: 5.800100426712619\n",
      "  time_since_restore: 42585.326812028885\n",
      "  time_this_iter_s: 281.3478829860687\n",
      "  time_total_s: 42585.326812028885\n",
      "  timers:\n",
      "    learn_throughput: 86.254\n",
      "    learn_time_ms: 173905.087\n",
      "    sample_throughput: 141.181\n",
      "    sample_time_ms: 106246.79\n",
      "    update_time_ms: 2.342\n",
      "  timestamp: 1666714104\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2205000\n",
      "  training_iteration: 147\n",
      "  trial_id: '12173_00000'\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Memory usage on this node: 10.9/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.38 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 accelerator_type:G, 0.0/6.0 CPU_group_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_5_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_1_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_3_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_2_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_4_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_0_edf1e7152919a3617c29d944024e5221)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_figure_eight\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                  | status   | loc                  |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_AccelEnv-v0_12173_00000 | RUNNING  | 141.225.10.229:30781 |    147 |          42585.3 | 2205000 |  182.137 |              190.545 |              171.672 |               1500 |\n",
      "+-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "Result for PPO_AccelEnv-v0_12173_00000:\n",
      "  agent_timesteps_total: 2220000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-25_11-13-03\n",
      "  done: false\n",
      "  episode_len_mean: 1500.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 196.19723342218927\n",
      "  episode_reward_mean: 184.03859285113424\n",
      "  episode_reward_min: 173.8382227844162\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1480\n",
      "  experiment_id: 3b224e0d4aa944209a188d13d8dd894d\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.253125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -4.6032919447300795\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.018043088447880825\n",
      "          policy_loss: 0.004933734714069356\n",
      "          total_loss: 192.2617624767756\n",
      "          vf_explained_var: -1.0102482045359906e-10\n",
      "          vf_loss: 192.25226186978614\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 2220000\n",
      "    num_steps_sampled: 2220000\n",
      "    num_steps_trained: 2220000\n",
      "  iterations_since_restore: 148\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 12.71754385964912\n",
      "    ram_util_percent: 35.73032581453634\n",
      "  pid: 30781\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08445308952926885\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.47406149535792\n",
      "    mean_inference_ms: 1.734284854538433\n",
      "    mean_raw_obs_processing_ms: 5.766972580576642\n",
      "  time_since_restore: 42864.89708566666\n",
      "  time_this_iter_s: 279.5702736377716\n",
      "  time_total_s: 42864.89708566666\n",
      "  timers:\n",
      "    learn_throughput: 86.494\n",
      "    learn_time_ms: 173422.592\n",
      "    sample_throughput: 141.196\n",
      "    sample_time_ms: 106235.196\n",
      "    update_time_ms: 2.318\n",
      "  timestamp: 1666714383\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2220000\n",
      "  training_iteration: 148\n",
      "  trial_id: '12173_00000'\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 10.9/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.38 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_4_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_3_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_5_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_0_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_1_edf1e7152919a3617c29d944024e5221, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_2_edf1e7152919a3617c29d944024e5221, 0.0/6.0 CPU_group_edf1e7152919a3617c29d944024e5221)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_figure_eight\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                  | status   | loc                  |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_AccelEnv-v0_12173_00000 | RUNNING  | 141.225.10.229:30781 |    148 |          42864.9 | 2220000 |  184.039 |              196.197 |              173.838 |               1500 |\n",
      "+-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "Result for PPO_AccelEnv-v0_12173_00000:\n",
      "  agent_timesteps_total: 2235000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-25_11-17-43\n",
      "  done: false\n",
      "  episode_len_mean: 1500.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 197.4064178258178\n",
      "  episode_reward_mean: 186.16464371226343\n",
      "  episode_reward_min: 174.17429261333493\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1490\n",
      "  experiment_id: 3b224e0d4aa944209a188d13d8dd894d\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.253125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -4.604554982104544\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0184429288307539\n",
      "          policy_loss: 0.004011644162158719\n",
      "          total_loss: 194.136293456514\n",
      "          vf_explained_var: -3.8389433854035815e-09\n",
      "          vf_loss: 194.12761291245283\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 2235000\n",
      "    num_steps_sampled: 2235000\n",
      "    num_steps_trained: 2235000\n",
      "  iterations_since_restore: 149\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 12.615037593984962\n",
      "    ram_util_percent: 35.764411027568926\n",
      "  pid: 30781\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08445348492709184\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.474066914810145\n",
      "    mean_inference_ms: 1.7342308186159505\n",
      "    mean_raw_obs_processing_ms: 5.734297673368792\n",
      "  time_since_restore: 43144.44792962074\n",
      "  time_this_iter_s: 279.5508439540863\n",
      "  time_total_s: 43144.44792962074\n",
      "  timers:\n",
      "    learn_throughput: 86.488\n",
      "    learn_time_ms: 173434.601\n",
      "    sample_throughput: 141.195\n",
      "    sample_time_ms: 106236.051\n",
      "    update_time_ms: 2.364\n",
      "  timestamp: 1666714663\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2235000\n",
      "  training_iteration: 149\n",
      "  trial_id: '12173_00000'\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 10.9/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.38 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_3_edf1e7152919a3617c29d944024e5221, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_4_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_1_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_2_edf1e7152919a3617c29d944024e5221, 0.0/6.0 CPU_group_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_0_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_5_edf1e7152919a3617c29d944024e5221)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_figure_eight\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                  | status   | loc                  |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_AccelEnv-v0_12173_00000 | RUNNING  | 141.225.10.229:30781 |    149 |          43144.4 | 2235000 |  186.165 |              197.406 |              174.174 |               1500 |\n",
      "+-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "Result for PPO_AccelEnv-v0_12173_00000:\n",
      "  agent_timesteps_total: 2250000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-25_11-22-22\n",
      "  done: false\n",
      "  episode_len_mean: 1500.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 203.61950166105456\n",
      "  episode_reward_mean: 188.8061472323164\n",
      "  episode_reward_min: 176.0401342262941\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1500\n",
      "  experiment_id: 3b224e0d4aa944209a188d13d8dd894d\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.253125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -4.564318166344853\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014801817277568755\n",
      "          policy_loss: 0.0030417496837691358\n",
      "          total_loss: 192.0346560656014\n",
      "          vf_explained_var: -1.4648599000466334e-09\n",
      "          vf_loss: 192.02786741095073\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 2250000\n",
      "    num_steps_sampled: 2250000\n",
      "    num_steps_trained: 2250000\n",
      "  iterations_since_restore: 150\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 12.694472361809046\n",
      "    ram_util_percent: 35.80804020100502\n",
      "  pid: 30781\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08445408106984674\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.474093923303176\n",
      "    mean_inference_ms: 1.7341710441399791\n",
      "    mean_raw_obs_processing_ms: 5.702068409940093\n",
      "  time_since_restore: 43423.37252664566\n",
      "  time_this_iter_s: 278.9245970249176\n",
      "  time_total_s: 43423.37252664566\n",
      "  timers:\n",
      "    learn_throughput: 86.497\n",
      "    learn_time_ms: 173415.85\n",
      "    sample_throughput: 141.18\n",
      "    sample_time_ms: 106247.333\n",
      "    update_time_ms: 2.314\n",
      "  timestamp: 1666714942\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2250000\n",
      "  training_iteration: 150\n",
      "  trial_id: '12173_00000'\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 10.9/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.38 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_2_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_1_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_4_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_0_edf1e7152919a3617c29d944024e5221, 0.0/6.0 CPU_group_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_5_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_3_edf1e7152919a3617c29d944024e5221, 0.0/1.0 accelerator_type:G)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_figure_eight\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                  | status   | loc                  |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_AccelEnv-v0_12173_00000 | RUNNING  | 141.225.10.229:30781 |    150 |          43423.4 | 2250000 |  188.806 |               203.62 |               176.04 |               1500 |\n",
      "+-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "Result for PPO_AccelEnv-v0_12173_00000:\n",
      "  agent_timesteps_total: 2265000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-25_11-27-03\n",
      "  done: false\n",
      "  episode_len_mean: 1500.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 208.88635940133875\n",
      "  episode_reward_mean: 191.54039129137774\n",
      "  episode_reward_min: 176.0401342262941\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1510\n",
      "  experiment_id: 3b224e0d4aa944209a188d13d8dd894d\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.253125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -4.596348591982308\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016284176745285026\n",
      "          policy_loss: 0.0042319642368041106\n",
      "          total_loss: 192.77608450550144\n",
      "          vf_explained_var: -2.7276700897971295e-09\n",
      "          vf_loss: 192.76773071289062\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 2265000\n",
      "    num_steps_sampled: 2265000\n",
      "    num_steps_trained: 2265000\n",
      "  iterations_since_restore: 151\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 12.63125\n",
      "    ram_util_percent: 35.781\n",
      "  pid: 30781\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08445404797922908\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.474131191560154\n",
      "    mean_inference_ms: 1.7340978724193696\n",
      "    mean_raw_obs_processing_ms: 5.670295767964917\n",
      "  time_since_restore: 43703.7469727993\n",
      "  time_this_iter_s: 280.37444615364075\n",
      "  time_total_s: 43703.7469727993\n",
      "  timers:\n",
      "    learn_throughput: 86.413\n",
      "    learn_time_ms: 173583.994\n",
      "    sample_throughput: 141.173\n",
      "    sample_time_ms: 106252.61\n",
      "    update_time_ms: 2.277\n",
      "  timestamp: 1666715223\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2265000\n",
      "  training_iteration: 151\n",
      "  trial_id: '12173_00000'\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 10.9/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.38 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_4_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_2_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_1_edf1e7152919a3617c29d944024e5221, 0.0/6.0 CPU_group_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_3_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_5_edf1e7152919a3617c29d944024e5221, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_0_edf1e7152919a3617c29d944024e5221)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_figure_eight\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                  | status   | loc                  |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_AccelEnv-v0_12173_00000 | RUNNING  | 141.225.10.229:30781 |    151 |          43703.7 | 2265000 |   191.54 |              208.886 |               176.04 |               1500 |\n",
      "+-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "Result for PPO_AccelEnv-v0_12173_00000:\n",
      "  agent_timesteps_total: 2280000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-25_11-31-42\n",
      "  done: false\n",
      "  episode_len_mean: 1500.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 208.88635940133875\n",
      "  episode_reward_mean: 193.6595231323138\n",
      "  episode_reward_min: 182.1095291806548\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1520\n",
      "  experiment_id: 3b224e0d4aa944209a188d13d8dd894d\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.253125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -4.61691471398887\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016365962652394123\n",
      "          policy_loss: 0.0031525015231158773\n",
      "          total_loss: 189.63060733019296\n",
      "          vf_explained_var: -3.030744544219033e-10\n",
      "          vf_loss: 189.62331238762806\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 2280000\n",
      "    num_steps_sampled: 2280000\n",
      "    num_steps_trained: 2280000\n",
      "  iterations_since_restore: 152\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 12.486180904522612\n",
      "    ram_util_percent: 35.801758793969846\n",
      "  pid: 30781\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08445402196098793\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.474178963997392\n",
      "    mean_inference_ms: 1.7340320649758412\n",
      "    mean_raw_obs_processing_ms: 5.638949668246828\n",
      "  time_since_restore: 43982.76226186752\n",
      "  time_this_iter_s: 279.01528906822205\n",
      "  time_total_s: 43982.76226186752\n",
      "  timers:\n",
      "    learn_throughput: 86.148\n",
      "    learn_time_ms: 174118.529\n",
      "    sample_throughput: 141.169\n",
      "    sample_time_ms: 106255.27\n",
      "    update_time_ms: 2.277\n",
      "  timestamp: 1666715502\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2280000\n",
      "  training_iteration: 152\n",
      "  trial_id: '12173_00000'\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Memory usage on this node: 10.9/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.38 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_3_edf1e7152919a3617c29d944024e5221, 0.0/6.0 CPU_group_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_1_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_0_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_4_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_5_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_2_edf1e7152919a3617c29d944024e5221)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_figure_eight\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                  | status   | loc                  |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_AccelEnv-v0_12173_00000 | RUNNING  | 141.225.10.229:30781 |    152 |          43982.8 | 2280000 |   193.66 |              208.886 |               182.11 |               1500 |\n",
      "+-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "Result for PPO_AccelEnv-v0_12173_00000:\n",
      "  agent_timesteps_total: 2295000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-25_11-36-24\n",
      "  done: false\n",
      "  episode_len_mean: 1500.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 214.4219859078227\n",
      "  episode_reward_mean: 196.12051055305278\n",
      "  episode_reward_min: 182.1095291806548\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1530\n",
      "  experiment_id: 3b224e0d4aa944209a188d13d8dd894d\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.253125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -4.598709877870851\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014763893417737069\n",
      "          policy_loss: 0.004838944068652089\n",
      "          total_loss: 193.16490082498325\n",
      "          vf_explained_var: -5.202778208257541e-09\n",
      "          vf_loss: 193.15632537906453\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 2295000\n",
      "    num_steps_sampled: 2295000\n",
      "    num_steps_trained: 2295000\n",
      "  iterations_since_restore: 153\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 12.273134328358207\n",
      "    ram_util_percent: 35.77537313432836\n",
      "  pid: 30781\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08445364360730384\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.474247572572448\n",
      "    mean_inference_ms: 1.7339448475008914\n",
      "    mean_raw_obs_processing_ms: 5.608037279450416\n",
      "  time_since_restore: 44264.76292657852\n",
      "  time_this_iter_s: 282.00066471099854\n",
      "  time_total_s: 44264.76292657852\n",
      "  timers:\n",
      "    learn_throughput: 86.186\n",
      "    learn_time_ms: 174042.227\n",
      "    sample_throughput: 141.16\n",
      "    sample_time_ms: 106262.411\n",
      "    update_time_ms: 2.324\n",
      "  timestamp: 1666715784\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2295000\n",
      "  training_iteration: 153\n",
      "  trial_id: '12173_00000'\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 10.9/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.38 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_2_edf1e7152919a3617c29d944024e5221, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_4_edf1e7152919a3617c29d944024e5221, 0.0/6.0 CPU_group_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_1_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_3_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_0_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_5_edf1e7152919a3617c29d944024e5221)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_figure_eight\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                  | status   | loc                  |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_AccelEnv-v0_12173_00000 | RUNNING  | 141.225.10.229:30781 |    153 |          44264.8 | 2295000 |  196.121 |              214.422 |               182.11 |               1500 |\n",
      "+-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_AccelEnv-v0_12173_00000:\n",
      "  agent_timesteps_total: 2310000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-25_11-41-03\n",
      "  done: false\n",
      "  episode_len_mean: 1500.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 214.4219859078227\n",
      "  episode_reward_mean: 198.24075067307206\n",
      "  episode_reward_min: 182.1095291806548\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1540\n",
      "  experiment_id: 3b224e0d4aa944209a188d13d8dd894d\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.253125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -4.606476375208063\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01472045526611895\n",
      "          policy_loss: 0.0031238248553995232\n",
      "          total_loss: 192.82606348587294\n",
      "          vf_explained_var: -3.1822817714299845e-09\n",
      "          vf_loss: 192.8192141516734\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 2310000\n",
      "    num_steps_sampled: 2310000\n",
      "    num_steps_trained: 2310000\n",
      "  iterations_since_restore: 154\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 12.600250626566416\n",
      "    ram_util_percent: 35.78546365914787\n",
      "  pid: 30781\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08445364331469879\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.474305312701507\n",
      "    mean_inference_ms: 1.7338744697140192\n",
      "    mean_raw_obs_processing_ms: 5.5775504889282566\n",
      "  time_since_restore: 44543.86034607887\n",
      "  time_this_iter_s: 279.09741950035095\n",
      "  time_total_s: 44543.86034607887\n",
      "  timers:\n",
      "    learn_throughput: 86.233\n",
      "    learn_time_ms: 173948.188\n",
      "    sample_throughput: 141.144\n",
      "    sample_time_ms: 106274.279\n",
      "    update_time_ms: 2.373\n",
      "  timestamp: 1666716063\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2310000\n",
      "  training_iteration: 154\n",
      "  trial_id: '12173_00000'\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 10.9/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.38 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_4_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_2_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_0_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_1_edf1e7152919a3617c29d944024e5221, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_3_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_5_edf1e7152919a3617c29d944024e5221, 0.0/6.0 CPU_group_edf1e7152919a3617c29d944024e5221)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_figure_eight\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                  | status   | loc                  |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_AccelEnv-v0_12173_00000 | RUNNING  | 141.225.10.229:30781 |    154 |          44543.9 | 2310000 |  198.241 |              214.422 |               182.11 |               1500 |\n",
      "+-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "Result for PPO_AccelEnv-v0_12173_00000:\n",
      "  agent_timesteps_total: 2325000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-25_11-45-44\n",
      "  done: false\n",
      "  episode_len_mean: 1500.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 214.75586343296382\n",
      "  episode_reward_mean: 200.81097928935296\n",
      "  episode_reward_min: 182.1095291806548\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1550\n",
      "  experiment_id: 3b224e0d4aa944209a188d13d8dd894d\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.253125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -4.600992853762739\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014432387329613364\n",
      "          policy_loss: 0.004157290408403565\n",
      "          total_loss: 192.22552972567283\n",
      "          vf_explained_var: -2.9802322831784522e-09\n",
      "          vf_loss: 192.21771930112678\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 2325000\n",
      "    num_steps_sampled: 2325000\n",
      "    num_steps_trained: 2325000\n",
      "  iterations_since_restore: 155\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 12.255\n",
      "    ram_util_percent: 35.75075000000001\n",
      "  pid: 30781\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08445463017280325\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.474358771434574\n",
      "    mean_inference_ms: 1.7338110401987266\n",
      "    mean_raw_obs_processing_ms: 5.547458673679395\n",
      "  time_since_restore: 44824.56253147125\n",
      "  time_this_iter_s: 280.70218539237976\n",
      "  time_total_s: 44824.56253147125\n",
      "  timers:\n",
      "    learn_throughput: 86.176\n",
      "    learn_time_ms: 174062.555\n",
      "    sample_throughput: 141.145\n",
      "    sample_time_ms: 106273.668\n",
      "    update_time_ms: 2.357\n",
      "  timestamp: 1666716344\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2325000\n",
      "  training_iteration: 155\n",
      "  trial_id: '12173_00000'\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 10.9/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.38 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_4_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_2_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_1_edf1e7152919a3617c29d944024e5221, 0.0/1.0 accelerator_type:G, 0.0/6.0 CPU_group_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_0_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_5_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_3_edf1e7152919a3617c29d944024e5221)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_figure_eight\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                  | status   | loc                  |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_AccelEnv-v0_12173_00000 | RUNNING  | 141.225.10.229:30781 |    155 |          44824.6 | 2325000 |  200.811 |              214.756 |               182.11 |               1500 |\n",
      "+-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "Result for PPO_AccelEnv-v0_12173_00000:\n",
      "  agent_timesteps_total: 2340000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-25_11-50-24\n",
      "  done: false\n",
      "  episode_len_mean: 1500.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 214.75586343296382\n",
      "  episode_reward_mean: 203.16095167272653\n",
      "  episode_reward_min: 185.2854510263143\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1560\n",
      "  experiment_id: 3b224e0d4aa944209a188d13d8dd894d\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.253125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -4.580093036667775\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015297130087782223\n",
      "          policy_loss: 0.004852168465830323\n",
      "          total_loss: 194.15444752321406\n",
      "          vf_explained_var: 1.5153722721095164e-10\n",
      "          vf_loss: 194.14572291616665\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 2340000\n",
      "    num_steps_sampled: 2340000\n",
      "    num_steps_trained: 2340000\n",
      "  iterations_since_restore: 156\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 12.69825\n",
      "    ram_util_percent: 35.79175\n",
      "  pid: 30781\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0844554223018942\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.474394877680325\n",
      "    mean_inference_ms: 1.7337457060818027\n",
      "    mean_raw_obs_processing_ms: 5.517775337819112\n",
      "  time_since_restore: 45104.44807887077\n",
      "  time_this_iter_s: 279.8855473995209\n",
      "  time_total_s: 45104.44807887077\n",
      "  timers:\n",
      "    learn_throughput: 86.323\n",
      "    learn_time_ms: 173766.968\n",
      "    sample_throughput: 141.155\n",
      "    sample_time_ms: 106266.428\n",
      "    update_time_ms: 2.359\n",
      "  timestamp: 1666716624\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2340000\n",
      "  training_iteration: 156\n",
      "  trial_id: '12173_00000'\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 10.9/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.38 GiB heap, 0.0/14.65 GiB objects (0.0/6.0 CPU_group_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_0_edf1e7152919a3617c29d944024e5221, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_2_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_3_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_5_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_4_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_1_edf1e7152919a3617c29d944024e5221)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_figure_eight\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                  | status   | loc                  |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_AccelEnv-v0_12173_00000 | RUNNING  | 141.225.10.229:30781 |    156 |          45104.4 | 2340000 |  203.161 |              214.756 |              185.285 |               1500 |\n",
      "+-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "Result for PPO_AccelEnv-v0_12173_00000:\n",
      "  agent_timesteps_total: 2355000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-25_11-55-04\n",
      "  done: false\n",
      "  episode_len_mean: 1500.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 218.93371408062\n",
      "  episode_reward_mean: 206.10622214998975\n",
      "  episode_reward_min: 191.77121333634162\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1570\n",
      "  experiment_id: 3b224e0d4aa944209a188d13d8dd894d\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.253125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -4.590428531371941\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015167878848112612\n",
      "          policy_loss: 0.0036962692325903197\n",
      "          total_loss: 196.41839708877822\n",
      "          vf_explained_var: -2.0710089199127424e-09\n",
      "          vf_loss: 196.41086121252027\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 2355000\n",
      "    num_steps_sampled: 2355000\n",
      "    num_steps_trained: 2355000\n",
      "  iterations_since_restore: 157\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 12.70250626566416\n",
      "    ram_util_percent: 35.81528822055137\n",
      "  pid: 30781\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08445508169732677\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.474432195079427\n",
      "    mean_inference_ms: 1.7336700781750016\n",
      "    mean_raw_obs_processing_ms: 5.488484314914444\n",
      "  time_since_restore: 45384.37258410454\n",
      "  time_this_iter_s: 279.92450523376465\n",
      "  time_total_s: 45384.37258410454\n",
      "  timers:\n",
      "    learn_throughput: 86.394\n",
      "    learn_time_ms: 173623.61\n",
      "    sample_throughput: 141.154\n",
      "    sample_time_ms: 106267.105\n",
      "    update_time_ms: 2.353\n",
      "  timestamp: 1666716904\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2355000\n",
      "  training_iteration: 157\n",
      "  trial_id: '12173_00000'\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Memory usage on this node: 11.0/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.38 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_1_edf1e7152919a3617c29d944024e5221, 0.0/6.0 CPU_group_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_2_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_3_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_5_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_0_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_4_edf1e7152919a3617c29d944024e5221, 0.0/1.0 accelerator_type:G)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_figure_eight\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                  | status   | loc                  |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_AccelEnv-v0_12173_00000 | RUNNING  | 141.225.10.229:30781 |    157 |          45384.4 | 2355000 |  206.106 |              218.934 |              191.771 |               1500 |\n",
      "+-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "Result for PPO_AccelEnv-v0_12173_00000:\n",
      "  agent_timesteps_total: 2370000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-25_11-59-43\n",
      "  done: false\n",
      "  episode_len_mean: 1500.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 221.7920659077274\n",
      "  episode_reward_mean: 208.65955094745254\n",
      "  episode_reward_min: 194.90433979579544\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1580\n",
      "  experiment_id: 3b224e0d4aa944209a188d13d8dd894d\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.253125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -4.590084386680085\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016648931048449427\n",
      "          policy_loss: 0.0031701539418170764\n",
      "          total_loss: 196.07693999985517\n",
      "          vf_explained_var: -3.788430902318396e-09\n",
      "          vf_loss: 196.06955500133967\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 2370000\n",
      "    num_steps_sampled: 2370000\n",
      "    num_steps_trained: 2370000\n",
      "  iterations_since_restore: 158\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 12.622864321608041\n",
      "    ram_util_percent: 35.794723618090444\n",
      "  pid: 30781\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08445431287256269\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.474512841704946\n",
      "    mean_inference_ms: 1.7335620029153391\n",
      "    mean_raw_obs_processing_ms: 5.45957744934498\n",
      "  time_since_restore: 45663.37572813034\n",
      "  time_this_iter_s: 279.0031440258026\n",
      "  time_total_s: 45663.37572813034\n",
      "  timers:\n",
      "    learn_throughput: 86.428\n",
      "    learn_time_ms: 173555.33\n",
      "    sample_throughput: 141.138\n",
      "    sample_time_ms: 106278.805\n",
      "    update_time_ms: 2.435\n",
      "  timestamp: 1666717183\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2370000\n",
      "  training_iteration: 158\n",
      "  trial_id: '12173_00000'\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 10.9/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.38 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_1_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_5_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_0_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_2_edf1e7152919a3617c29d944024e5221, 0.0/6.0 CPU_group_edf1e7152919a3617c29d944024e5221, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_4_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_3_edf1e7152919a3617c29d944024e5221)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_figure_eight\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                  | status   | loc                  |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_AccelEnv-v0_12173_00000 | RUNNING  | 141.225.10.229:30781 |    158 |          45663.4 | 2370000 |   208.66 |              221.792 |              194.904 |               1500 |\n",
      "+-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_AccelEnv-v0_12173_00000:\n",
      "  agent_timesteps_total: 2385000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-25_12-04-22\n",
      "  done: false\n",
      "  episode_len_mean: 1500.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 221.7920659077274\n",
      "  episode_reward_mean: 211.02727759302547\n",
      "  episode_reward_min: 197.7165924181346\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1590\n",
      "  experiment_id: 3b224e0d4aa944209a188d13d8dd894d\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.253125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -4.591347980095168\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01619522382566945\n",
      "          policy_loss: 0.005104759791313466\n",
      "          total_loss: 195.18674785807983\n",
      "          vf_explained_var: -2.2730584081642746e-09\n",
      "          vf_loss: 195.17754379531084\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 2385000\n",
      "    num_steps_sampled: 2385000\n",
      "    num_steps_trained: 2385000\n",
      "  iterations_since_restore: 159\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 12.74436090225564\n",
      "    ram_util_percent: 35.748120300751886\n",
      "  pid: 30781\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08445347893022827\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.474587359509986\n",
      "    mean_inference_ms: 1.7334481099323862\n",
      "    mean_raw_obs_processing_ms: 5.431045598288311\n",
      "  time_since_restore: 45942.359573841095\n",
      "  time_this_iter_s: 278.9838457107544\n",
      "  time_total_s: 45942.359573841095\n",
      "  timers:\n",
      "    learn_throughput: 86.453\n",
      "    learn_time_ms: 173505.647\n",
      "    sample_throughput: 141.147\n",
      "    sample_time_ms: 106272.01\n",
      "    update_time_ms: 2.433\n",
      "  timestamp: 1666717462\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2385000\n",
      "  training_iteration: 159\n",
      "  trial_id: '12173_00000'\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 10.9/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.38 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_5_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_3_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_4_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_2_edf1e7152919a3617c29d944024e5221, 0.0/1.0 accelerator_type:G, 0.0/6.0 CPU_group_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_1_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_0_edf1e7152919a3617c29d944024e5221)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_figure_eight\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                  | status   | loc                  |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_AccelEnv-v0_12173_00000 | RUNNING  | 141.225.10.229:30781 |    159 |          45942.4 | 2385000 |  211.027 |              221.792 |              197.717 |               1500 |\n",
      "+-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "Result for PPO_AccelEnv-v0_12173_00000:\n",
      "  agent_timesteps_total: 2400000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-25_12-09-01\n",
      "  done: false\n",
      "  episode_len_mean: 1500.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 226.92051704212844\n",
      "  episode_reward_mean: 213.45245707726176\n",
      "  episode_reward_min: 197.7165924181346\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1600\n",
      "  experiment_id: 3b224e0d4aa944209a188d13d8dd894d\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.253125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -4.601755888179198\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014825914422835472\n",
      "          policy_loss: 0.0030522451270371675\n",
      "          total_loss: 197.9027317758334\n",
      "          vf_explained_var: -1.7679343544685366e-09\n",
      "          vf_loss: 197.8959268149683\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 2400000\n",
      "    num_steps_sampled: 2400000\n",
      "    num_steps_trained: 2400000\n",
      "  iterations_since_restore: 160\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 12.70678391959799\n",
      "    ram_util_percent: 35.7748743718593\n",
      "  pid: 30781\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08445242901256986\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.474677160655265\n",
      "    mean_inference_ms: 1.7333295550872334\n",
      "    mean_raw_obs_processing_ms: 5.4028703516817815\n",
      "  time_since_restore: 46221.81934952736\n",
      "  time_this_iter_s: 279.45977568626404\n",
      "  time_total_s: 46221.81934952736\n",
      "  timers:\n",
      "    learn_throughput: 86.424\n",
      "    learn_time_ms: 173562.681\n",
      "    sample_throughput: 141.151\n",
      "    sample_time_ms: 106269.05\n",
      "    update_time_ms: 2.422\n",
      "  timestamp: 1666717741\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2400000\n",
      "  training_iteration: 160\n",
      "  trial_id: '12173_00000'\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 10.9/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.38 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_4_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_2_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_1_edf1e7152919a3617c29d944024e5221, 0.0/6.0 CPU_group_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_0_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_5_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_3_edf1e7152919a3617c29d944024e5221, 0.0/1.0 accelerator_type:G)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_figure_eight\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                  | status   | loc                  |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_AccelEnv-v0_12173_00000 | RUNNING  | 141.225.10.229:30781 |    160 |          46221.8 | 2400000 |  213.452 |              226.921 |              197.717 |               1500 |\n",
      "+-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "Result for PPO_AccelEnv-v0_12173_00000:\n",
      "  agent_timesteps_total: 2415000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-10-25_12-13-29\n",
      "  done: false\n",
      "  episode_len_mean: 1500.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 232.52601168044282\n",
      "  episode_reward_mean: 215.8116808760109\n",
      "  episode_reward_min: 197.7165924181346\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1610\n",
      "  experiment_id: 3b224e0d4aa944209a188d13d8dd894d\n",
      "  hostname: michael\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.253125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: -4.56879795001725\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0149169318819092\n",
      "          policy_loss: 0.003603301890652185\n",
      "          total_loss: 197.47643526368222\n",
      "          vf_explained_var: -9.092234187768611e-10\n",
      "          vf_loss: 197.46905621674102\n",
      "        model: {}\n",
      "    num_agent_steps_sampled: 2415000\n",
      "    num_steps_sampled: 2415000\n",
      "    num_steps_trained: 2415000\n",
      "  iterations_since_restore: 161\n",
      "  node_ip: 141.225.10.229\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 12.934293193717277\n",
      "    ram_util_percent: 35.786649214659676\n",
      "  pid: 30781\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08445126654462637\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.474755363656772\n",
      "    mean_inference_ms: 1.7332212486689678\n",
      "    mean_raw_obs_processing_ms: 5.375054820857088\n",
      "  time_since_restore: 46489.13457584381\n",
      "  time_this_iter_s: 267.315226316452\n",
      "  time_total_s: 46489.13457584381\n",
      "  timers:\n",
      "    learn_throughput: 87.078\n",
      "    learn_time_ms: 172258.998\n",
      "    sample_throughput: 141.155\n",
      "    sample_time_ms: 106266.134\n",
      "    update_time_ms: 2.473\n",
      "  timestamp: 1666718009\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2415000\n",
      "  training_iteration: 161\n",
      "  trial_id: '12173_00000'\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 10.9/31.2 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 6.0/6 CPUs, 0/6 GPUs, 0.0/7.38 GiB heap, 0.0/14.65 GiB objects (0.0/1.0 CPU_group_2_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_0_edf1e7152919a3617c29d944024e5221, 0.0/6.0 CPU_group_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_4_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_5_edf1e7152919a3617c29d944024e5221, 0.0/1.0 accelerator_type:G, 0.0/1.0 CPU_group_1_edf1e7152919a3617c29d944024e5221, 0.0/1.0 CPU_group_3_edf1e7152919a3617c29d944024e5221)\n",
      "Result logdir: /home/michael-lab/ray_results/singleagent_figure_eight\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "| Trial name                  | status   | loc                  |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
      "|-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
      "| PPO_AccelEnv-v0_12173_00000 | RUNNING  | 141.225.10.229:30781 |    161 |          46489.1 | 2415000 |  215.812 |              232.526 |              197.717 |               1500 |\n",
      "+-----------------------------+----------+----------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30785)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.458526-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30780)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.505609-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30783)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.486337-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30782)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.4470596-0_emission.csv ./michael_files/emission_collection/\n",
      "\u001b[2m\u001b[36m(pid=30784)\u001b[0m ./michael_files/emission_collection/singleagent_figure_eight_20221024-2318281666671508.488508-0_emission.csv ./michael_files/emission_collection/\n"
=======
      "| PPO_AccelEnv-v0_669a1_00000 | PENDING  |       |\n",
      "+-----------------------------+----------+-------+\n",
      "\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=22422)\u001b[0m 2022-10-27 15:17:39,243\tINFO trainer.py:720 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=22422)\u001b[0m 2022-10-27 15:17:42,161\tWARNING util.py:55 -- Install gputil for GPU system monitoring.\n",
      "^C\n"
>>>>>>> 45ba339a35ad2740fb9ec6babd1e5d31fe54e2af
     ]
    }
   ],
   "source": [
    "!python examples/train.py singleagent_figure_eight"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
